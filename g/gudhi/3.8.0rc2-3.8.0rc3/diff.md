# Comparing `tmp/gudhi-3.8.0rc2-cp39-cp39-win_amd64.whl.zip` & `tmp/gudhi-3.8.0rc3-cp38-cp38-macosx_10_15_universal2.whl.zip`

## zipinfo {}

```diff
@@ -1,60 +1,60 @@
-Zip file size: 2560300 bytes, number of entries: 58
--rw-rw-rw-  2.0 fat     2766 b- defN 23-Apr-11 15:26 gudhi/__init__.py
--rw-rw-rw-  2.0 fat   217088 b- defN 23-Apr-11 15:31 gudhi/_persline.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   916992 b- defN 23-Apr-11 15:30 gudhi/alpha_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   152064 b- defN 23-Apr-11 15:31 gudhi/bottleneck.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    35108 b- defN 23-Apr-11 14:53 gudhi/cover_complex.py
--rw-rw-rw-  2.0 fat   197120 b- defN 23-Apr-11 15:27 gudhi/cubical_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2198 b- defN 23-Apr-11 14:53 gudhi/dtm_rips_complex.py
--rw-rw-rw-  2.0 fat    95744 b- defN 23-Apr-11 15:29 gudhi/euclidean_strong_witness_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    94720 b- defN 23-Apr-11 15:29 gudhi/euclidean_witness_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   421376 b- defN 23-Apr-11 15:16 gudhi/gmp-10.dll
--rw-rw-rw-  2.0 fat    26112 b- defN 23-Apr-11 15:16 gudhi/gmpxx-4.dll
--rw-rw-rw-  2.0 fat   412672 b- defN 23-Apr-11 15:23 gudhi/mpfr-6.dll
--rw-rw-rw-  2.0 fat   300544 b- defN 23-Apr-11 15:27 gudhi/nerve_gic.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    74240 b- defN 23-Apr-11 15:26 gudhi/off_utils.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   203264 b- defN 23-Apr-11 15:27 gudhi/periodic_cubical_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    23430 b- defN 23-Apr-11 14:53 gudhi/persistence_graphical_tools.py
--rw-rw-rw-  2.0 fat    78336 b- defN 23-Apr-11 15:27 gudhi/reader_utils.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   135168 b- defN 23-Apr-11 15:26 gudhi/rips_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   475648 b- defN 23-Apr-11 15:26 gudhi/simplex_tree.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    68608 b- defN 23-Apr-11 15:27 gudhi/strong_witness_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   128000 b- defN 23-Apr-11 15:27 gudhi/subsampling.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   659456 b- defN 23-Apr-11 15:28 gudhi/tangential_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2555 b- defN 23-Apr-11 14:53 gudhi/weighted_rips_complex.py
--rw-rw-rw-  2.0 fat    67072 b- defN 23-Apr-11 15:27 gudhi/witness_complex.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-11 14:53 gudhi/clustering/__init__.py
--rw-rw-rw-  2.0 fat   139776 b- defN 23-Apr-11 15:30 gudhi/clustering/_tomato.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat    15868 b- defN 23-Apr-11 14:53 gudhi/clustering/tomato.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-11 14:53 gudhi/datasets/__init__.py
--rw-rw-rw-  2.0 fat    12572 b- defN 23-Apr-11 14:53 gudhi/datasets/remote.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-11 14:53 gudhi/datasets/generators/__init__.py
--rw-rw-rw-  2.0 fat   133632 b- defN 23-Apr-11 15:31 gudhi/datasets/generators/_points.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2457 b- defN 23-Apr-11 14:53 gudhi/datasets/generators/points.py
--rw-rw-rw-  2.0 fat      189 b- defN 23-Apr-11 14:53 gudhi/hera/__init__.py
--rw-rw-rw-  2.0 fat   183808 b- defN 23-Apr-11 15:31 gudhi/hera/bottleneck.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat   187392 b- defN 23-Apr-11 15:30 gudhi/hera/wasserstein.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-11 14:53 gudhi/point_cloud/__init__.py
--rw-rw-rw-  2.0 fat     7591 b- defN 23-Apr-11 14:53 gudhi/point_cloud/dtm.py
--rw-rw-rw-  2.0 fat    16635 b- defN 23-Apr-11 14:53 gudhi/point_cloud/knn.py
--rw-rw-rw-  2.0 fat     3300 b- defN 23-Apr-11 14:53 gudhi/point_cloud/timedelay.py
--rw-rw-rw-  2.0 fat      194 b- defN 23-Apr-11 14:53 gudhi/representations/__init__.py
--rw-rw-rw-  2.0 fat    19274 b- defN 23-Apr-11 14:53 gudhi/representations/kernel_methods.py
--rw-rw-rw-  2.0 fat    24025 b- defN 23-Apr-11 14:53 gudhi/representations/metrics.py
--rw-rw-rw-  2.0 fat    19030 b- defN 23-Apr-11 14:53 gudhi/representations/preprocessing.py
--rw-rw-rw-  2.0 fat    41757 b- defN 23-Apr-11 14:53 gudhi/representations/vector_methods.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Apr-11 14:53 gudhi/sklearn/__init__.py
--rw-rw-rw-  2.0 fat     5507 b- defN 23-Apr-11 14:53 gudhi/sklearn/cubical_persistence.py
--rw-rw-rw-  2.0 fat      218 b- defN 23-Apr-11 14:53 gudhi/tensorflow/__init__.py
--rw-rw-rw-  2.0 fat     4454 b- defN 23-Apr-11 14:53 gudhi/tensorflow/cubical_layer.py
--rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 14:53 gudhi/tensorflow/lower_star_simplex_tree_layer.py
--rw-rw-rw-  2.0 fat    15686 b- defN 23-Apr-11 14:53 gudhi/tensorflow/perslay.py
--rw-rw-rw-  2.0 fat     4979 b- defN 23-Apr-11 14:53 gudhi/tensorflow/rips_layer.py
--rw-rw-rw-  2.0 fat       47 b- defN 23-Apr-11 14:53 gudhi/wasserstein/__init__.py
--rw-rw-rw-  2.0 fat     6506 b- defN 23-Apr-11 14:53 gudhi/wasserstein/barycenter.py
--rw-rw-rw-  2.0 fat    15711 b- defN 23-Apr-11 14:53 gudhi/wasserstein/wasserstein.py
--rw-rw-rw-  2.0 fat     1676 b- defN 23-Apr-11 15:31 gudhi-3.8.0rc2.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Apr-11 15:31 gudhi-3.8.0rc2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Apr-11 15:31 gudhi-3.8.0rc2.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     5219 b- defN 23-Apr-11 15:31 gudhi-3.8.0rc2.dist-info/RECORD
-58 files, 5662779 bytes uncompressed, 2551988 bytes compressed:  54.9%
+Zip file size: 5749158 bytes, number of entries: 58
+-rwxr-xr-x  2.0 unx   406912 b- defN 23-Apr-14 12:31 gudhi/euclidean_witness_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   518880 b- defN 23-Apr-14 12:31 gudhi/rips_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx  3893040 b- defN 23-Apr-14 12:31 gudhi/alpha_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx  1034320 b- defN 23-Apr-14 12:31 gudhi/nerve_gic.cpython-38-darwin.so
+-rw-r--r--  2.0 unx    34462 b- defN 23-Apr-14 12:16 gudhi/cover_complex.py
+-rwxr-xr-x  2.0 unx   303568 b- defN 23-Apr-14 12:31 gudhi/reader_utils.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   612624 b- defN 23-Apr-14 12:31 gudhi/cubical_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   304400 b- defN 23-Apr-14 12:31 gudhi/off_utils.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   486144 b- defN 23-Apr-14 12:31 gudhi/subsampling.cpython-38-darwin.so
+-rw-r--r--  2.0 unx     2147 b- defN 23-Apr-14 12:16 gudhi/dtm_rips_complex.py
+-rwxr-xr-x  2.0 unx   559056 b- defN 23-Apr-14 12:31 gudhi/bottleneck.cpython-38-darwin.so
+-rw-r--r--  2.0 unx     2808 b- defN 23-Apr-14 12:20 gudhi/__init__.py
+-rw-r--r--  2.0 unx    22900 b- defN 23-Apr-14 12:16 gudhi/persistence_graphical_tools.py
+-rwxr-xr-x  2.0 unx   685696 b- defN 23-Apr-14 12:31 gudhi/periodic_cubical_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx  1395712 b- defN 23-Apr-14 12:31 gudhi/simplex_tree.cpython-38-darwin.so
+-rw-r--r--  2.0 unx     2494 b- defN 23-Apr-14 12:16 gudhi/weighted_rips_complex.py
+-rwxr-xr-x  2.0 unx  1399216 b- defN 23-Apr-14 12:31 gudhi/_persline.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   409392 b- defN 23-Apr-14 12:31 gudhi/euclidean_strong_witness_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx  2070704 b- defN 23-Apr-14 12:31 gudhi/tangential_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   282656 b- defN 23-Apr-14 12:31 gudhi/strong_witness_complex.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   279264 b- defN 23-Apr-14 12:31 gudhi/witness_complex.cpython-38-darwin.so
+-rw-r--r--  2.0 unx    23601 b- defN 23-Apr-14 12:16 gudhi/representations/metrics.py
+-rw-r--r--  2.0 unx    40936 b- defN 23-Apr-14 12:16 gudhi/representations/vector_methods.py
+-rw-r--r--  2.0 unx      188 b- defN 23-Apr-14 12:16 gudhi/representations/__init__.py
+-rw-r--r--  2.0 unx    18971 b- defN 23-Apr-14 12:16 gudhi/representations/kernel_methods.py
+-rw-r--r--  2.0 unx    18596 b- defN 23-Apr-14 12:16 gudhi/representations/preprocessing.py
+-rw-r--r--  2.0 unx     7412 b- defN 23-Apr-14 12:16 gudhi/point_cloud/dtm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-14 12:16 gudhi/point_cloud/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-Apr-14 12:16 gudhi/point_cloud/timedelay.py
+-rw-r--r--  2.0 unx    16291 b- defN 23-Apr-14 12:16 gudhi/point_cloud/knn.py
+-rw-r--r--  2.0 unx     6360 b- defN 23-Apr-14 12:16 gudhi/wasserstein/barycenter.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Apr-14 12:16 gudhi/wasserstein/__init__.py
+-rw-r--r--  2.0 unx    15356 b- defN 23-Apr-14 12:16 gudhi/wasserstein/wasserstein.py
+-rw-r--r--  2.0 unx    12238 b- defN 23-Apr-14 12:16 gudhi/datasets/remote.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-14 12:16 gudhi/datasets/__init__.py
+-rw-r--r--  2.0 unx     2398 b- defN 23-Apr-14 12:16 gudhi/datasets/generators/points.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-14 12:16 gudhi/datasets/generators/__init__.py
+-rwxr-xr-x  2.0 unx   497456 b- defN 23-Apr-14 12:31 gudhi/datasets/generators/_points.cpython-38-darwin.so
+-rwxr-xr-x  2.0 unx   516864 b- defN 23-Apr-14 12:31 gudhi/clustering/_tomato.cpython-38-darwin.so
+-rw-r--r--  2.0 unx    15547 b- defN 23-Apr-14 12:16 gudhi/clustering/tomato.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-14 12:16 gudhi/clustering/__init__.py
+-r--r--r--  2.0 unx   180064 b- defN 23-Apr-14 12:31 gudhi/.dylibs/libgmpxx.4.dylib
+-r--r--r--  2.0 unx   988448 b- defN 23-Apr-14 12:31 gudhi/.dylibs/libgmp.10.dylib
+-r--r--r--  2.0 unx   977248 b- defN 23-Apr-14 12:31 gudhi/.dylibs/libmpfr.6.dylib
+-rw-r--r--  2.0 unx     5390 b- defN 23-Apr-14 12:16 gudhi/sklearn/cubical_persistence.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-14 12:16 gudhi/sklearn/__init__.py
+-rw-r--r--  2.0 unx     4802 b- defN 23-Apr-14 12:16 gudhi/tensorflow/lower_star_simplex_tree_layer.py
+-rw-r--r--  2.0 unx      213 b- defN 23-Apr-14 12:16 gudhi/tensorflow/__init__.py
+-rw-r--r--  2.0 unx     4886 b- defN 23-Apr-14 12:16 gudhi/tensorflow/rips_layer.py
+-rw-r--r--  2.0 unx    15402 b- defN 23-Apr-14 12:16 gudhi/tensorflow/perslay.py
+-rw-r--r--  2.0 unx     4372 b- defN 23-Apr-14 12:16 gudhi/tensorflow/cubical_layer.py
+-rwxr-xr-x  2.0 unx   667312 b- defN 23-Apr-14 12:31 gudhi/hera/bottleneck.cpython-38-darwin.so
+-rw-r--r--  2.0 unx      182 b- defN 23-Apr-14 12:16 gudhi/hera/__init__.py
+-rwxr-xr-x  2.0 unx   690288 b- defN 23-Apr-14 12:31 gudhi/hera/wasserstein.cpython-38-darwin.so
+-rw-rw-r--  2.0 unx     5368 b- defN 23-Apr-14 12:31 gudhi-3.8.0rc3.dist-info/RECORD
+-rw-r--r--  2.0 unx      114 b- defN 23-Apr-14 12:31 gudhi-3.8.0rc3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 23-Apr-14 12:31 gudhi-3.8.0rc3.dist-info/top_level.txt
+-rw-r--r--  2.0 unx     1681 b- defN 23-Apr-14 12:31 gudhi-3.8.0rc3.dist-info/METADATA
+58 files, 19447638 bytes uncompressed, 5740688 bytes compressed:  70.5%
```

## zipnote {}

```diff
@@ -1,175 +1,175 @@
-Filename: gudhi/__init__.py
+Filename: gudhi/euclidean_witness_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/_persline.cp39-win_amd64.pyd
+Filename: gudhi/rips_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/alpha_complex.cp39-win_amd64.pyd
+Filename: gudhi/alpha_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/bottleneck.cp39-win_amd64.pyd
+Filename: gudhi/nerve_gic.cpython-38-darwin.so
 Comment: 
 
 Filename: gudhi/cover_complex.py
 Comment: 
 
-Filename: gudhi/cubical_complex.cp39-win_amd64.pyd
+Filename: gudhi/reader_utils.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/dtm_rips_complex.py
+Filename: gudhi/cubical_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/euclidean_strong_witness_complex.cp39-win_amd64.pyd
+Filename: gudhi/off_utils.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/euclidean_witness_complex.cp39-win_amd64.pyd
+Filename: gudhi/subsampling.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/gmp-10.dll
+Filename: gudhi/dtm_rips_complex.py
 Comment: 
 
-Filename: gudhi/gmpxx-4.dll
+Filename: gudhi/bottleneck.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/mpfr-6.dll
+Filename: gudhi/__init__.py
 Comment: 
 
-Filename: gudhi/nerve_gic.cp39-win_amd64.pyd
+Filename: gudhi/persistence_graphical_tools.py
 Comment: 
 
-Filename: gudhi/off_utils.cp39-win_amd64.pyd
+Filename: gudhi/periodic_cubical_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/periodic_cubical_complex.cp39-win_amd64.pyd
+Filename: gudhi/simplex_tree.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/persistence_graphical_tools.py
+Filename: gudhi/weighted_rips_complex.py
 Comment: 
 
-Filename: gudhi/reader_utils.cp39-win_amd64.pyd
+Filename: gudhi/_persline.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/rips_complex.cp39-win_amd64.pyd
+Filename: gudhi/euclidean_strong_witness_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/simplex_tree.cp39-win_amd64.pyd
+Filename: gudhi/tangential_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/strong_witness_complex.cp39-win_amd64.pyd
+Filename: gudhi/strong_witness_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/subsampling.cp39-win_amd64.pyd
+Filename: gudhi/witness_complex.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/tangential_complex.cp39-win_amd64.pyd
+Filename: gudhi/representations/metrics.py
 Comment: 
 
-Filename: gudhi/weighted_rips_complex.py
+Filename: gudhi/representations/vector_methods.py
 Comment: 
 
-Filename: gudhi/witness_complex.cp39-win_amd64.pyd
+Filename: gudhi/representations/__init__.py
 Comment: 
 
-Filename: gudhi/clustering/__init__.py
+Filename: gudhi/representations/kernel_methods.py
 Comment: 
 
-Filename: gudhi/clustering/_tomato.cp39-win_amd64.pyd
+Filename: gudhi/representations/preprocessing.py
 Comment: 
 
-Filename: gudhi/clustering/tomato.py
+Filename: gudhi/point_cloud/dtm.py
 Comment: 
 
-Filename: gudhi/datasets/__init__.py
+Filename: gudhi/point_cloud/__init__.py
 Comment: 
 
-Filename: gudhi/datasets/remote.py
+Filename: gudhi/point_cloud/timedelay.py
 Comment: 
 
-Filename: gudhi/datasets/generators/__init__.py
+Filename: gudhi/point_cloud/knn.py
 Comment: 
 
-Filename: gudhi/datasets/generators/_points.cp39-win_amd64.pyd
+Filename: gudhi/wasserstein/barycenter.py
 Comment: 
 
-Filename: gudhi/datasets/generators/points.py
+Filename: gudhi/wasserstein/__init__.py
 Comment: 
 
-Filename: gudhi/hera/__init__.py
+Filename: gudhi/wasserstein/wasserstein.py
+Comment: 
+
+Filename: gudhi/datasets/remote.py
 Comment: 
 
-Filename: gudhi/hera/bottleneck.cp39-win_amd64.pyd
+Filename: gudhi/datasets/__init__.py
 Comment: 
 
-Filename: gudhi/hera/wasserstein.cp39-win_amd64.pyd
+Filename: gudhi/datasets/generators/points.py
 Comment: 
 
-Filename: gudhi/point_cloud/__init__.py
+Filename: gudhi/datasets/generators/__init__.py
 Comment: 
 
-Filename: gudhi/point_cloud/dtm.py
+Filename: gudhi/datasets/generators/_points.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/point_cloud/knn.py
+Filename: gudhi/clustering/_tomato.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/point_cloud/timedelay.py
+Filename: gudhi/clustering/tomato.py
 Comment: 
 
-Filename: gudhi/representations/__init__.py
+Filename: gudhi/clustering/__init__.py
 Comment: 
 
-Filename: gudhi/representations/kernel_methods.py
+Filename: gudhi/.dylibs/libgmpxx.4.dylib
 Comment: 
 
-Filename: gudhi/representations/metrics.py
+Filename: gudhi/.dylibs/libgmp.10.dylib
 Comment: 
 
-Filename: gudhi/representations/preprocessing.py
+Filename: gudhi/.dylibs/libmpfr.6.dylib
 Comment: 
 
-Filename: gudhi/representations/vector_methods.py
+Filename: gudhi/sklearn/cubical_persistence.py
 Comment: 
 
 Filename: gudhi/sklearn/__init__.py
 Comment: 
 
-Filename: gudhi/sklearn/cubical_persistence.py
+Filename: gudhi/tensorflow/lower_star_simplex_tree_layer.py
 Comment: 
 
 Filename: gudhi/tensorflow/__init__.py
 Comment: 
 
-Filename: gudhi/tensorflow/cubical_layer.py
-Comment: 
-
-Filename: gudhi/tensorflow/lower_star_simplex_tree_layer.py
+Filename: gudhi/tensorflow/rips_layer.py
 Comment: 
 
 Filename: gudhi/tensorflow/perslay.py
 Comment: 
 
-Filename: gudhi/tensorflow/rips_layer.py
+Filename: gudhi/tensorflow/cubical_layer.py
 Comment: 
 
-Filename: gudhi/wasserstein/__init__.py
+Filename: gudhi/hera/bottleneck.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi/wasserstein/barycenter.py
+Filename: gudhi/hera/__init__.py
 Comment: 
 
-Filename: gudhi/wasserstein/wasserstein.py
+Filename: gudhi/hera/wasserstein.cpython-38-darwin.so
 Comment: 
 
-Filename: gudhi-3.8.0rc2.dist-info/METADATA
+Filename: gudhi-3.8.0rc3.dist-info/RECORD
 Comment: 
 
-Filename: gudhi-3.8.0rc2.dist-info/WHEEL
+Filename: gudhi-3.8.0rc3.dist-info/WHEEL
 Comment: 
 
-Filename: gudhi-3.8.0rc2.dist-info/top_level.txt
+Filename: gudhi-3.8.0rc3.dist-info/top_level.txt
 Comment: 
 
-Filename: gudhi-3.8.0rc2.dist-info/RECORD
+Filename: gudhi-3.8.0rc3.dist-info/METADATA
 Comment: 
 
 Zip file comment:
```

## gudhi/__init__.py

```diff
@@ -1,50 +1,50 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-#  See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-#  Author(s):       Vincent Rouvreau
-#
-# Copyright (C) 2016  Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-from importlib import import_module
-from sys import exc_info
-
-__author__ = "GUDHI Editorial Board"
-__copyright__ = "Copyright (C) 2016 Inria"
-__license__ = "https://gudhi.inria.fr/licensing/"
-__version__ = "3.8.0rc2"
-# This variable is used by doctest to find files
-__root_source_dir__ = "D:/a/gudhi-devel/gudhi-devel"
-__debug_info__ =     "Pybind11 version 2.10.4 \n" \
-    "Python version 3.9.13\n" \
-    "Cython version 0.29.34 \n" \
-    "Numpy version 1.24.2 \n" \
-    "Eigen3 version 3.4.0\n" \
-    "Boost version 108100\n" \
-    "CGAL header only version 5.5.1\n" \
-    "GMP_LIBRARIES = C:/vcpkg/installed/x64-windows/lib/gmp.lib\n" \
-    "GMPXX_LIBRARIES = C:/vcpkg/installed/x64-windows/lib/gmpxx.lib\n" \
-    "MPFR_LIBRARIES = C:/vcpkg/installed/x64-windows/lib/mpfr.lib\n" \
-
-
-__all__ = ['bottleneck', 'off_utils', 'simplex_tree', 'rips_complex', 'cubical_complex', 'periodic_cubical_complex', 'persistence_graphical_tools', 'reader_utils', 'witness_complex', 'strong_witness_complex', 'nerve_gic', 'subsampling', 'tangential_complex', 'alpha_complex', 'euclidean_witness_complex', 'euclidean_strong_witness_complex',  'hera', 'clustering', 'datasets', 'representations', 'sklearn', 'tensorflow', 'wasserstein', 'point_cloud', 'weighted_rips_complex', 'dtm_rips_complex', 'cover_complex', ]
-
-__available_modules = ''
-__missing_modules = ''
-
-# Try to import * from gudhi.__module_name for default modules.
-# Extra modules require an explicit import by the user (mostly because of
-# unusual dependencies, but also to avoid cluttering namespace gudhi and
-# speed up the basic import)
-for __module_name in ['bottleneck', 'off_utils', 'simplex_tree', 'rips_complex', 'cubical_complex', 'periodic_cubical_complex', 'persistence_graphical_tools', 'reader_utils', 'witness_complex', 'strong_witness_complex', 'nerve_gic', 'subsampling', 'tangential_complex', 'alpha_complex', 'euclidean_witness_complex', 'euclidean_strong_witness_complex', ]:
-    try:
-        __module = import_module('gudhi.' + __module_name)
-        try:
-            __to_import = __module.__all__
-        except AttributeError:
-            __to_import = [name for name in __module.__dict__ if not name.startswith('_')]
-        globals().update({name: __module.__dict__[name] for name in __to_import})
-        __available_modules += __module_name + ";"
-    except:
-        __missing_modules += __module_name + ";"
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+#  See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+#  Author(s):       Vincent Rouvreau
+#
+# Copyright (C) 2016  Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+from importlib import import_module
+from sys import exc_info
+
+__author__ = "GUDHI Editorial Board"
+__copyright__ = "Copyright (C) 2016 Inria"
+__license__ = "https://gudhi.inria.fr/licensing/"
+__version__ = "3.8.0rc3"
+# This variable is used by doctest to find files
+__root_source_dir__ = "/Users/runner/work/gudhi-devel/gudhi-devel"
+__debug_info__ =     "Pybind11 version 2.10.4 \n" \
+    "Python version 3.8.16\n" \
+    "Cython version 0.29.34 \n" \
+    "Numpy version 1.21.6 \n" \
+    "Eigen3 version 3.4.0\n" \
+    "Boost version 1.81.0\n" \
+    "CGAL header only version 5.5.2\n" \
+    "GMP_LIBRARIES = /Users/runner/work/gudhi-devel/gudhi-devel/deps-uni/lib/libgmp.dylib\n" \
+    "GMPXX_LIBRARIES = /Users/runner/work/gudhi-devel/gudhi-devel/deps-uni/lib/libgmpxx.dylib\n" \
+    "MPFR_LIBRARIES = /Users/runner/work/gudhi-devel/gudhi-devel/deps-uni/lib/libmpfr.dylib\n" \
+
+
+__all__ = ['bottleneck', 'off_utils', 'simplex_tree', 'rips_complex', 'cubical_complex', 'periodic_cubical_complex', 'persistence_graphical_tools', 'reader_utils', 'witness_complex', 'strong_witness_complex', 'nerve_gic', 'subsampling', 'tangential_complex', 'alpha_complex', 'euclidean_witness_complex', 'euclidean_strong_witness_complex',  'hera', 'clustering', 'datasets', 'representations', 'sklearn', 'tensorflow', 'wasserstein', 'point_cloud', 'weighted_rips_complex', 'dtm_rips_complex', 'cover_complex', ]
+
+__available_modules = ''
+__missing_modules = ''
+
+# Try to import * from gudhi.__module_name for default modules.
+# Extra modules require an explicit import by the user (mostly because of
+# unusual dependencies, but also to avoid cluttering namespace gudhi and
+# speed up the basic import)
+for __module_name in ['bottleneck', 'off_utils', 'simplex_tree', 'rips_complex', 'cubical_complex', 'periodic_cubical_complex', 'persistence_graphical_tools', 'reader_utils', 'witness_complex', 'strong_witness_complex', 'nerve_gic', 'subsampling', 'tangential_complex', 'alpha_complex', 'euclidean_witness_complex', 'euclidean_strong_witness_complex', ]:
+    try:
+        __module = import_module('gudhi.' + __module_name)
+        try:
+            __to_import = __module.__all__
+        except AttributeError:
+            __to_import = [name for name in __module.__dict__ if not name.startswith('_')]
+        globals().update({name: __module.__dict__[name] for name in __to_import})
+        __available_modules += __module_name + ";"
+    except:
+        __missing_modules += __module_name + ";"
```

## gudhi/cover_complex.py

 * *Ordering differences only*

```diff
@@ -1,646 +1,646 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Mathieu Carri√®re
-#
-# Copyright (C) 2021 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-import itertools
-import matplotlib
-import matplotlib.pyplot as plt
-
-from sklearn.base            import BaseEstimator
-from sklearn.cluster         import DBSCAN, AgglomerativeClustering
-from sklearn.metrics         import pairwise_distances
-from scipy.spatial.distance  import directed_hausdorff
-
-from . import SimplexTree, CoverComplex
-
-class CoverComplexPy(BaseEstimator):
-    """
-    This is a mother class for MapperComplex, GraphInducedComplex and NerveComplex.
-
-    Attributes:
-        simplex_tree_ (gudhi SimplexTree): simplicial complex representing the cover complex computed after calling the fit() method.
-        node_info_ (dictionary): various information associated to the nodes of the cover complex.
-    """
-    def __init__(self, verbose=False):
-        """
-        Constructor for the CoverComplexPy class.
-
-        Parameters
-        ----------
-        """
-        self.verbose = verbose
-
-    def get_networkx(self, set_attributes_from_colors=False):
-        """
-        Turn the 1-skeleton of the cover complex computed after calling fit() method into a networkx graph.
-        This function requires networkx (https://networkx.org/documentation/stable/install.html).
-
-        Parameters
-        ----------
-        set_attributes_from_colors : bool
-            if True, the color functions will be used as attributes for the networkx graph.
-
-        Returns
-        -------
-        G : networkx graph
-            graph representing the 1-skeleton of the cover complex.
-        """
-        import networkx as nx
-        st = self.simplex_tree_
-        G = nx.Graph()
-        for (splx,_) in st.get_skeleton(1):	
-            if len(splx) == 1:
-                G.add_node(splx[0])
-            if len(splx) == 2:
-                G.add_edge(splx[0], splx[1])
-        if set_attributes_from_colors:
-            attrs = {k: {"attr_name": self.node_info_[k]["colors"]} for k in G.nodes()}
-            nx.set_node_attributes(G, attrs)
-        return G
-
-    def save_to_dot(self, file_name="cover_complex", color_name="color", eps_color=.1, eps_size=.1):
-        """
-        Write the 0-skeleton of the cover complex in a DOT file called "{file_name}.dot", that can be processed with, e.g., neato. The vertices of the cover complex are colored with the first color function, ie, the first column of self.colors.  This function also produces an extra pdf file "colorbar_{color_name}.pdf" containing a colorbar corresponding to the node colors in the DOT file.
-
-        Parameters
-        ----------
-        file_name : string
-            name for the output .dot file, default "cover_complex" 
-        color_name : string
-            name for the output .pdf showing the colorbar of the color used for the Mapper nodes, default "color"
-        eps_color : float
-            scale the node colors between [eps_color, 1-eps_color]. Should be between 0 and 1/2. When close to 0., the color varies a lot across the nodes, if close to 1/2, the color tends to be more uniform.
-        eps_size : float
-            scale the node sizes between [eps_size, 1-eps_size]. Should be between 0 and 1/2. When close to 0., the size varies a lot across the nodes, if close to 1/2, the nodes tend to have the same size.
-        """
-        st = self.simplex_tree_
-        node_info_ = self.node_info_
-
-        maxv, minv = max([node_info_[k]["colors"][0] for k in node_info_.keys()]), min([node_info_[k]["colors"][0] for k in node_info_.keys()])
-        maxs, mins = max([node_info_[k]["size"]      for k in node_info_.keys()]), min([node_info_[k]["size"]      for k in node_info_.keys()])
-
-        if not file_name.lower().endswith(".dot"):
-            file_name += ".dot"
-
-        with open(file_name, "w") as f:
-            f.write("graph MAP{")
-            cols = []
-            for (simplex,_) in st.get_skeleton(0):
-                cnode = (1.-2*eps_color) * (node_info_[simplex[0]]["colors"][0] - minv)/(maxv-minv) + eps_color if maxv != minv else 0
-                snode = (1.-2*eps_size) * (node_info_[simplex[0]]["size"]-mins)/(maxs-mins) + eps_size if maxs != mins else 1
-                f.write(  str(simplex[0]) + "[shape=circle width=" + str(snode) + " fontcolor=black color=black label=\""  + "\" style=filled fillcolor=\"" + str(cnode) + ", 1, 1\"]")
-                cols.append(cnode)
-            for (simplex,_) in st.get_simplices():
-                if len(simplex) == 2:
-                    f.write("  " + str(simplex[0]) + " -- " + str(simplex[1]) + " [weight=15];")
-            f.write("}")
-        
-        L = np.linspace(eps_color, 1.-eps_color, 100)
-        colsrgb = []
-        import colorsys
-        for c in L:
-            colsrgb.append(colorsys.hsv_to_rgb(c,1,1))
-        fig, ax = plt.subplots(figsize=(6, 1))
-        fig.subplots_adjust(bottom=0.5)
-        my_cmap = matplotlib.colors.ListedColormap(colsrgb, name=color_name)
-        cb = matplotlib.colorbar.ColorbarBase(ax, cmap=my_cmap, norm=matplotlib.colors.Normalize(vmin=minv, vmax=maxv), orientation="horizontal")
-        cb.set_label(color_name)
-        fig.savefig("colorbar_" + color_name + ".pdf", format="pdf")
-        plt.close()
-        
-    def save_to_txt(self, file_name="cover_complex", data_name="data", cover_name="cover", color_name="color"):
-        """
-        Write the cover complex to a TXT file called "{file_name}.txt", that can be processed with the KeplerMapper Python script "KeplerMapperVisuFromTxtFile.py" available under "src/Nerve_GIC/utilities/".
-
-        Parameters
-        ----------
-        file_name : string
-            name for the output .txt file, default "cover_complex" 
-        data_name : string
-            name to use for the data on which the cover complex was computed, default "data". It will be used when generating an html visualization with KeplerMapperVisuFromTxtFile.py 
-        cover_name : string
-            name to use for the cover used to compute the cover complex, default "cover". It will be used when generating an html visualization with KeplerMapperVisuFromTxtFile.py
-        color_name : string
-            name to use for the color used to color the cover complex nodes, default "color". It will be used when generating an html visualization with KeplerMapperVisuFromTxtFile.py
-        """
-        st = self.simplex_tree_
-
-        if not file_name.lower().endswith(".txt"):
-            file_name += ".txt"
-
-        with open(file_name, "w") as f:
-            f.write(data_name + "\n")
-            f.write(cover_name + "\n")
-            f.write(color_name + "\n")
-            f.write("0 0\n")
-            f.write(str(st.num_vertices()) + " " + str(len(list(st.get_skeleton(1)))-st.num_vertices()) + "\n")
-            name2id = {}
-            idv = 0
-            for s,_ in st.get_skeleton(0):
-                f.write(str(idv) + " " + str(self.node_info_[s[0]]["colors"][0]) + " " + str(self.node_info_[s[0]]["size"]) + "\n")
-                name2id[s[0]] = idv
-                idv += 1
-            for s,_ in st.get_skeleton(1):
-                if len(s) == 2:
-                    f.write(str(name2id[s[0]]) + " " + str(name2id[s[1]]) + "\n")
-    
-    class _constant_clustering():
-        def fit_predict(X):
-            return np.zeros([len(X)], dtype=np.int32)
-
-
-class MapperComplex(CoverComplexPy):
-    """
-    This is a class for computing Mapper simplicial complexes on point clouds or distance matrices.
-    """
-    def __init__(self, *, input_type="point cloud", colors=None, min_points_per_node=0, filter_bnds=None, resolutions=None, gains=None, clustering=DBSCAN(), N=100, beta=0., C=10., verbose=False):
-        """
-        Constructor for the MapperComplex class.
-
-        Parameters
-        ----------
-        input_type : string
-            type of input data. Either "point cloud" or "distance matrix".
-        min_points_per_node : int
-            threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **min_points_per_node** points will be removed.
-        filter_bnds : list of lists or numpy array of shape (num_filters) x 2)
-            limits of each filter, of the form [[f_1^min, f_1^max], ..., [f_n^min, f_n^max]]. If one of the values is numpy.nan, it can be computed from the dataset with the fit() method.
-        resolutions : list or numpy array of shape num_filters containing integers
-            resolution of each filter function, ie number of intervals required to cover each filter image. If None, it is estimated from data.
-        gains : list or numpy array of shape num_filters containing doubles in [0,1]
-            gain of each filter function, ie overlap percentage of the intervals covering each filter image. If None, it is set as 1/3 for all filters, since in the automatic parameter selection method in http://www.jmlr.org/papers/volume19/17-291/17-291.pdf, any arbitrary value between 1/3 and 1/2 works, so we go with the minimal one (ensuring that the complex is a graph if only given one filter).
-        clustering : class
-            clustering class (default sklearn.cluster.DBSCAN()). Common clustering classes can be found in the scikit-learn library (such as AgglomerativeClustering for instance). If None, it is set to hierarchical clustering, with scale estimated from data.
-        N : int
-            subsampling iterations (default 100) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-        beta : float
-            exponent parameter (default 0.) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-        C : float 
-            constant parameter (default 10.) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-        verbose : bool
-            whether to display info while computing.
-        """
-
-        self.filter_bnds, self.resolutions, self.gains, self.clustering = filter_bnds, resolutions, gains, clustering
-        self.input_type, self.min_points_per_node, self.N, self.beta, self.C = input_type, min_points_per_node, N, beta, C
-        CoverComplexPy.__init__(self, verbose)
-
-    def estimate_scale(self, X, N=100, beta=0., C=10.):
-        """
-        Compute estimated scale of a point cloud or a distance matrix.
-
-        Parameters
-        ----------
-            X : numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix
-                input point cloud or distance matrix.
-            N : int
-                subsampling iterations (default 100). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            beta : float 
-                exponent parameter (default 0.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            C : float
-                constant parameter (default 10.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-
-        Returns
-        -------
-        delta : float
-            estimated scale that can be used with, e.g., agglomerative clustering.
-        """
-        num_pts = X.shape[0]
-        delta, m = 0., int(  num_pts / np.exp((1+beta) * np.log(np.log(num_pts)/np.log(C)))  )
-        for _ in range(N):
-            subpop = np.random.choice(num_pts, size=m, replace=False)
-            if self.input_type == "point cloud":
-                d, _, _ = directed_hausdorff(X, X[subpop,:])
-            if self.input_type == "distance matrix":
-                d = np.max(np.min(X[:,subpop], axis=1), axis=0)
-            delta += d/N
-        return delta
-
-    def get_optimal_parameters_for_agglomerative_clustering(self, X, beta=0., C=10., N=100):
-        """
-        Compute optimal scale and resolutions for a point cloud or a distance matrix.
-
-        Parameters
-        ----------
-            X : numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix
-                input point cloud or distance matrix.
-            beta : float
-                exponent parameter (default 0.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            C : float 
-                constant parameter (default 10.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            N : int
-                subsampling iterations (default 100). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-
-        Returns
-        -------
-        delta : float
-            optimal scale that can be used with agglomerative clustering.
-        resolutions : numpy array of shape (num_filters)
-            optimal resolutions associated to each filter.
-        """
-        num_filt, delta = self.filters.shape[1], 0
-        delta = self.estimate_scale(X=X, N=N, C=C, beta=beta)
-
-        pairwise = pairwise_distances(X, metric="euclidean") if self.input_type == "point cloud" else X
-        pairs = np.argwhere(pairwise <= delta)
-        num_pairs = pairs.shape[0]
-        res = []
-        for f in range(num_filt):
-            F = self.filters[:,f]
-            resf = 0
-            for p in range(num_pairs):
-                resf = max(resf, abs(F[pairs[p,0]] - F[pairs[p,1]]))
-            res.append(resf)
-
-        return delta, np.array(res)
-
-    def fit(self, X, y=None, filters=None, colors=None):
-        """
-        Fit the MapperComplex class on a point cloud or a distance matrix: compute the Mapper complex and store it in a simplex tree called `simplex_tree_`.
-
-        Parameters
-        ----------
-            X : numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix
-                input point cloud or distance matrix.
-            y : n x 1 array
-                point labels (unused).
-            filters : list of lists or numpy array of shape (num_points) x (num_filters) 
-                filter functions (sometimes called lenses) used to compute the cover. Each column of the numpy array defines a scalar function defined on the input points.
-            colors : list of lists or numpy array of shape (num_points) x (num_colors)
-                functions used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of these functions on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.
-        """
-
-        if self.resolutions is not None:
-            self.resolutions = np.array(self.resolutions)
-            if len(self.resolutions.shape) == 0:
-                self.resolutions = np.broadcast_to(self.resolutions, 1)
-        if self.gains is not None:
-            self.gains = np.array(self.gains)
-            if len(self.gains.shape) == 0:
-                self.gains = np.broadcast_to(self.gains, 1)
-        if self.filter_bnds is not None:
-            self.filter_bnds = np.array(self.filter_bnds)
-
-        self.filters, self.colors = filters, colors
-
-        if self.filters is None:
-            if self.input_type == "point cloud":
-                self.filters = X[:,0:1]
-            elif self.input_type == "distance matrix":
-                self.filters = X.max(axis=0)[:,None]
-        else:
-            if isinstance(self.filters, np.ndarray) == False:
-                self.filters = np.array(self.filters).T
-
-        if self.colors is None:
-            if self.input_type == "point cloud":
-                self.colors = X[:,0:1]
-            elif self.input_type == "distance matrix":
-                self.colors = X.max(axis=0)[:,None]
-        else:
-            if isinstance(self.colors, np.ndarray) == False:
-                self.colors = np.array(self.colors).T
-
-        if len(self.filters.shape) == 1: # if self.filters is a 1D filter, convert it to an array of shape [n,1]
-            self.filters = np.reshape(self.filters, [len(X),1])
-        if len(self.colors.shape) == 1: # if self.colors is a 1D filter, convert it to an array of shape [n,1]
-            self.colors = np.reshape(self.colors, [len(X),1])
-      
-        num_pts, num_filters = self.filters.shape[0], self.filters.shape[1]
-
-        # If some filter limits are unspecified, automatically compute them
-        if self.filter_bnds is None:
-            self.filter_bnds = np.hstack([np.min(self.filters, axis=0)[:,np.newaxis], np.max(self.filters, axis=0)[:,np.newaxis]])
-
-        # If some resolutions are not specified, automatically compute them
-        if self.gains is None:
-            self.gains = np.broadcast_to(1/3, num_filters)
-        if self.resolutions is None or self.clustering is None:
-            delta, resolutions = self.get_optimal_parameters_for_agglomerative_clustering(X=X, beta=self.beta, C=self.C, N=self.N)
-            if self.clustering is None:
-                if self.input_type == "point cloud":
-                    self.clustering = AgglomerativeClustering(n_clusters=None, linkage="single", distance_threshold=delta, affinity="euclidean")
-                else:
-                    self.clustering = AgglomerativeClustering(n_clusters=None, linkage="single", distance_threshold=delta, affinity="precomputed")
-            if self.resolutions is None:
-                self.resolutions = np.multiply(resolutions, 1./self.gains)
-                self.resolutions = np.array([int( (self.filter_bnds[ir,1]-self.filter_bnds[ir,0])/r) for ir, r in enumerate(self.resolutions)])
-
-        # Initialize attributes
-        self.simplex_tree_, self.node_info_ = SimplexTree(), {}
-
-        if np.all(self.gains < .5):
-
-            # Compute which points fall in which patch or patch intersections
-            interval_inds, intersec_inds = np.empty(self.filters.shape), np.empty(self.filters.shape)
-            for i in range(num_filters):
-                f, r, g = self.filters[:,i], self.resolutions[i], self.gains[i]
-                min_f, max_f = self.filter_bnds[i,0], np.nextafter(self.filter_bnds[i,1], np.inf)
-                interval_endpoints, l = np.linspace(min_f, max_f, num=r+1, retstep=True)
-                intersec_endpoints = []
-                for j in range(1, len(interval_endpoints)-1):
-                    intersec_endpoints.append(interval_endpoints[j] - g*l / (2 - 2*g))
-                    intersec_endpoints.append(interval_endpoints[j] + g*l / (2 - 2*g))
-                interval_inds[:,i] = np.digitize(f, interval_endpoints)
-                intersec_inds[:,i] = 0.5 * (np.digitize(f, intersec_endpoints) + 1)
-
-            # Build the binned_data map that takes a patch or a patch intersection and outputs the indices of the points contained in it
-            binned_data = {}
-            for i in range(num_pts):
-                list_preimage = []
-                for j in range(num_filters):
-                    a, b = interval_inds[i,j], intersec_inds[i,j]
-                    list_preimage.append([a])
-                    if b == a:
-                        list_preimage[j].append(a+1)
-                    if b == a-1:
-                        list_preimage[j].append(a-1)
-                list_preimage = list(itertools.product(*list_preimage))
-                for pre_idx in list_preimage:
-                    try:
-                        binned_data[pre_idx].append(i)
-                    except KeyError:
-                        binned_data[pre_idx] = [i]
-
-        else:
-
-            # Compute interval endpoints for each filter
-            l_int, r_int = [], []
-            for i in range(num_filters):
-                L, R = [], []
-                f, r, g = self.filters[:,i], self.resolutions[i], self.gains[i]
-                min_f, max_f = self.filter_bnds[i,0], np.nextafter(self.filter_bnds[i,1], np.inf)
-                interval_endpoints, l = np.linspace(min_f, max_f, num=r+1, retstep=True)
-                for j in range(len(interval_endpoints)-1):
-                    L.append(interval_endpoints[j]   - g*l / (2 - 2*g))
-                    R.append(interval_endpoints[j+1] + g*l / (2 - 2*g))
-                l_int.append(L)
-                r_int.append(R)
-
-            # Build the binned_data map that takes a patch or a patch intersection and outputs the indices of the points contained in it
-            binned_data = {}
-            for i in range(num_pts):
-                list_preimage = []
-                for j in range(num_filters):
-                    fval = self.filters[i,j]
-                    start, end = int(min(np.argwhere(np.array(r_int[j]) >= fval))), int(max(np.argwhere(np.array(l_int[j]) <= fval)))
-                    list_preimage.append(list(range(start, end+1)))
-                list_preimage = list(itertools.product(*list_preimage))
-                for pre_idx in list_preimage:
-                    try:
-                        binned_data[pre_idx].append(i)
-                    except KeyError:
-                        binned_data[pre_idx] = [i]
-
-        # Initialize the cover map, that takes a point and outputs the clusters to which it belongs
-        cover, clus_base = [[] for _ in range(num_pts)], 0
-
-        # For each patch
-        for preimage in binned_data:
-
-            # Apply clustering on the corresponding subpopulation
-            idxs = np.array(binned_data[preimage])
-            if len(idxs) > 1:
-                clusters = self.clustering.fit_predict(X[idxs,:]) if self.input_type == "point cloud" else self.clustering.fit_predict(X[idxs,:][:,idxs])
-            elif len(idxs) == 1:
-                clusters = np.array([0])
-            else:
-                continue
-
-            # Collect various information on each cluster
-            num_clus_pre = np.max(clusters) + 1
-            for clus_i in range(num_clus_pre):
-                node_name = clus_base + clus_i
-                subpopulation = idxs[clusters == clus_i]
-                self.node_info_[node_name] = {}
-                self.node_info_[node_name]["indices"] = subpopulation
-                self.node_info_[node_name]["size"] = len(subpopulation)
-                self.node_info_[node_name]["colors"] = np.mean(self.colors[subpopulation,:], axis=0)
-                self.node_info_[node_name]["patch"] = preimage
-
-            # Update the cover map
-            for pt in range(clusters.shape[0]):
-                node_name = clus_base + clusters[pt]
-                if clusters[pt] != -1 and self.node_info_[node_name]["size"] >= self.min_points_per_node:
-                    cover[idxs[pt]].append(node_name)
-
-            clus_base += np.max(clusters) + 1
-
-        # Insert the simplices of the Mapper complex
-        for i in range(num_pts):
-            self.simplex_tree_.insert(cover[i])
-
-        return self
-
-class GraphInducedComplex(CoverComplexPy):
-    """
-    This is a class for computing graph induced simplicial complexes on point clouds or distance matrices.
-    """
-    def __init__(self, *, input_type="point cloud", cover="functional", min_points_per_node=0,
-                          voronoi_samples=100, assignments=None,  filter_bnds=None, resolution=None, gain=None, N=100, beta=0., C=10.,
-                          graph="rips", rips_threshold=None, verbose=False):
-        """
-        Constructor for the GraphInducedComplex class.
-
-        Parameters:
-            input_type (string): type of input data. Either "point cloud" or "distance matrix".
-            cover (string): specifies the cover. Either "functional" (preimages of filter function), "voronoi" or "precomputed".
-            min_points_per_node (int): threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **min_points_per_node** points will be removed.
-            voronoi_samples (int): number of Voronoi germs used for partitioning the input dataset. Used only if cover = "voronoi".
-            assignments (list of length (num_points) of lists of integers): cover assignment for each point. Used only if cover = "precomputed".
-            filter_bnds (list or numpy array of shape 2): limits of the filter function, of the form [f^min, f^max]. If one of the values is numpy.nan, it can be computed from the dataset with the fit() method. Used only if cover = "functional".
-            resolution (int): resolution of the filter function, ie number of intervals required to cover each filter image. Used only if cover = "functional". If None, it is estimated from data.
-            gain (double in [0,1]): gain of the filter function, ie overlap percentage of the intervals covering each filter image. Used only if cover = "functional".
-            N (int): subsampling iterations (default 100) for estimating scale and resolutions. Used only if cover = "functional". See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            beta (double): exponent parameter (default 0.) for estimating scale and resolutions. Used only if cover = "functional". See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            C (double): constant parameter (default 10.) for estimating scale and resolutions. Used only if cover = "functional". See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
-            graph (string): type of graph to use for GIC. Currently accepts "rips" only.
-            rips_threshold (float): Rips parameter. Used only if graph = "rips".
-            verbose (bool): whether to display info while computing.
-        """
-
-        self.input_type, self.cover, self.min_points_per_node = input_type, cover, min_points_per_node
-        self.voronoi_samples, self.assignments, self.filter_bnds, self.resolution, self.gain = voronoi_samples, assignments, filter_bnds, resolution, gain
-        self.graph, self.rips_threshold, self.N, self.beta, self.C = graph, rips_threshold, N, beta, C
-        CoverComplexPy.__init__(self, verbose)
-
-    def fit(self, X, y=None, filter=None, color=None):
-        """
-        Fit the GraphInducedComplex class on a point cloud or a distance matrix: compute the graph induced complex and store it in a simplex tree called `simplex_tree_`.
-
-        Parameters:
-            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.
-            y (n x 1 array): point labels (unused).
-            filter (list or numpy array of shape (num_points)): filter function (sometimes called lens) used to compute the cover. Used only if cover = "functional".
-            color (list or numpy array of shape (num_points)): function used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of this function on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.
-        """
-        self.data, self.filter, self.color = X, filter, color
-        self.complex = CoverComplex()
-        self.complex.set_type("GIC")
-        self.complex.set_verbose(self.verbose)
-
-        if self.input_type == "point cloud":
-            self.complex.set_point_cloud_from_range(X)
-        elif self.input_type == "distance matrix":
-            self.complex.set_distances_from_range(X)
-
-        # Set vertex color
-        if self.color is None:
-            if self.input_type == "point cloud":
-                self.color = X[:,0]
-            elif self.input_type == "distance matrix":
-                self.color = X.max(axis=0)
-        else:
-            self.color = np.array(self.color)
-
-        self.complex.set_color_from_range(self.color)
-
-        # Set underlying graph
-        if self.graph == "rips":
-            if self.rips_threshold is not None:
-                self.complex.set_graph_from_rips(self.rips_threshold)
-            else:
-                self.complex.set_subsampling(self.C, self.beta)
-                self.complex.set_graph_from_automatic_rips(self.N)
-
-        # Set vertex cover
-        if self.cover == "voronoi":
-            self.complex.set_cover_from_Voronoi(self.voronoi_samples)
-
-        elif self.cover == "functional":
-
-            if self.filter is None:
-                if self.input_type == "point cloud":
-                    self.filter = X[:,0]
-                elif self.input_type == "distance matrix":
-                    self.filter = X.max(axis=0)
-            else:
-                self.filter = np.array(self.filter)
-
-            self.complex.set_function_from_range(self.filter)
-
-            if self.resolution is None:
-                self.complex.set_automatic_resolution()
-            else:
-                self.complex.set_resolution_with_interval_number(self.resolution)
-
-            if self.gain is None:
-                self.complex.set_gain(.33)
-            else:
-                self.complex.set_gain(self.gain)
-
-            self.complex.set_cover_from_function()
-
-        elif self.cover == "precomputed":
-            self.complex.set_cover_from_range(self.assignments)
-
-        # Compute simplex tree
-        self.complex.set_mask(self.min_points_per_node)
-        self.complex.find_simplices()
-        simplex_tree_ = self.complex.create_simplex_tree()
-
-        # Normalize vertex names of simplex tree
-        self.simplex_tree_ = SimplexTree()
-        idv, names = 0, {}
-        for v,_ in simplex_tree_.get_skeleton(0):
-            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
-                names[v[0]] = idv
-                self.simplex_tree_.insert([idv])
-                idv += 1
-        for s,_ in simplex_tree_.get_simplices():
-            if len(s) >= 2 and np.all([len(self.complex.subpopulation(v)) > self.min_points_per_node for v in s]):
-                self.simplex_tree_.insert([names[v] for v in s])
-
-        # Store vertex info
-        self.node_info_ = {}
-        for v,_ in simplex_tree_.get_skeleton(0):
-            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
-                node = names[v[0]]
-                pop = self.complex.subpopulation(v[0])
-                self.node_info_[node] = {"indices": pop, "size": len(pop), "colors": [self.complex.subcolor(v[0])]}
-
-        return self
-
-class NerveComplex(CoverComplexPy):
-    """
-    This is a class for computing nerve simplicial complexes on point clouds or distance matrices.
-    """
-    def __init__(self, *, input_type="point cloud", min_points_per_node=0, verbose=False):
-        """
-        Constructor for the NerveComplex class.
-
-        Parameters:
-            input_type (string): type of input data. Either "point cloud" or "distance matrix".
-            min_points_per_node (int): threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **min_points_per_node** points will be removed.
-            verbose (bool): whether to display info while computing.
-        """
-
-        self.input_type, self.min_points_per_node = input_type, min_points_per_node
-        CoverComplexPy.__init__(self, verbose)
-
-    def fit(self, X, y=None, assignments=None, color=None):
-        """
-        Fit the NerveComplex class on a point cloud or a distance matrix: compute the nerve complex and store it in a simplex tree called `simplex_tree_`.
-
-        Parameters:
-            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.
-            y (n x 1 array): point labels (unused).
-            assignments (list of length (num_points) of lists of integers): cover assignment for each point.
-            color (numpy array of shape (num_points) x (num_colors)): functions used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of these functions on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.
-        """
-        self.data, self.color = X, color
-        self.assignments = assignments
-        self.complex = CoverComplex()
-        self.complex.set_type("Nerve")
-        self.complex.set_verbose(self.verbose)
-
-        if self.input_type == "point cloud":
-            self.complex.set_point_cloud_from_range(X)
-        elif self.input_type == "distance matrix":
-            self.complex.set_distances_from_range(X)
-
-        # Set vertex color
-        if self.color is None:
-            if self.input_type == "point cloud":
-                self.color = X[:,0]
-            elif self.input_type == "distance matrix":
-                self.color = X.max(axis=0)
-        else:
-            self.color = np.array(self.color)
-        self.complex.set_color_from_range(self.color)
-
-        # Set vertex cover
-        self.complex.set_cover_from_range(self.assignments)
-
-        # Compute simplex tree
-        self.complex.set_mask(self.min_points_per_node)
-        self.complex.find_simplices()
-        simplex_tree_ = self.complex.create_simplex_tree()
-
-        # Normalize vertex names of simplex tree
-        self.simplex_tree_ = SimplexTree()
-        idv, names = 0, {}
-        for v,_ in simplex_tree_.get_skeleton(0):
-            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
-                names[v[0]] = idv
-                self.simplex_tree_.insert([idv])
-                idv += 1
-        for s,_ in simplex_tree_.get_simplices():
-            if len(s) >= 2 and np.all([len(self.complex.subpopulation(v)) > self.min_points_per_node for v in s]):
-                self.simplex_tree_.insert([names[v] for v in s])
-
-        # Store vertex info
-        self.node_info_ = {}
-        for v,_ in simplex_tree_.get_skeleton(0):
-            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
-                node = names[v[0]]
-                pop = self.complex.subpopulation(v[0])
-                self.node_info_[node] = {"indices": pop, "size": len(pop), "colors": [self.complex.subcolor(v[0])]}
-
-        return self
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Mathieu Carri√®re
+#
+# Copyright (C) 2021 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+import itertools
+import matplotlib
+import matplotlib.pyplot as plt
+
+from sklearn.base            import BaseEstimator
+from sklearn.cluster         import DBSCAN, AgglomerativeClustering
+from sklearn.metrics         import pairwise_distances
+from scipy.spatial.distance  import directed_hausdorff
+
+from . import SimplexTree, CoverComplex
+
+class CoverComplexPy(BaseEstimator):
+    """
+    This is a mother class for MapperComplex, GraphInducedComplex and NerveComplex.
+
+    Attributes:
+        simplex_tree_ (gudhi SimplexTree): simplicial complex representing the cover complex computed after calling the fit() method.
+        node_info_ (dictionary): various information associated to the nodes of the cover complex.
+    """
+    def __init__(self, verbose=False):
+        """
+        Constructor for the CoverComplexPy class.
+
+        Parameters
+        ----------
+        """
+        self.verbose = verbose
+
+    def get_networkx(self, set_attributes_from_colors=False):
+        """
+        Turn the 1-skeleton of the cover complex computed after calling fit() method into a networkx graph.
+        This function requires networkx (https://networkx.org/documentation/stable/install.html).
+
+        Parameters
+        ----------
+        set_attributes_from_colors : bool
+            if True, the color functions will be used as attributes for the networkx graph.
+
+        Returns
+        -------
+        G : networkx graph
+            graph representing the 1-skeleton of the cover complex.
+        """
+        import networkx as nx
+        st = self.simplex_tree_
+        G = nx.Graph()
+        for (splx,_) in st.get_skeleton(1):	
+            if len(splx) == 1:
+                G.add_node(splx[0])
+            if len(splx) == 2:
+                G.add_edge(splx[0], splx[1])
+        if set_attributes_from_colors:
+            attrs = {k: {"attr_name": self.node_info_[k]["colors"]} for k in G.nodes()}
+            nx.set_node_attributes(G, attrs)
+        return G
+
+    def save_to_dot(self, file_name="cover_complex", color_name="color", eps_color=.1, eps_size=.1):
+        """
+        Write the 0-skeleton of the cover complex in a DOT file called "{file_name}.dot", that can be processed with, e.g., neato. The vertices of the cover complex are colored with the first color function, ie, the first column of self.colors.  This function also produces an extra pdf file "colorbar_{color_name}.pdf" containing a colorbar corresponding to the node colors in the DOT file.
+
+        Parameters
+        ----------
+        file_name : string
+            name for the output .dot file, default "cover_complex" 
+        color_name : string
+            name for the output .pdf showing the colorbar of the color used for the Mapper nodes, default "color"
+        eps_color : float
+            scale the node colors between [eps_color, 1-eps_color]. Should be between 0 and 1/2. When close to 0., the color varies a lot across the nodes, if close to 1/2, the color tends to be more uniform.
+        eps_size : float
+            scale the node sizes between [eps_size, 1-eps_size]. Should be between 0 and 1/2. When close to 0., the size varies a lot across the nodes, if close to 1/2, the nodes tend to have the same size.
+        """
+        st = self.simplex_tree_
+        node_info_ = self.node_info_
+
+        maxv, minv = max([node_info_[k]["colors"][0] for k in node_info_.keys()]), min([node_info_[k]["colors"][0] for k in node_info_.keys()])
+        maxs, mins = max([node_info_[k]["size"]      for k in node_info_.keys()]), min([node_info_[k]["size"]      for k in node_info_.keys()])
+
+        if not file_name.lower().endswith(".dot"):
+            file_name += ".dot"
+
+        with open(file_name, "w") as f:
+            f.write("graph MAP{")
+            cols = []
+            for (simplex,_) in st.get_skeleton(0):
+                cnode = (1.-2*eps_color) * (node_info_[simplex[0]]["colors"][0] - minv)/(maxv-minv) + eps_color if maxv != minv else 0
+                snode = (1.-2*eps_size) * (node_info_[simplex[0]]["size"]-mins)/(maxs-mins) + eps_size if maxs != mins else 1
+                f.write(  str(simplex[0]) + "[shape=circle width=" + str(snode) + " fontcolor=black color=black label=\""  + "\" style=filled fillcolor=\"" + str(cnode) + ", 1, 1\"]")
+                cols.append(cnode)
+            for (simplex,_) in st.get_simplices():
+                if len(simplex) == 2:
+                    f.write("  " + str(simplex[0]) + " -- " + str(simplex[1]) + " [weight=15];")
+            f.write("}")
+        
+        L = np.linspace(eps_color, 1.-eps_color, 100)
+        colsrgb = []
+        import colorsys
+        for c in L:
+            colsrgb.append(colorsys.hsv_to_rgb(c,1,1))
+        fig, ax = plt.subplots(figsize=(6, 1))
+        fig.subplots_adjust(bottom=0.5)
+        my_cmap = matplotlib.colors.ListedColormap(colsrgb, name=color_name)
+        cb = matplotlib.colorbar.ColorbarBase(ax, cmap=my_cmap, norm=matplotlib.colors.Normalize(vmin=minv, vmax=maxv), orientation="horizontal")
+        cb.set_label(color_name)
+        fig.savefig("colorbar_" + color_name + ".pdf", format="pdf")
+        plt.close()
+        
+    def save_to_txt(self, file_name="cover_complex", data_name="data", cover_name="cover", color_name="color"):
+        """
+        Write the cover complex to a TXT file called "{file_name}.txt", that can be processed with the KeplerMapper Python script "KeplerMapperVisuFromTxtFile.py" available under "src/Nerve_GIC/utilities/".
+
+        Parameters
+        ----------
+        file_name : string
+            name for the output .txt file, default "cover_complex" 
+        data_name : string
+            name to use for the data on which the cover complex was computed, default "data". It will be used when generating an html visualization with KeplerMapperVisuFromTxtFile.py 
+        cover_name : string
+            name to use for the cover used to compute the cover complex, default "cover". It will be used when generating an html visualization with KeplerMapperVisuFromTxtFile.py
+        color_name : string
+            name to use for the color used to color the cover complex nodes, default "color". It will be used when generating an html visualization with KeplerMapperVisuFromTxtFile.py
+        """
+        st = self.simplex_tree_
+
+        if not file_name.lower().endswith(".txt"):
+            file_name += ".txt"
+
+        with open(file_name, "w") as f:
+            f.write(data_name + "\n")
+            f.write(cover_name + "\n")
+            f.write(color_name + "\n")
+            f.write("0 0\n")
+            f.write(str(st.num_vertices()) + " " + str(len(list(st.get_skeleton(1)))-st.num_vertices()) + "\n")
+            name2id = {}
+            idv = 0
+            for s,_ in st.get_skeleton(0):
+                f.write(str(idv) + " " + str(self.node_info_[s[0]]["colors"][0]) + " " + str(self.node_info_[s[0]]["size"]) + "\n")
+                name2id[s[0]] = idv
+                idv += 1
+            for s,_ in st.get_skeleton(1):
+                if len(s) == 2:
+                    f.write(str(name2id[s[0]]) + " " + str(name2id[s[1]]) + "\n")
+    
+    class _constant_clustering():
+        def fit_predict(X):
+            return np.zeros([len(X)], dtype=np.int32)
+
+
+class MapperComplex(CoverComplexPy):
+    """
+    This is a class for computing Mapper simplicial complexes on point clouds or distance matrices.
+    """
+    def __init__(self, *, input_type="point cloud", colors=None, min_points_per_node=0, filter_bnds=None, resolutions=None, gains=None, clustering=DBSCAN(), N=100, beta=0., C=10., verbose=False):
+        """
+        Constructor for the MapperComplex class.
+
+        Parameters
+        ----------
+        input_type : string
+            type of input data. Either "point cloud" or "distance matrix".
+        min_points_per_node : int
+            threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **min_points_per_node** points will be removed.
+        filter_bnds : list of lists or numpy array of shape (num_filters) x 2)
+            limits of each filter, of the form [[f_1^min, f_1^max], ..., [f_n^min, f_n^max]]. If one of the values is numpy.nan, it can be computed from the dataset with the fit() method.
+        resolutions : list or numpy array of shape num_filters containing integers
+            resolution of each filter function, ie number of intervals required to cover each filter image. If None, it is estimated from data.
+        gains : list or numpy array of shape num_filters containing doubles in [0,1]
+            gain of each filter function, ie overlap percentage of the intervals covering each filter image. If None, it is set as 1/3 for all filters, since in the automatic parameter selection method in http://www.jmlr.org/papers/volume19/17-291/17-291.pdf, any arbitrary value between 1/3 and 1/2 works, so we go with the minimal one (ensuring that the complex is a graph if only given one filter).
+        clustering : class
+            clustering class (default sklearn.cluster.DBSCAN()). Common clustering classes can be found in the scikit-learn library (such as AgglomerativeClustering for instance). If None, it is set to hierarchical clustering, with scale estimated from data.
+        N : int
+            subsampling iterations (default 100) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+        beta : float
+            exponent parameter (default 0.) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+        C : float 
+            constant parameter (default 10.) for estimating scale and resolutions. Used only if clustering or resolutions = None. See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+        verbose : bool
+            whether to display info while computing.
+        """
+
+        self.filter_bnds, self.resolutions, self.gains, self.clustering = filter_bnds, resolutions, gains, clustering
+        self.input_type, self.min_points_per_node, self.N, self.beta, self.C = input_type, min_points_per_node, N, beta, C
+        CoverComplexPy.__init__(self, verbose)
+
+    def estimate_scale(self, X, N=100, beta=0., C=10.):
+        """
+        Compute estimated scale of a point cloud or a distance matrix.
+
+        Parameters
+        ----------
+            X : numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix
+                input point cloud or distance matrix.
+            N : int
+                subsampling iterations (default 100). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            beta : float 
+                exponent parameter (default 0.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            C : float
+                constant parameter (default 10.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+
+        Returns
+        -------
+        delta : float
+            estimated scale that can be used with, e.g., agglomerative clustering.
+        """
+        num_pts = X.shape[0]
+        delta, m = 0., int(  num_pts / np.exp((1+beta) * np.log(np.log(num_pts)/np.log(C)))  )
+        for _ in range(N):
+            subpop = np.random.choice(num_pts, size=m, replace=False)
+            if self.input_type == "point cloud":
+                d, _, _ = directed_hausdorff(X, X[subpop,:])
+            if self.input_type == "distance matrix":
+                d = np.max(np.min(X[:,subpop], axis=1), axis=0)
+            delta += d/N
+        return delta
+
+    def get_optimal_parameters_for_agglomerative_clustering(self, X, beta=0., C=10., N=100):
+        """
+        Compute optimal scale and resolutions for a point cloud or a distance matrix.
+
+        Parameters
+        ----------
+            X : numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix
+                input point cloud or distance matrix.
+            beta : float
+                exponent parameter (default 0.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            C : float 
+                constant parameter (default 10.). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            N : int
+                subsampling iterations (default 100). See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+
+        Returns
+        -------
+        delta : float
+            optimal scale that can be used with agglomerative clustering.
+        resolutions : numpy array of shape (num_filters)
+            optimal resolutions associated to each filter.
+        """
+        num_filt, delta = self.filters.shape[1], 0
+        delta = self.estimate_scale(X=X, N=N, C=C, beta=beta)
+
+        pairwise = pairwise_distances(X, metric="euclidean") if self.input_type == "point cloud" else X
+        pairs = np.argwhere(pairwise <= delta)
+        num_pairs = pairs.shape[0]
+        res = []
+        for f in range(num_filt):
+            F = self.filters[:,f]
+            resf = 0
+            for p in range(num_pairs):
+                resf = max(resf, abs(F[pairs[p,0]] - F[pairs[p,1]]))
+            res.append(resf)
+
+        return delta, np.array(res)
+
+    def fit(self, X, y=None, filters=None, colors=None):
+        """
+        Fit the MapperComplex class on a point cloud or a distance matrix: compute the Mapper complex and store it in a simplex tree called `simplex_tree_`.
+
+        Parameters
+        ----------
+            X : numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix
+                input point cloud or distance matrix.
+            y : n x 1 array
+                point labels (unused).
+            filters : list of lists or numpy array of shape (num_points) x (num_filters) 
+                filter functions (sometimes called lenses) used to compute the cover. Each column of the numpy array defines a scalar function defined on the input points.
+            colors : list of lists or numpy array of shape (num_points) x (num_colors)
+                functions used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of these functions on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.
+        """
+
+        if self.resolutions is not None:
+            self.resolutions = np.array(self.resolutions)
+            if len(self.resolutions.shape) == 0:
+                self.resolutions = np.broadcast_to(self.resolutions, 1)
+        if self.gains is not None:
+            self.gains = np.array(self.gains)
+            if len(self.gains.shape) == 0:
+                self.gains = np.broadcast_to(self.gains, 1)
+        if self.filter_bnds is not None:
+            self.filter_bnds = np.array(self.filter_bnds)
+
+        self.filters, self.colors = filters, colors
+
+        if self.filters is None:
+            if self.input_type == "point cloud":
+                self.filters = X[:,0:1]
+            elif self.input_type == "distance matrix":
+                self.filters = X.max(axis=0)[:,None]
+        else:
+            if isinstance(self.filters, np.ndarray) == False:
+                self.filters = np.array(self.filters).T
+
+        if self.colors is None:
+            if self.input_type == "point cloud":
+                self.colors = X[:,0:1]
+            elif self.input_type == "distance matrix":
+                self.colors = X.max(axis=0)[:,None]
+        else:
+            if isinstance(self.colors, np.ndarray) == False:
+                self.colors = np.array(self.colors).T
+
+        if len(self.filters.shape) == 1: # if self.filters is a 1D filter, convert it to an array of shape [n,1]
+            self.filters = np.reshape(self.filters, [len(X),1])
+        if len(self.colors.shape) == 1: # if self.colors is a 1D filter, convert it to an array of shape [n,1]
+            self.colors = np.reshape(self.colors, [len(X),1])
+      
+        num_pts, num_filters = self.filters.shape[0], self.filters.shape[1]
+
+        # If some filter limits are unspecified, automatically compute them
+        if self.filter_bnds is None:
+            self.filter_bnds = np.hstack([np.min(self.filters, axis=0)[:,np.newaxis], np.max(self.filters, axis=0)[:,np.newaxis]])
+
+        # If some resolutions are not specified, automatically compute them
+        if self.gains is None:
+            self.gains = np.broadcast_to(1/3, num_filters)
+        if self.resolutions is None or self.clustering is None:
+            delta, resolutions = self.get_optimal_parameters_for_agglomerative_clustering(X=X, beta=self.beta, C=self.C, N=self.N)
+            if self.clustering is None:
+                if self.input_type == "point cloud":
+                    self.clustering = AgglomerativeClustering(n_clusters=None, linkage="single", distance_threshold=delta, affinity="euclidean")
+                else:
+                    self.clustering = AgglomerativeClustering(n_clusters=None, linkage="single", distance_threshold=delta, affinity="precomputed")
+            if self.resolutions is None:
+                self.resolutions = np.multiply(resolutions, 1./self.gains)
+                self.resolutions = np.array([int( (self.filter_bnds[ir,1]-self.filter_bnds[ir,0])/r) for ir, r in enumerate(self.resolutions)])
+
+        # Initialize attributes
+        self.simplex_tree_, self.node_info_ = SimplexTree(), {}
+
+        if np.all(self.gains < .5):
+
+            # Compute which points fall in which patch or patch intersections
+            interval_inds, intersec_inds = np.empty(self.filters.shape), np.empty(self.filters.shape)
+            for i in range(num_filters):
+                f, r, g = self.filters[:,i], self.resolutions[i], self.gains[i]
+                min_f, max_f = self.filter_bnds[i,0], np.nextafter(self.filter_bnds[i,1], np.inf)
+                interval_endpoints, l = np.linspace(min_f, max_f, num=r+1, retstep=True)
+                intersec_endpoints = []
+                for j in range(1, len(interval_endpoints)-1):
+                    intersec_endpoints.append(interval_endpoints[j] - g*l / (2 - 2*g))
+                    intersec_endpoints.append(interval_endpoints[j] + g*l / (2 - 2*g))
+                interval_inds[:,i] = np.digitize(f, interval_endpoints)
+                intersec_inds[:,i] = 0.5 * (np.digitize(f, intersec_endpoints) + 1)
+
+            # Build the binned_data map that takes a patch or a patch intersection and outputs the indices of the points contained in it
+            binned_data = {}
+            for i in range(num_pts):
+                list_preimage = []
+                for j in range(num_filters):
+                    a, b = interval_inds[i,j], intersec_inds[i,j]
+                    list_preimage.append([a])
+                    if b == a:
+                        list_preimage[j].append(a+1)
+                    if b == a-1:
+                        list_preimage[j].append(a-1)
+                list_preimage = list(itertools.product(*list_preimage))
+                for pre_idx in list_preimage:
+                    try:
+                        binned_data[pre_idx].append(i)
+                    except KeyError:
+                        binned_data[pre_idx] = [i]
+
+        else:
+
+            # Compute interval endpoints for each filter
+            l_int, r_int = [], []
+            for i in range(num_filters):
+                L, R = [], []
+                f, r, g = self.filters[:,i], self.resolutions[i], self.gains[i]
+                min_f, max_f = self.filter_bnds[i,0], np.nextafter(self.filter_bnds[i,1], np.inf)
+                interval_endpoints, l = np.linspace(min_f, max_f, num=r+1, retstep=True)
+                for j in range(len(interval_endpoints)-1):
+                    L.append(interval_endpoints[j]   - g*l / (2 - 2*g))
+                    R.append(interval_endpoints[j+1] + g*l / (2 - 2*g))
+                l_int.append(L)
+                r_int.append(R)
+
+            # Build the binned_data map that takes a patch or a patch intersection and outputs the indices of the points contained in it
+            binned_data = {}
+            for i in range(num_pts):
+                list_preimage = []
+                for j in range(num_filters):
+                    fval = self.filters[i,j]
+                    start, end = int(min(np.argwhere(np.array(r_int[j]) >= fval))), int(max(np.argwhere(np.array(l_int[j]) <= fval)))
+                    list_preimage.append(list(range(start, end+1)))
+                list_preimage = list(itertools.product(*list_preimage))
+                for pre_idx in list_preimage:
+                    try:
+                        binned_data[pre_idx].append(i)
+                    except KeyError:
+                        binned_data[pre_idx] = [i]
+
+        # Initialize the cover map, that takes a point and outputs the clusters to which it belongs
+        cover, clus_base = [[] for _ in range(num_pts)], 0
+
+        # For each patch
+        for preimage in binned_data:
+
+            # Apply clustering on the corresponding subpopulation
+            idxs = np.array(binned_data[preimage])
+            if len(idxs) > 1:
+                clusters = self.clustering.fit_predict(X[idxs,:]) if self.input_type == "point cloud" else self.clustering.fit_predict(X[idxs,:][:,idxs])
+            elif len(idxs) == 1:
+                clusters = np.array([0])
+            else:
+                continue
+
+            # Collect various information on each cluster
+            num_clus_pre = np.max(clusters) + 1
+            for clus_i in range(num_clus_pre):
+                node_name = clus_base + clus_i
+                subpopulation = idxs[clusters == clus_i]
+                self.node_info_[node_name] = {}
+                self.node_info_[node_name]["indices"] = subpopulation
+                self.node_info_[node_name]["size"] = len(subpopulation)
+                self.node_info_[node_name]["colors"] = np.mean(self.colors[subpopulation,:], axis=0)
+                self.node_info_[node_name]["patch"] = preimage
+
+            # Update the cover map
+            for pt in range(clusters.shape[0]):
+                node_name = clus_base + clusters[pt]
+                if clusters[pt] != -1 and self.node_info_[node_name]["size"] >= self.min_points_per_node:
+                    cover[idxs[pt]].append(node_name)
+
+            clus_base += np.max(clusters) + 1
+
+        # Insert the simplices of the Mapper complex
+        for i in range(num_pts):
+            self.simplex_tree_.insert(cover[i])
+
+        return self
+
+class GraphInducedComplex(CoverComplexPy):
+    """
+    This is a class for computing graph induced simplicial complexes on point clouds or distance matrices.
+    """
+    def __init__(self, *, input_type="point cloud", cover="functional", min_points_per_node=0,
+                          voronoi_samples=100, assignments=None,  filter_bnds=None, resolution=None, gain=None, N=100, beta=0., C=10.,
+                          graph="rips", rips_threshold=None, verbose=False):
+        """
+        Constructor for the GraphInducedComplex class.
+
+        Parameters:
+            input_type (string): type of input data. Either "point cloud" or "distance matrix".
+            cover (string): specifies the cover. Either "functional" (preimages of filter function), "voronoi" or "precomputed".
+            min_points_per_node (int): threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **min_points_per_node** points will be removed.
+            voronoi_samples (int): number of Voronoi germs used for partitioning the input dataset. Used only if cover = "voronoi".
+            assignments (list of length (num_points) of lists of integers): cover assignment for each point. Used only if cover = "precomputed".
+            filter_bnds (list or numpy array of shape 2): limits of the filter function, of the form [f^min, f^max]. If one of the values is numpy.nan, it can be computed from the dataset with the fit() method. Used only if cover = "functional".
+            resolution (int): resolution of the filter function, ie number of intervals required to cover each filter image. Used only if cover = "functional". If None, it is estimated from data.
+            gain (double in [0,1]): gain of the filter function, ie overlap percentage of the intervals covering each filter image. Used only if cover = "functional".
+            N (int): subsampling iterations (default 100) for estimating scale and resolutions. Used only if cover = "functional". See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            beta (double): exponent parameter (default 0.) for estimating scale and resolutions. Used only if cover = "functional". See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            C (double): constant parameter (default 10.) for estimating scale and resolutions. Used only if cover = "functional". See http://www.jmlr.org/papers/volume19/17-291/17-291.pdf for details.
+            graph (string): type of graph to use for GIC. Currently accepts "rips" only.
+            rips_threshold (float): Rips parameter. Used only if graph = "rips".
+            verbose (bool): whether to display info while computing.
+        """
+
+        self.input_type, self.cover, self.min_points_per_node = input_type, cover, min_points_per_node
+        self.voronoi_samples, self.assignments, self.filter_bnds, self.resolution, self.gain = voronoi_samples, assignments, filter_bnds, resolution, gain
+        self.graph, self.rips_threshold, self.N, self.beta, self.C = graph, rips_threshold, N, beta, C
+        CoverComplexPy.__init__(self, verbose)
+
+    def fit(self, X, y=None, filter=None, color=None):
+        """
+        Fit the GraphInducedComplex class on a point cloud or a distance matrix: compute the graph induced complex and store it in a simplex tree called `simplex_tree_`.
+
+        Parameters:
+            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.
+            y (n x 1 array): point labels (unused).
+            filter (list or numpy array of shape (num_points)): filter function (sometimes called lens) used to compute the cover. Used only if cover = "functional".
+            color (list or numpy array of shape (num_points)): function used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of this function on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.
+        """
+        self.data, self.filter, self.color = X, filter, color
+        self.complex = CoverComplex()
+        self.complex.set_type("GIC")
+        self.complex.set_verbose(self.verbose)
+
+        if self.input_type == "point cloud":
+            self.complex.set_point_cloud_from_range(X)
+        elif self.input_type == "distance matrix":
+            self.complex.set_distances_from_range(X)
+
+        # Set vertex color
+        if self.color is None:
+            if self.input_type == "point cloud":
+                self.color = X[:,0]
+            elif self.input_type == "distance matrix":
+                self.color = X.max(axis=0)
+        else:
+            self.color = np.array(self.color)
+
+        self.complex.set_color_from_range(self.color)
+
+        # Set underlying graph
+        if self.graph == "rips":
+            if self.rips_threshold is not None:
+                self.complex.set_graph_from_rips(self.rips_threshold)
+            else:
+                self.complex.set_subsampling(self.C, self.beta)
+                self.complex.set_graph_from_automatic_rips(self.N)
+
+        # Set vertex cover
+        if self.cover == "voronoi":
+            self.complex.set_cover_from_Voronoi(self.voronoi_samples)
+
+        elif self.cover == "functional":
+
+            if self.filter is None:
+                if self.input_type == "point cloud":
+                    self.filter = X[:,0]
+                elif self.input_type == "distance matrix":
+                    self.filter = X.max(axis=0)
+            else:
+                self.filter = np.array(self.filter)
+
+            self.complex.set_function_from_range(self.filter)
+
+            if self.resolution is None:
+                self.complex.set_automatic_resolution()
+            else:
+                self.complex.set_resolution_with_interval_number(self.resolution)
+
+            if self.gain is None:
+                self.complex.set_gain(.33)
+            else:
+                self.complex.set_gain(self.gain)
+
+            self.complex.set_cover_from_function()
+
+        elif self.cover == "precomputed":
+            self.complex.set_cover_from_range(self.assignments)
+
+        # Compute simplex tree
+        self.complex.set_mask(self.min_points_per_node)
+        self.complex.find_simplices()
+        simplex_tree_ = self.complex.create_simplex_tree()
+
+        # Normalize vertex names of simplex tree
+        self.simplex_tree_ = SimplexTree()
+        idv, names = 0, {}
+        for v,_ in simplex_tree_.get_skeleton(0):
+            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
+                names[v[0]] = idv
+                self.simplex_tree_.insert([idv])
+                idv += 1
+        for s,_ in simplex_tree_.get_simplices():
+            if len(s) >= 2 and np.all([len(self.complex.subpopulation(v)) > self.min_points_per_node for v in s]):
+                self.simplex_tree_.insert([names[v] for v in s])
+
+        # Store vertex info
+        self.node_info_ = {}
+        for v,_ in simplex_tree_.get_skeleton(0):
+            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
+                node = names[v[0]]
+                pop = self.complex.subpopulation(v[0])
+                self.node_info_[node] = {"indices": pop, "size": len(pop), "colors": [self.complex.subcolor(v[0])]}
+
+        return self
+
+class NerveComplex(CoverComplexPy):
+    """
+    This is a class for computing nerve simplicial complexes on point clouds or distance matrices.
+    """
+    def __init__(self, *, input_type="point cloud", min_points_per_node=0, verbose=False):
+        """
+        Constructor for the NerveComplex class.
+
+        Parameters:
+            input_type (string): type of input data. Either "point cloud" or "distance matrix".
+            min_points_per_node (int): threshold on the size of the cover complex nodes (default 0). Any node associated to a subpopulation with less than **min_points_per_node** points will be removed.
+            verbose (bool): whether to display info while computing.
+        """
+
+        self.input_type, self.min_points_per_node = input_type, min_points_per_node
+        CoverComplexPy.__init__(self, verbose)
+
+    def fit(self, X, y=None, assignments=None, color=None):
+        """
+        Fit the NerveComplex class on a point cloud or a distance matrix: compute the nerve complex and store it in a simplex tree called `simplex_tree_`.
+
+        Parameters:
+            X (numpy array of shape (num_points) x (num_coordinates) if point cloud and (num_points) x (num_points) if distance matrix): input point cloud or distance matrix.
+            y (n x 1 array): point labels (unused).
+            assignments (list of length (num_points) of lists of integers): cover assignment for each point.
+            color (numpy array of shape (num_points) x (num_colors)): functions used to color the nodes of the cover complex. More specifically, coloring is done by computing the means of these functions on the subpopulations corresponding to each node. If None, first coordinate is used if input is point cloud, and eccentricity is used if input is distance matrix.
+        """
+        self.data, self.color = X, color
+        self.assignments = assignments
+        self.complex = CoverComplex()
+        self.complex.set_type("Nerve")
+        self.complex.set_verbose(self.verbose)
+
+        if self.input_type == "point cloud":
+            self.complex.set_point_cloud_from_range(X)
+        elif self.input_type == "distance matrix":
+            self.complex.set_distances_from_range(X)
+
+        # Set vertex color
+        if self.color is None:
+            if self.input_type == "point cloud":
+                self.color = X[:,0]
+            elif self.input_type == "distance matrix":
+                self.color = X.max(axis=0)
+        else:
+            self.color = np.array(self.color)
+        self.complex.set_color_from_range(self.color)
+
+        # Set vertex cover
+        self.complex.set_cover_from_range(self.assignments)
+
+        # Compute simplex tree
+        self.complex.set_mask(self.min_points_per_node)
+        self.complex.find_simplices()
+        simplex_tree_ = self.complex.create_simplex_tree()
+
+        # Normalize vertex names of simplex tree
+        self.simplex_tree_ = SimplexTree()
+        idv, names = 0, {}
+        for v,_ in simplex_tree_.get_skeleton(0):
+            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
+                names[v[0]] = idv
+                self.simplex_tree_.insert([idv])
+                idv += 1
+        for s,_ in simplex_tree_.get_simplices():
+            if len(s) >= 2 and np.all([len(self.complex.subpopulation(v)) > self.min_points_per_node for v in s]):
+                self.simplex_tree_.insert([names[v] for v in s])
+
+        # Store vertex info
+        self.node_info_ = {}
+        for v,_ in simplex_tree_.get_skeleton(0):
+            if len(self.complex.subpopulation(v[0])) > self.min_points_per_node:
+                node = names[v[0]]
+                pop = self.complex.subpopulation(v[0])
+                self.node_info_[node] = {"indices": pop, "size": len(pop), "colors": [self.complex.subcolor(v[0])]}
+
+        return self
```

## gudhi/dtm_rips_complex.py

 * *Ordering differences only*

```diff
@@ -1,51 +1,51 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Yuichi Ike, Rapha√´l Tinarrage
-#
-# Copyright (C) 2020 Inria, Copyright (C) 2020 FUjitsu Laboratories Ltd.
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-
-from gudhi.weighted_rips_complex import WeightedRipsComplex
-from gudhi.point_cloud.dtm import DistanceToMeasure
-from scipy.spatial.distance import cdist
-
-class DTMRipsComplex(WeightedRipsComplex):
-    """
-    Class to generate a DTM Rips complex from a distance matrix or a point set, 
-    in the way described in :cite:`dtmfiltrations`.
-    Remark that all the filtration values are doubled compared to the definition in the paper 
-    for the consistency with RipsComplex.
-    :Requires: `SciPy <installation.html#scipy>`_
-    """
-    def __init__(self, 
-                 points=None, 
-                 distance_matrix=None, 
-                 k=1, 
-                 q=2,
-                 max_filtration=float('inf')):
-        """
-        Args:
-            points (numpy.ndarray): array of points.
-            distance_matrix (numpy.ndarray): full distance matrix.
-            k (int): number of neighbors for the computation of DTM. Defaults to 1, which is equivalent to the usual Rips complex.
-            q (float): order used to compute the distance to measure. Defaults to 2.
-            max_filtration (float): specifies the maximal filtration value to be considered.      
-        """
-        if distance_matrix is None:
-            if points is None:
-                # Empty Rips construction
-                points=[]
-            distance_matrix = cdist(points,points)
-        self.distance_matrix = distance_matrix
-
-        # TODO: address the error when k is too large 
-        if k <= 1:
-            self.weights = [0] * len(distance_matrix)
-        else:
-            dtm = DistanceToMeasure(k, q=q, metric="precomputed")        
-            self.weights = dtm.fit_transform(distance_matrix)
-        self.max_filtration = max_filtration
-        
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Yuichi Ike, Rapha√´l Tinarrage
+#
+# Copyright (C) 2020 Inria, Copyright (C) 2020 FUjitsu Laboratories Ltd.
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+
+from gudhi.weighted_rips_complex import WeightedRipsComplex
+from gudhi.point_cloud.dtm import DistanceToMeasure
+from scipy.spatial.distance import cdist
+
+class DTMRipsComplex(WeightedRipsComplex):
+    """
+    Class to generate a DTM Rips complex from a distance matrix or a point set, 
+    in the way described in :cite:`dtmfiltrations`.
+    Remark that all the filtration values are doubled compared to the definition in the paper 
+    for the consistency with RipsComplex.
+    :Requires: `SciPy <installation.html#scipy>`_
+    """
+    def __init__(self, 
+                 points=None, 
+                 distance_matrix=None, 
+                 k=1, 
+                 q=2,
+                 max_filtration=float('inf')):
+        """
+        Args:
+            points (numpy.ndarray): array of points.
+            distance_matrix (numpy.ndarray): full distance matrix.
+            k (int): number of neighbors for the computation of DTM. Defaults to 1, which is equivalent to the usual Rips complex.
+            q (float): order used to compute the distance to measure. Defaults to 2.
+            max_filtration (float): specifies the maximal filtration value to be considered.      
+        """
+        if distance_matrix is None:
+            if points is None:
+                # Empty Rips construction
+                points=[]
+            distance_matrix = cdist(points,points)
+        self.distance_matrix = distance_matrix
+
+        # TODO: address the error when k is too large 
+        if k <= 1:
+            self.weights = [0] * len(distance_matrix)
+        else:
+            dtm = DistanceToMeasure(k, q=q, metric="precomputed")        
+            self.weights = dtm.fit_transform(distance_matrix)
+        self.max_filtration = max_filtration
+
```

## gudhi/persistence_graphical_tools.py

 * *Ordering differences only*

```diff
@@ -1,530 +1,530 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Vincent Rouvreau, Bertrand Michel
-#
-# Copyright (C) 2016 Inria
-#
-# Modification(s):
-#   - 2020/02 Theo Lacombe: Added more options for improved rendering and more flexibility.
-#   - 2022/11 Vincent Rouvreau: "Automatic" legend display detected by _array_handler that returns if the persistence
-#                               was a nx2 array.
-#   - YYYY/MM Author: Description of the modification
-
-from os import path
-from math import isfinite
-import numpy as np
-from functools import lru_cache
-import warnings
-import errno
-import os
-
-from gudhi.reader_utils import read_persistence_intervals_in_dimension
-from gudhi.reader_utils import read_persistence_intervals_grouped_by_dimension
-
-__author__ = "Vincent Rouvreau, Bertrand Michel, Theo Lacombe"
-__copyright__ = "Copyright (C) 2016 Inria"
-__license__ = "MIT"
-
-_gudhi_matplotlib_use_tex = True
-
-
-def __min_birth_max_death(persistence, band=0.0):
-    """This function returns (min_birth, max_death) from the persistence.
-
-    :param persistence: The persistence to plot.
-    :type persistence: list of tuples(dimension, tuple(birth, death)).
-    :param band: band
-    :type band: float.
-    :returns: (float, float) -- (min_birth, max_death).
-    """
-    # Look for minimum birth date and maximum death date for plot optimisation
-    max_death = 0
-    min_birth = persistence[0][1][0]
-    for interval in reversed(persistence):
-        if float(interval[1][1]) != float("inf"):
-            if float(interval[1][1]) > max_death:
-                max_death = float(interval[1][1])
-        if float(interval[1][0]) > max_death:
-            max_death = float(interval[1][0])
-        if float(interval[1][0]) < min_birth:
-            min_birth = float(interval[1][0])
-    if band > 0.0:
-        max_death += band
-    # can happen if only points at inf death
-    if min_birth == max_death:
-        max_death = max_death + 1.0
-    return (min_birth, max_death)
-
-
-def _array_handler(a):
-    """
-    :param a: if array, assumes it is a (n x 2) np.array and returns a
-                persistence-compatible list (padding with 0), so that the
-                plot can be performed seamlessly.
-    :returns: * List[dimension, [birth, death]] Persistence, compatible with plot functions, list.
-              * boolean Modification status (True if output is different from input)
-    """
-    if isinstance(a[0][1], (np.floating, float)):
-        return [[0, x] for x in a], True
-    else:
-        return a, False
-
-
-def _limit_to_max_intervals(persistence, max_intervals, key):
-    """This function returns truncated persistence if length is bigger than max_intervals.
-    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
-    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death).
-    :param max_intervals: maximal number of intervals to display.
-        Selected intervals are those with the longest life time. Set it
-        to 0 to see all. Default value is 1000.
-    :type max_intervals: int.
-    :param key: key function for sort algorithm.
-    :type key: function or lambda.
-    """
-    if max_intervals > 0 and max_intervals < len(persistence):
-        warnings.warn(
-            "There are %s intervals given as input, whereas max_intervals is set to %s."
-            % (len(persistence), max_intervals)
-        )
-        # Sort by life time, then takes only the max_intervals elements
-        return sorted(persistence, key=key, reverse=True)[:max_intervals]
-    else:
-        return persistence
-
-
-@lru_cache(maxsize=1)
-def _matplotlib_can_use_tex():
-    """This function returns True if matplotlib can deal with LaTeX, False otherwise.
-    The returned value is cached.
-    """
-    try:
-        from matplotlib import checkdep_usetex
-
-        return checkdep_usetex(True)
-    except ImportError as import_error:
-        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
-
-
-def plot_persistence_barcode(
-    persistence=[],
-    persistence_file="",
-    alpha=0.6,
-    max_intervals=20000,
-    inf_delta=0.1,
-    legend=None,
-    colormap=None,
-    axes=None,
-    fontsize=16,
-):
-    """This function plots the persistence bar code from persistence values list
-    , a np.array of shape (N x 2) (representing a diagram
-    in a single homology dimension),
-    or from a `persistence diagram <fileformats.html#persistence-diagram>`_ file.
-
-    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
-    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death)
-    :param persistence_file: A `persistence diagram <fileformats.html#persistence-diagram>`_ file style name
-        (reset persistence if both are set).
-    :type persistence_file: string
-    :param alpha: barcode transparency value (0.0 transparent through 1.0
-        opaque - default is 0.6).
-    :type alpha: float
-    :param max_intervals: maximal number of intervals to display.
-        Selected intervals are those with the longest life time. Set it
-        to 0 to see all. Default value is 20000.
-    :type max_intervals: int
-    :param inf_delta: Infinity is placed at :code:`((max_death - min_birth) x
-        inf_delta)` above :code:`max_death` value. A reasonable value is
-        between 0.05 and 0.5 - default is 0.1.
-    :type inf_delta: float
-    :param legend: Display the dimension color legend. Default is None, meaning the legend is displayed if dimension
-        is specified in the persistence argument, and not displayed if dimension is not specified.
-    :type legend: boolean or None
-    :param colormap: A matplotlib-like qualitative colormaps. Default is None
-        which means :code:`matplotlib.cm.Set1.colors`.
-    :type colormap: tuple of colors (3-tuple of float between 0. and 1.)
-    :param axes: A matplotlib-like subplot axes. If None, the plot is drawn on
-        a new set of axes.
-    :type axes: `matplotlib.axes.Axes`
-    :param fontsize: Fontsize to use in axis.
-    :type fontsize: int
-    :returns: (`matplotlib.axes.Axes`): The axes on which the plot was drawn.
-    """
-    try:
-        import matplotlib.pyplot as plt
-        import matplotlib.patches as mpatches
-        from matplotlib import rc
-
-        if _gudhi_matplotlib_use_tex and _matplotlib_can_use_tex():
-            plt.rc("text", usetex=True)
-            plt.rc("font", family="serif")
-        else:
-            plt.rc("text", usetex=False)
-            plt.rc("font", family="DejaVu Sans")
-
-        # By default, let's say the persistence is not an array of shape (N x 2) - Can be from a persistence file
-        nx2_array = False
-        if persistence_file != "":
-            if path.isfile(persistence_file):
-                # Reset persistence
-                persistence = []
-                diag = read_persistence_intervals_grouped_by_dimension(persistence_file=persistence_file)
-                for key in diag.keys():
-                    for persistence_interval in diag[key]:
-                        persistence.append((key, persistence_interval))
-            else:
-                raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), persistence_file)
-
-        try:
-            persistence, nx2_array = _array_handler(persistence)
-            persistence = _limit_to_max_intervals(
-                persistence, max_intervals, key=lambda life_time: life_time[1][1] - life_time[1][0]
-            )
-            (min_birth, max_death) = __min_birth_max_death(persistence)
-            persistence = sorted(persistence, key=lambda birth: birth[1][0])
-        except IndexError:
-            min_birth, max_death = 0.0, 1.0
-            pass
-
-        delta = (max_death - min_birth) * inf_delta
-        # Replace infinity values with max_death + delta for bar code to be more
-        # readable
-        infinity = max_death + delta
-        axis_start = min_birth - delta
-
-        if axes is None:
-            _, axes = plt.subplots(1, 1)
-        if colormap is None:
-            colormap = plt.cm.Set1.colors
-
-        x = [birth for (dim, (birth, death)) in persistence]
-        y = [(death - birth) if death != float("inf") else (infinity - birth) for (dim, (birth, death)) in persistence]
-        c = [colormap[dim] for (dim, (birth, death)) in persistence]
-
-        axes.barh(range(len(x)), y, left=x, alpha=alpha, color=c, linewidth=0)
-
-        if legend is None and not nx2_array:
-            # By default, if persistence is an array of (dimension, (birth, death)), display the legend
-            legend = True
-
-        if legend:
-            dimensions = {item[0] for item in persistence}
-            axes.legend(
-                handles=[mpatches.Patch(color=colormap[dim], label=str(dim)) for dim in dimensions],
-                loc="best",
-            )
-
-        axes.set_title("Persistence barcode", fontsize=fontsize)
-        axes.set_yticks([])
-        axes.invert_yaxis()
-
-        # Ends plot on infinity value and starts a little bit before min_birth
-        if len(x) != 0:
-            axes.set_xlim((axis_start, infinity))
-        return axes
-
-    except ImportError as import_error:
-        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
-
-
-def plot_persistence_diagram(
-    persistence=[],
-    persistence_file="",
-    alpha=0.6,
-    band=0.0,
-    max_intervals=1000000,
-    inf_delta=0.1,
-    legend=None,
-    colormap=None,
-    axes=None,
-    fontsize=16,
-    greyblock=True,
-):
-    r"""This function plots the persistence diagram from persistence values
-    list, a np.array of shape (N x 2) representing a diagram in a single
-    homology dimension, or from a `persistence diagram <fileformats.html#persistence-diagram>`_ file`.
-
-    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
-    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death)
-    :param persistence_file: A `persistence diagram <fileformats.html#persistence-diagram>`_ file style name
-        (reset persistence if both are set).
-    :type persistence_file: string
-    :param alpha: plot transparency value (0.0 transparent through 1.0
-        opaque - default is 0.6).
-    :type alpha: float
-    :param band: band (not displayed if :math:`\leq` 0. - default is 0.)
-    :type band: float
-    :param max_intervals: maximal number of intervals to display.
-        Selected intervals are those with the longest life time. Set it
-        to 0 to see all. Default value is 1000000.
-    :type max_intervals: int
-    :param inf_delta: Infinity is placed at :code:`((max_death - min_birth) x
-        inf_delta)` above :code:`max_death` value. A reasonable value is
-        between 0.05 and 0.5 - default is 0.1.
-    :type inf_delta: float
-    :param legend: Display the dimension color legend. Default is None, meaning the legend is displayed if dimension
-        is specified in the persistence argument, and not displayed if dimension is not specified.
-    :type legend: boolean or None
-    :param colormap: A matplotlib-like qualitative colormaps. Default is None
-        which means :code:`matplotlib.cm.Set1.colors`.
-    :type colormap: tuple of colors (3-tuple of float between 0. and 1.)
-    :param axes: A matplotlib-like subplot axes. If None, the plot is drawn on
-        a new set of axes.
-    :type axes: `matplotlib.axes.Axes`
-    :param fontsize: Fontsize to use in axis.
-    :type fontsize: int
-    :param greyblock: if we want to plot a grey patch on the lower half plane for nicer rendering. Default True.
-    :type greyblock: boolean
-    :returns: (`matplotlib.axes.Axes`): The axes on which the plot was drawn.
-    """
-    try:
-        import matplotlib.pyplot as plt
-        import matplotlib.patches as mpatches
-        from matplotlib import rc
-
-        if _gudhi_matplotlib_use_tex and _matplotlib_can_use_tex():
-            plt.rc("text", usetex=True)
-            plt.rc("font", family="serif")
-        else:
-            plt.rc("text", usetex=False)
-            plt.rc("font", family="DejaVu Sans")
-
-        # By default, let's say the persistence is not an array of shape (N x 2) - Can be from a persistence file
-        nx2_array = False
-        if persistence_file != "":
-            if path.isfile(persistence_file):
-                # Reset persistence
-                persistence = []
-                diag = read_persistence_intervals_grouped_by_dimension(persistence_file=persistence_file)
-                for key in diag.keys():
-                    for persistence_interval in diag[key]:
-                        persistence.append((key, persistence_interval))
-            else:
-                raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), persistence_file)
-
-        try:
-            persistence, nx2_array = _array_handler(persistence)
-            persistence = _limit_to_max_intervals(
-                persistence, max_intervals, key=lambda life_time: life_time[1][1] - life_time[1][0]
-            )
-            min_birth, max_death = __min_birth_max_death(persistence, band)
-        except IndexError:
-            min_birth, max_death = 0.0, 1.0
-            pass
-
-        delta = (max_death - min_birth) * inf_delta
-        # Replace infinity values with max_death + delta for diagram to be more
-        # readable
-        infinity = max_death + delta
-        axis_end = max_death + delta / 2
-        axis_start = min_birth - delta
-
-        if axes is None:
-            _, axes = plt.subplots(1, 1)
-        if colormap is None:
-            colormap = plt.cm.Set1.colors
-        # bootstrap band
-        if band > 0.0:
-            x = np.linspace(axis_start, infinity, 1000)
-            axes.fill_between(x, x, x + band, alpha=alpha, facecolor="red")
-        # lower diag patch
-        if greyblock:
-            axes.add_patch(
-                mpatches.Polygon(
-                    [[axis_start, axis_start], [axis_end, axis_start], [axis_end, axis_end]],
-                    fill=True,
-                    color="lightgrey",
-                )
-            )
-        # line display of equation : birth = death
-        axes.plot([axis_start, axis_end], [axis_start, axis_end], linewidth=1.0, color="k")
-
-        x = [birth for (dim, (birth, death)) in persistence]
-        y = [death if death != float("inf") else infinity for (dim, (birth, death)) in persistence]
-        c = [colormap[dim] for (dim, (birth, death)) in persistence]
-
-        axes.scatter(x, y, alpha=alpha, color=c)
-        if float("inf") in (death for (dim, (birth, death)) in persistence):
-            # infinity line and text
-            axes.plot([axis_start, axis_end], [infinity, infinity], linewidth=1.0, color="k", alpha=alpha)
-            # Infinity label
-            yt = axes.get_yticks()
-            yt = yt[np.where(yt < axis_end)]  # to avoid plotting ticklabel higher than infinity
-            yt = np.append(yt, infinity)
-            ytl = ["%.3f" % e for e in yt]  # to avoid float precision error
-            ytl[-1] = r"$+\infty$"
-            axes.set_yticks(yt)
-            axes.set_yticklabels(ytl)
-
-        if legend is None and not nx2_array:
-            # By default, if persistence is an array of (dimension, (birth, death)), display the legend
-            legend = True
-
-        if legend:
-            dimensions = list({item[0] for item in persistence})
-            axes.legend(
-                handles=[mpatches.Patch(color=colormap[dim], label=str(dim)) for dim in dimensions],
-                loc="lower right",
-            )
-
-        axes.set_xlabel("Birth", fontsize=fontsize)
-        axes.set_ylabel("Death", fontsize=fontsize)
-        axes.set_title("Persistence diagram", fontsize=fontsize)
-        # Ends plot on infinity value and starts a little bit before min_birth
-        axes.axis([axis_start, axis_end, axis_start, infinity + delta / 2])
-        return axes
-
-    except ImportError as import_error:
-        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
-
-
-def plot_persistence_density(
-    persistence=[],
-    persistence_file="",
-    nbins=300,
-    bw_method=None,
-    max_intervals=1000,
-    dimension=None,
-    cmap=None,
-    legend=True,
-    axes=None,
-    fontsize=16,
-    greyblock=False,
-):
-    """This function plots the persistence density from persistence values list, np.array of shape (N x 2) representing
-    a diagram in a single homology dimension, or from a `persistence diagram <fileformats.html#persistence-diagram>`_
-    file. Be aware that this function does not distinguish the dimension, it is up to you to select the required one.
-    This function also does not handle degenerate data set (scipy correlation matrix inversion can fail).
-
-    :Requires: `SciPy <installation.html#scipy>`_
-
-    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
-    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death)
-    :param persistence_file: A `persistence diagram <fileformats.html#persistence-diagram>`_
-        file style name (reset persistence if both are set).
-    :type persistence_file: string
-    :param nbins: Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents (default is 300)
-    :type nbins: int
-    :param bw_method: The method used to calculate the estimator bandwidth. This can be 'scott', 'silverman', a scalar
-        constant or a callable. If a scalar, this will be used directly as kde.factor. If a callable, it should take a
-        gaussian_kde instance as only parameter and return a scalar. If None (default), 'scott' is used. See
-        `scipy.stats.gaussian_kde documentation
-        <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html>`_
-        for more details.
-    :type bw_method: str, scalar or callable, optional
-    :param max_intervals: maximal number of points used in the density estimation. Selected intervals are those with
-        the longest life time. Set it to 0 to see all. Default value is 1000.
-    :type max_intervals: int
-    :param dimension: the dimension to be selected in the intervals (default is None to mix all dimensions).
-    :type dimension: int
-    :param cmap: A matplotlib colormap (default is matplotlib.pyplot.cm.hot_r).
-    :type cmap: cf. matplotlib colormap
-    :param legend: Display the color bar values (default is True).
-    :type legend: boolean
-    :param axes: A matplotlib-like subplot axes. If None, the plot is drawn on a new set of axes.
-    :type axes: `matplotlib.axes.Axes`
-    :param fontsize: Fontsize to use in axis.
-    :type fontsize: int
-    :param greyblock: if we want to plot a grey patch on the lower half plane for nicer rendering. Default False.
-    :type greyblock: boolean
-    :returns: (`matplotlib.axes.Axes`): The axes on which the plot was drawn.
-    """
-    try:
-        import matplotlib.pyplot as plt
-        import matplotlib.patches as mpatches
-        from scipy.stats import kde
-        from matplotlib import rc
-
-        if _gudhi_matplotlib_use_tex and _matplotlib_can_use_tex():
-            plt.rc("text", usetex=True)
-            plt.rc("font", family="serif")
-        else:
-            plt.rc("text", usetex=False)
-            plt.rc("font", family="DejaVu Sans")
-
-        if persistence_file != "":
-            if dimension is None:
-                # All dimension case
-                dimension = -1
-            if path.isfile(persistence_file):
-                persistence_dim = read_persistence_intervals_in_dimension(
-                    persistence_file=persistence_file, only_this_dim=dimension
-                )
-            else:
-                raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), persistence_file)
-
-        # default cmap value cannot be done at argument definition level as matplotlib is not yet defined.
-        if cmap is None:
-            cmap = plt.cm.hot_r
-        if axes is None:
-            _, axes = plt.subplots(1, 1)
-
-        try:
-            # if not read from file but given by an argument
-            persistence, _ = _array_handler(persistence)
-            persistence_dim = np.array(
-                [
-                    (dim_interval[1][0], dim_interval[1][1])
-                    for dim_interval in persistence
-                    if (dim_interval[0] == dimension) or (dimension is None)
-                ]
-            )
-            persistence_dim = persistence_dim[np.isfinite(persistence_dim[:, 1])]
-            persistence_dim = np.array(
-                _limit_to_max_intervals(
-                    persistence_dim, max_intervals, key=lambda life_time: life_time[1] - life_time[0]
-                )
-            )
-
-            # Set as numpy array birth and death (remove undefined values - inf and NaN)
-            birth = persistence_dim[:, 0]
-            death = persistence_dim[:, 1]
-            birth_min = birth.min()
-            birth_max = birth.max()
-            death_min = death.min()
-            death_max = death.max()
-
-            # Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents
-            k = kde.gaussian_kde([birth, death], bw_method=bw_method)
-            xi, yi = np.mgrid[
-                birth_min : birth_max : nbins * 1j,
-                death_min : death_max : nbins * 1j,
-            ]
-            zi = k(np.vstack([xi.flatten(), yi.flatten()]))
-            # Make the plot
-            img = axes.pcolormesh(xi, yi, zi.reshape(xi.shape), cmap=cmap, shading="auto")
-            plot_success = True
-
-        # IndexError on empty diagrams, ValueError on only inf death values
-        except (IndexError, ValueError):
-            birth_min = 0.0
-            birth_max = 1.0
-            death_min = 0.0
-            death_max = 1.0
-            plot_success = False
-            pass
-
-        # line display of equation : birth = death
-        x = np.linspace(death_min, birth_max, 1000)
-        axes.plot(x, x, color="k", linewidth=1.0)
-
-        if greyblock:
-            axes.add_patch(
-                mpatches.Polygon(
-                    [[birth_min, birth_min], [death_max, birth_min], [death_max, death_max]],
-                    fill=True,
-                    color="lightgrey",
-                )
-            )
-
-        if plot_success and legend:
-            plt.colorbar(img, ax=axes)
-
-        axes.set_xlabel("Birth", fontsize=fontsize)
-        axes.set_ylabel("Death", fontsize=fontsize)
-        axes.set_title("Persistence density", fontsize=fontsize)
-
-        return axes
-
-    except ImportError as import_error:
-        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Vincent Rouvreau, Bertrand Michel
+#
+# Copyright (C) 2016 Inria
+#
+# Modification(s):
+#   - 2020/02 Theo Lacombe: Added more options for improved rendering and more flexibility.
+#   - 2022/11 Vincent Rouvreau: "Automatic" legend display detected by _array_handler that returns if the persistence
+#                               was a nx2 array.
+#   - YYYY/MM Author: Description of the modification
+
+from os import path
+from math import isfinite
+import numpy as np
+from functools import lru_cache
+import warnings
+import errno
+import os
+
+from gudhi.reader_utils import read_persistence_intervals_in_dimension
+from gudhi.reader_utils import read_persistence_intervals_grouped_by_dimension
+
+__author__ = "Vincent Rouvreau, Bertrand Michel, Theo Lacombe"
+__copyright__ = "Copyright (C) 2016 Inria"
+__license__ = "MIT"
+
+_gudhi_matplotlib_use_tex = True
+
+
+def __min_birth_max_death(persistence, band=0.0):
+    """This function returns (min_birth, max_death) from the persistence.
+
+    :param persistence: The persistence to plot.
+    :type persistence: list of tuples(dimension, tuple(birth, death)).
+    :param band: band
+    :type band: float.
+    :returns: (float, float) -- (min_birth, max_death).
+    """
+    # Look for minimum birth date and maximum death date for plot optimisation
+    max_death = 0
+    min_birth = persistence[0][1][0]
+    for interval in reversed(persistence):
+        if float(interval[1][1]) != float("inf"):
+            if float(interval[1][1]) > max_death:
+                max_death = float(interval[1][1])
+        if float(interval[1][0]) > max_death:
+            max_death = float(interval[1][0])
+        if float(interval[1][0]) < min_birth:
+            min_birth = float(interval[1][0])
+    if band > 0.0:
+        max_death += band
+    # can happen if only points at inf death
+    if min_birth == max_death:
+        max_death = max_death + 1.0
+    return (min_birth, max_death)
+
+
+def _array_handler(a):
+    """
+    :param a: if array, assumes it is a (n x 2) np.array and returns a
+                persistence-compatible list (padding with 0), so that the
+                plot can be performed seamlessly.
+    :returns: * List[dimension, [birth, death]] Persistence, compatible with plot functions, list.
+              * boolean Modification status (True if output is different from input)
+    """
+    if isinstance(a[0][1], (np.floating, float)):
+        return [[0, x] for x in a], True
+    else:
+        return a, False
+
+
+def _limit_to_max_intervals(persistence, max_intervals, key):
+    """This function returns truncated persistence if length is bigger than max_intervals.
+    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
+    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death).
+    :param max_intervals: maximal number of intervals to display.
+        Selected intervals are those with the longest life time. Set it
+        to 0 to see all. Default value is 1000.
+    :type max_intervals: int.
+    :param key: key function for sort algorithm.
+    :type key: function or lambda.
+    """
+    if max_intervals > 0 and max_intervals < len(persistence):
+        warnings.warn(
+            "There are %s intervals given as input, whereas max_intervals is set to %s."
+            % (len(persistence), max_intervals)
+        )
+        # Sort by life time, then takes only the max_intervals elements
+        return sorted(persistence, key=key, reverse=True)[:max_intervals]
+    else:
+        return persistence
+
+
+@lru_cache(maxsize=1)
+def _matplotlib_can_use_tex():
+    """This function returns True if matplotlib can deal with LaTeX, False otherwise.
+    The returned value is cached.
+    """
+    try:
+        from matplotlib import checkdep_usetex
+
+        return checkdep_usetex(True)
+    except ImportError as import_error:
+        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
+
+
+def plot_persistence_barcode(
+    persistence=[],
+    persistence_file="",
+    alpha=0.6,
+    max_intervals=20000,
+    inf_delta=0.1,
+    legend=None,
+    colormap=None,
+    axes=None,
+    fontsize=16,
+):
+    """This function plots the persistence bar code from persistence values list
+    , a np.array of shape (N x 2) (representing a diagram
+    in a single homology dimension),
+    or from a `persistence diagram <fileformats.html#persistence-diagram>`_ file.
+
+    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
+    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death)
+    :param persistence_file: A `persistence diagram <fileformats.html#persistence-diagram>`_ file style name
+        (reset persistence if both are set).
+    :type persistence_file: string
+    :param alpha: barcode transparency value (0.0 transparent through 1.0
+        opaque - default is 0.6).
+    :type alpha: float
+    :param max_intervals: maximal number of intervals to display.
+        Selected intervals are those with the longest life time. Set it
+        to 0 to see all. Default value is 20000.
+    :type max_intervals: int
+    :param inf_delta: Infinity is placed at :code:`((max_death - min_birth) x
+        inf_delta)` above :code:`max_death` value. A reasonable value is
+        between 0.05 and 0.5 - default is 0.1.
+    :type inf_delta: float
+    :param legend: Display the dimension color legend. Default is None, meaning the legend is displayed if dimension
+        is specified in the persistence argument, and not displayed if dimension is not specified.
+    :type legend: boolean or None
+    :param colormap: A matplotlib-like qualitative colormaps. Default is None
+        which means :code:`matplotlib.cm.Set1.colors`.
+    :type colormap: tuple of colors (3-tuple of float between 0. and 1.)
+    :param axes: A matplotlib-like subplot axes. If None, the plot is drawn on
+        a new set of axes.
+    :type axes: `matplotlib.axes.Axes`
+    :param fontsize: Fontsize to use in axis.
+    :type fontsize: int
+    :returns: (`matplotlib.axes.Axes`): The axes on which the plot was drawn.
+    """
+    try:
+        import matplotlib.pyplot as plt
+        import matplotlib.patches as mpatches
+        from matplotlib import rc
+
+        if _gudhi_matplotlib_use_tex and _matplotlib_can_use_tex():
+            plt.rc("text", usetex=True)
+            plt.rc("font", family="serif")
+        else:
+            plt.rc("text", usetex=False)
+            plt.rc("font", family="DejaVu Sans")
+
+        # By default, let's say the persistence is not an array of shape (N x 2) - Can be from a persistence file
+        nx2_array = False
+        if persistence_file != "":
+            if path.isfile(persistence_file):
+                # Reset persistence
+                persistence = []
+                diag = read_persistence_intervals_grouped_by_dimension(persistence_file=persistence_file)
+                for key in diag.keys():
+                    for persistence_interval in diag[key]:
+                        persistence.append((key, persistence_interval))
+            else:
+                raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), persistence_file)
+
+        try:
+            persistence, nx2_array = _array_handler(persistence)
+            persistence = _limit_to_max_intervals(
+                persistence, max_intervals, key=lambda life_time: life_time[1][1] - life_time[1][0]
+            )
+            (min_birth, max_death) = __min_birth_max_death(persistence)
+            persistence = sorted(persistence, key=lambda birth: birth[1][0])
+        except IndexError:
+            min_birth, max_death = 0.0, 1.0
+            pass
+
+        delta = (max_death - min_birth) * inf_delta
+        # Replace infinity values with max_death + delta for bar code to be more
+        # readable
+        infinity = max_death + delta
+        axis_start = min_birth - delta
+
+        if axes is None:
+            _, axes = plt.subplots(1, 1)
+        if colormap is None:
+            colormap = plt.cm.Set1.colors
+
+        x = [birth for (dim, (birth, death)) in persistence]
+        y = [(death - birth) if death != float("inf") else (infinity - birth) for (dim, (birth, death)) in persistence]
+        c = [colormap[dim] for (dim, (birth, death)) in persistence]
+
+        axes.barh(range(len(x)), y, left=x, alpha=alpha, color=c, linewidth=0)
+
+        if legend is None and not nx2_array:
+            # By default, if persistence is an array of (dimension, (birth, death)), display the legend
+            legend = True
+
+        if legend:
+            dimensions = {item[0] for item in persistence}
+            axes.legend(
+                handles=[mpatches.Patch(color=colormap[dim], label=str(dim)) for dim in dimensions],
+                loc="best",
+            )
+
+        axes.set_title("Persistence barcode", fontsize=fontsize)
+        axes.set_yticks([])
+        axes.invert_yaxis()
+
+        # Ends plot on infinity value and starts a little bit before min_birth
+        if len(x) != 0:
+            axes.set_xlim((axis_start, infinity))
+        return axes
+
+    except ImportError as import_error:
+        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
+
+
+def plot_persistence_diagram(
+    persistence=[],
+    persistence_file="",
+    alpha=0.6,
+    band=0.0,
+    max_intervals=1000000,
+    inf_delta=0.1,
+    legend=None,
+    colormap=None,
+    axes=None,
+    fontsize=16,
+    greyblock=True,
+):
+    r"""This function plots the persistence diagram from persistence values
+    list, a np.array of shape (N x 2) representing a diagram in a single
+    homology dimension, or from a `persistence diagram <fileformats.html#persistence-diagram>`_ file`.
+
+    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
+    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death)
+    :param persistence_file: A `persistence diagram <fileformats.html#persistence-diagram>`_ file style name
+        (reset persistence if both are set).
+    :type persistence_file: string
+    :param alpha: plot transparency value (0.0 transparent through 1.0
+        opaque - default is 0.6).
+    :type alpha: float
+    :param band: band (not displayed if :math:`\leq` 0. - default is 0.)
+    :type band: float
+    :param max_intervals: maximal number of intervals to display.
+        Selected intervals are those with the longest life time. Set it
+        to 0 to see all. Default value is 1000000.
+    :type max_intervals: int
+    :param inf_delta: Infinity is placed at :code:`((max_death - min_birth) x
+        inf_delta)` above :code:`max_death` value. A reasonable value is
+        between 0.05 and 0.5 - default is 0.1.
+    :type inf_delta: float
+    :param legend: Display the dimension color legend. Default is None, meaning the legend is displayed if dimension
+        is specified in the persistence argument, and not displayed if dimension is not specified.
+    :type legend: boolean or None
+    :param colormap: A matplotlib-like qualitative colormaps. Default is None
+        which means :code:`matplotlib.cm.Set1.colors`.
+    :type colormap: tuple of colors (3-tuple of float between 0. and 1.)
+    :param axes: A matplotlib-like subplot axes. If None, the plot is drawn on
+        a new set of axes.
+    :type axes: `matplotlib.axes.Axes`
+    :param fontsize: Fontsize to use in axis.
+    :type fontsize: int
+    :param greyblock: if we want to plot a grey patch on the lower half plane for nicer rendering. Default True.
+    :type greyblock: boolean
+    :returns: (`matplotlib.axes.Axes`): The axes on which the plot was drawn.
+    """
+    try:
+        import matplotlib.pyplot as plt
+        import matplotlib.patches as mpatches
+        from matplotlib import rc
+
+        if _gudhi_matplotlib_use_tex and _matplotlib_can_use_tex():
+            plt.rc("text", usetex=True)
+            plt.rc("font", family="serif")
+        else:
+            plt.rc("text", usetex=False)
+            plt.rc("font", family="DejaVu Sans")
+
+        # By default, let's say the persistence is not an array of shape (N x 2) - Can be from a persistence file
+        nx2_array = False
+        if persistence_file != "":
+            if path.isfile(persistence_file):
+                # Reset persistence
+                persistence = []
+                diag = read_persistence_intervals_grouped_by_dimension(persistence_file=persistence_file)
+                for key in diag.keys():
+                    for persistence_interval in diag[key]:
+                        persistence.append((key, persistence_interval))
+            else:
+                raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), persistence_file)
+
+        try:
+            persistence, nx2_array = _array_handler(persistence)
+            persistence = _limit_to_max_intervals(
+                persistence, max_intervals, key=lambda life_time: life_time[1][1] - life_time[1][0]
+            )
+            min_birth, max_death = __min_birth_max_death(persistence, band)
+        except IndexError:
+            min_birth, max_death = 0.0, 1.0
+            pass
+
+        delta = (max_death - min_birth) * inf_delta
+        # Replace infinity values with max_death + delta for diagram to be more
+        # readable
+        infinity = max_death + delta
+        axis_end = max_death + delta / 2
+        axis_start = min_birth - delta
+
+        if axes is None:
+            _, axes = plt.subplots(1, 1)
+        if colormap is None:
+            colormap = plt.cm.Set1.colors
+        # bootstrap band
+        if band > 0.0:
+            x = np.linspace(axis_start, infinity, 1000)
+            axes.fill_between(x, x, x + band, alpha=alpha, facecolor="red")
+        # lower diag patch
+        if greyblock:
+            axes.add_patch(
+                mpatches.Polygon(
+                    [[axis_start, axis_start], [axis_end, axis_start], [axis_end, axis_end]],
+                    fill=True,
+                    color="lightgrey",
+                )
+            )
+        # line display of equation : birth = death
+        axes.plot([axis_start, axis_end], [axis_start, axis_end], linewidth=1.0, color="k")
+
+        x = [birth for (dim, (birth, death)) in persistence]
+        y = [death if death != float("inf") else infinity for (dim, (birth, death)) in persistence]
+        c = [colormap[dim] for (dim, (birth, death)) in persistence]
+
+        axes.scatter(x, y, alpha=alpha, color=c)
+        if float("inf") in (death for (dim, (birth, death)) in persistence):
+            # infinity line and text
+            axes.plot([axis_start, axis_end], [infinity, infinity], linewidth=1.0, color="k", alpha=alpha)
+            # Infinity label
+            yt = axes.get_yticks()
+            yt = yt[np.where(yt < axis_end)]  # to avoid plotting ticklabel higher than infinity
+            yt = np.append(yt, infinity)
+            ytl = ["%.3f" % e for e in yt]  # to avoid float precision error
+            ytl[-1] = r"$+\infty$"
+            axes.set_yticks(yt)
+            axes.set_yticklabels(ytl)
+
+        if legend is None and not nx2_array:
+            # By default, if persistence is an array of (dimension, (birth, death)), display the legend
+            legend = True
+
+        if legend:
+            dimensions = list({item[0] for item in persistence})
+            axes.legend(
+                handles=[mpatches.Patch(color=colormap[dim], label=str(dim)) for dim in dimensions],
+                loc="lower right",
+            )
+
+        axes.set_xlabel("Birth", fontsize=fontsize)
+        axes.set_ylabel("Death", fontsize=fontsize)
+        axes.set_title("Persistence diagram", fontsize=fontsize)
+        # Ends plot on infinity value and starts a little bit before min_birth
+        axes.axis([axis_start, axis_end, axis_start, infinity + delta / 2])
+        return axes
+
+    except ImportError as import_error:
+        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
+
+
+def plot_persistence_density(
+    persistence=[],
+    persistence_file="",
+    nbins=300,
+    bw_method=None,
+    max_intervals=1000,
+    dimension=None,
+    cmap=None,
+    legend=True,
+    axes=None,
+    fontsize=16,
+    greyblock=False,
+):
+    """This function plots the persistence density from persistence values list, np.array of shape (N x 2) representing
+    a diagram in a single homology dimension, or from a `persistence diagram <fileformats.html#persistence-diagram>`_
+    file. Be aware that this function does not distinguish the dimension, it is up to you to select the required one.
+    This function also does not handle degenerate data set (scipy correlation matrix inversion can fail).
+
+    :Requires: `SciPy <installation.html#scipy>`_
+
+    :param persistence: Persistence intervals values list. Can be grouped by dimension or not.
+    :type persistence: an array of (dimension, (birth, death)) or an array of (birth, death)
+    :param persistence_file: A `persistence diagram <fileformats.html#persistence-diagram>`_
+        file style name (reset persistence if both are set).
+    :type persistence_file: string
+    :param nbins: Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents (default is 300)
+    :type nbins: int
+    :param bw_method: The method used to calculate the estimator bandwidth. This can be 'scott', 'silverman', a scalar
+        constant or a callable. If a scalar, this will be used directly as kde.factor. If a callable, it should take a
+        gaussian_kde instance as only parameter and return a scalar. If None (default), 'scott' is used. See
+        `scipy.stats.gaussian_kde documentation
+        <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html>`_
+        for more details.
+    :type bw_method: str, scalar or callable, optional
+    :param max_intervals: maximal number of points used in the density estimation. Selected intervals are those with
+        the longest life time. Set it to 0 to see all. Default value is 1000.
+    :type max_intervals: int
+    :param dimension: the dimension to be selected in the intervals (default is None to mix all dimensions).
+    :type dimension: int
+    :param cmap: A matplotlib colormap (default is matplotlib.pyplot.cm.hot_r).
+    :type cmap: cf. matplotlib colormap
+    :param legend: Display the color bar values (default is True).
+    :type legend: boolean
+    :param axes: A matplotlib-like subplot axes. If None, the plot is drawn on a new set of axes.
+    :type axes: `matplotlib.axes.Axes`
+    :param fontsize: Fontsize to use in axis.
+    :type fontsize: int
+    :param greyblock: if we want to plot a grey patch on the lower half plane for nicer rendering. Default False.
+    :type greyblock: boolean
+    :returns: (`matplotlib.axes.Axes`): The axes on which the plot was drawn.
+    """
+    try:
+        import matplotlib.pyplot as plt
+        import matplotlib.patches as mpatches
+        from scipy.stats import kde
+        from matplotlib import rc
+
+        if _gudhi_matplotlib_use_tex and _matplotlib_can_use_tex():
+            plt.rc("text", usetex=True)
+            plt.rc("font", family="serif")
+        else:
+            plt.rc("text", usetex=False)
+            plt.rc("font", family="DejaVu Sans")
+
+        if persistence_file != "":
+            if dimension is None:
+                # All dimension case
+                dimension = -1
+            if path.isfile(persistence_file):
+                persistence_dim = read_persistence_intervals_in_dimension(
+                    persistence_file=persistence_file, only_this_dim=dimension
+                )
+            else:
+                raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), persistence_file)
+
+        # default cmap value cannot be done at argument definition level as matplotlib is not yet defined.
+        if cmap is None:
+            cmap = plt.cm.hot_r
+        if axes is None:
+            _, axes = plt.subplots(1, 1)
+
+        try:
+            # if not read from file but given by an argument
+            persistence, _ = _array_handler(persistence)
+            persistence_dim = np.array(
+                [
+                    (dim_interval[1][0], dim_interval[1][1])
+                    for dim_interval in persistence
+                    if (dim_interval[0] == dimension) or (dimension is None)
+                ]
+            )
+            persistence_dim = persistence_dim[np.isfinite(persistence_dim[:, 1])]
+            persistence_dim = np.array(
+                _limit_to_max_intervals(
+                    persistence_dim, max_intervals, key=lambda life_time: life_time[1] - life_time[0]
+                )
+            )
+
+            # Set as numpy array birth and death (remove undefined values - inf and NaN)
+            birth = persistence_dim[:, 0]
+            death = persistence_dim[:, 1]
+            birth_min = birth.min()
+            birth_max = birth.max()
+            death_min = death.min()
+            death_max = death.max()
+
+            # Evaluate a gaussian kde on a regular grid of nbins x nbins over data extents
+            k = kde.gaussian_kde([birth, death], bw_method=bw_method)
+            xi, yi = np.mgrid[
+                birth_min : birth_max : nbins * 1j,
+                death_min : death_max : nbins * 1j,
+            ]
+            zi = k(np.vstack([xi.flatten(), yi.flatten()]))
+            # Make the plot
+            img = axes.pcolormesh(xi, yi, zi.reshape(xi.shape), cmap=cmap, shading="auto")
+            plot_success = True
+
+        # IndexError on empty diagrams, ValueError on only inf death values
+        except (IndexError, ValueError):
+            birth_min = 0.0
+            birth_max = 1.0
+            death_min = 0.0
+            death_max = 1.0
+            plot_success = False
+            pass
+
+        # line display of equation : birth = death
+        x = np.linspace(death_min, birth_max, 1000)
+        axes.plot(x, x, color="k", linewidth=1.0)
+
+        if greyblock:
+            axes.add_patch(
+                mpatches.Polygon(
+                    [[birth_min, birth_min], [death_max, birth_min], [death_max, death_max]],
+                    fill=True,
+                    color="lightgrey",
+                )
+            )
+
+        if plot_success and legend:
+            plt.colorbar(img, ax=axes)
+
+        axes.set_xlabel("Birth", fontsize=fontsize)
+        axes.set_ylabel("Death", fontsize=fontsize)
+        axes.set_title("Persistence density", fontsize=fontsize)
+
+        return axes
+
+    except ImportError as import_error:
+        warnings.warn(f"This function is not available.\nModuleNotFoundError: No module named '{import_error.name}'.")
```

## gudhi/weighted_rips_complex.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Rapha√´l Tinarrage, Yuichi Ike, Masatoshi Takenouchi
-#
-# Copyright (C) 2020 Inria, Copyright (C) 2020 FUjitsu Laboratories Ltd.
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-from gudhi import SimplexTree
-
-class WeightedRipsComplex:
-    """
-    Class to generate a weighted Rips complex from a distance matrix and weights on vertices, 
-    in the way described in :cite:`dtmfiltrations` with `p=1`. The filtration value of vertex `i` is `2*weights[i]`,
-    and the filtration value of edge `ij` is `distance_matrix[i][j]+weights[i]+weights[j]`,
-    or the maximum of the filtrations of its extremities, whichever is largest.
-    Remark that all the filtration values are doubled compared to the definition in the paper 
-    for consistency with RipsComplex.
-    """
-    def __init__(self, 
-                distance_matrix, 
-                weights=None,
-                max_filtration=float('inf')):
-        """
-        Args:
-            distance_matrix (Sequence[Sequence[float]]): distance matrix (full square or lower triangular).
-            weights (Sequence[float]): (one half of) weight for each vertex.
-            max_filtration (float): specifies the maximal filtration value to be considered.      
-        """
-        self.distance_matrix = distance_matrix
-        if weights is not None:
-            self.weights = weights
-        else:
-            self.weights = [0] * len(distance_matrix)
-        self.max_filtration = max_filtration
-            
-    def create_simplex_tree(self, max_dimension):
-        """
-        Args:
-            max_dimension (int): graph expansion until this given dimension.
-        """
-        dist = self.distance_matrix
-        F = self.weights
-        num_pts = len(dist)
-        
-        st = SimplexTree()
-        
-        for i in range(num_pts):
-            if 2*F[i] <= self.max_filtration:
-                st.insert([i], 2*F[i])
-        for i in range(num_pts):
-            for j in range(i):
-                value = max(2*F[i], 2*F[j], dist[i][j] + F[i] + F[j])
-                # max is needed when F is not 1-Lipschitz
-                if value <= self.max_filtration:
-                    st.insert([i,j], filtration=value)
-                    
-        st.expansion(max_dimension) 
-        return st
-        
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Rapha√´l Tinarrage, Yuichi Ike, Masatoshi Takenouchi
+#
+# Copyright (C) 2020 Inria, Copyright (C) 2020 FUjitsu Laboratories Ltd.
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+from gudhi import SimplexTree
+
+class WeightedRipsComplex:
+    """
+    Class to generate a weighted Rips complex from a distance matrix and weights on vertices, 
+    in the way described in :cite:`dtmfiltrations` with `p=1`. The filtration value of vertex `i` is `2*weights[i]`,
+    and the filtration value of edge `ij` is `distance_matrix[i][j]+weights[i]+weights[j]`,
+    or the maximum of the filtrations of its extremities, whichever is largest.
+    Remark that all the filtration values are doubled compared to the definition in the paper 
+    for consistency with RipsComplex.
+    """
+    def __init__(self, 
+                distance_matrix, 
+                weights=None,
+                max_filtration=float('inf')):
+        """
+        Args:
+            distance_matrix (Sequence[Sequence[float]]): distance matrix (full square or lower triangular).
+            weights (Sequence[float]): (one half of) weight for each vertex.
+            max_filtration (float): specifies the maximal filtration value to be considered.      
+        """
+        self.distance_matrix = distance_matrix
+        if weights is not None:
+            self.weights = weights
+        else:
+            self.weights = [0] * len(distance_matrix)
+        self.max_filtration = max_filtration
+            
+    def create_simplex_tree(self, max_dimension):
+        """
+        Args:
+            max_dimension (int): graph expansion until this given dimension.
+        """
+        dist = self.distance_matrix
+        F = self.weights
+        num_pts = len(dist)
+        
+        st = SimplexTree()
+        
+        for i in range(num_pts):
+            if 2*F[i] <= self.max_filtration:
+                st.insert([i], 2*F[i])
+        for i in range(num_pts):
+            for j in range(i):
+                value = max(2*F[i], 2*F[j], dist[i][j] + F[i] + F[j])
+                # max is needed when F is not 1-Lipschitz
+                if value <= self.max_filtration:
+                    st.insert([i,j], filtration=value)
+                    
+        st.expansion(max_dimension) 
+        return st
+
```

## gudhi/clustering/tomato.py

 * *Ordering differences only*

```diff
@@ -1,321 +1,321 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Marc Glisse
-#
-# Copyright (C) 2020 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy
-from ..point_cloud.knn import KNearestNeighbors
-from ..point_cloud.dtm import DTMDensity
-from ._tomato import *
-
-# The fit/predict interface is not so well suited...
-
-
-class Tomato:
-    """
-    This clustering algorithm needs a neighborhood graph on the points, and an estimation of the density at each point.
-    A few possible graph constructions and density estimators are provided for convenience, but it is perfectly natural
-    to provide your own.
-
-    :Requires: `SciPy <installation.html#scipy>`_, `Scikit-learn <installation.html#scikit-learn>`_ or others
-        (see :class:`~gudhi.point_cloud.knn.KNearestNeighbors`) in function of the options.
-
-    Attributes
-    ----------
-    n_clusters_: int
-        The number of clusters. Writing to it automatically adjusts `labels_`.
-    merge_threshold_: float
-        minimum prominence of a cluster so it doesn't get merged. Writing to it automatically adjusts `labels_`.
-    n_leaves_: int
-        number of leaves (unstable clusters) in the hierarchical tree
-    leaf_labels_: ndarray of shape (n_samples,)
-        cluster labels for each point, at the very bottom of the hierarchy
-    labels_: ndarray of shape (n_samples,)
-        cluster labels for each point, after merging
-    diagram_: ndarray of shape (`n_leaves_`, 2)
-        persistence diagram (only the finite points)
-    max_weight_per_cc_: ndarray of shape (n_connected_components,)
-        maximum of the density function on each connected component. This corresponds to the abscissa of infinite
-        points in the diagram
-    children_: ndarray of shape (`n_leaves_`-n_connected_components, 2)
-        The children of each non-leaf node. Values less than `n_leaves_` correspond to leaves of the tree.
-        A node i greater than or equal to `n_leaves_` is a non-leaf node and has children children_[i - `n_leaves_`].
-        Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_leaves_` + i
-    weights_: ndarray of shape (n_samples,)
-        weights of the points, as computed by the density estimator or provided by the user
-    params_: dict
-        Parameters like metric, etc
-    """
-
-    def __init__(
-        self,
-        graph_type="knn",
-        density_type="logDTM",
-        n_clusters=None,
-        merge_threshold=None,
-        #       eliminate_threshold=None,
-        #           eliminate_threshold (float): minimum max weight of a cluster so it doesn't get eliminated
-        **params
-    ):
-        """
-        Args:
-            graph_type (str): 'manual', 'knn' or 'radius'. Default is 'knn'.
-            density_type (str): 'manual', 'DTM', 'logDTM', 'KDE' or 'logKDE'. When you have many points,
-                'KDE' and 'logKDE' tend to be slower. Default is 'logDTM'.
-            metric (str|Callable): metric used when calculating the distance between instances in a feature array.
-                Defaults to Minkowski of parameter p.
-            kde_params (dict): if density_type is 'KDE' or 'logKDE', additional parameters passed directly to
-                sklearn.neighbors.KernelDensity.
-            k (int): number of neighbors for a knn graph (including the vertex itself). Defaults to 10.
-            k_DTM (int): number of neighbors for the DTM density estimation (including the vertex itself).
-                Defaults to k.
-            r (float): size of a neighborhood if graph_type is 'radius'. Also used as default bandwidth in kde_params.
-            eps (float): (1+eps) approximation factor when computing distances (ignored in many cases).
-            n_clusters (int): number of clusters requested. Defaults to None, i.e. no merging occurs and we get
-                the maximal number of clusters.
-            merge_threshold (float): minimum prominence of a cluster so it doesn't get merged.
-            symmetrize_graph (bool): whether we should add edges to make the neighborhood graph symmetric.
-                This can be useful with k-NN for small k. Defaults to false.
-            p (float): norm L^p on input points. Defaults to 2.
-            q (float): order used to compute the distance to measure. Defaults to dim.
-                Beware that when the dimension is large, this can easily cause overflows.
-            dim (float): final exponent in DTM density estimation, representing the dimension. Defaults to the
-                dimension, or 2 when the dimension cannot be read from the input (metric is "precomputed").
-            n_jobs (int): Number of jobs to schedule for parallel processing on the CPU.
-                If -1 is given all processors are used. Default: 1.
-            params: extra parameters are passed to :class:`~gudhi.point_cloud.knn.KNearestNeighbors` and
-                :class:`~gudhi.point_cloud.dtm.DTMDensity`.
-        """
-        # Should metric='precomputed' mean input_type='distance_matrix'?
-        # Should we be able to pass metric='minkowski' (what None does currently)?
-        self.graph_type_ = graph_type
-        self.density_type_ = density_type
-        self.params_ = params
-        self.__n_clusters = n_clusters
-        self.__merge_threshold = merge_threshold
-        # self.eliminate_threshold_ = eliminate_threshold
-        if n_clusters and merge_threshold:
-            raise ValueError("Cannot specify both a merge threshold and a number of clusters")
-
-    def fit(self, X, y=None, weights=None):
-        """
-        Args:
-            X ((n,d)-array of float|(n,n)-array of float|Sequence[Iterable[int]]): coordinates of the points,
-                or distance matrix (full, not just a triangle) if metric is "precomputed", or list of neighbors
-                for each point (points are represented by their index, starting from 0) if graph_type is "manual".
-                The number of points is currently limited to about 2 billion.
-            weights (ndarray of shape (n_samples)): if density_type is 'manual', a density estimate at each point
-            y: Not used, present here for API consistency with scikit-learn by convention.
-        """
-        # TODO: First detect if this is a new call with the same data (only threshold changed?)
-        # TODO: less code duplication (subroutines?), less spaghetti, but don't compute neighbors twice if not needed. Clear error message for missing or contradictory parameters.
-        if weights is not None:
-            density_type = "manual"
-        else:
-            density_type = self.density_type_
-            if density_type == "manual":
-                raise ValueError("If density_type is 'manual', you must provide weights to fit()")
-
-        if self.graph_type_ == "manual":
-            self.neighbors_ = X
-            # FIXME: uniformize "message 'option'" vs 'message "option"'
-            assert density_type == "manual", 'If graph_type is "manual", density_type must be as well'
-        else:
-            metric = self.params_.get("metric", "minkowski")
-            if metric != "precomputed":
-                self.points_ = X
-
-        # Slight complication to avoid computing knn twice.
-        need_knn = 0
-        need_knn_ngb = False
-        need_knn_dist = False
-        if self.graph_type_ == "knn":
-            k_graph = self.params_.get("k", 10)
-            # If X has fewer than k points...
-            if k_graph > len(X):
-                k_graph = len(X)
-            need_knn = k_graph
-            need_knn_ngb = True
-        if self.density_type_ in ["DTM", "logDTM"]:
-            k = self.params_.get("k", 10)
-            k_DTM = self.params_.get("k_DTM", k)
-            # If X has fewer than k points...
-            if k_DTM > len(X):
-                k_DTM = len(X)
-            need_knn = max(need_knn, k_DTM)
-            need_knn_dist = True
-            # if we ask for more neighbors for the graph than the DTM, getting the distances is a slight waste,
-            # but it looks negligible
-        if need_knn > 0:
-            knn_args = dict(self.params_)
-            knn_args["k"] = need_knn
-            knn = KNearestNeighbors(return_index=need_knn_ngb, return_distance=need_knn_dist, **knn_args).fit_transform(
-                X
-            )
-            if need_knn_ngb:
-                if need_knn_dist:
-                    self.neighbors_ = knn[0][:, 0:k_graph]
-                    knn_dist = knn[1]
-                else:
-                    self.neighbors_ = knn
-            elif need_knn_dist:
-                knn_dist = knn
-        if self.density_type_ in ["DTM", "logDTM"]:
-            dim = self.params_.get("dim")
-            if dim is None:
-                dim = len(X[0]) if metric != "precomputed" else 2
-            q = self.params_.get("q", dim)
-            weights = DTMDensity(k=k_DTM, metric="neighbors", dim=dim, q=q).fit_transform(knn_dist)
-            if self.density_type_ == "logDTM":
-                weights = numpy.log(weights)
-
-        if self.graph_type_ == "radius":
-            if metric in ["minkowski", "euclidean", "manhattan", "chebyshev"]:
-                from scipy.spatial import cKDTree
-
-                tree = cKDTree(X)
-                # TODO: handle "l1" and "l2" aliases?
-                p = self.params_.get("p")
-                if metric == "euclidean":
-                    assert p is None or p == 2, "p=" + str(p) + " is not consistent with metric='euclidean'"
-                    p = 2
-                elif metric == "manhattan":
-                    assert p is None or p == 1, "p=" + str(p) + " is not consistent with metric='manhattan'"
-                    p = 1
-                elif metric == "chebyshev":
-                    assert p is None or p == numpy.inf, "p=" + str(p) + " is not consistent with metric='chebyshev'"
-                    p = numpy.inf
-                elif p is None:
-                    p = 2  # the default
-                eps = self.params_.get("eps", 0)
-                self.neighbors_ = tree.query_ball_tree(tree, r=self.params_["r"], p=p, eps=eps)
-
-            # TODO: sklearn's NearestNeighbors.radius_neighbors can handle more metrics efficiently via its BallTree
-            # (don't bother with the _graph variant, it just calls radius_neighbors).
-            elif metric != "precomputed":
-                from sklearn.metrics import pairwise_distances
-
-                X = pairwise_distances(X, metric=metric, n_jobs=self.params_.get("n_jobs"))
-                metric = "precomputed"
-
-            if metric == "precomputed":
-                # TODO: parallelize? May not be worth it.
-                X = numpy.asarray(X)
-                r = self.params_["r"]
-                self.neighbors_ = [numpy.flatnonzero(l <= r) for l in X]
-
-        if self.density_type_ in {"KDE", "logKDE"}:
-            # Slow...
-            assert (
-                self.graph_type_ != "manual" and metric != "precomputed"
-            ), "Scikit-learn's KernelDensity requires point coordinates"
-            kde_params = dict(self.params_.get("kde_params", dict()))
-            kde_params.setdefault("metric", metric)
-            r = self.params_.get("r")
-            if r is not None:
-                kde_params.setdefault("bandwidth", r)
-            # Should we default rtol to eps?
-            from sklearn.neighbors import KernelDensity
-
-            weights = KernelDensity(**kde_params).fit(self.points_).score_samples(self.points_)
-            if self.density_type_ == "KDE":
-                weights = numpy.exp(weights)
-
-        # TODO: do it at the C++ level and/or in parallel if this is too slow?
-        if self.params_.get("symmetrize_graph"):
-            self.neighbors_ = [set(line) for line in self.neighbors_]
-            for i, line in enumerate(self.neighbors_):
-                line.discard(i)
-                for j in line:
-                    self.neighbors_[j].add(i)
-
-        self.weights_ = weights
-        # This is where the main computation happens
-        self.leaf_labels_, self.children_, self.diagram_, self.max_weight_per_cc_ = hierarchy(self.neighbors_, weights)
-        self.n_leaves_ = len(self.max_weight_per_cc_) + len(self.children_)
-        assert self.leaf_labels_.max() + 1 == len(self.max_weight_per_cc_) + len(self.children_)
-        # TODO: deduplicate this code with the setters below
-        if self.__merge_threshold:
-            assert not self.__n_clusters
-            self.__n_clusters = numpy.count_nonzero(
-                self.diagram_[:, 0] - self.diagram_[:, 1] > self.__merge_threshold
-            ) + len(self.max_weight_per_cc_)
-        if self.__n_clusters:
-            # TODO: set corresponding merge_threshold?
-            renaming = merge(self.children_, self.n_leaves_, self.__n_clusters)
-            self.labels_ = renaming[self.leaf_labels_]
-            # In case the user asked for something impossible.
-            # TODO: check for impossible situations before calling merge.
-            self.__n_clusters = self.labels_.max() + 1
-        else:
-            self.labels_ = self.leaf_labels_
-            self.__n_clusters = self.n_leaves_
-        return self
-
-    def fit_predict(self, X, y=None, weights=None):
-        """
-        Equivalent to fit(), and returns the `labels_`.
-        """
-        return self.fit(X, y, weights).labels_
-
-    # TODO: add argument k or threshold? Have a version where you can click and it shows the line and the corresponding k?
-    def plot_diagram(self):
-        """
-        """
-        import matplotlib.pyplot as plt
-
-        l = self.max_weight_per_cc_.min()
-        r = self.max_weight_per_cc_.max()
-        if self.diagram_.size > 0:
-            plt.plot(self.diagram_[:, 0], self.diagram_[:, 1], "o", color="red")
-            l = min(l, self.diagram_[:, 1].min())
-            r = max(r, self.diagram_[:, 0].max())
-        if l == r:
-            if l > 0:
-                l, r = 0.9 * l, 1.1 * r
-            elif l < 0:
-                l, r = 1.1 * l, 0.9 * r
-            else:
-                l, r = -1.0, 1.0
-        plt.plot([l, r], [l, r])
-        plt.plot(
-            self.max_weight_per_cc_, numpy.full(self.max_weight_per_cc_.shape, 1.1 * l - 0.1 * r), "o", color="green"
-        )
-        plt.show()
-
-    # Use set_params instead?
-    @property
-    def n_clusters_(self):
-        return self.__n_clusters
-
-    @n_clusters_.setter
-    def n_clusters_(self, n_clusters):
-        if n_clusters == self.__n_clusters:
-            return
-        self.__n_clusters = n_clusters
-        self.__merge_threshold = None
-        if hasattr(self, "leaf_labels_"):
-            renaming = merge(self.children_, self.n_leaves_, self.__n_clusters)
-            self.labels_ = renaming[self.leaf_labels_]
-            # In case the user asked for something impossible
-            self.__n_clusters = self.labels_.max() + 1
-
-    @property
-    def merge_threshold_(self):
-        return self.__merge_threshold
-
-    @merge_threshold_.setter
-    def merge_threshold_(self, merge_threshold):
-        if merge_threshold == self.__merge_threshold:
-            return
-        if hasattr(self, "leaf_labels_"):
-            self.n_clusters_ = numpy.count_nonzero(self.diagram_[:, 0] - self.diagram_[:, 1] > merge_threshold) + len(
-                self.max_weight_per_cc_
-            )
-        else:
-            self.__n_clusters = None
-        self.__merge_threshold = merge_threshold
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Marc Glisse
+#
+# Copyright (C) 2020 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy
+from ..point_cloud.knn import KNearestNeighbors
+from ..point_cloud.dtm import DTMDensity
+from ._tomato import *
+
+# The fit/predict interface is not so well suited...
+
+
+class Tomato:
+    """
+    This clustering algorithm needs a neighborhood graph on the points, and an estimation of the density at each point.
+    A few possible graph constructions and density estimators are provided for convenience, but it is perfectly natural
+    to provide your own.
+
+    :Requires: `SciPy <installation.html#scipy>`_, `Scikit-learn <installation.html#scikit-learn>`_ or others
+        (see :class:`~gudhi.point_cloud.knn.KNearestNeighbors`) in function of the options.
+
+    Attributes
+    ----------
+    n_clusters_: int
+        The number of clusters. Writing to it automatically adjusts `labels_`.
+    merge_threshold_: float
+        minimum prominence of a cluster so it doesn't get merged. Writing to it automatically adjusts `labels_`.
+    n_leaves_: int
+        number of leaves (unstable clusters) in the hierarchical tree
+    leaf_labels_: ndarray of shape (n_samples,)
+        cluster labels for each point, at the very bottom of the hierarchy
+    labels_: ndarray of shape (n_samples,)
+        cluster labels for each point, after merging
+    diagram_: ndarray of shape (`n_leaves_`, 2)
+        persistence diagram (only the finite points)
+    max_weight_per_cc_: ndarray of shape (n_connected_components,)
+        maximum of the density function on each connected component. This corresponds to the abscissa of infinite
+        points in the diagram
+    children_: ndarray of shape (`n_leaves_`-n_connected_components, 2)
+        The children of each non-leaf node. Values less than `n_leaves_` correspond to leaves of the tree.
+        A node i greater than or equal to `n_leaves_` is a non-leaf node and has children children_[i - `n_leaves_`].
+        Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_leaves_` + i
+    weights_: ndarray of shape (n_samples,)
+        weights of the points, as computed by the density estimator or provided by the user
+    params_: dict
+        Parameters like metric, etc
+    """
+
+    def __init__(
+        self,
+        graph_type="knn",
+        density_type="logDTM",
+        n_clusters=None,
+        merge_threshold=None,
+        #       eliminate_threshold=None,
+        #           eliminate_threshold (float): minimum max weight of a cluster so it doesn't get eliminated
+        **params
+    ):
+        """
+        Args:
+            graph_type (str): 'manual', 'knn' or 'radius'. Default is 'knn'.
+            density_type (str): 'manual', 'DTM', 'logDTM', 'KDE' or 'logKDE'. When you have many points,
+                'KDE' and 'logKDE' tend to be slower. Default is 'logDTM'.
+            metric (str|Callable): metric used when calculating the distance between instances in a feature array.
+                Defaults to Minkowski of parameter p.
+            kde_params (dict): if density_type is 'KDE' or 'logKDE', additional parameters passed directly to
+                sklearn.neighbors.KernelDensity.
+            k (int): number of neighbors for a knn graph (including the vertex itself). Defaults to 10.
+            k_DTM (int): number of neighbors for the DTM density estimation (including the vertex itself).
+                Defaults to k.
+            r (float): size of a neighborhood if graph_type is 'radius'. Also used as default bandwidth in kde_params.
+            eps (float): (1+eps) approximation factor when computing distances (ignored in many cases).
+            n_clusters (int): number of clusters requested. Defaults to None, i.e. no merging occurs and we get
+                the maximal number of clusters.
+            merge_threshold (float): minimum prominence of a cluster so it doesn't get merged.
+            symmetrize_graph (bool): whether we should add edges to make the neighborhood graph symmetric.
+                This can be useful with k-NN for small k. Defaults to false.
+            p (float): norm L^p on input points. Defaults to 2.
+            q (float): order used to compute the distance to measure. Defaults to dim.
+                Beware that when the dimension is large, this can easily cause overflows.
+            dim (float): final exponent in DTM density estimation, representing the dimension. Defaults to the
+                dimension, or 2 when the dimension cannot be read from the input (metric is "precomputed").
+            n_jobs (int): Number of jobs to schedule for parallel processing on the CPU.
+                If -1 is given all processors are used. Default: 1.
+            params: extra parameters are passed to :class:`~gudhi.point_cloud.knn.KNearestNeighbors` and
+                :class:`~gudhi.point_cloud.dtm.DTMDensity`.
+        """
+        # Should metric='precomputed' mean input_type='distance_matrix'?
+        # Should we be able to pass metric='minkowski' (what None does currently)?
+        self.graph_type_ = graph_type
+        self.density_type_ = density_type
+        self.params_ = params
+        self.__n_clusters = n_clusters
+        self.__merge_threshold = merge_threshold
+        # self.eliminate_threshold_ = eliminate_threshold
+        if n_clusters and merge_threshold:
+            raise ValueError("Cannot specify both a merge threshold and a number of clusters")
+
+    def fit(self, X, y=None, weights=None):
+        """
+        Args:
+            X ((n,d)-array of float|(n,n)-array of float|Sequence[Iterable[int]]): coordinates of the points,
+                or distance matrix (full, not just a triangle) if metric is "precomputed", or list of neighbors
+                for each point (points are represented by their index, starting from 0) if graph_type is "manual".
+                The number of points is currently limited to about 2 billion.
+            weights (ndarray of shape (n_samples)): if density_type is 'manual', a density estimate at each point
+            y: Not used, present here for API consistency with scikit-learn by convention.
+        """
+        # TODO: First detect if this is a new call with the same data (only threshold changed?)
+        # TODO: less code duplication (subroutines?), less spaghetti, but don't compute neighbors twice if not needed. Clear error message for missing or contradictory parameters.
+        if weights is not None:
+            density_type = "manual"
+        else:
+            density_type = self.density_type_
+            if density_type == "manual":
+                raise ValueError("If density_type is 'manual', you must provide weights to fit()")
+
+        if self.graph_type_ == "manual":
+            self.neighbors_ = X
+            # FIXME: uniformize "message 'option'" vs 'message "option"'
+            assert density_type == "manual", 'If graph_type is "manual", density_type must be as well'
+        else:
+            metric = self.params_.get("metric", "minkowski")
+            if metric != "precomputed":
+                self.points_ = X
+
+        # Slight complication to avoid computing knn twice.
+        need_knn = 0
+        need_knn_ngb = False
+        need_knn_dist = False
+        if self.graph_type_ == "knn":
+            k_graph = self.params_.get("k", 10)
+            # If X has fewer than k points...
+            if k_graph > len(X):
+                k_graph = len(X)
+            need_knn = k_graph
+            need_knn_ngb = True
+        if self.density_type_ in ["DTM", "logDTM"]:
+            k = self.params_.get("k", 10)
+            k_DTM = self.params_.get("k_DTM", k)
+            # If X has fewer than k points...
+            if k_DTM > len(X):
+                k_DTM = len(X)
+            need_knn = max(need_knn, k_DTM)
+            need_knn_dist = True
+            # if we ask for more neighbors for the graph than the DTM, getting the distances is a slight waste,
+            # but it looks negligible
+        if need_knn > 0:
+            knn_args = dict(self.params_)
+            knn_args["k"] = need_knn
+            knn = KNearestNeighbors(return_index=need_knn_ngb, return_distance=need_knn_dist, **knn_args).fit_transform(
+                X
+            )
+            if need_knn_ngb:
+                if need_knn_dist:
+                    self.neighbors_ = knn[0][:, 0:k_graph]
+                    knn_dist = knn[1]
+                else:
+                    self.neighbors_ = knn
+            elif need_knn_dist:
+                knn_dist = knn
+        if self.density_type_ in ["DTM", "logDTM"]:
+            dim = self.params_.get("dim")
+            if dim is None:
+                dim = len(X[0]) if metric != "precomputed" else 2
+            q = self.params_.get("q", dim)
+            weights = DTMDensity(k=k_DTM, metric="neighbors", dim=dim, q=q).fit_transform(knn_dist)
+            if self.density_type_ == "logDTM":
+                weights = numpy.log(weights)
+
+        if self.graph_type_ == "radius":
+            if metric in ["minkowski", "euclidean", "manhattan", "chebyshev"]:
+                from scipy.spatial import cKDTree
+
+                tree = cKDTree(X)
+                # TODO: handle "l1" and "l2" aliases?
+                p = self.params_.get("p")
+                if metric == "euclidean":
+                    assert p is None or p == 2, "p=" + str(p) + " is not consistent with metric='euclidean'"
+                    p = 2
+                elif metric == "manhattan":
+                    assert p is None or p == 1, "p=" + str(p) + " is not consistent with metric='manhattan'"
+                    p = 1
+                elif metric == "chebyshev":
+                    assert p is None or p == numpy.inf, "p=" + str(p) + " is not consistent with metric='chebyshev'"
+                    p = numpy.inf
+                elif p is None:
+                    p = 2  # the default
+                eps = self.params_.get("eps", 0)
+                self.neighbors_ = tree.query_ball_tree(tree, r=self.params_["r"], p=p, eps=eps)
+
+            # TODO: sklearn's NearestNeighbors.radius_neighbors can handle more metrics efficiently via its BallTree
+            # (don't bother with the _graph variant, it just calls radius_neighbors).
+            elif metric != "precomputed":
+                from sklearn.metrics import pairwise_distances
+
+                X = pairwise_distances(X, metric=metric, n_jobs=self.params_.get("n_jobs"))
+                metric = "precomputed"
+
+            if metric == "precomputed":
+                # TODO: parallelize? May not be worth it.
+                X = numpy.asarray(X)
+                r = self.params_["r"]
+                self.neighbors_ = [numpy.flatnonzero(l <= r) for l in X]
+
+        if self.density_type_ in {"KDE", "logKDE"}:
+            # Slow...
+            assert (
+                self.graph_type_ != "manual" and metric != "precomputed"
+            ), "Scikit-learn's KernelDensity requires point coordinates"
+            kde_params = dict(self.params_.get("kde_params", dict()))
+            kde_params.setdefault("metric", metric)
+            r = self.params_.get("r")
+            if r is not None:
+                kde_params.setdefault("bandwidth", r)
+            # Should we default rtol to eps?
+            from sklearn.neighbors import KernelDensity
+
+            weights = KernelDensity(**kde_params).fit(self.points_).score_samples(self.points_)
+            if self.density_type_ == "KDE":
+                weights = numpy.exp(weights)
+
+        # TODO: do it at the C++ level and/or in parallel if this is too slow?
+        if self.params_.get("symmetrize_graph"):
+            self.neighbors_ = [set(line) for line in self.neighbors_]
+            for i, line in enumerate(self.neighbors_):
+                line.discard(i)
+                for j in line:
+                    self.neighbors_[j].add(i)
+
+        self.weights_ = weights
+        # This is where the main computation happens
+        self.leaf_labels_, self.children_, self.diagram_, self.max_weight_per_cc_ = hierarchy(self.neighbors_, weights)
+        self.n_leaves_ = len(self.max_weight_per_cc_) + len(self.children_)
+        assert self.leaf_labels_.max() + 1 == len(self.max_weight_per_cc_) + len(self.children_)
+        # TODO: deduplicate this code with the setters below
+        if self.__merge_threshold:
+            assert not self.__n_clusters
+            self.__n_clusters = numpy.count_nonzero(
+                self.diagram_[:, 0] - self.diagram_[:, 1] > self.__merge_threshold
+            ) + len(self.max_weight_per_cc_)
+        if self.__n_clusters:
+            # TODO: set corresponding merge_threshold?
+            renaming = merge(self.children_, self.n_leaves_, self.__n_clusters)
+            self.labels_ = renaming[self.leaf_labels_]
+            # In case the user asked for something impossible.
+            # TODO: check for impossible situations before calling merge.
+            self.__n_clusters = self.labels_.max() + 1
+        else:
+            self.labels_ = self.leaf_labels_
+            self.__n_clusters = self.n_leaves_
+        return self
+
+    def fit_predict(self, X, y=None, weights=None):
+        """
+        Equivalent to fit(), and returns the `labels_`.
+        """
+        return self.fit(X, y, weights).labels_
+
+    # TODO: add argument k or threshold? Have a version where you can click and it shows the line and the corresponding k?
+    def plot_diagram(self):
+        """
+        """
+        import matplotlib.pyplot as plt
+
+        l = self.max_weight_per_cc_.min()
+        r = self.max_weight_per_cc_.max()
+        if self.diagram_.size > 0:
+            plt.plot(self.diagram_[:, 0], self.diagram_[:, 1], "o", color="red")
+            l = min(l, self.diagram_[:, 1].min())
+            r = max(r, self.diagram_[:, 0].max())
+        if l == r:
+            if l > 0:
+                l, r = 0.9 * l, 1.1 * r
+            elif l < 0:
+                l, r = 1.1 * l, 0.9 * r
+            else:
+                l, r = -1.0, 1.0
+        plt.plot([l, r], [l, r])
+        plt.plot(
+            self.max_weight_per_cc_, numpy.full(self.max_weight_per_cc_.shape, 1.1 * l - 0.1 * r), "o", color="green"
+        )
+        plt.show()
+
+    # Use set_params instead?
+    @property
+    def n_clusters_(self):
+        return self.__n_clusters
+
+    @n_clusters_.setter
+    def n_clusters_(self, n_clusters):
+        if n_clusters == self.__n_clusters:
+            return
+        self.__n_clusters = n_clusters
+        self.__merge_threshold = None
+        if hasattr(self, "leaf_labels_"):
+            renaming = merge(self.children_, self.n_leaves_, self.__n_clusters)
+            self.labels_ = renaming[self.leaf_labels_]
+            # In case the user asked for something impossible
+            self.__n_clusters = self.labels_.max() + 1
+
+    @property
+    def merge_threshold_(self):
+        return self.__merge_threshold
+
+    @merge_threshold_.setter
+    def merge_threshold_(self, merge_threshold):
+        if merge_threshold == self.__merge_threshold:
+            return
+        if hasattr(self, "leaf_labels_"):
+            self.n_clusters_ = numpy.count_nonzero(self.diagram_[:, 0] - self.diagram_[:, 1] > merge_threshold) + len(
+                self.max_weight_per_cc_
+            )
+        else:
+            self.__n_clusters = None
+        self.__merge_threshold = merge_threshold
```

## gudhi/datasets/remote.py

 * *Ordering differences only*

```diff
@@ -1,334 +1,334 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Hind Montassif
-#
-# Copyright (C) 2021 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-#   - 2022/09 Vincent Rouvreau: factorize _fetch_remote_license
-#                               fetch activities dataset
-
-from os.path import join, split, exists, expanduser
-from os import makedirs, remove, environ
-
-from urllib.request import urlretrieve
-import hashlib
-import shutil
-from functools import lru_cache
-
-import numpy as np
-from numpy.lib import recfunctions as rfn
-
-
-def _get_data_home(data_home=None):
-    """
-    Return the path of the remote datasets directory.
-    This folder is used to store remotely fetched datasets.
-    By default the datasets directory is set to a folder named 'gudhi_data' in the user home folder.
-    Alternatively, it can be set by the 'GUDHI_DATA' environment variable.
-    The '~' symbol is expanded to the user home folder.
-    If the folder does not already exist, it is automatically created.
-
-    Parameters
-    ----------
-    data_home : string
-        The path to remote datasets directory.
-        Default is `None`, meaning that the data home directory will be set to "~/gudhi_data",
-        if the 'GUDHI_DATA' environment variable does not exist.
-
-    Returns
-    -------
-    data_home: string
-        The path to remote datasets directory.
-    """
-    if data_home is None:
-        data_home = environ.get("GUDHI_DATA", join("~", "gudhi_data"))
-    data_home = expanduser(data_home)
-    makedirs(data_home, exist_ok=True)
-    return data_home
-
-
-def clear_data_home(data_home=None):
-    """
-    Delete the data home cache directory and all its content.
-
-    Parameters
-    ----------
-    data_home : string, default is None.
-        The path to remote datasets directory.
-        If `None` and the 'GUDHI_DATA' environment variable does not exist,
-        the default directory to be removed is set to "~/gudhi_data".
-    """
-    # On windows, needs to clear cache before removing a file - no lazy file deletion
-    _load_and_cache_activity.cache_clear()
-    data_home = _get_data_home(data_home)
-    shutil.rmtree(data_home)
-
-
-def _checksum_sha256(file_path):
-    """
-    Compute the file checksum using sha256.
-
-    Parameters
-    ----------
-    file_path: string
-        Full path of the created file including filename.
-
-    Returns
-    -------
-        The hex digest of file_path.
-    """
-    sha256_hash = hashlib.sha256()
-    chunk_size = 4096
-    with open(file_path, "rb") as f:
-        # Read and update hash string value in blocks of 4K
-        while True:
-            buffer = f.read(chunk_size)
-            if not buffer:
-                break
-            sha256_hash.update(buffer)
-    return sha256_hash.hexdigest()
-
-
-def _fetch_remote(url, file_path, file_checksum=None):
-    """
-    Fetch the wanted dataset from the given url and save it in file_path.
-
-    Parameters
-    ----------
-    url : string
-        The url to fetch the dataset from.
-    file_path : string
-        Full path of the downloaded file including filename.
-    file_checksum : string
-        The file checksum using sha256 to check against the one computed on the downloaded file.
-        Default is 'None', which means the checksum is not checked.
-
-    Raises
-    ------
-    OSError
-        If the computed SHA256 checksum of file does not match the one given by the user.
-    """
-
-    # Get the file
-    urlretrieve(url, file_path)
-
-    if file_checksum is not None:
-        checksum = _checksum_sha256(file_path)
-        if file_checksum != checksum:
-            # Remove file and raise error
-            remove(file_path)
-            raise OSError(
-                "{} has a SHA256 checksum : {}, "
-                "different from expected : {}."
-                "The file may be corrupted or the given url may be wrong !".format(file_path, checksum, file_checksum)
-            )
-
-
-def _fetch_remote_license(license_url, license_path, license_checksum=None, accept_license=False):
-    """
-    Fetch the wanted license from the given url and save it in file_path.
-
-    Parameters
-    ----------
-    license_url : string
-        The url to fetch the license file from.
-    license_path : string
-        Full path of the downloaded file including filename.
-    license_checksum : string
-        The license file checksum using sha256 to check against the one computed on the downloaded file.
-        Default is 'None', which means the checksum is not checked.
-
-    Raises
-    ------
-    OSError
-        If the computed SHA256 checksum of file does not match the one given by the user.
-    """
-    _fetch_remote(license_url, license_path, license_checksum)
-    # Print license terms unless accept_license is set to True
-    if not accept_license:
-        if exists(license_path):
-            with open(license_path) as f:
-                print(f.read())
-
-
-def _get_archive_path(file_path, label):
-    """
-    Get archive path based on file_path given by user and label.
-
-    Parameters
-    ----------
-    file_path: string
-        Full path of the file to get including filename, or None.
-    label: string
-        Label used along with 'data_home' to get archive path, in case 'file_path' is None.
-
-    Returns
-    -------
-        Full path of archive including filename.
-    """
-    if file_path is None:
-        archive_path = join(_get_data_home(), label)
-        dirname = split(archive_path)[0]
-        makedirs(dirname, exist_ok=True)
-    else:
-        archive_path = file_path
-        dirname = split(archive_path)[0]
-        makedirs(dirname, exist_ok=True)
-
-    return archive_path
-
-
-def fetch_spiral_2d(file_path=None):
-    """
-    Load the spiral_2d dataset.
-
-    Note that if the dataset already exists in the target location, it is not downloaded again,
-    and the corresponding array is returned from cache.
-
-    Parameters
-    ----------
-    file_path : string
-        Full path of the downloaded file including filename.
-
-        Default is None, meaning that it's set to "data_home/points/spiral_2d/spiral_2d.npy".
-
-        The "data_home" directory is set by default to "~/gudhi_data",
-        unless the 'GUDHI_DATA' environment variable is set.
-
-    Returns
-    -------
-    points: numpy array
-        Array of shape (114562, 2).
-    """
-    file_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/spiral_2d/spiral_2d.npy"
-    file_checksum = "2226024da76c073dd2f24b884baefbfd14928b52296df41ad2d9b9dc170f2401"
-
-    archive_path = _get_archive_path(file_path, "points/spiral_2d/spiral_2d.npy")
-
-    if not exists(archive_path):
-        _fetch_remote(file_url, archive_path, file_checksum)
-
-    return np.load(archive_path, mmap_mode="r")
-
-
-def fetch_bunny(file_path=None, accept_license=False):
-    """
-    Load the Stanford bunny dataset.
-
-    This dataset contains 35947 vertices.
-
-    Note that if the dataset already exists in the target location, it is not downloaded again,
-    and the corresponding array is returned from cache.
-
-    Parameters
-    ----------
-    file_path : string
-        Full path of the downloaded file including filename.
-
-        Default is None, meaning that it's set to "data_home/points/bunny/bunny.npy".
-        In this case, the LICENSE file would be downloaded as "data_home/points/bunny/bunny.LICENSE".
-
-        The "data_home" directory is set by default to "~/gudhi_data",
-        unless the 'GUDHI_DATA' environment variable is set.
-
-    accept_license : boolean
-        Flag to specify if user accepts the file LICENSE and prevents from printing the corresponding license terms.
-
-        Default is False.
-
-    Returns
-    -------
-    points: numpy array
-        Array of shape (35947, 3).
-    """
-
-    file_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/bunny/bunny.npy"
-    file_checksum = "f382482fd89df8d6444152dc8fd454444fe597581b193fd139725a85af4a6c6e"
-    license_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/bunny/bunny.LICENSE"
-    license_checksum = "b763dbe1b2fc6015d05cbf7bcc686412a2eb100a1f2220296e3b4a644c69633a"
-
-    archive_path = _get_archive_path(file_path, "points/bunny/bunny.npy")
-
-    if not exists(archive_path):
-        _fetch_remote(file_url, archive_path, file_checksum)
-        license_path = join(split(archive_path)[0], "bunny.LICENSE")
-        _fetch_remote_license(license_url, license_path, license_checksum, accept_license)
-
-    return np.load(archive_path, mmap_mode="r")
-
-
-@lru_cache(maxsize=None)
-def _load_and_cache_activity(file_path):
-    return np.load(file_path, mmap_mode="r")
-
-
-def fetch_daily_activities(file_path=None, subset=None, accept_license=False):
-    """
-    Load a subset of the Daily and Sports Activities dataset. This dataset comes from
-    https://archive.ics.uci.edu/ml/datasets/daily+and+sports+activities (CC BY 4.0 license).
-
-    Note that if the dataset already exists in the target location, it is not downloaded again,
-    and the corresponding dataset is read from cache.
-
-    Parameters
-    ----------
-    file_path : string
-        Full path of the downloaded file including filename.
-
-        Default is None, meaning that it's set to "data_home/points/activities/activities_p1_left_leg.npy".
-        In this case, the LICENSE file would be downloaded as "data_home/points/activities/activities.LICENSE".
-
-        The "data_home" directory is set by default to "~/gudhi_data",
-        unless the 'GUDHI_DATA' environment variable is set.
-
-    subset : string
-        This argument allows to download the following subsets:
-         * 'cross_training' Only left leg magnetometer of cross training activity performed by the person 1. It contains 7.500 vertices in dimension 3.
-         * 'jumping' Only left leg magnetometer of jumping activity performed by the person 1. It contains 7.500 vertices in dimension 3.
-         * 'stepper' Only left leg magnetometer of stepper activity performed by the person 1. It contains 7.500 vertices in dimension 3.
-         * 'walking' Only left leg magnetometer of walking activity performed by the person 1. It contains 7.500 vertices in dimension 3.
-         * None (default value) This dataset contains 30.000 vertices in dimension 3 + activity type column (`14.` for 'cross_training', `18.` for 'jumping', `13.` for 'stepper', or `9.` for 'walking')
-
-    accept_license : boolean
-        Flag to specify if user accepts the file LICENSE and prevents from printing the corresponding license terms.
-
-        Default is False.
-
-    Returns
-    -------
-    points: numpy array
-        Depending on subset value:
-         * Array of shape (7.500, 3, dtype = float).
-
-        Or
-         * Array of shape (30.000, 4, dtype = float) when `subset is None`.
-    """
-    # To avoid download when subset is not correct
-    if subset not in ["walking", "stepper", "cross_training", "jumping", None]:
-        raise ValueError("Unknown subset value")
-
-    file_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/activities/activities_p1_left_leg.npy"
-    file_checksum = "ff813f717dbd3c8f8a95e59a7d8496d0b43c440e85a4db1f2b338cbfa9c02f25"
-    activities_license_url = (
-        "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/activities/activities.LICENSE"
-    )
-    activities_license_checksum = "f5ce6749fa9d5359d7b0c4c37d0b61e5d9520f9494cd53be94295d3967ee4023"
-    gudhi_data_set_path = "points/activities/activities_p1_left_leg.npy"
-
-    archive_path = _get_archive_path(file_path, gudhi_data_set_path)
-    if not exists(archive_path):
-        _fetch_remote(file_url, archive_path, file_checksum)
-        license_path = join(split(archive_path)[0], "activities.LICENSE")
-        _fetch_remote_license(activities_license_url, license_path, activities_license_checksum, accept_license)
-
-    ds = _load_and_cache_activity(archive_path)
-
-    if subset is not None:
-        activity_pos = {"walking": 0, "stepper": 1, "cross_training": 2, "jumping": 3}
-        per_activity = 7500
-        start_pos = activity_pos[subset] * per_activity
-        ds = ds[start_pos : (start_pos + per_activity), 0:3]
-
-    return ds
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Hind Montassif
+#
+# Copyright (C) 2021 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+#   - 2022/09 Vincent Rouvreau: factorize _fetch_remote_license
+#                               fetch activities dataset
+
+from os.path import join, split, exists, expanduser
+from os import makedirs, remove, environ
+
+from urllib.request import urlretrieve
+import hashlib
+import shutil
+from functools import lru_cache
+
+import numpy as np
+from numpy.lib import recfunctions as rfn
+
+
+def _get_data_home(data_home=None):
+    """
+    Return the path of the remote datasets directory.
+    This folder is used to store remotely fetched datasets.
+    By default the datasets directory is set to a folder named 'gudhi_data' in the user home folder.
+    Alternatively, it can be set by the 'GUDHI_DATA' environment variable.
+    The '~' symbol is expanded to the user home folder.
+    If the folder does not already exist, it is automatically created.
+
+    Parameters
+    ----------
+    data_home : string
+        The path to remote datasets directory.
+        Default is `None`, meaning that the data home directory will be set to "~/gudhi_data",
+        if the 'GUDHI_DATA' environment variable does not exist.
+
+    Returns
+    -------
+    data_home: string
+        The path to remote datasets directory.
+    """
+    if data_home is None:
+        data_home = environ.get("GUDHI_DATA", join("~", "gudhi_data"))
+    data_home = expanduser(data_home)
+    makedirs(data_home, exist_ok=True)
+    return data_home
+
+
+def clear_data_home(data_home=None):
+    """
+    Delete the data home cache directory and all its content.
+
+    Parameters
+    ----------
+    data_home : string, default is None.
+        The path to remote datasets directory.
+        If `None` and the 'GUDHI_DATA' environment variable does not exist,
+        the default directory to be removed is set to "~/gudhi_data".
+    """
+    # On windows, needs to clear cache before removing a file - no lazy file deletion
+    _load_and_cache_activity.cache_clear()
+    data_home = _get_data_home(data_home)
+    shutil.rmtree(data_home)
+
+
+def _checksum_sha256(file_path):
+    """
+    Compute the file checksum using sha256.
+
+    Parameters
+    ----------
+    file_path: string
+        Full path of the created file including filename.
+
+    Returns
+    -------
+        The hex digest of file_path.
+    """
+    sha256_hash = hashlib.sha256()
+    chunk_size = 4096
+    with open(file_path, "rb") as f:
+        # Read and update hash string value in blocks of 4K
+        while True:
+            buffer = f.read(chunk_size)
+            if not buffer:
+                break
+            sha256_hash.update(buffer)
+    return sha256_hash.hexdigest()
+
+
+def _fetch_remote(url, file_path, file_checksum=None):
+    """
+    Fetch the wanted dataset from the given url and save it in file_path.
+
+    Parameters
+    ----------
+    url : string
+        The url to fetch the dataset from.
+    file_path : string
+        Full path of the downloaded file including filename.
+    file_checksum : string
+        The file checksum using sha256 to check against the one computed on the downloaded file.
+        Default is 'None', which means the checksum is not checked.
+
+    Raises
+    ------
+    OSError
+        If the computed SHA256 checksum of file does not match the one given by the user.
+    """
+
+    # Get the file
+    urlretrieve(url, file_path)
+
+    if file_checksum is not None:
+        checksum = _checksum_sha256(file_path)
+        if file_checksum != checksum:
+            # Remove file and raise error
+            remove(file_path)
+            raise OSError(
+                "{} has a SHA256 checksum : {}, "
+                "different from expected : {}."
+                "The file may be corrupted or the given url may be wrong !".format(file_path, checksum, file_checksum)
+            )
+
+
+def _fetch_remote_license(license_url, license_path, license_checksum=None, accept_license=False):
+    """
+    Fetch the wanted license from the given url and save it in file_path.
+
+    Parameters
+    ----------
+    license_url : string
+        The url to fetch the license file from.
+    license_path : string
+        Full path of the downloaded file including filename.
+    license_checksum : string
+        The license file checksum using sha256 to check against the one computed on the downloaded file.
+        Default is 'None', which means the checksum is not checked.
+
+    Raises
+    ------
+    OSError
+        If the computed SHA256 checksum of file does not match the one given by the user.
+    """
+    _fetch_remote(license_url, license_path, license_checksum)
+    # Print license terms unless accept_license is set to True
+    if not accept_license:
+        if exists(license_path):
+            with open(license_path) as f:
+                print(f.read())
+
+
+def _get_archive_path(file_path, label):
+    """
+    Get archive path based on file_path given by user and label.
+
+    Parameters
+    ----------
+    file_path: string
+        Full path of the file to get including filename, or None.
+    label: string
+        Label used along with 'data_home' to get archive path, in case 'file_path' is None.
+
+    Returns
+    -------
+        Full path of archive including filename.
+    """
+    if file_path is None:
+        archive_path = join(_get_data_home(), label)
+        dirname = split(archive_path)[0]
+        makedirs(dirname, exist_ok=True)
+    else:
+        archive_path = file_path
+        dirname = split(archive_path)[0]
+        makedirs(dirname, exist_ok=True)
+
+    return archive_path
+
+
+def fetch_spiral_2d(file_path=None):
+    """
+    Load the spiral_2d dataset.
+
+    Note that if the dataset already exists in the target location, it is not downloaded again,
+    and the corresponding array is returned from cache.
+
+    Parameters
+    ----------
+    file_path : string
+        Full path of the downloaded file including filename.
+
+        Default is None, meaning that it's set to "data_home/points/spiral_2d/spiral_2d.npy".
+
+        The "data_home" directory is set by default to "~/gudhi_data",
+        unless the 'GUDHI_DATA' environment variable is set.
+
+    Returns
+    -------
+    points: numpy array
+        Array of shape (114562, 2).
+    """
+    file_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/spiral_2d/spiral_2d.npy"
+    file_checksum = "2226024da76c073dd2f24b884baefbfd14928b52296df41ad2d9b9dc170f2401"
+
+    archive_path = _get_archive_path(file_path, "points/spiral_2d/spiral_2d.npy")
+
+    if not exists(archive_path):
+        _fetch_remote(file_url, archive_path, file_checksum)
+
+    return np.load(archive_path, mmap_mode="r")
+
+
+def fetch_bunny(file_path=None, accept_license=False):
+    """
+    Load the Stanford bunny dataset.
+
+    This dataset contains 35947 vertices.
+
+    Note that if the dataset already exists in the target location, it is not downloaded again,
+    and the corresponding array is returned from cache.
+
+    Parameters
+    ----------
+    file_path : string
+        Full path of the downloaded file including filename.
+
+        Default is None, meaning that it's set to "data_home/points/bunny/bunny.npy".
+        In this case, the LICENSE file would be downloaded as "data_home/points/bunny/bunny.LICENSE".
+
+        The "data_home" directory is set by default to "~/gudhi_data",
+        unless the 'GUDHI_DATA' environment variable is set.
+
+    accept_license : boolean
+        Flag to specify if user accepts the file LICENSE and prevents from printing the corresponding license terms.
+
+        Default is False.
+
+    Returns
+    -------
+    points: numpy array
+        Array of shape (35947, 3).
+    """
+
+    file_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/bunny/bunny.npy"
+    file_checksum = "f382482fd89df8d6444152dc8fd454444fe597581b193fd139725a85af4a6c6e"
+    license_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/bunny/bunny.LICENSE"
+    license_checksum = "b763dbe1b2fc6015d05cbf7bcc686412a2eb100a1f2220296e3b4a644c69633a"
+
+    archive_path = _get_archive_path(file_path, "points/bunny/bunny.npy")
+
+    if not exists(archive_path):
+        _fetch_remote(file_url, archive_path, file_checksum)
+        license_path = join(split(archive_path)[0], "bunny.LICENSE")
+        _fetch_remote_license(license_url, license_path, license_checksum, accept_license)
+
+    return np.load(archive_path, mmap_mode="r")
+
+
+@lru_cache(maxsize=None)
+def _load_and_cache_activity(file_path):
+    return np.load(file_path, mmap_mode="r")
+
+
+def fetch_daily_activities(file_path=None, subset=None, accept_license=False):
+    """
+    Load a subset of the Daily and Sports Activities dataset. This dataset comes from
+    https://archive.ics.uci.edu/ml/datasets/daily+and+sports+activities (CC BY 4.0 license).
+
+    Note that if the dataset already exists in the target location, it is not downloaded again,
+    and the corresponding dataset is read from cache.
+
+    Parameters
+    ----------
+    file_path : string
+        Full path of the downloaded file including filename.
+
+        Default is None, meaning that it's set to "data_home/points/activities/activities_p1_left_leg.npy".
+        In this case, the LICENSE file would be downloaded as "data_home/points/activities/activities.LICENSE".
+
+        The "data_home" directory is set by default to "~/gudhi_data",
+        unless the 'GUDHI_DATA' environment variable is set.
+
+    subset : string
+        This argument allows to download the following subsets:
+         * 'cross_training' Only left leg magnetometer of cross training activity performed by the person 1. It contains 7.500 vertices in dimension 3.
+         * 'jumping' Only left leg magnetometer of jumping activity performed by the person 1. It contains 7.500 vertices in dimension 3.
+         * 'stepper' Only left leg magnetometer of stepper activity performed by the person 1. It contains 7.500 vertices in dimension 3.
+         * 'walking' Only left leg magnetometer of walking activity performed by the person 1. It contains 7.500 vertices in dimension 3.
+         * None (default value) This dataset contains 30.000 vertices in dimension 3 + activity type column (`14.` for 'cross_training', `18.` for 'jumping', `13.` for 'stepper', or `9.` for 'walking')
+
+    accept_license : boolean
+        Flag to specify if user accepts the file LICENSE and prevents from printing the corresponding license terms.
+
+        Default is False.
+
+    Returns
+    -------
+    points: numpy array
+        Depending on subset value:
+         * Array of shape (7.500, 3, dtype = float).
+
+        Or
+         * Array of shape (30.000, 4, dtype = float) when `subset is None`.
+    """
+    # To avoid download when subset is not correct
+    if subset not in ["walking", "stepper", "cross_training", "jumping", None]:
+        raise ValueError("Unknown subset value")
+
+    file_url = "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/activities/activities_p1_left_leg.npy"
+    file_checksum = "ff813f717dbd3c8f8a95e59a7d8496d0b43c440e85a4db1f2b338cbfa9c02f25"
+    activities_license_url = (
+        "https://raw.githubusercontent.com/GUDHI/gudhi-data/main/points/activities/activities.LICENSE"
+    )
+    activities_license_checksum = "f5ce6749fa9d5359d7b0c4c37d0b61e5d9520f9494cd53be94295d3967ee4023"
+    gudhi_data_set_path = "points/activities/activities_p1_left_leg.npy"
+
+    archive_path = _get_archive_path(file_path, gudhi_data_set_path)
+    if not exists(archive_path):
+        _fetch_remote(file_url, archive_path, file_checksum)
+        license_path = join(split(archive_path)[0], "activities.LICENSE")
+        _fetch_remote_license(activities_license_url, license_path, activities_license_checksum, accept_license)
+
+    ds = _load_and_cache_activity(archive_path)
+
+    if subset is not None:
+        activity_pos = {"walking": 0, "stepper": 1, "cross_training": 2, "jumping": 3}
+        per_activity = 7500
+        start_pos = activity_pos[subset] * per_activity
+        ds = ds[start_pos : (start_pos + per_activity), 0:3]
+
+    return ds
```

## gudhi/datasets/generators/points.py

 * *Ordering differences only*

```diff
@@ -1,59 +1,59 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Hind Montassif
-#
-# Copyright (C) 2021 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-
-from ._points import ctorus
-from ._points import sphere
-
-def _generate_random_points_on_torus(n_samples, dim):
-
-    # Generate random angles of size n_samples*dim
-    alpha = 2*np.pi*np.random.rand(n_samples*dim)
-
-    # Based on angles, construct points of size n_samples*dim on a circle and reshape the result in a n_samples*2*dim array
-    array_points = np.column_stack([np.cos(alpha), np.sin(alpha)]).reshape(-1, 2*dim)
-
-    return array_points
-
-def _generate_grid_points_on_torus(n_samples, dim):
-
-    # Generate points on a dim-torus as a grid
-    n_samples_grid = int((n_samples+.5)**(1./dim)) # add .5 to avoid rounding down with numerical approximations
-    alpha = np.linspace(0, 2*np.pi, n_samples_grid, endpoint=False)
-
-    array_points = np.column_stack([np.cos(alpha), np.sin(alpha)])
-    array_points_idx = np.empty([n_samples_grid]*dim + [dim], dtype=int)
-    for i, x in enumerate(np.ix_(*([np.arange(n_samples_grid)]*dim))):
-        array_points_idx[...,i] = x
-    return array_points[array_points_idx].reshape(-1, 2*dim)
-
-def torus(n_samples, dim, sample='random'):
-    """
-    Generate points on a flat dim-torus in R^2dim either randomly or on a grid
-
-    :param n_samples: The number of points to be generated.
-    :param dim: The dimension of the torus on which points would be generated in R^2*dim.
-    :param sample: The sample type of the generated points. Can be 'random' or 'grid'.
-    :returns: numpy array containing the generated points on a torus.
-
-    The shape of returned numpy array is:
-
-    If sample is 'random': (n_samples, 2*dim).
-
-    If sample is 'grid': (‚åän_samples**(1./dim)‚åã**dim, 2*dim), where shape[0] is rounded down to the closest perfect 'dim'th power.
-    """
-    if sample == 'random':
-        # Generate points randomly
-        return _generate_random_points_on_torus(n_samples, dim)
-    elif sample == 'grid':
-        # Generate points on a grid
-        return _generate_grid_points_on_torus(n_samples, dim)
-    else:
-        raise ValueError("Sample type '{}' is not supported".format(sample))
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Hind Montassif
+#
+# Copyright (C) 2021 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+
+from ._points import ctorus
+from ._points import sphere
+
+def _generate_random_points_on_torus(n_samples, dim):
+
+    # Generate random angles of size n_samples*dim
+    alpha = 2*np.pi*np.random.rand(n_samples*dim)
+
+    # Based on angles, construct points of size n_samples*dim on a circle and reshape the result in a n_samples*2*dim array
+    array_points = np.column_stack([np.cos(alpha), np.sin(alpha)]).reshape(-1, 2*dim)
+
+    return array_points
+
+def _generate_grid_points_on_torus(n_samples, dim):
+
+    # Generate points on a dim-torus as a grid
+    n_samples_grid = int((n_samples+.5)**(1./dim)) # add .5 to avoid rounding down with numerical approximations
+    alpha = np.linspace(0, 2*np.pi, n_samples_grid, endpoint=False)
+
+    array_points = np.column_stack([np.cos(alpha), np.sin(alpha)])
+    array_points_idx = np.empty([n_samples_grid]*dim + [dim], dtype=int)
+    for i, x in enumerate(np.ix_(*([np.arange(n_samples_grid)]*dim))):
+        array_points_idx[...,i] = x
+    return array_points[array_points_idx].reshape(-1, 2*dim)
+
+def torus(n_samples, dim, sample='random'):
+    """
+    Generate points on a flat dim-torus in R^2dim either randomly or on a grid
+
+    :param n_samples: The number of points to be generated.
+    :param dim: The dimension of the torus on which points would be generated in R^2*dim.
+    :param sample: The sample type of the generated points. Can be 'random' or 'grid'.
+    :returns: numpy array containing the generated points on a torus.
+
+    The shape of returned numpy array is:
+
+    If sample is 'random': (n_samples, 2*dim).
+
+    If sample is 'grid': (‚åän_samples**(1./dim)‚åã**dim, 2*dim), where shape[0] is rounded down to the closest perfect 'dim'th power.
+    """
+    if sample == 'random':
+        # Generate points randomly
+        return _generate_random_points_on_torus(n_samples, dim)
+    elif sample == 'grid':
+        # Generate points on a grid
+        return _generate_grid_points_on_torus(n_samples, dim)
+    else:
+        raise ValueError("Sample type '{}' is not supported".format(sample))
```

## gudhi/hera/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .wasserstein import wasserstein_distance
-from .bottleneck import bottleneck_distance
-
-
-__author__ = "Marc Glisse"
-__copyright__ = "Copyright (C) 2020 Inria"
-__license__ = "MIT"
+from .wasserstein import wasserstein_distance
+from .bottleneck import bottleneck_distance
+
+
+__author__ = "Marc Glisse"
+__copyright__ = "Copyright (C) 2020 Inria"
+__license__ = "MIT"
```

## gudhi/point_cloud/dtm.py

 * *Ordering differences only*

```diff
@@ -1,179 +1,179 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Marc Glisse
-#
-# Copyright (C) 2020 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-from .knn import KNearestNeighbors
-import numpy as np
-
-__author__ = "Marc Glisse"
-__copyright__ = "Copyright (C) 2020 Inria"
-__license__ = "MIT"
-
-
-class DistanceToMeasure:
-    """
-    Class to compute the distance to the empirical measure defined by a point set, as introduced in :cite:`dtm`.
-    """
-
-    def __init__(self, k, q=2, **kwargs):
-        """
-        Args:
-            k (int): number of neighbors (possibly including the point itself).
-            q (float): order used to compute the distance to measure. Defaults to 2.
-            kwargs: same parameters as :class:`~gudhi.point_cloud.knn.KNearestNeighbors`, except that
-                metric="neighbors" means that :func:`transform` expects an array with the distances
-                to the k nearest neighbors.
-        """
-        self.k = k
-        self.q = q
-        self.params = kwargs
-
-    def fit_transform(self, X, y=None):
-        return self.fit(X).transform(X)
-
-    def fit(self, X, y=None):
-        """
-        Args:
-            X (numpy.array): coordinates for mass points.
-        """
-        if self.params.setdefault("metric", "euclidean") != "neighbors":
-            self.knn = KNearestNeighbors(
-                self.k, return_index=False, return_distance=True, sort_results=False, **self.params
-            )
-            self.knn.fit(X)
-        return self
-
-    def transform(self, X):
-        """
-        Args:
-            X (numpy.array): coordinates for query points, or distance matrix if metric is "precomputed",
-                or distances to the k nearest neighbors if metric is "neighbors" (if the array has more
-                than k columns, the remaining ones are ignored).
-
-        Returns:
-            numpy.array: a 1-d array with, for each point of X, its distance to the measure defined
-            by the argument of :func:`fit`.
-        """
-        if self.params["metric"] == "neighbors":
-            distances = X[:, : self.k]
-        else:
-            distances = self.knn.transform(X)
-        distances = distances ** self.q
-        dtm = distances.sum(-1) / self.k
-        dtm = dtm ** (1.0 / self.q)
-        # We compute too many powers, 1/p in knn then q in dtm, 1/q in dtm then q or some log in the caller.
-        # Add option to skip the final root?
-        return dtm
-
-
-class DTMDensity:
-    """
-    Density estimator based on the distance to the empirical measure defined by a point set, as defined
-    in :cite:`dtmdensity`. Note that this implementation only renormalizes when asked, and the renormalization
-    only works for a Euclidean metric, so in other cases the total measure may not be 1.
-
-    .. note:: When the dimension is high, using it as an exponent can quickly lead to under- or overflows.
-        We recommend using a small fixed value instead in those cases, even if it won't have the same nice
-        theoretical properties as the dimension.
-    """
-
-    def __init__(self, k=None, weights=None, q=None, dim=None, normalize=False, n_samples=None, **kwargs):
-        """
-        Args:
-            k (int): number of neighbors (possibly including the point itself). Optional if it can be guessed
-                from weights or metric="neighbors".
-            weights (numpy.array): weights of each of the k neighbors, optional. They are supposed to sum to 1.
-            q (float): order used to compute the distance to measure. Defaults to dim.
-            dim (float): final exponent representing the dimension. Defaults to the dimension, and must be specified
-                when the dimension cannot be read from the input (metric is "neighbors" or "precomputed").
-            normalize (bool): normalize the density so it corresponds to a probability measure on ‚Ñù·µà.
-                Only available for the Euclidean metric, defaults to False.
-            n_samples (int): number of sample points used for fitting. Only needed if `normalize` is True and
-                metric is "neighbors".
-            kwargs: same parameters as :class:`~gudhi.point_cloud.knn.KNearestNeighbors`, except that
-                metric="neighbors" means that :func:`transform` expects an array with the distances to
-                the k nearest neighbors.
-        """
-        if weights is None:
-            self.k = k
-            if k is None:
-                assert kwargs.get("metric") == "neighbors", 'Must specify k or weights, unless metric is "neighbors"'
-                self.weights = None
-            else:
-                self.weights = np.full(k, 1.0 / k)
-        else:
-            self.weights = weights
-            self.k = len(weights)
-            assert k is None or k == self.k, "k differs from the length of weights"
-        self.q = q
-        self.dim = dim
-        self.params = kwargs
-        self.normalize = normalize
-        self.n_samples = n_samples
-
-    def fit_transform(self, X, y=None):
-        return self.fit(X).transform(X)
-
-    def fit(self, X, y=None):
-        """
-        Args:
-            X (numpy.array): coordinates for mass points.
-        """
-        if self.params.setdefault("metric", "euclidean") != "neighbors":
-            self.knn = KNearestNeighbors(
-                self.k, return_index=False, return_distance=True, sort_results=False, **self.params
-            )
-            self.knn.fit(X)
-            if self.params["metric"] != "precomputed":
-                self.n_samples = len(X)
-        return self
-
-    def transform(self, X):
-        """
-        Args:
-            X (numpy.array): coordinates for query points, or distance matrix if metric is "precomputed",
-                or distances to the k nearest neighbors if metric is "neighbors" (if the array has more
-                than k columns, the remaining ones are ignored).
-        """
-        q = self.q
-        dim = self.dim
-        if dim is None:
-            assert self.params["metric"] not in {
-                "neighbors",
-                "precomputed",
-            }, "dim not specified and cannot guess the dimension"
-            dim = len(X[0])
-        if q is None:
-            q = dim
-        k = self.k
-        weights = self.weights
-        if self.params["metric"] == "neighbors":
-            distances = np.asarray(X)
-            if weights is None:
-                k = distances.shape[1]
-                weights = np.full(k, 1.0 / k)
-            else:
-                distances = distances[:, :k]
-        else:
-            distances = self.knn.transform(X)
-        distances = distances ** q
-        dtm = (distances * weights).sum(-1)
-        if self.normalize:
-            dtm /= (np.arange(1, k + 1) ** (q / dim) * weights).sum()
-        density = dtm ** (-dim / q)
-        if self.normalize:
-            import math
-
-            if self.params["metric"] == "precomputed":
-                self.n_samples = len(X[0])
-            # Volume of d-ball
-            Vd = math.pi ** (dim / 2) / math.gamma(dim / 2 + 1)
-            density /= self.n_samples * Vd
-        return density
-        # We compute too many powers, 1/p in knn then q in dtm, d/q in dtm then whatever in the caller.
-        # Add option to skip the final root?
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Marc Glisse
+#
+# Copyright (C) 2020 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+from .knn import KNearestNeighbors
+import numpy as np
+
+__author__ = "Marc Glisse"
+__copyright__ = "Copyright (C) 2020 Inria"
+__license__ = "MIT"
+
+
+class DistanceToMeasure:
+    """
+    Class to compute the distance to the empirical measure defined by a point set, as introduced in :cite:`dtm`.
+    """
+
+    def __init__(self, k, q=2, **kwargs):
+        """
+        Args:
+            k (int): number of neighbors (possibly including the point itself).
+            q (float): order used to compute the distance to measure. Defaults to 2.
+            kwargs: same parameters as :class:`~gudhi.point_cloud.knn.KNearestNeighbors`, except that
+                metric="neighbors" means that :func:`transform` expects an array with the distances
+                to the k nearest neighbors.
+        """
+        self.k = k
+        self.q = q
+        self.params = kwargs
+
+    def fit_transform(self, X, y=None):
+        return self.fit(X).transform(X)
+
+    def fit(self, X, y=None):
+        """
+        Args:
+            X (numpy.array): coordinates for mass points.
+        """
+        if self.params.setdefault("metric", "euclidean") != "neighbors":
+            self.knn = KNearestNeighbors(
+                self.k, return_index=False, return_distance=True, sort_results=False, **self.params
+            )
+            self.knn.fit(X)
+        return self
+
+    def transform(self, X):
+        """
+        Args:
+            X (numpy.array): coordinates for query points, or distance matrix if metric is "precomputed",
+                or distances to the k nearest neighbors if metric is "neighbors" (if the array has more
+                than k columns, the remaining ones are ignored).
+
+        Returns:
+            numpy.array: a 1-d array with, for each point of X, its distance to the measure defined
+            by the argument of :func:`fit`.
+        """
+        if self.params["metric"] == "neighbors":
+            distances = X[:, : self.k]
+        else:
+            distances = self.knn.transform(X)
+        distances = distances ** self.q
+        dtm = distances.sum(-1) / self.k
+        dtm = dtm ** (1.0 / self.q)
+        # We compute too many powers, 1/p in knn then q in dtm, 1/q in dtm then q or some log in the caller.
+        # Add option to skip the final root?
+        return dtm
+
+
+class DTMDensity:
+    """
+    Density estimator based on the distance to the empirical measure defined by a point set, as defined
+    in :cite:`dtmdensity`. Note that this implementation only renormalizes when asked, and the renormalization
+    only works for a Euclidean metric, so in other cases the total measure may not be 1.
+
+    .. note:: When the dimension is high, using it as an exponent can quickly lead to under- or overflows.
+        We recommend using a small fixed value instead in those cases, even if it won't have the same nice
+        theoretical properties as the dimension.
+    """
+
+    def __init__(self, k=None, weights=None, q=None, dim=None, normalize=False, n_samples=None, **kwargs):
+        """
+        Args:
+            k (int): number of neighbors (possibly including the point itself). Optional if it can be guessed
+                from weights or metric="neighbors".
+            weights (numpy.array): weights of each of the k neighbors, optional. They are supposed to sum to 1.
+            q (float): order used to compute the distance to measure. Defaults to dim.
+            dim (float): final exponent representing the dimension. Defaults to the dimension, and must be specified
+                when the dimension cannot be read from the input (metric is "neighbors" or "precomputed").
+            normalize (bool): normalize the density so it corresponds to a probability measure on ‚Ñù·µà.
+                Only available for the Euclidean metric, defaults to False.
+            n_samples (int): number of sample points used for fitting. Only needed if `normalize` is True and
+                metric is "neighbors".
+            kwargs: same parameters as :class:`~gudhi.point_cloud.knn.KNearestNeighbors`, except that
+                metric="neighbors" means that :func:`transform` expects an array with the distances to
+                the k nearest neighbors.
+        """
+        if weights is None:
+            self.k = k
+            if k is None:
+                assert kwargs.get("metric") == "neighbors", 'Must specify k or weights, unless metric is "neighbors"'
+                self.weights = None
+            else:
+                self.weights = np.full(k, 1.0 / k)
+        else:
+            self.weights = weights
+            self.k = len(weights)
+            assert k is None or k == self.k, "k differs from the length of weights"
+        self.q = q
+        self.dim = dim
+        self.params = kwargs
+        self.normalize = normalize
+        self.n_samples = n_samples
+
+    def fit_transform(self, X, y=None):
+        return self.fit(X).transform(X)
+
+    def fit(self, X, y=None):
+        """
+        Args:
+            X (numpy.array): coordinates for mass points.
+        """
+        if self.params.setdefault("metric", "euclidean") != "neighbors":
+            self.knn = KNearestNeighbors(
+                self.k, return_index=False, return_distance=True, sort_results=False, **self.params
+            )
+            self.knn.fit(X)
+            if self.params["metric"] != "precomputed":
+                self.n_samples = len(X)
+        return self
+
+    def transform(self, X):
+        """
+        Args:
+            X (numpy.array): coordinates for query points, or distance matrix if metric is "precomputed",
+                or distances to the k nearest neighbors if metric is "neighbors" (if the array has more
+                than k columns, the remaining ones are ignored).
+        """
+        q = self.q
+        dim = self.dim
+        if dim is None:
+            assert self.params["metric"] not in {
+                "neighbors",
+                "precomputed",
+            }, "dim not specified and cannot guess the dimension"
+            dim = len(X[0])
+        if q is None:
+            q = dim
+        k = self.k
+        weights = self.weights
+        if self.params["metric"] == "neighbors":
+            distances = np.asarray(X)
+            if weights is None:
+                k = distances.shape[1]
+                weights = np.full(k, 1.0 / k)
+            else:
+                distances = distances[:, :k]
+        else:
+            distances = self.knn.transform(X)
+        distances = distances ** q
+        dtm = (distances * weights).sum(-1)
+        if self.normalize:
+            dtm /= (np.arange(1, k + 1) ** (q / dim) * weights).sum()
+        density = dtm ** (-dim / q)
+        if self.normalize:
+            import math
+
+            if self.params["metric"] == "precomputed":
+                self.n_samples = len(X[0])
+            # Volume of d-ball
+            Vd = math.pi ** (dim / 2) / math.gamma(dim / 2 + 1)
+            density /= self.n_samples * Vd
+        return density
+        # We compute too many powers, 1/p in knn then q in dtm, d/q in dtm then whatever in the caller.
+        # Add option to skip the final root?
```

## gudhi/point_cloud/knn.py

 * *Ordering differences only*

```diff
@@ -1,344 +1,344 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Marc Glisse
-#
-# Copyright (C) 2020 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy
-import warnings
-
-# TODO: https://github.com/facebookresearch/faiss
-
-__author__ = "Marc Glisse"
-__copyright__ = "Copyright (C) 2020 Inria"
-__license__ = "MIT"
-
-
-class KNearestNeighbors:
-    """
-    Class wrapping several implementations for computing the k nearest neighbors in a point set.
-
-    :Requires: `PyKeOps <installation.html#pykeops>`_, `SciPy <installation.html#scipy>`_,
-        `Scikit-learn <installation.html#scikit-learn>`_, and/or `Hnswlib <installation.html#hnswlib>`_
-        in function of the selected `implementation`.
-    """
-
-    def __init__(self, k, return_index=True, return_distance=False, metric="euclidean", **kwargs):
-        """
-        Args:
-            k (int): number of neighbors (possibly including the point itself).
-            return_index (bool): if True, return the index of each neighbor.
-            return_distance (bool): if True, return the distance to each neighbor.
-            implementation (str): choice of the library that does the real work.
-
-                * 'keops' for a brute-force, CUDA implementation through pykeops. Useful when the dimension becomes large (10+) but the number of points remains low (less than a million). Only "minkowski" and its aliases are supported.
-                * 'ckdtree' for scipy's cKDTree. Only "minkowski" and its aliases are supported.
-                * 'sklearn' for scikit-learn's NearestNeighbors. Note that this provides in particular an option algorithm="brute".
-                * 'hnsw' for hnswlib.Index. It can be very fast but does not provide guarantees. Only supports "euclidean" for now.
-                * None will try to select a sensible one (scipy if possible, scikit-learn otherwise).
-            metric (str): see `sklearn.neighbors.NearestNeighbors`.
-            eps (float): relative error when computing nearest neighbors with the cKDTree.
-            p (float): norm L^p on input points (including numpy.inf) if metric is "minkowski". Defaults to 2.
-            n_jobs (int): number of jobs to schedule for parallel processing of nearest neighbors on the CPU.
-                If -1 is given all processors are used. Default: 1.
-            sort_results (bool): if True, then distances and indices of each point are
-                sorted on return, so that the first column contains the closest points.
-                Otherwise, neighbors are returned in an arbitrary order. Defaults to True.
-            enable_autodiff (bool): if the input is a torch.tensor or tensorflow.Tensor, this
-                instructs the function to compute distances in a way that works with automatic differentiation.
-                This is experimental, not supported for all metrics, and requires the package EagerPy.
-                Defaults to False.
-            kwargs: additional parameters are forwarded to the backends.
-        """
-        self.k = k
-        self.return_index = return_index
-        self.return_distance = return_distance
-        self.metric = metric
-        self.params = kwargs
-        # canonicalize
-        if metric == "euclidean":
-            self.params["p"] = 2
-            self.metric = "minkowski"
-        elif metric == "manhattan":
-            self.params["p"] = 1
-            self.metric = "minkowski"
-        elif metric == "chebyshev":
-            self.params["p"] = numpy.inf
-            self.metric = "minkowski"
-        elif metric == "minkowski":
-            self.params["p"] = kwargs.get("p", 2)
-        if self.params.get("implementation") in {"keops", "ckdtree"}:
-            assert self.metric == "minkowski"
-        if self.params.get("implementation") == "hnsw":
-            assert self.metric == "minkowski" and self.params["p"] == 2
-        if not self.params.get("implementation"):
-            if self.metric == "minkowski":
-                self.params["implementation"] = "ckdtree"
-            else:
-                self.params["implementation"] = "sklearn"
-        if not return_distance:
-            self.params["enable_autodiff"] = False
-
-    def fit_transform(self, X, y=None):
-        return self.fit(X).transform(X)
-
-    def fit(self, X, y=None):
-        """
-        Args:
-            X (numpy.array): coordinates for reference points.
-        """
-        self.ref_points = X
-        if self.params.get("enable_autodiff", False):
-            import eagerpy as ep
-
-            X = ep.astensor(X)
-            if self.params["implementation"] != "keops" or not isinstance(X, ep.PyTorchTensor):
-                # I don't know a clever way to reuse a GPU tensor from tensorflow in pytorch
-                # without copying to/from the CPU.
-                X = X.numpy()
-        if self.params["implementation"] == "ckdtree":
-            # sklearn could handle this, but it is much slower
-            from scipy.spatial import cKDTree
-
-            self.kdtree = cKDTree(X)
-
-        if self.params["implementation"] == "sklearn" and self.metric != "precomputed":
-            # FIXME: sklearn badly handles "precomputed"
-            from sklearn.neighbors import NearestNeighbors
-
-            nargs = {
-                k: v for k, v in self.params.items() if k in {"p", "n_jobs", "metric_params", "algorithm", "leaf_size"}
-            }
-            self.nn = NearestNeighbors(n_neighbors=self.k, metric=self.metric, **nargs)
-            self.nn.fit(X)
-
-        if self.params["implementation"] == "hnsw":
-            import hnswlib
-
-            self.graph = hnswlib.Index("l2", len(X[0]))  # Actually returns squared distances
-            self.graph.init_index(
-                len(X), **{k: v for k, v in self.params.items() if k in {"ef_construction", "M", "random_seed"}}
-            )
-            n = self.params.get("num_threads")
-            if n is None:
-                n = self.params.get("n_jobs", 1)
-                self.params["num_threads"] = n
-            self.graph.add_items(X, num_threads=n)
-
-        return self
-
-    def transform(self, X):
-        """
-        Args:
-            X (numpy.array): coordinates for query points, or distance matrix if metric is "precomputed".
-
-        Returns:
-            numpy.array: if return_index, an array of shape (len(X), k) with the indices (in the argument
-            of :func:`fit`) of the k nearest neighbors to the points of X. If return_distance, an array of the
-            same shape with the distances to those neighbors. If both, a tuple with the two arrays, in this order.
-        """
-        if self.params.get("enable_autodiff", False):
-            # pykeops does not support autodiff for kmin yet, but when it does in the future,
-            # we may want a special path.
-            import eagerpy as ep
-
-            save_return_index = self.return_index
-            self.return_index = True
-            self.return_distance = False
-            self.params["enable_autodiff"] = False
-            try:
-                newX = ep.astensor(X)
-                if self.params["implementation"] != "keops" or (
-                    not isinstance(newX, ep.PyTorchTensor) and not isinstance(newX, ep.NumPyTensor)
-                ):
-                    newX = newX.numpy()
-                else:
-                    newX = newX.raw
-                neighbors = self.transform(newX)
-            finally:
-                self.return_index = save_return_index
-                self.return_distance = True
-                self.params["enable_autodiff"] = True
-            # We can implement more later as needed
-            assert self.metric == "minkowski"
-            p = self.params["p"]
-            Y = ep.astensor(self.ref_points)
-            neighbor_pts = Y[
-                neighbors,
-            ]
-            diff = neighbor_pts - X[:, None, :]
-            if isinstance(diff, ep.PyTorchTensor):
-                # https://github.com/jonasrauber/eagerpy/issues/6
-                distances = ep.astensor(diff.raw.norm(p, -1))
-            else:
-                distances = diff.norms.lp(p, -1)
-            if self.return_index:
-                return neighbors, distances.raw
-            else:
-                return distances.raw
-
-        metric = self.metric
-        k = self.k
-
-        if metric == "precomputed":
-            # scikit-learn could handle that, but they insist on calling fit() with an unused square array, which is too unnatural.
-            if self.return_index:
-                n_jobs = self.params.get("n_jobs", 1)
-                # Supposedly numpy can be compiled with OpenMP and handle this, but nobody does that?!
-                if n_jobs == 1:
-                    neighbors = numpy.argpartition(X, k - 1)[:, 0:k]
-                    if self.params.get("sort_results", True):
-                        X = numpy.take_along_axis(X, neighbors, axis=-1)
-                        ngb_order = numpy.argsort(X, axis=-1)
-                        neighbors = numpy.take_along_axis(neighbors, ngb_order, axis=-1)
-                    else:
-                        ngb_order = neighbors
-                    if self.return_distance:
-                        distances = numpy.take_along_axis(X, ngb_order, axis=-1)
-                        return neighbors, distances
-                    else:
-                        return neighbors
-                else:
-                    from joblib import Parallel, delayed, effective_n_jobs
-                    from sklearn.utils import gen_even_slices
-
-                    slices = gen_even_slices(len(X), effective_n_jobs(n_jobs))
-                    parallel = Parallel(prefer="threads", n_jobs=n_jobs)
-                    if self.params.get("sort_results", True):
-
-                        def func(M):
-                            neighbors = numpy.argpartition(M, k - 1)[:, 0:k]
-                            Y = numpy.take_along_axis(M, neighbors, axis=-1)
-                            ngb_order = numpy.argsort(Y, axis=-1)
-                            return numpy.take_along_axis(neighbors, ngb_order, axis=-1)
-
-                    else:
-
-                        def func(M):
-                            return numpy.argpartition(M, k - 1)[:, 0:k]
-
-                    neighbors = numpy.concatenate(parallel(delayed(func)(X[s]) for s in slices))
-                    if self.return_distance:
-                        distances = numpy.take_along_axis(X, neighbors, axis=-1)
-                        return neighbors, distances
-                    else:
-                        return neighbors
-            if self.return_distance:
-                n_jobs = self.params.get("n_jobs", 1)
-                if n_jobs == 1:
-                    distances = numpy.partition(X, k - 1)[:, 0:k]
-                    if self.params.get("sort_results"):
-                        # partition is not guaranteed to sort the lower half, although it often does
-                        distances.sort(axis=-1)
-                else:
-                    from joblib import Parallel, delayed, effective_n_jobs
-                    from sklearn.utils import gen_even_slices
-
-                    if self.params.get("sort_results"):
-
-                        def func(M):
-                            # Not partitioning in place, because we should not modify the user's array?
-                            r = numpy.partition(M, k - 1)[:, 0:k]
-                            r.sort(axis=-1)
-                            return r
-
-                    else:
-                        func = lambda M: numpy.partition(M, k - 1)[:, 0:k]
-                    slices = gen_even_slices(len(X), effective_n_jobs(n_jobs))
-                    parallel = Parallel(prefer="threads", n_jobs=n_jobs)
-                    distances = numpy.concatenate(parallel(delayed(func)(X[s]) for s in slices))
-                return distances
-            return None
-
-        if self.params["implementation"] == "hnsw":
-            ef = self.params.get("ef")
-            if ef is not None:
-                self.graph.set_ef(ef)
-            neighbors, distances = self.graph.knn_query(X, k, num_threads=self.params["num_threads"])
-            with warnings.catch_warnings():
-                if not(numpy.all(numpy.isfinite(distances))):
-                    warnings.warn("Overflow/infinite value encountered while computing 'distances'", RuntimeWarning)
-            # The k nearest neighbors are always sorted. I couldn't find it in the doc, but the code calls searchKnn,
-            # which returns a priority_queue, and then fills the return array backwards with top/pop on the queue.
-            if self.return_index:
-                if self.return_distance:
-                    return neighbors, numpy.sqrt(distances)
-                else:
-                    return neighbors
-            if self.return_distance:
-                return numpy.sqrt(distances)
-            return None
-
-        if self.params["implementation"] == "keops":
-            import torch
-            from pykeops.torch import LazyTensor
-
-            # 'float64' is slow except on super expensive GPUs. Allow it with some param?
-            XX = torch.as_tensor(X, dtype=torch.float32)
-            if X is self.ref_points:
-                YY = XX
-            else:
-                YY = torch.as_tensor(self.ref_points, dtype=torch.float32)
-            p = self.params["p"]
-            if p == numpy.inf:
-                # Requires pykeops 1.4 or later
-                mat = (LazyTensor(XX[:, None, :]) - LazyTensor(YY[None, :, :])).abs().max(-1)
-            elif p == 2:  # Any even integer?
-                mat = ((LazyTensor(XX[:, None, :]) - LazyTensor(YY[None, :, :])) ** p).sum(-1)
-            else:
-                mat = ((LazyTensor(XX[:, None, :]) - LazyTensor(YY[None, :, :])).abs() ** p).sum(-1)
-
-            if self.return_index:
-                if self.return_distance:
-                    distances, neighbors = mat.Kmin_argKmin(k, dim=1)
-                    with warnings.catch_warnings():
-                        if not(torch.isfinite(distances).all()):
-                            warnings.warn("Overflow/infinite value encountered while computing 'distances'", RuntimeWarning)
-                    if p != numpy.inf:
-                        distances = distances ** (1.0 / p)
-                    return neighbors, distances
-                else:
-                    neighbors = mat.argKmin(k, dim=1)
-                    return neighbors
-            if self.return_distance:
-                distances = mat.Kmin(k, dim=1)
-                with warnings.catch_warnings():
-                    if not(torch.isfinite(distances).all()):
-                        warnings.warn("Overflow/infinite value encountered while computing 'distances'", RuntimeWarning)
-                if p != numpy.inf:
-                    distances = distances ** (1.0 / p)
-                return distances
-            return None
-
-        if self.params["implementation"] == "ckdtree":
-            qargs = {key: val for key, val in self.params.items() if key in {"p", "eps"}}
-            # SciPy renamed n_jobs to workers
-            qargs["workers"] = self.params.get("workers") or self.params.get("n_jobs") or 1
-            distances, neighbors = self.kdtree.query(X, k=self.k, **qargs)
-            if k == 1:
-                # SciPy decided to squeeze the last dimension for k=1
-                distances = distances[:, None]
-                neighbors = neighbors[:, None]
-            if self.return_index:
-                if self.return_distance:
-                    return neighbors, distances
-                else:
-                    return neighbors
-            if self.return_distance:
-                return distances
-            return None
-
-        assert self.params["implementation"] == "sklearn"
-        if self.return_distance:
-            distances, neighbors = self.nn.kneighbors(X, return_distance=True)
-            if self.return_index:
-                return neighbors, distances
-            else:
-                return distances
-        if self.return_index:
-            neighbors = self.nn.kneighbors(X, return_distance=False)
-            return neighbors
-        return None
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Marc Glisse
+#
+# Copyright (C) 2020 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy
+import warnings
+
+# TODO: https://github.com/facebookresearch/faiss
+
+__author__ = "Marc Glisse"
+__copyright__ = "Copyright (C) 2020 Inria"
+__license__ = "MIT"
+
+
+class KNearestNeighbors:
+    """
+    Class wrapping several implementations for computing the k nearest neighbors in a point set.
+
+    :Requires: `PyKeOps <installation.html#pykeops>`_, `SciPy <installation.html#scipy>`_,
+        `Scikit-learn <installation.html#scikit-learn>`_, and/or `Hnswlib <installation.html#hnswlib>`_
+        in function of the selected `implementation`.
+    """
+
+    def __init__(self, k, return_index=True, return_distance=False, metric="euclidean", **kwargs):
+        """
+        Args:
+            k (int): number of neighbors (possibly including the point itself).
+            return_index (bool): if True, return the index of each neighbor.
+            return_distance (bool): if True, return the distance to each neighbor.
+            implementation (str): choice of the library that does the real work.
+
+                * 'keops' for a brute-force, CUDA implementation through pykeops. Useful when the dimension becomes large (10+) but the number of points remains low (less than a million). Only "minkowski" and its aliases are supported.
+                * 'ckdtree' for scipy's cKDTree. Only "minkowski" and its aliases are supported.
+                * 'sklearn' for scikit-learn's NearestNeighbors. Note that this provides in particular an option algorithm="brute".
+                * 'hnsw' for hnswlib.Index. It can be very fast but does not provide guarantees. Only supports "euclidean" for now.
+                * None will try to select a sensible one (scipy if possible, scikit-learn otherwise).
+            metric (str): see `sklearn.neighbors.NearestNeighbors`.
+            eps (float): relative error when computing nearest neighbors with the cKDTree.
+            p (float): norm L^p on input points (including numpy.inf) if metric is "minkowski". Defaults to 2.
+            n_jobs (int): number of jobs to schedule for parallel processing of nearest neighbors on the CPU.
+                If -1 is given all processors are used. Default: 1.
+            sort_results (bool): if True, then distances and indices of each point are
+                sorted on return, so that the first column contains the closest points.
+                Otherwise, neighbors are returned in an arbitrary order. Defaults to True.
+            enable_autodiff (bool): if the input is a torch.tensor or tensorflow.Tensor, this
+                instructs the function to compute distances in a way that works with automatic differentiation.
+                This is experimental, not supported for all metrics, and requires the package EagerPy.
+                Defaults to False.
+            kwargs: additional parameters are forwarded to the backends.
+        """
+        self.k = k
+        self.return_index = return_index
+        self.return_distance = return_distance
+        self.metric = metric
+        self.params = kwargs
+        # canonicalize
+        if metric == "euclidean":
+            self.params["p"] = 2
+            self.metric = "minkowski"
+        elif metric == "manhattan":
+            self.params["p"] = 1
+            self.metric = "minkowski"
+        elif metric == "chebyshev":
+            self.params["p"] = numpy.inf
+            self.metric = "minkowski"
+        elif metric == "minkowski":
+            self.params["p"] = kwargs.get("p", 2)
+        if self.params.get("implementation") in {"keops", "ckdtree"}:
+            assert self.metric == "minkowski"
+        if self.params.get("implementation") == "hnsw":
+            assert self.metric == "minkowski" and self.params["p"] == 2
+        if not self.params.get("implementation"):
+            if self.metric == "minkowski":
+                self.params["implementation"] = "ckdtree"
+            else:
+                self.params["implementation"] = "sklearn"
+        if not return_distance:
+            self.params["enable_autodiff"] = False
+
+    def fit_transform(self, X, y=None):
+        return self.fit(X).transform(X)
+
+    def fit(self, X, y=None):
+        """
+        Args:
+            X (numpy.array): coordinates for reference points.
+        """
+        self.ref_points = X
+        if self.params.get("enable_autodiff", False):
+            import eagerpy as ep
+
+            X = ep.astensor(X)
+            if self.params["implementation"] != "keops" or not isinstance(X, ep.PyTorchTensor):
+                # I don't know a clever way to reuse a GPU tensor from tensorflow in pytorch
+                # without copying to/from the CPU.
+                X = X.numpy()
+        if self.params["implementation"] == "ckdtree":
+            # sklearn could handle this, but it is much slower
+            from scipy.spatial import cKDTree
+
+            self.kdtree = cKDTree(X)
+
+        if self.params["implementation"] == "sklearn" and self.metric != "precomputed":
+            # FIXME: sklearn badly handles "precomputed"
+            from sklearn.neighbors import NearestNeighbors
+
+            nargs = {
+                k: v for k, v in self.params.items() if k in {"p", "n_jobs", "metric_params", "algorithm", "leaf_size"}
+            }
+            self.nn = NearestNeighbors(n_neighbors=self.k, metric=self.metric, **nargs)
+            self.nn.fit(X)
+
+        if self.params["implementation"] == "hnsw":
+            import hnswlib
+
+            self.graph = hnswlib.Index("l2", len(X[0]))  # Actually returns squared distances
+            self.graph.init_index(
+                len(X), **{k: v for k, v in self.params.items() if k in {"ef_construction", "M", "random_seed"}}
+            )
+            n = self.params.get("num_threads")
+            if n is None:
+                n = self.params.get("n_jobs", 1)
+                self.params["num_threads"] = n
+            self.graph.add_items(X, num_threads=n)
+
+        return self
+
+    def transform(self, X):
+        """
+        Args:
+            X (numpy.array): coordinates for query points, or distance matrix if metric is "precomputed".
+
+        Returns:
+            numpy.array: if return_index, an array of shape (len(X), k) with the indices (in the argument
+            of :func:`fit`) of the k nearest neighbors to the points of X. If return_distance, an array of the
+            same shape with the distances to those neighbors. If both, a tuple with the two arrays, in this order.
+        """
+        if self.params.get("enable_autodiff", False):
+            # pykeops does not support autodiff for kmin yet, but when it does in the future,
+            # we may want a special path.
+            import eagerpy as ep
+
+            save_return_index = self.return_index
+            self.return_index = True
+            self.return_distance = False
+            self.params["enable_autodiff"] = False
+            try:
+                newX = ep.astensor(X)
+                if self.params["implementation"] != "keops" or (
+                    not isinstance(newX, ep.PyTorchTensor) and not isinstance(newX, ep.NumPyTensor)
+                ):
+                    newX = newX.numpy()
+                else:
+                    newX = newX.raw
+                neighbors = self.transform(newX)
+            finally:
+                self.return_index = save_return_index
+                self.return_distance = True
+                self.params["enable_autodiff"] = True
+            # We can implement more later as needed
+            assert self.metric == "minkowski"
+            p = self.params["p"]
+            Y = ep.astensor(self.ref_points)
+            neighbor_pts = Y[
+                neighbors,
+            ]
+            diff = neighbor_pts - X[:, None, :]
+            if isinstance(diff, ep.PyTorchTensor):
+                # https://github.com/jonasrauber/eagerpy/issues/6
+                distances = ep.astensor(diff.raw.norm(p, -1))
+            else:
+                distances = diff.norms.lp(p, -1)
+            if self.return_index:
+                return neighbors, distances.raw
+            else:
+                return distances.raw
+
+        metric = self.metric
+        k = self.k
+
+        if metric == "precomputed":
+            # scikit-learn could handle that, but they insist on calling fit() with an unused square array, which is too unnatural.
+            if self.return_index:
+                n_jobs = self.params.get("n_jobs", 1)
+                # Supposedly numpy can be compiled with OpenMP and handle this, but nobody does that?!
+                if n_jobs == 1:
+                    neighbors = numpy.argpartition(X, k - 1)[:, 0:k]
+                    if self.params.get("sort_results", True):
+                        X = numpy.take_along_axis(X, neighbors, axis=-1)
+                        ngb_order = numpy.argsort(X, axis=-1)
+                        neighbors = numpy.take_along_axis(neighbors, ngb_order, axis=-1)
+                    else:
+                        ngb_order = neighbors
+                    if self.return_distance:
+                        distances = numpy.take_along_axis(X, ngb_order, axis=-1)
+                        return neighbors, distances
+                    else:
+                        return neighbors
+                else:
+                    from joblib import Parallel, delayed, effective_n_jobs
+                    from sklearn.utils import gen_even_slices
+
+                    slices = gen_even_slices(len(X), effective_n_jobs(n_jobs))
+                    parallel = Parallel(prefer="threads", n_jobs=n_jobs)
+                    if self.params.get("sort_results", True):
+
+                        def func(M):
+                            neighbors = numpy.argpartition(M, k - 1)[:, 0:k]
+                            Y = numpy.take_along_axis(M, neighbors, axis=-1)
+                            ngb_order = numpy.argsort(Y, axis=-1)
+                            return numpy.take_along_axis(neighbors, ngb_order, axis=-1)
+
+                    else:
+
+                        def func(M):
+                            return numpy.argpartition(M, k - 1)[:, 0:k]
+
+                    neighbors = numpy.concatenate(parallel(delayed(func)(X[s]) for s in slices))
+                    if self.return_distance:
+                        distances = numpy.take_along_axis(X, neighbors, axis=-1)
+                        return neighbors, distances
+                    else:
+                        return neighbors
+            if self.return_distance:
+                n_jobs = self.params.get("n_jobs", 1)
+                if n_jobs == 1:
+                    distances = numpy.partition(X, k - 1)[:, 0:k]
+                    if self.params.get("sort_results"):
+                        # partition is not guaranteed to sort the lower half, although it often does
+                        distances.sort(axis=-1)
+                else:
+                    from joblib import Parallel, delayed, effective_n_jobs
+                    from sklearn.utils import gen_even_slices
+
+                    if self.params.get("sort_results"):
+
+                        def func(M):
+                            # Not partitioning in place, because we should not modify the user's array?
+                            r = numpy.partition(M, k - 1)[:, 0:k]
+                            r.sort(axis=-1)
+                            return r
+
+                    else:
+                        func = lambda M: numpy.partition(M, k - 1)[:, 0:k]
+                    slices = gen_even_slices(len(X), effective_n_jobs(n_jobs))
+                    parallel = Parallel(prefer="threads", n_jobs=n_jobs)
+                    distances = numpy.concatenate(parallel(delayed(func)(X[s]) for s in slices))
+                return distances
+            return None
+
+        if self.params["implementation"] == "hnsw":
+            ef = self.params.get("ef")
+            if ef is not None:
+                self.graph.set_ef(ef)
+            neighbors, distances = self.graph.knn_query(X, k, num_threads=self.params["num_threads"])
+            with warnings.catch_warnings():
+                if not(numpy.all(numpy.isfinite(distances))):
+                    warnings.warn("Overflow/infinite value encountered while computing 'distances'", RuntimeWarning)
+            # The k nearest neighbors are always sorted. I couldn't find it in the doc, but the code calls searchKnn,
+            # which returns a priority_queue, and then fills the return array backwards with top/pop on the queue.
+            if self.return_index:
+                if self.return_distance:
+                    return neighbors, numpy.sqrt(distances)
+                else:
+                    return neighbors
+            if self.return_distance:
+                return numpy.sqrt(distances)
+            return None
+
+        if self.params["implementation"] == "keops":
+            import torch
+            from pykeops.torch import LazyTensor
+
+            # 'float64' is slow except on super expensive GPUs. Allow it with some param?
+            XX = torch.as_tensor(X, dtype=torch.float32)
+            if X is self.ref_points:
+                YY = XX
+            else:
+                YY = torch.as_tensor(self.ref_points, dtype=torch.float32)
+            p = self.params["p"]
+            if p == numpy.inf:
+                # Requires pykeops 1.4 or later
+                mat = (LazyTensor(XX[:, None, :]) - LazyTensor(YY[None, :, :])).abs().max(-1)
+            elif p == 2:  # Any even integer?
+                mat = ((LazyTensor(XX[:, None, :]) - LazyTensor(YY[None, :, :])) ** p).sum(-1)
+            else:
+                mat = ((LazyTensor(XX[:, None, :]) - LazyTensor(YY[None, :, :])).abs() ** p).sum(-1)
+
+            if self.return_index:
+                if self.return_distance:
+                    distances, neighbors = mat.Kmin_argKmin(k, dim=1)
+                    with warnings.catch_warnings():
+                        if not(torch.isfinite(distances).all()):
+                            warnings.warn("Overflow/infinite value encountered while computing 'distances'", RuntimeWarning)
+                    if p != numpy.inf:
+                        distances = distances ** (1.0 / p)
+                    return neighbors, distances
+                else:
+                    neighbors = mat.argKmin(k, dim=1)
+                    return neighbors
+            if self.return_distance:
+                distances = mat.Kmin(k, dim=1)
+                with warnings.catch_warnings():
+                    if not(torch.isfinite(distances).all()):
+                        warnings.warn("Overflow/infinite value encountered while computing 'distances'", RuntimeWarning)
+                if p != numpy.inf:
+                    distances = distances ** (1.0 / p)
+                return distances
+            return None
+
+        if self.params["implementation"] == "ckdtree":
+            qargs = {key: val for key, val in self.params.items() if key in {"p", "eps"}}
+            # SciPy renamed n_jobs to workers
+            qargs["workers"] = self.params.get("workers") or self.params.get("n_jobs") or 1
+            distances, neighbors = self.kdtree.query(X, k=self.k, **qargs)
+            if k == 1:
+                # SciPy decided to squeeze the last dimension for k=1
+                distances = distances[:, None]
+                neighbors = neighbors[:, None]
+            if self.return_index:
+                if self.return_distance:
+                    return neighbors, distances
+                else:
+                    return neighbors
+            if self.return_distance:
+                return distances
+            return None
+
+        assert self.params["implementation"] == "sklearn"
+        if self.return_distance:
+            distances, neighbors = self.nn.kneighbors(X, return_distance=True)
+            if self.return_index:
+                return neighbors, distances
+            else:
+                return distances
+        if self.return_index:
+            neighbors = self.nn.kneighbors(X, return_distance=False)
+            return neighbors
+        return None
```

## gudhi/point_cloud/timedelay.py

 * *Ordering differences only*

```diff
@@ -1,93 +1,93 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Martin Royer, Yuichi Ike, Masatoshi Takenouchi
-#
-# Copyright (C) 2020 Inria, Copyright (C) 2020 Fujitsu Laboratories Ltd.
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-
-
-class TimeDelayEmbedding:
-    """Point cloud transformation class. Embeds time-series data in the R^d according to
-    `Takens' Embedding Theorem <https://en.wikipedia.org/wiki/Takens%27s_theorem>`_ and obtains the
-    coordinates of each point.
-
-    Example
-    -------
-
-    Given delay=3 and skip=2, a point cloud which is obtained by embedding
-    a scalar time-series into R^3 is as follows::
-
-        time-series = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
-        point cloud = [[1, 4, 7],
-                       [3, 6, 9]]
-
-    Given delay=1 and skip=1, a point cloud which is obtained by embedding
-    a 2D vector time-series data into R^4 is as follows::
-
-        time-series = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]
-        point cloud = [[0, 1, 2, 3],
-                       [2, 3, 4, 5],
-                       [4, 5, 6, 7],
-                       [6, 7, 8, 9]]
-    """
-
-    def __init__(self, dim=3, delay=1, skip=1):
-        """
-        Constructor for the TimeDelayEmbedding class.
-
-        Parameters:
-            dim (int): `d` of R^d to be embedded. Optional (default=3).
-            delay (int): Time-Delay embedding. Optional (default=1).
-            skip (int): How often to skip embedded points. Optional (default=1).
-        """
-        self._dim = dim
-        self._delay = delay
-        self._skip = skip
-
-    def __call__(self, ts):
-        """Transform method for single time-series data.
-
-        Parameters
-        ----------
-        ts : Iterable[float] or Iterable[Iterable[float]]
-            A single time-series data, with scalar or vector values.
-
-        Returns
-        -------
-        point cloud : n x dim numpy arrays
-            Makes point cloud from a single time-series data.
-        """
-        return self._transform(np.array(ts))
-
-    def fit(self, ts, y=None):
-        return self
-    
-    def _transform(self, ts):
-        """Guts of transform method."""
-        if ts.ndim == 1:
-            repeat = self._dim
-        else:
-            assert self._dim % ts.shape[1] == 0
-            repeat = self._dim // ts.shape[1]
-        end = len(ts) - self._delay * (repeat - 1)
-        short = np.arange(0, end, self._skip)
-        vertical = np.arange(0, repeat * self._delay, self._delay)
-        return ts[np.add.outer(short, vertical)].reshape(len(short), -1)
-
-    def transform(self, ts):
-        """Transform method for multiple time-series data.
-
-        Parameters
-        ----------
-        ts : Iterable[Iterable[float]] or Iterable[Iterable[Iterable[float]]]
-            Multiple time-series data, with scalar or vector values.
-
-        Returns
-        -------
-        point clouds : list of n x dim numpy arrays
-            Makes point cloud from each time-series data.
-        """
-        return [self._transform(np.array(s)) for s in ts]
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Martin Royer, Yuichi Ike, Masatoshi Takenouchi
+#
+# Copyright (C) 2020 Inria, Copyright (C) 2020 Fujitsu Laboratories Ltd.
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+
+
+class TimeDelayEmbedding:
+    """Point cloud transformation class. Embeds time-series data in the R^d according to
+    `Takens' Embedding Theorem <https://en.wikipedia.org/wiki/Takens%27s_theorem>`_ and obtains the
+    coordinates of each point.
+
+    Example
+    -------
+
+    Given delay=3 and skip=2, a point cloud which is obtained by embedding
+    a scalar time-series into R^3 is as follows::
+
+        time-series = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
+        point cloud = [[1, 4, 7],
+                       [3, 6, 9]]
+
+    Given delay=1 and skip=1, a point cloud which is obtained by embedding
+    a 2D vector time-series data into R^4 is as follows::
+
+        time-series = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]
+        point cloud = [[0, 1, 2, 3],
+                       [2, 3, 4, 5],
+                       [4, 5, 6, 7],
+                       [6, 7, 8, 9]]
+    """
+
+    def __init__(self, dim=3, delay=1, skip=1):
+        """
+        Constructor for the TimeDelayEmbedding class.
+
+        Parameters:
+            dim (int): `d` of R^d to be embedded. Optional (default=3).
+            delay (int): Time-Delay embedding. Optional (default=1).
+            skip (int): How often to skip embedded points. Optional (default=1).
+        """
+        self._dim = dim
+        self._delay = delay
+        self._skip = skip
+
+    def __call__(self, ts):
+        """Transform method for single time-series data.
+
+        Parameters
+        ----------
+        ts : Iterable[float] or Iterable[Iterable[float]]
+            A single time-series data, with scalar or vector values.
+
+        Returns
+        -------
+        point cloud : n x dim numpy arrays
+            Makes point cloud from a single time-series data.
+        """
+        return self._transform(np.array(ts))
+
+    def fit(self, ts, y=None):
+        return self
+    
+    def _transform(self, ts):
+        """Guts of transform method."""
+        if ts.ndim == 1:
+            repeat = self._dim
+        else:
+            assert self._dim % ts.shape[1] == 0
+            repeat = self._dim // ts.shape[1]
+        end = len(ts) - self._delay * (repeat - 1)
+        short = np.arange(0, end, self._skip)
+        vertical = np.arange(0, repeat * self._delay, self._delay)
+        return ts[np.add.outer(short, vertical)].reshape(len(short), -1)
+
+    def transform(self, ts):
+        """Transform method for multiple time-series data.
+
+        Parameters
+        ----------
+        ts : Iterable[Iterable[float]] or Iterable[Iterable[Iterable[float]]]
+            Multiple time-series data, with scalar or vector values.
+
+        Returns
+        -------
+        point clouds : list of n x dim numpy arrays
+            Makes point cloud from each time-series data.
+        """
+        return [self._transform(np.array(s)) for s in ts]
```

## gudhi/representations/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-from .kernel_methods import *
-from .metrics import *
-from .preprocessing import *
-from .vector_methods import *
-
-__all__ = ["kernel_methods", "metrics", "preprocessing", "vector_methods"]
+from .kernel_methods import *
+from .metrics import *
+from .preprocessing import *
+from .vector_methods import *
+
+__all__ = ["kernel_methods", "metrics", "preprocessing", "vector_methods"]
```

## gudhi/representations/kernel_methods.py

 * *Ordering differences only*

```diff
@@ -1,303 +1,303 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Mathieu Carri√®re
-#
-# Copyright (C) 2018-2019 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.metrics import pairwise_distances, pairwise_kernels
-from .metrics import SlicedWassersteinDistance, PersistenceFisherDistance, _sklearn_wrapper, _pairwise, pairwise_persistence_diagram_distances, _sliced_wasserstein_distance, _persistence_fisher_distance
-from .preprocessing import Padding
-
-#############################################
-# Kernel methods ############################
-#############################################
-
-def _persistence_weighted_gaussian_kernel(D1, D2, weight=lambda x: 1, kernel_approx=None, bandwidth=1.):
-    """
-    This is a function for computing the persistence weighted Gaussian kernel value from two persistence diagrams. The persistence weighted Gaussian kernel is computed by convolving the persistence diagram points with weighted Gaussian kernels. See http://proceedings.mlr.press/v48/kusano16.html for more details.
-
-    Parameters:
-        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram. Must not contain essential points (i.e. with infinite coordinate).
-        D2: (m x 2) numpy.array encoding the second diagram.
-        bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved
-        weight: weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie lists or numpy arrays of the form [p_x,p_y].
-        kernel_approx: kernel approximation class used to speed up computation. Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
-
-    Returns:
-        float: the persistence weighted Gaussian kernel value between persistence diagrams. 
-    """
-    ws1 = np.array([weight(D1[j,:]) for j in range(len(D1))])
-    ws2 = np.array([weight(D2[j,:]) for j in range(len(D2))])
-    if kernel_approx is not None:
-        approx1 = np.sum(np.multiply(ws1[:,np.newaxis], kernel_approx.transform(D1)), axis=0)
-        approx2 = np.sum(np.multiply(ws2[:,np.newaxis], kernel_approx.transform(D2)), axis=0)
-        return (1./(np.sqrt(2*np.pi)*bandwidth)) * np.matmul(approx1, approx2.T)
-    else:
-        W = np.matmul(ws1[:,np.newaxis], ws2[np.newaxis,:])
-        E = (1./(np.sqrt(2*np.pi)*bandwidth)) * np.exp(-np.square(pairwise_distances(D1,D2))/(2*bandwidth*bandwidth))
-        return np.sum(np.multiply(W, E))
-
-def _persistence_scale_space_kernel(D1, D2, kernel_approx=None, bandwidth=1.):
-    """
-    This is a function for computing the persistence scale space kernel value from two persistence diagrams. The persistence scale space kernel is computed by adding the symmetric to the diagonal of each point in each persistence diagram, with negative weight, and then convolving the points with a Gaussian kernel. See https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper.pdf for more details.
-    
-    Parameters:
-        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram. Must not contain essential points (i.e. with infinite coordinate).
-        D2: (m x 2) numpy.array encoding the second diagram.
-        bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved
-        kernel_approx: kernel approximation class used to speed up computation. Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
-    
-    Returns:
-        float: the persistence scale space kernel value between persistence diagrams. 
-    """
-    DD1 = np.concatenate([D1, D1[:,[1,0]]], axis=0)
-    DD2 = np.concatenate([D2, D2[:,[1,0]]], axis=0)
-    weight_pss = lambda x: 1 if x[1] >= x[0] else -1
-    return 0.5 * _persistence_weighted_gaussian_kernel(DD1, DD2, weight=weight_pss, kernel_approx=kernel_approx, bandwidth=bandwidth)
-
-
-def pairwise_persistence_diagram_kernels(X, Y=None, kernel="sliced_wasserstein", n_jobs=None, **kwargs):
-    """
-    This function computes the kernel matrix between two lists of persistence diagrams given as numpy arrays of shape (nx2).
-
-    Parameters:    
-        X (list of n numpy arrays of shape (numx2)): first list of persistence diagrams. 
-        Y (list of m numpy arrays of shape (numx2)): second list of persistence diagrams (optional). If None, pairwise kernel values are computed from the first list only.
-        kernel: kernel to use. It can be either a string ("sliced_wasserstein", "persistence_scale_space", "persistence_weighted_gaussian", "persistence_fisher") or a function taking two numpy arrays of shape (nx2) and (mx2) as inputs. If it is a function, make sure that it is symmetric.
-        n_jobs (int): number of jobs to use for the computation. This uses joblib.Parallel(prefer="threads"), so kernels that do not release the GIL may not scale unless run inside a `joblib.parallel_backend <https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend>`_ block.
-        **kwargs: optional keyword parameters. Any further parameters are passed directly to the kernel function. See the docs of the various kernel classes in this module.
-
-    Returns: 
-        numpy array of shape (nxm): kernel matrix.
-    """    
-    XX = np.reshape(np.arange(len(X)), [-1,1])
-    YY = None if Y is None or Y is X else np.reshape(np.arange(len(Y)), [-1,1])
-    if kernel == "sliced_wasserstein":
-        return np.exp(-pairwise_persistence_diagram_distances(X, Y, metric="sliced_wasserstein", num_directions=kwargs["num_directions"], n_jobs=n_jobs) / kwargs["bandwidth"])
-    elif kernel == "persistence_fisher":
-        return np.exp(-pairwise_persistence_diagram_distances(X, Y, metric="persistence_fisher", kernel_approx=kwargs["kernel_approx"], bandwidth=kwargs["bandwidth_fisher"], n_jobs=n_jobs) / kwargs["bandwidth"])
-    elif kernel == "persistence_scale_space":
-        return _pairwise(pairwise_kernels, False, XX, YY, metric=_sklearn_wrapper(_persistence_scale_space_kernel, X, Y, **kwargs), n_jobs=n_jobs)
-    elif kernel == "persistence_weighted_gaussian":
-        return _pairwise(pairwise_kernels, False, XX, YY, metric=_sklearn_wrapper(_persistence_weighted_gaussian_kernel, X, Y, **kwargs), n_jobs=n_jobs)
-    else:
-        return _pairwise(pairwise_kernels, False, XX, YY, metric=_sklearn_wrapper(metric, **kwargs), n_jobs=n_jobs)
-
-class SlicedWassersteinKernel(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the sliced Wasserstein kernel matrix from a list of persistence diagrams. The sliced Wasserstein kernel is computed by exponentiating the corresponding sliced Wasserstein distance with a Gaussian kernel. See http://proceedings.mlr.press/v70/carriere17a.html for more details. 
-    """
-    def __init__(self, num_directions=10, bandwidth=1.0, n_jobs=None):
-        """
-        Constructor for the SlicedWassersteinKernel class.
-
-        Parameters:
-            bandwidth (double): bandwidth of the Gaussian kernel applied to the sliced Wasserstein distance (default 1.).
-            num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the kernel computation (default 10).
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
-        """
-        self.bandwidth = bandwidth
-        self.num_directions = num_directions
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the SlicedWassersteinKernel class on a list of persistence diagrams: an instance of the SlicedWassersteinDistance class is fitted on the diagrams and then stored. 
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all sliced Wasserstein kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise sliced Wasserstein kernel values.
-        """
-        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="sliced_wasserstein", bandwidth=self.bandwidth, num_directions=self.num_directions, n_jobs=self.n_jobs)
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply SlicedWassersteinKernel on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: sliced Wasserstein kernel value.
-        """
-        return np.exp(-_sliced_wasserstein_distance(diag1, diag2, num_directions=self.num_directions) / self.bandwidth)
-
-class PersistenceWeightedGaussianKernel(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the persistence weighted Gaussian kernel matrix from a list of persistence diagrams. The persistence weighted Gaussian kernel is computed by convolving the persistence diagram points with weighted Gaussian kernels. See http://proceedings.mlr.press/v48/kusano16.html for more details. 
-    """
-    def __init__(self, bandwidth=1., weight=lambda x: 1, kernel_approx=None, n_jobs=None):
-        """
-        Constructor for the PersistenceWeightedGaussianKernel class.
-  
-        Parameters:
-            bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved (default 1.)
-            weight (function): weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie lists or numpy arrays of the form [p_x,p_y].
-            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
-        """
-        self.bandwidth, self.weight = bandwidth, weight
-        self.kernel_approx = kernel_approx
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the PersistenceWeightedGaussianKernel class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams** and the kernel approximation class (if not None) is applied on them. 
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all persistence weighted Gaussian kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise persistence weighted Gaussian kernel values.
-        """
-        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="persistence_weighted_gaussian", bandwidth=self.bandwidth, weight=self.weight, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply PersistenceWeightedGaussianKernel on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: persistence weighted Gaussian kernel value.
-        """
-        return _persistence_weighted_gaussian_kernel(diag1, diag2, weight=self.weight, kernel_approx=self.kernel_approx, bandwidth=self.bandwidth)
-
-class PersistenceScaleSpaceKernel(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the persistence scale space kernel matrix from a list of persistence diagrams. The persistence scale space kernel is computed by adding the symmetric to the diagonal of each point in each persistence diagram, with negative weight, and then convolving the points with a Gaussian kernel. See https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper.pdf for more details. 
-    """
-    def __init__(self, bandwidth=1., kernel_approx=None, n_jobs=None):
-        """
-        Constructor for the PersistenceScaleSpaceKernel class.
-  
-        Parameters:
-            bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved (default 1.)
-            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
-        """
-        self.bandwidth, self.kernel_approx = bandwidth, kernel_approx
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the PersistenceScaleSpaceKernel class on a list of persistence diagrams: symmetric to the diagonal of all points are computed and an instance of the PersistenceWeightedGaussianKernel class is fitted on the diagrams and then stored. 
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all persistence scale space kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise persistence scale space kernel values.
-        """
-        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="persistence_scale_space", bandwidth=self.bandwidth, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply PersistenceScaleSpaceKernel on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: persistence scale space kernel value.
-        """
-        return _persistence_scale_space_kernel(diag1, diag2, bandwidth=self.bandwidth, kernel_approx=self.kernel_approx)
-
-class PersistenceFisherKernel(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the persistence Fisher kernel matrix from a list of persistence diagrams. The persistence Fisher kernel is computed by exponentiating the corresponding persistence Fisher distance with a Gaussian kernel. See papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams for more details. 
-    """
-    def __init__(self, bandwidth_fisher=1., bandwidth=1., kernel_approx=None, n_jobs=None):
-        """
-        Constructor for the PersistenceFisherKernel class.
-
-        Parameters:
-            bandwidth (double): bandwidth of the Gaussian kernel applied to the persistence Fisher distance (default 1.).
-            bandwidth_fisher (double): bandwidth of the Gaussian kernel used to turn persistence diagrams into probability distributions by PersistenceFisherDistance class (default 1.).
-            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
-        """
-        self.bandwidth = bandwidth
-        self.bandwidth_fisher, self.kernel_approx = bandwidth_fisher, kernel_approx
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the PersistenceFisherKernel class on a list of persistence diagrams: an instance of the PersistenceFisherDistance class is fitted on the diagrams and then stored. 
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all persistence Fisher kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise persistence Fisher kernel values.
-        """
-        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="persistence_fisher", bandwidth=self.bandwidth, bandwidth_fisher=self.bandwidth_fisher, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply PersistenceFisherKernel on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: persistence Fisher kernel value.
-        """
-        return np.exp(-_persistence_fisher_distance(diag1, diag2, bandwidth=self.bandwidth_fisher, kernel_approx=self.kernel_approx) / self.bandwidth)
-
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Mathieu Carri√®re
+#
+# Copyright (C) 2018-2019 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.metrics import pairwise_distances, pairwise_kernels
+from .metrics import SlicedWassersteinDistance, PersistenceFisherDistance, _sklearn_wrapper, _pairwise, pairwise_persistence_diagram_distances, _sliced_wasserstein_distance, _persistence_fisher_distance
+from .preprocessing import Padding
+
+#############################################
+# Kernel methods ############################
+#############################################
+
+def _persistence_weighted_gaussian_kernel(D1, D2, weight=lambda x: 1, kernel_approx=None, bandwidth=1.):
+    """
+    This is a function for computing the persistence weighted Gaussian kernel value from two persistence diagrams. The persistence weighted Gaussian kernel is computed by convolving the persistence diagram points with weighted Gaussian kernels. See http://proceedings.mlr.press/v48/kusano16.html for more details.
+
+    Parameters:
+        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram. Must not contain essential points (i.e. with infinite coordinate).
+        D2: (m x 2) numpy.array encoding the second diagram.
+        bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved
+        weight: weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie lists or numpy arrays of the form [p_x,p_y].
+        kernel_approx: kernel approximation class used to speed up computation. Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
+
+    Returns:
+        float: the persistence weighted Gaussian kernel value between persistence diagrams. 
+    """
+    ws1 = np.array([weight(D1[j,:]) for j in range(len(D1))])
+    ws2 = np.array([weight(D2[j,:]) for j in range(len(D2))])
+    if kernel_approx is not None:
+        approx1 = np.sum(np.multiply(ws1[:,np.newaxis], kernel_approx.transform(D1)), axis=0)
+        approx2 = np.sum(np.multiply(ws2[:,np.newaxis], kernel_approx.transform(D2)), axis=0)
+        return (1./(np.sqrt(2*np.pi)*bandwidth)) * np.matmul(approx1, approx2.T)
+    else:
+        W = np.matmul(ws1[:,np.newaxis], ws2[np.newaxis,:])
+        E = (1./(np.sqrt(2*np.pi)*bandwidth)) * np.exp(-np.square(pairwise_distances(D1,D2))/(2*bandwidth*bandwidth))
+        return np.sum(np.multiply(W, E))
+
+def _persistence_scale_space_kernel(D1, D2, kernel_approx=None, bandwidth=1.):
+    """
+    This is a function for computing the persistence scale space kernel value from two persistence diagrams. The persistence scale space kernel is computed by adding the symmetric to the diagonal of each point in each persistence diagram, with negative weight, and then convolving the points with a Gaussian kernel. See https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper.pdf for more details.
+    
+    Parameters:
+        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram. Must not contain essential points (i.e. with infinite coordinate).
+        D2: (m x 2) numpy.array encoding the second diagram.
+        bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved
+        kernel_approx: kernel approximation class used to speed up computation. Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
+    
+    Returns:
+        float: the persistence scale space kernel value between persistence diagrams. 
+    """
+    DD1 = np.concatenate([D1, D1[:,[1,0]]], axis=0)
+    DD2 = np.concatenate([D2, D2[:,[1,0]]], axis=0)
+    weight_pss = lambda x: 1 if x[1] >= x[0] else -1
+    return 0.5 * _persistence_weighted_gaussian_kernel(DD1, DD2, weight=weight_pss, kernel_approx=kernel_approx, bandwidth=bandwidth)
+
+
+def pairwise_persistence_diagram_kernels(X, Y=None, kernel="sliced_wasserstein", n_jobs=None, **kwargs):
+    """
+    This function computes the kernel matrix between two lists of persistence diagrams given as numpy arrays of shape (nx2).
+
+    Parameters:    
+        X (list of n numpy arrays of shape (numx2)): first list of persistence diagrams. 
+        Y (list of m numpy arrays of shape (numx2)): second list of persistence diagrams (optional). If None, pairwise kernel values are computed from the first list only.
+        kernel: kernel to use. It can be either a string ("sliced_wasserstein", "persistence_scale_space", "persistence_weighted_gaussian", "persistence_fisher") or a function taking two numpy arrays of shape (nx2) and (mx2) as inputs. If it is a function, make sure that it is symmetric.
+        n_jobs (int): number of jobs to use for the computation. This uses joblib.Parallel(prefer="threads"), so kernels that do not release the GIL may not scale unless run inside a `joblib.parallel_backend <https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend>`_ block.
+        **kwargs: optional keyword parameters. Any further parameters are passed directly to the kernel function. See the docs of the various kernel classes in this module.
+
+    Returns: 
+        numpy array of shape (nxm): kernel matrix.
+    """    
+    XX = np.reshape(np.arange(len(X)), [-1,1])
+    YY = None if Y is None or Y is X else np.reshape(np.arange(len(Y)), [-1,1])
+    if kernel == "sliced_wasserstein":
+        return np.exp(-pairwise_persistence_diagram_distances(X, Y, metric="sliced_wasserstein", num_directions=kwargs["num_directions"], n_jobs=n_jobs) / kwargs["bandwidth"])
+    elif kernel == "persistence_fisher":
+        return np.exp(-pairwise_persistence_diagram_distances(X, Y, metric="persistence_fisher", kernel_approx=kwargs["kernel_approx"], bandwidth=kwargs["bandwidth_fisher"], n_jobs=n_jobs) / kwargs["bandwidth"])
+    elif kernel == "persistence_scale_space":
+        return _pairwise(pairwise_kernels, False, XX, YY, metric=_sklearn_wrapper(_persistence_scale_space_kernel, X, Y, **kwargs), n_jobs=n_jobs)
+    elif kernel == "persistence_weighted_gaussian":
+        return _pairwise(pairwise_kernels, False, XX, YY, metric=_sklearn_wrapper(_persistence_weighted_gaussian_kernel, X, Y, **kwargs), n_jobs=n_jobs)
+    else:
+        return _pairwise(pairwise_kernels, False, XX, YY, metric=_sklearn_wrapper(metric, **kwargs), n_jobs=n_jobs)
+
+class SlicedWassersteinKernel(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the sliced Wasserstein kernel matrix from a list of persistence diagrams. The sliced Wasserstein kernel is computed by exponentiating the corresponding sliced Wasserstein distance with a Gaussian kernel. See http://proceedings.mlr.press/v70/carriere17a.html for more details. 
+    """
+    def __init__(self, num_directions=10, bandwidth=1.0, n_jobs=None):
+        """
+        Constructor for the SlicedWassersteinKernel class.
+
+        Parameters:
+            bandwidth (double): bandwidth of the Gaussian kernel applied to the sliced Wasserstein distance (default 1.).
+            num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the kernel computation (default 10).
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
+        """
+        self.bandwidth = bandwidth
+        self.num_directions = num_directions
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the SlicedWassersteinKernel class on a list of persistence diagrams: an instance of the SlicedWassersteinDistance class is fitted on the diagrams and then stored. 
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all sliced Wasserstein kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise sliced Wasserstein kernel values.
+        """
+        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="sliced_wasserstein", bandwidth=self.bandwidth, num_directions=self.num_directions, n_jobs=self.n_jobs)
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply SlicedWassersteinKernel on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: sliced Wasserstein kernel value.
+        """
+        return np.exp(-_sliced_wasserstein_distance(diag1, diag2, num_directions=self.num_directions) / self.bandwidth)
+
+class PersistenceWeightedGaussianKernel(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the persistence weighted Gaussian kernel matrix from a list of persistence diagrams. The persistence weighted Gaussian kernel is computed by convolving the persistence diagram points with weighted Gaussian kernels. See http://proceedings.mlr.press/v48/kusano16.html for more details. 
+    """
+    def __init__(self, bandwidth=1., weight=lambda x: 1, kernel_approx=None, n_jobs=None):
+        """
+        Constructor for the PersistenceWeightedGaussianKernel class.
+  
+        Parameters:
+            bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved (default 1.)
+            weight (function): weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie lists or numpy arrays of the form [p_x,p_y].
+            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
+        """
+        self.bandwidth, self.weight = bandwidth, weight
+        self.kernel_approx = kernel_approx
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the PersistenceWeightedGaussianKernel class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams** and the kernel approximation class (if not None) is applied on them. 
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all persistence weighted Gaussian kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise persistence weighted Gaussian kernel values.
+        """
+        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="persistence_weighted_gaussian", bandwidth=self.bandwidth, weight=self.weight, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply PersistenceWeightedGaussianKernel on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: persistence weighted Gaussian kernel value.
+        """
+        return _persistence_weighted_gaussian_kernel(diag1, diag2, weight=self.weight, kernel_approx=self.kernel_approx, bandwidth=self.bandwidth)
+
+class PersistenceScaleSpaceKernel(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the persistence scale space kernel matrix from a list of persistence diagrams. The persistence scale space kernel is computed by adding the symmetric to the diagonal of each point in each persistence diagram, with negative weight, and then convolving the points with a Gaussian kernel. See https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper.pdf for more details. 
+    """
+    def __init__(self, bandwidth=1., kernel_approx=None, n_jobs=None):
+        """
+        Constructor for the PersistenceScaleSpaceKernel class.
+  
+        Parameters:
+            bandwidth (double): bandwidth of the Gaussian kernel with which persistence diagrams will be convolved (default 1.)
+            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
+        """
+        self.bandwidth, self.kernel_approx = bandwidth, kernel_approx
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the PersistenceScaleSpaceKernel class on a list of persistence diagrams: symmetric to the diagonal of all points are computed and an instance of the PersistenceWeightedGaussianKernel class is fitted on the diagrams and then stored. 
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all persistence scale space kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise persistence scale space kernel values.
+        """
+        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="persistence_scale_space", bandwidth=self.bandwidth, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply PersistenceScaleSpaceKernel on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: persistence scale space kernel value.
+        """
+        return _persistence_scale_space_kernel(diag1, diag2, bandwidth=self.bandwidth, kernel_approx=self.kernel_approx)
+
+class PersistenceFisherKernel(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the persistence Fisher kernel matrix from a list of persistence diagrams. The persistence Fisher kernel is computed by exponentiating the corresponding persistence Fisher distance with a Gaussian kernel. See papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams for more details. 
+    """
+    def __init__(self, bandwidth_fisher=1., bandwidth=1., kernel_approx=None, n_jobs=None):
+        """
+        Constructor for the PersistenceFisherKernel class.
+
+        Parameters:
+            bandwidth (double): bandwidth of the Gaussian kernel applied to the persistence Fisher distance (default 1.).
+            bandwidth_fisher (double): bandwidth of the Gaussian kernel used to turn persistence diagrams into probability distributions by PersistenceFisherDistance class (default 1.).
+            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_kernels` for details.
+        """
+        self.bandwidth = bandwidth
+        self.bandwidth_fisher, self.kernel_approx = bandwidth_fisher, kernel_approx
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the PersistenceFisherKernel class on a list of persistence diagrams: an instance of the PersistenceFisherDistance class is fitted on the diagrams and then stored. 
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all persistence Fisher kernel values between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in X) x (number of diagrams in **diagrams**): matrix of pairwise persistence Fisher kernel values.
+        """
+        return pairwise_persistence_diagram_kernels(X, self.diagrams_, kernel="persistence_fisher", bandwidth=self.bandwidth, bandwidth_fisher=self.bandwidth_fisher, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply PersistenceFisherKernel on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: persistence Fisher kernel value.
+        """
+        return np.exp(-_persistence_fisher_distance(diag1, diag2, bandwidth=self.bandwidth_fisher, kernel_approx=self.kernel_approx) / self.bandwidth)
+
```

## gudhi/representations/metrics.py

 * *Ordering differences only*

```diff
@@ -1,424 +1,424 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Mathieu Carri√®re
-#
-# Copyright (C) 2018-2019 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-from sklearn.base import BaseEstimator, TransformerMixin
-from sklearn.metrics import pairwise_distances
-from gudhi.hera import wasserstein_distance as hera_wasserstein_distance
-from .preprocessing import Padding
-from joblib import Parallel, delayed
-
-#############################################
-# Metrics ###################################
-#############################################
-
-def _sliced_wasserstein_distance(D1, D2, num_directions):
-    """
-    This is a function for computing the sliced Wasserstein distance from two persistence diagrams. The Sliced Wasserstein distance is computed by projecting the persistence diagrams onto lines, comparing the projections with the 1-norm, and finally averaging over the lines. See http://proceedings.mlr.press/v70/carriere17a.html for more details.
-    
-    Parameters:
-        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram. Must not contain essential points (i.e. with infinite coordinate).
-        D2: (m x 2) numpy.array encoding the second diagram.
-        num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the distance computation.
-
-    Returns: 
-        float: the sliced Wasserstein distance between persistence diagrams. 
-    """
-    thetas = np.linspace(-np.pi/2, np.pi/2, num=num_directions+1)[np.newaxis,:-1]
-    lines = np.concatenate([np.cos(thetas), np.sin(thetas)], axis=0)
-    approx1 = np.matmul(D1, lines)
-    approx_diag1 = np.matmul(np.broadcast_to(D1.sum(-1,keepdims=True)/2,(len(D1),2)), lines)
-    approx2 = np.matmul(D2, lines)
-    approx_diag2 = np.matmul(np.broadcast_to(D2.sum(-1,keepdims=True)/2,(len(D2),2)), lines)
-    A = np.sort(np.concatenate([approx1, approx_diag2], axis=0), axis=0)
-    B = np.sort(np.concatenate([approx2, approx_diag1], axis=0), axis=0)
-    L1 = np.sum(np.abs(A-B), axis=0)
-    return np.mean(L1)
-
-def _compute_persistence_diagram_projections(X, num_directions):
-    """
-    This is a function for projecting the points of a list of persistence diagrams (as well as their diagonal projections) onto a fixed number of lines sampled uniformly on [-pi/2, pi/2]. This function can be used as a preprocessing step in order to speed up the running time for computing all pairwise sliced Wasserstein distances / kernel values on a list of persistence diagrams. 
-
-    Parameters:
-        X (list of n numpy arrays of shape (numx2)): list of persistence diagrams. 
-        num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the distance computation.
-
-    Returns: 
-        list of n numpy arrays of shape (2*numx2): list of projected persistence diagrams.
-    """
-    thetas = np.linspace(-np.pi/2, np.pi/2, num=num_directions+1)[np.newaxis,:-1]
-    lines = np.concatenate([np.cos(thetas), np.sin(thetas)], axis=0)
-    XX = [np.vstack([np.matmul(D, lines), np.matmul(np.matmul(D, .5 * np.ones((2,2))), lines)]) for D in X]
-    return XX
-
-def _sliced_wasserstein_distance_on_projections(D1, D2):
-    """
-    This is a function for computing the sliced Wasserstein distance between two persistence diagrams that have already been projected onto some lines. It simply amounts to comparing the sorted projections with the 1-norm, and averaging over the lines. See http://proceedings.mlr.press/v70/carriere17a.html for more details.
-
-    Parameters: 
-        D1: (2n x number_of_lines) numpy.array containing the n projected points of the first diagram, and the n projections of their diagonal projections.
-        D2: (2m x number_of_lines) numpy.array containing the m projected points of the second diagram, and the m projections of their diagonal projections.
-
-    Returns: 
-        float: the sliced Wasserstein distance between the projected persistence diagrams. 
-    """
-    lim1, lim2 = int(len(D1)/2), int(len(D2)/2)
-    approx1, approx_diag1, approx2, approx_diag2 = D1[:lim1], D1[lim1:], D2[:lim2], D2[lim2:]
-    A = np.sort(np.concatenate([approx1, approx_diag2], axis=0), axis=0)
-    B = np.sort(np.concatenate([approx2, approx_diag1], axis=0), axis=0)
-    L1 = np.sum(np.abs(A-B), axis=0)
-    return np.mean(L1)
-
-def _persistence_fisher_distance(D1, D2, kernel_approx=None, bandwidth=1.):
-    """
-    This is a function for computing the persistence Fisher distance from two persistence diagrams. The persistence Fisher distance is obtained by computing the original Fisher distance between the probability distributions associated to the persistence diagrams given by convolving them with a Gaussian kernel. See http://papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams for more details.
-
-    Parameters: 
-        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram). Must not contain essential points (i.e. with infinite coordinate).
-        D2: (m x 2) numpy.array encoding the second diagram.
-        bandwidth (float): bandwidth of the Gaussian kernel used to turn persistence diagrams into probability distributions.
-        kernel_approx: kernel approximation class used to speed up computation. Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).   
-
-    Returns: 
-        float: the persistence Fisher distance between persistence diagrams. 
-    """
-    projection = (1./2) * np.ones((2,2))
-    diagonal_projections1 = np.matmul(D1, projection)
-    diagonal_projections2 = np.matmul(D2, projection)
-    if kernel_approx is not None:
-        approx1 = kernel_approx.transform(D1)
-        approx_diagonal1 = kernel_approx.transform(diagonal_projections1)
-        approx2 = kernel_approx.transform(D2)
-        approx_diagonal2 = kernel_approx.transform(diagonal_projections2)
-        Z = np.concatenate([approx1, approx_diagonal1, approx2, approx_diagonal2], axis=0)
-        U, V = np.sum(np.concatenate([approx1, approx_diagonal2], axis=0), axis=0), np.sum(np.concatenate([approx2, approx_diagonal1], axis=0), axis=0) 
-        vectori, vectorj = np.abs(np.matmul(Z, U.T)), np.abs(np.matmul(Z, V.T))
-        vectori_sum, vectorj_sum = np.sum(vectori), np.sum(vectorj)
-        if vectori_sum != 0:
-            vectori = vectori/vectori_sum
-        if vectorj_sum != 0:
-            vectorj = vectorj/vectorj_sum
-        return np.arccos(  min(np.dot(np.sqrt(vectori), np.sqrt(vectorj)), 1.)  )
-    else:
-        Z = np.concatenate([D1, diagonal_projections1, D2, diagonal_projections2], axis=0)
-        U, V = np.concatenate([D1, diagonal_projections2], axis=0), np.concatenate([D2, diagonal_projections1], axis=0) 
-        vectori = np.sum(np.exp(-np.square(pairwise_distances(Z,U))/(2 * np.square(bandwidth)))/(bandwidth * np.sqrt(2*np.pi)), axis=1)
-        vectorj = np.sum(np.exp(-np.square(pairwise_distances(Z,V))/(2 * np.square(bandwidth)))/(bandwidth * np.sqrt(2*np.pi)), axis=1)
-        vectori_sum, vectorj_sum = np.sum(vectori), np.sum(vectorj)
-        if vectori_sum != 0:
-            vectori = vectori/vectori_sum
-        if vectorj_sum != 0:
-            vectorj = vectorj/vectorj_sum
-        return np.arccos(  min(np.dot(np.sqrt(vectori), np.sqrt(vectorj)), 1.)  )
-
-def _pairwise(fallback, skipdiag, X, Y, metric, n_jobs):
-    if Y is not None:
-        return fallback(X, Y, metric=metric, n_jobs=n_jobs)
-    triu = np.triu_indices(len(X), k=skipdiag)
-    tril = (triu[1], triu[0])
-    par = Parallel(n_jobs=n_jobs, prefer="threads")
-    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))
-    m = np.empty((len(X), len(X)))
-    m[triu] = d
-    m[tril] = d
-    if skipdiag:
-        np.fill_diagonal(m, 0)
-    return m
-
-def _sklearn_wrapper(metric, X, Y, **kwargs):
-    """
-    This function is a wrapper for any metric between two persistence diagrams that takes two numpy arrays of shapes (nx2) and (mx2) as arguments.
-    """
-    if Y is None:
-        def flat_metric(a, b):
-            return metric(X[int(a[0])], X[int(b[0])], **kwargs)
-    else:
-        def flat_metric(a, b):
-            return metric(X[int(a[0])], Y[int(b[0])], **kwargs)
-    return flat_metric
-
-PAIRWISE_DISTANCE_FUNCTIONS = {
-    "wasserstein": hera_wasserstein_distance,
-    "hera_wasserstein": hera_wasserstein_distance,
-    "persistence_fisher": _persistence_fisher_distance,
-}
-
-def pairwise_persistence_diagram_distances(X, Y=None, metric="bottleneck", n_jobs=None, **kwargs):
-    """
-    This function computes the distance matrix between two lists of persistence diagrams given as numpy arrays of shape (nx2).
-
-    Parameters:
-        X (list of n numpy arrays of shape (numx2)): first list of persistence diagrams. 
-        Y (list of m numpy arrays of shape (numx2)): second list of persistence diagrams (optional). If None, pairwise distances are computed from the first list only.
-        metric: distance to use. It can be either a string ("sliced_wasserstein", "wasserstein", "hera_wasserstein" (Wasserstein distance computed with Hera---note that Hera is also used for the default option "wasserstein"), "pot_wasserstein" (Wasserstein distance computed with POT), "bottleneck", "persistence_fisher") or a function taking two numpy arrays of shape (nx2) and (mx2) as inputs. If it is a function, make sure that it is symmetric and that it outputs 0 if called on the same two arrays. 
-        n_jobs (int): number of jobs to use for the computation. This uses joblib.Parallel(prefer="threads"), so metrics that do not release the GIL may not scale unless run inside a `joblib.parallel_backend <https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend>`_ block.
-        **kwargs: optional keyword parameters. Any further parameters are passed directly to the distance function. See the docs of the various distance classes in this module.
-
-    Returns: 
-        numpy array of shape (nxm): distance matrix
-    """
-    XX = np.reshape(np.arange(len(X)), [-1,1])
-    YY = None if Y is None or Y is X else np.reshape(np.arange(len(Y)), [-1,1])
-    if metric == "bottleneck":
-        try: 
-            from .. import bottleneck_distance
-            return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(bottleneck_distance, X, Y, **kwargs), n_jobs=n_jobs)
-        except ImportError:
-            print("Gudhi built without CGAL")
-            raise
-    elif metric == "pot_wasserstein":
-        try:
-            from gudhi.wasserstein import wasserstein_distance as pot_wasserstein_distance
-            return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(pot_wasserstein_distance,  X, Y, **kwargs), n_jobs=n_jobs)
-        except ImportError:
-            print("POT (Python Optimal Transport) is not installed. Please install POT or use metric='wasserstein' or metric='hera_wasserstein'")
-            raise
-    elif metric == "sliced_wasserstein":
-        Xproj = _compute_persistence_diagram_projections(X, **kwargs)
-        Yproj = None if Y is None else _compute_persistence_diagram_projections(Y, **kwargs)
-        return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(_sliced_wasserstein_distance_on_projections, Xproj, Yproj), n_jobs=n_jobs)
-    elif type(metric) == str:
-        return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(PAIRWISE_DISTANCE_FUNCTIONS[metric], X, Y, **kwargs), n_jobs=n_jobs)
-    else:
-        return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(metric, X, Y, **kwargs), n_jobs=n_jobs)
-
-class SlicedWassersteinDistance(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the sliced Wasserstein distance matrix from a list of persistence diagrams. The Sliced Wasserstein distance is computed by projecting the persistence diagrams onto lines, comparing the projections with the 1-norm, and finally integrating over all possible lines. See http://proceedings.mlr.press/v70/carriere17a.html for more details. 
-    """
-    def __init__(self, num_directions=10, n_jobs=None):
-        """
-        Constructor for the SlicedWassersteinDistance class.
-
-        Parameters:
-            num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the distance computation (default 10). 
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
-        """
-        self.num_directions = num_directions
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the SlicedWassersteinDistance class on a list of persistence diagrams: persistence diagrams are projected onto the different lines. The diagrams themselves and their projections are then stored in numpy arrays, called **diagrams_** and **approx_diag_**.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all sliced Wasserstein distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise sliced Wasserstein distances.
-        """
-        return pairwise_persistence_diagram_distances(X, self.diagrams_, metric="sliced_wasserstein", num_directions=self.num_directions, n_jobs=self.n_jobs)
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply SlicedWassersteinDistance on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: sliced Wasserstein distance.
-        """
-        return _sliced_wasserstein_distance(diag1, diag2, num_directions=self.num_directions)
-
-class BottleneckDistance(BaseEstimator, TransformerMixin):
-    r"""
-    This is a class for computing the bottleneck distance matrix from a list of persistence diagrams.
-
-    :Requires: `CGAL <installation.html#cgal>`_ :math:`\geq` 4.11.0
-    """
-    def __init__(self, epsilon=None, n_jobs=None):
-        """
-        Constructor for the BottleneckDistance class.
-
-        Parameters:
-            epsilon (double): absolute (additive) error tolerated on the distance (default is the smallest positive float), see :func:`gudhi.bottleneck_distance`.
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
-        """
-        self.epsilon = epsilon
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the BottleneckDistance class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams**.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all bottleneck distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise bottleneck distances.
-        """
-        Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric="bottleneck", e=self.epsilon, n_jobs=self.n_jobs)
-        return Xfit
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply BottleneckDistance on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: bottleneck distance.
-        """
-        try: 
-            from .. import bottleneck_distance
-            return bottleneck_distance(diag1, diag2, e=self.epsilon)
-        except ImportError:
-            print("Gudhi built without CGAL")
-            raise
-
-class PersistenceFisherDistance(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the persistence Fisher distance matrix from a list of persistence diagrams. The persistence Fisher distance is obtained by computing the original Fisher distance between the probability distributions associated to the persistence diagrams given by convolving them with a Gaussian kernel. See http://papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams for more details. 
-    """
-    def __init__(self, bandwidth=1., kernel_approx=None, n_jobs=None):
-        """
-        Constructor for the PersistenceFisherDistance class.
-
-        Parameters:
-            bandwidth (double): bandwidth of the Gaussian kernel used to turn persistence diagrams into probability distributions (default 1.).
-            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).   
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
-        """
-        self.bandwidth, self.kernel_approx = bandwidth, kernel_approx
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the PersistenceFisherDistance class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams** and the kernel approximation class (if not None) is applied on them.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all persistence Fisher distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise persistence Fisher distances.
-        """
-        return pairwise_persistence_diagram_distances(X, self.diagrams_, metric="persistence_fisher", bandwidth=self.bandwidth, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply PersistenceFisherDistance on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: persistence Fisher distance.
-        """
-        return _persistence_fisher_distance(diag1, diag2, bandwidth=self.bandwidth, kernel_approx=self.kernel_approx)
-
-
-class WassersteinDistance(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the Wasserstein distance matrix from a list of persistence diagrams. 
-    """
-
-    def __init__(self, order=1, internal_p=np.inf, mode="hera", delta=0.01, n_jobs=None):
-        """
-        Constructor for the WassersteinDistance class.
-
-        Parameters:
-            order (int): exponent for Wasserstein, default value is 1., see :func:`gudhi.wasserstein.wasserstein_distance`.
-            internal_p (int): ground metric on the (upper-half) plane (i.e. norm l_p in R^2), default value is `np.inf`, see :func:`gudhi.wasserstein.wasserstein_distance`.
-            mode (str): method for computing Wasserstein distance. Either "pot" or "hera". Default set to "hera".
-            delta (float): relative error 1+delta. Used only if mode == "hera".
-            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
-        """
-        self.order, self.internal_p, self.mode = order, internal_p, mode
-        self.delta = delta
-        self.n_jobs = n_jobs
-
-    def fit(self, X, y=None):
-        """
-        Fit the WassersteinDistance class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams**.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        if self.mode not in ("pot", "hera"):
-            raise NameError("Unknown mode. Current available values for mode are 'hera' and 'pot'")
-        self.diagrams_ = X
-        return self
-
-    def transform(self, X):
-        """
-        Compute all Wasserstein distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-
-        Returns:
-            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise Wasserstein distances.
-        """
-        if self.mode == "hera":
-            Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric="hera_wasserstein", order=self.order, internal_p=self.internal_p, delta=self.delta, n_jobs=self.n_jobs)
-        else:
-            Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric="pot_wasserstein", order=self.order, internal_p=self.internal_p, matching=False, n_jobs=self.n_jobs)
-        return Xfit
-
-    def __call__(self, diag1, diag2):
-        """
-        Apply WassersteinDistance on a single pair of persistence diagrams and outputs the result.
-
-        Parameters:
-            diag1 (n x 2 numpy array): first input persistence diagram.
-            diag2 (n x 2 numpy array): second input persistence diagram.
-
-        Returns:
-            float: Wasserstein distance.
-        """
-        if self.mode == "hera":
-            return hera_wasserstein_distance(diag1, diag2, order=self.order, internal_p=self.internal_p, delta=self.delta)
-        elif self.mode == "pot":
-            try:
-                from gudhi.wasserstein import wasserstein_distance as pot_wasserstein_distance
-                return pot_wasserstein_distance(diag1, diag2, order=self.order, internal_p=self.internal_p, matching=False)
-            except ImportError:
-                print("POT (Python Optimal Transport) is not installed. Please install POT or use mode='hera'")
-                raise
-        else:
-            raise NameError("Unknown mode. Current available values for mode are 'hera' and 'pot'")
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Mathieu Carri√®re
+#
+# Copyright (C) 2018-2019 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.metrics import pairwise_distances
+from gudhi.hera import wasserstein_distance as hera_wasserstein_distance
+from .preprocessing import Padding
+from joblib import Parallel, delayed
+
+#############################################
+# Metrics ###################################
+#############################################
+
+def _sliced_wasserstein_distance(D1, D2, num_directions):
+    """
+    This is a function for computing the sliced Wasserstein distance from two persistence diagrams. The Sliced Wasserstein distance is computed by projecting the persistence diagrams onto lines, comparing the projections with the 1-norm, and finally averaging over the lines. See http://proceedings.mlr.press/v70/carriere17a.html for more details.
+    
+    Parameters:
+        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram. Must not contain essential points (i.e. with infinite coordinate).
+        D2: (m x 2) numpy.array encoding the second diagram.
+        num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the distance computation.
+
+    Returns: 
+        float: the sliced Wasserstein distance between persistence diagrams. 
+    """
+    thetas = np.linspace(-np.pi/2, np.pi/2, num=num_directions+1)[np.newaxis,:-1]
+    lines = np.concatenate([np.cos(thetas), np.sin(thetas)], axis=0)
+    approx1 = np.matmul(D1, lines)
+    approx_diag1 = np.matmul(np.broadcast_to(D1.sum(-1,keepdims=True)/2,(len(D1),2)), lines)
+    approx2 = np.matmul(D2, lines)
+    approx_diag2 = np.matmul(np.broadcast_to(D2.sum(-1,keepdims=True)/2,(len(D2),2)), lines)
+    A = np.sort(np.concatenate([approx1, approx_diag2], axis=0), axis=0)
+    B = np.sort(np.concatenate([approx2, approx_diag1], axis=0), axis=0)
+    L1 = np.sum(np.abs(A-B), axis=0)
+    return np.mean(L1)
+
+def _compute_persistence_diagram_projections(X, num_directions):
+    """
+    This is a function for projecting the points of a list of persistence diagrams (as well as their diagonal projections) onto a fixed number of lines sampled uniformly on [-pi/2, pi/2]. This function can be used as a preprocessing step in order to speed up the running time for computing all pairwise sliced Wasserstein distances / kernel values on a list of persistence diagrams. 
+
+    Parameters:
+        X (list of n numpy arrays of shape (numx2)): list of persistence diagrams. 
+        num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the distance computation.
+
+    Returns: 
+        list of n numpy arrays of shape (2*numx2): list of projected persistence diagrams.
+    """
+    thetas = np.linspace(-np.pi/2, np.pi/2, num=num_directions+1)[np.newaxis,:-1]
+    lines = np.concatenate([np.cos(thetas), np.sin(thetas)], axis=0)
+    XX = [np.vstack([np.matmul(D, lines), np.matmul(np.matmul(D, .5 * np.ones((2,2))), lines)]) for D in X]
+    return XX
+
+def _sliced_wasserstein_distance_on_projections(D1, D2):
+    """
+    This is a function for computing the sliced Wasserstein distance between two persistence diagrams that have already been projected onto some lines. It simply amounts to comparing the sorted projections with the 1-norm, and averaging over the lines. See http://proceedings.mlr.press/v70/carriere17a.html for more details.
+
+    Parameters: 
+        D1: (2n x number_of_lines) numpy.array containing the n projected points of the first diagram, and the n projections of their diagonal projections.
+        D2: (2m x number_of_lines) numpy.array containing the m projected points of the second diagram, and the m projections of their diagonal projections.
+
+    Returns: 
+        float: the sliced Wasserstein distance between the projected persistence diagrams. 
+    """
+    lim1, lim2 = int(len(D1)/2), int(len(D2)/2)
+    approx1, approx_diag1, approx2, approx_diag2 = D1[:lim1], D1[lim1:], D2[:lim2], D2[lim2:]
+    A = np.sort(np.concatenate([approx1, approx_diag2], axis=0), axis=0)
+    B = np.sort(np.concatenate([approx2, approx_diag1], axis=0), axis=0)
+    L1 = np.sum(np.abs(A-B), axis=0)
+    return np.mean(L1)
+
+def _persistence_fisher_distance(D1, D2, kernel_approx=None, bandwidth=1.):
+    """
+    This is a function for computing the persistence Fisher distance from two persistence diagrams. The persistence Fisher distance is obtained by computing the original Fisher distance between the probability distributions associated to the persistence diagrams given by convolving them with a Gaussian kernel. See http://papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams for more details.
+
+    Parameters: 
+        D1: (n x 2) numpy.array encoding the (finite points of the) first diagram). Must not contain essential points (i.e. with infinite coordinate).
+        D2: (m x 2) numpy.array encoding the second diagram.
+        bandwidth (float): bandwidth of the Gaussian kernel used to turn persistence diagrams into probability distributions.
+        kernel_approx: kernel approximation class used to speed up computation. Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).   
+
+    Returns: 
+        float: the persistence Fisher distance between persistence diagrams. 
+    """
+    projection = (1./2) * np.ones((2,2))
+    diagonal_projections1 = np.matmul(D1, projection)
+    diagonal_projections2 = np.matmul(D2, projection)
+    if kernel_approx is not None:
+        approx1 = kernel_approx.transform(D1)
+        approx_diagonal1 = kernel_approx.transform(diagonal_projections1)
+        approx2 = kernel_approx.transform(D2)
+        approx_diagonal2 = kernel_approx.transform(diagonal_projections2)
+        Z = np.concatenate([approx1, approx_diagonal1, approx2, approx_diagonal2], axis=0)
+        U, V = np.sum(np.concatenate([approx1, approx_diagonal2], axis=0), axis=0), np.sum(np.concatenate([approx2, approx_diagonal1], axis=0), axis=0) 
+        vectori, vectorj = np.abs(np.matmul(Z, U.T)), np.abs(np.matmul(Z, V.T))
+        vectori_sum, vectorj_sum = np.sum(vectori), np.sum(vectorj)
+        if vectori_sum != 0:
+            vectori = vectori/vectori_sum
+        if vectorj_sum != 0:
+            vectorj = vectorj/vectorj_sum
+        return np.arccos(  min(np.dot(np.sqrt(vectori), np.sqrt(vectorj)), 1.)  )
+    else:
+        Z = np.concatenate([D1, diagonal_projections1, D2, diagonal_projections2], axis=0)
+        U, V = np.concatenate([D1, diagonal_projections2], axis=0), np.concatenate([D2, diagonal_projections1], axis=0) 
+        vectori = np.sum(np.exp(-np.square(pairwise_distances(Z,U))/(2 * np.square(bandwidth)))/(bandwidth * np.sqrt(2*np.pi)), axis=1)
+        vectorj = np.sum(np.exp(-np.square(pairwise_distances(Z,V))/(2 * np.square(bandwidth)))/(bandwidth * np.sqrt(2*np.pi)), axis=1)
+        vectori_sum, vectorj_sum = np.sum(vectori), np.sum(vectorj)
+        if vectori_sum != 0:
+            vectori = vectori/vectori_sum
+        if vectorj_sum != 0:
+            vectorj = vectorj/vectorj_sum
+        return np.arccos(  min(np.dot(np.sqrt(vectori), np.sqrt(vectorj)), 1.)  )
+
+def _pairwise(fallback, skipdiag, X, Y, metric, n_jobs):
+    if Y is not None:
+        return fallback(X, Y, metric=metric, n_jobs=n_jobs)
+    triu = np.triu_indices(len(X), k=skipdiag)
+    tril = (triu[1], triu[0])
+    par = Parallel(n_jobs=n_jobs, prefer="threads")
+    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))
+    m = np.empty((len(X), len(X)))
+    m[triu] = d
+    m[tril] = d
+    if skipdiag:
+        np.fill_diagonal(m, 0)
+    return m
+
+def _sklearn_wrapper(metric, X, Y, **kwargs):
+    """
+    This function is a wrapper for any metric between two persistence diagrams that takes two numpy arrays of shapes (nx2) and (mx2) as arguments.
+    """
+    if Y is None:
+        def flat_metric(a, b):
+            return metric(X[int(a[0])], X[int(b[0])], **kwargs)
+    else:
+        def flat_metric(a, b):
+            return metric(X[int(a[0])], Y[int(b[0])], **kwargs)
+    return flat_metric
+
+PAIRWISE_DISTANCE_FUNCTIONS = {
+    "wasserstein": hera_wasserstein_distance,
+    "hera_wasserstein": hera_wasserstein_distance,
+    "persistence_fisher": _persistence_fisher_distance,
+}
+
+def pairwise_persistence_diagram_distances(X, Y=None, metric="bottleneck", n_jobs=None, **kwargs):
+    """
+    This function computes the distance matrix between two lists of persistence diagrams given as numpy arrays of shape (nx2).
+
+    Parameters:
+        X (list of n numpy arrays of shape (numx2)): first list of persistence diagrams. 
+        Y (list of m numpy arrays of shape (numx2)): second list of persistence diagrams (optional). If None, pairwise distances are computed from the first list only.
+        metric: distance to use. It can be either a string ("sliced_wasserstein", "wasserstein", "hera_wasserstein" (Wasserstein distance computed with Hera---note that Hera is also used for the default option "wasserstein"), "pot_wasserstein" (Wasserstein distance computed with POT), "bottleneck", "persistence_fisher") or a function taking two numpy arrays of shape (nx2) and (mx2) as inputs. If it is a function, make sure that it is symmetric and that it outputs 0 if called on the same two arrays. 
+        n_jobs (int): number of jobs to use for the computation. This uses joblib.Parallel(prefer="threads"), so metrics that do not release the GIL may not scale unless run inside a `joblib.parallel_backend <https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend>`_ block.
+        **kwargs: optional keyword parameters. Any further parameters are passed directly to the distance function. See the docs of the various distance classes in this module.
+
+    Returns: 
+        numpy array of shape (nxm): distance matrix
+    """
+    XX = np.reshape(np.arange(len(X)), [-1,1])
+    YY = None if Y is None or Y is X else np.reshape(np.arange(len(Y)), [-1,1])
+    if metric == "bottleneck":
+        try: 
+            from .. import bottleneck_distance
+            return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(bottleneck_distance, X, Y, **kwargs), n_jobs=n_jobs)
+        except ImportError:
+            print("Gudhi built without CGAL")
+            raise
+    elif metric == "pot_wasserstein":
+        try:
+            from gudhi.wasserstein import wasserstein_distance as pot_wasserstein_distance
+            return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(pot_wasserstein_distance,  X, Y, **kwargs), n_jobs=n_jobs)
+        except ImportError:
+            print("POT (Python Optimal Transport) is not installed. Please install POT or use metric='wasserstein' or metric='hera_wasserstein'")
+            raise
+    elif metric == "sliced_wasserstein":
+        Xproj = _compute_persistence_diagram_projections(X, **kwargs)
+        Yproj = None if Y is None else _compute_persistence_diagram_projections(Y, **kwargs)
+        return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(_sliced_wasserstein_distance_on_projections, Xproj, Yproj), n_jobs=n_jobs)
+    elif type(metric) == str:
+        return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(PAIRWISE_DISTANCE_FUNCTIONS[metric], X, Y, **kwargs), n_jobs=n_jobs)
+    else:
+        return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(metric, X, Y, **kwargs), n_jobs=n_jobs)
+
+class SlicedWassersteinDistance(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the sliced Wasserstein distance matrix from a list of persistence diagrams. The Sliced Wasserstein distance is computed by projecting the persistence diagrams onto lines, comparing the projections with the 1-norm, and finally integrating over all possible lines. See http://proceedings.mlr.press/v70/carriere17a.html for more details. 
+    """
+    def __init__(self, num_directions=10, n_jobs=None):
+        """
+        Constructor for the SlicedWassersteinDistance class.
+
+        Parameters:
+            num_directions (int): number of lines evenly sampled from [-pi/2,pi/2] in order to approximate and speed up the distance computation (default 10). 
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
+        """
+        self.num_directions = num_directions
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the SlicedWassersteinDistance class on a list of persistence diagrams: persistence diagrams are projected onto the different lines. The diagrams themselves and their projections are then stored in numpy arrays, called **diagrams_** and **approx_diag_**.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all sliced Wasserstein distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise sliced Wasserstein distances.
+        """
+        return pairwise_persistence_diagram_distances(X, self.diagrams_, metric="sliced_wasserstein", num_directions=self.num_directions, n_jobs=self.n_jobs)
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply SlicedWassersteinDistance on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: sliced Wasserstein distance.
+        """
+        return _sliced_wasserstein_distance(diag1, diag2, num_directions=self.num_directions)
+
+class BottleneckDistance(BaseEstimator, TransformerMixin):
+    r"""
+    This is a class for computing the bottleneck distance matrix from a list of persistence diagrams.
+
+    :Requires: `CGAL <installation.html#cgal>`_ :math:`\geq` 4.11.0
+    """
+    def __init__(self, epsilon=None, n_jobs=None):
+        """
+        Constructor for the BottleneckDistance class.
+
+        Parameters:
+            epsilon (double): absolute (additive) error tolerated on the distance (default is the smallest positive float), see :func:`gudhi.bottleneck_distance`.
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
+        """
+        self.epsilon = epsilon
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the BottleneckDistance class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams**.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all bottleneck distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise bottleneck distances.
+        """
+        Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric="bottleneck", e=self.epsilon, n_jobs=self.n_jobs)
+        return Xfit
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply BottleneckDistance on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: bottleneck distance.
+        """
+        try: 
+            from .. import bottleneck_distance
+            return bottleneck_distance(diag1, diag2, e=self.epsilon)
+        except ImportError:
+            print("Gudhi built without CGAL")
+            raise
+
+class PersistenceFisherDistance(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the persistence Fisher distance matrix from a list of persistence diagrams. The persistence Fisher distance is obtained by computing the original Fisher distance between the probability distributions associated to the persistence diagrams given by convolving them with a Gaussian kernel. See http://papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams for more details. 
+    """
+    def __init__(self, bandwidth=1., kernel_approx=None, n_jobs=None):
+        """
+        Constructor for the PersistenceFisherDistance class.
+
+        Parameters:
+            bandwidth (double): bandwidth of the Gaussian kernel used to turn persistence diagrams into probability distributions (default 1.).
+            kernel_approx (class): kernel approximation class used to speed up computation (default None). Common kernel approximations classes can be found in the scikit-learn library (such as RBFSampler for instance).   
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
+        """
+        self.bandwidth, self.kernel_approx = bandwidth, kernel_approx
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the PersistenceFisherDistance class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams** and the kernel approximation class (if not None) is applied on them.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all persistence Fisher distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise persistence Fisher distances.
+        """
+        return pairwise_persistence_diagram_distances(X, self.diagrams_, metric="persistence_fisher", bandwidth=self.bandwidth, kernel_approx=self.kernel_approx, n_jobs=self.n_jobs)
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply PersistenceFisherDistance on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: persistence Fisher distance.
+        """
+        return _persistence_fisher_distance(diag1, diag2, bandwidth=self.bandwidth, kernel_approx=self.kernel_approx)
+
+
+class WassersteinDistance(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the Wasserstein distance matrix from a list of persistence diagrams. 
+    """
+
+    def __init__(self, order=1, internal_p=np.inf, mode="hera", delta=0.01, n_jobs=None):
+        """
+        Constructor for the WassersteinDistance class.
+
+        Parameters:
+            order (int): exponent for Wasserstein, default value is 1., see :func:`gudhi.wasserstein.wasserstein_distance`.
+            internal_p (int): ground metric on the (upper-half) plane (i.e. norm l_p in R^2), default value is `np.inf`, see :func:`gudhi.wasserstein.wasserstein_distance`.
+            mode (str): method for computing Wasserstein distance. Either "pot" or "hera". Default set to "hera".
+            delta (float): relative error 1+delta. Used only if mode == "hera".
+            n_jobs (int): number of jobs to use for the computation. See :func:`pairwise_persistence_diagram_distances` for details.
+        """
+        self.order, self.internal_p, self.mode = order, internal_p, mode
+        self.delta = delta
+        self.n_jobs = n_jobs
+
+    def fit(self, X, y=None):
+        """
+        Fit the WassersteinDistance class on a list of persistence diagrams: persistence diagrams are stored in a numpy array called **diagrams**.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        if self.mode not in ("pot", "hera"):
+            raise NameError("Unknown mode. Current available values for mode are 'hera' and 'pot'")
+        self.diagrams_ = X
+        return self
+
+    def transform(self, X):
+        """
+        Compute all Wasserstein distances between the persistence diagrams that were stored after calling the fit() method, and a given list of (possibly different) persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+
+        Returns:
+            numpy array of shape (number of diagrams in **diagrams**) x (number of diagrams in X): matrix of pairwise Wasserstein distances.
+        """
+        if self.mode == "hera":
+            Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric="hera_wasserstein", order=self.order, internal_p=self.internal_p, delta=self.delta, n_jobs=self.n_jobs)
+        else:
+            Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric="pot_wasserstein", order=self.order, internal_p=self.internal_p, matching=False, n_jobs=self.n_jobs)
+        return Xfit
+
+    def __call__(self, diag1, diag2):
+        """
+        Apply WassersteinDistance on a single pair of persistence diagrams and outputs the result.
+
+        Parameters:
+            diag1 (n x 2 numpy array): first input persistence diagram.
+            diag2 (n x 2 numpy array): second input persistence diagram.
+
+        Returns:
+            float: Wasserstein distance.
+        """
+        if self.mode == "hera":
+            return hera_wasserstein_distance(diag1, diag2, order=self.order, internal_p=self.internal_p, delta=self.delta)
+        elif self.mode == "pot":
+            try:
+                from gudhi.wasserstein import wasserstein_distance as pot_wasserstein_distance
+                return pot_wasserstein_distance(diag1, diag2, order=self.order, internal_p=self.internal_p, matching=False)
+            except ImportError:
+                print("POT (Python Optimal Transport) is not installed. Please install POT or use mode='hera'")
+                raise
+        else:
+            raise NameError("Unknown mode. Current available values for mode are 'hera' and 'pot'")
```

## gudhi/representations/preprocessing.py

 * *Ordering differences only*

```diff
@@ -1,434 +1,434 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Mathieu Carri√®re, Vincent Rouvreau
-#
-# Copyright (C) 2018-2019 Inria
-#
-# Modification(s):
-#   - 2021/10 Vincent Rouvreau: Add DimensionSelector
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-from sklearn.base          import BaseEstimator, TransformerMixin
-from sklearn.preprocessing import StandardScaler
-
-
-#############################################
-# Utils #####################################
-#############################################
-
-def _maybe_fit_transform(obj, attr, diag):
-    """
-    In __call__, use transform on the object itself if it has been fitted,
-    otherwise fit_transform on a clone of the object so it doesn't affect future calls.
-    """
-    if hasattr(obj, attr):
-        result = obj.transform([diag])
-    else:
-        result = obj.__class__(**obj.get_params()).fit_transform([diag])
-    return result[0]
-
-class Clamping(BaseEstimator, TransformerMixin):
-    """
-    This is a class for clamping a list of values. It is not meant to be called directly on (a list of) persistence diagrams, but it is rather meant to be used as a parameter for the DiagramScaler class. As such it has the same methods and purpose as common scalers from sklearn.preprocessing such as MinMaxScaler, RobustScaler, StandardScaler, etc. A typical use would be for instance if you want to clamp abscissae or ordinates (or both) of persistence diagrams within a pre-defined interval.
-    """
-    def __init__(self, minimum=-np.inf, maximum=np.inf):
-        """
-        Constructor for the Clamping class.
-
-        Parameters:
-            limit (float): clamping value (default np.inf).
-        """
-        self.minimum = minimum
-        self.maximum = maximum
-
-    def fit(self, X, y=None):
-        """
-        Fit the Clamping class on a list of values (this function actually does nothing but is useful when Clamping is included in a scikit-learn Pipeline).
-
-        Parameters:
-            X (numpy array of size n): input values.
-            y (n x 1 array): value labels (unused).
-        """
-        return self
-
-    def transform(self, X):
-        """
-        Clamp list of values.
-
-        Parameters:
-            X (numpy array of size n): input list of values.
-
-        Returns:
-            numpy array of size n: output list of values.
-        """
-        Xfit = np.clip(X, self.minimum, self.maximum)
-        return Xfit
-
-
-#############################################
-# Preprocessing #############################
-#############################################
-
-class BirthPersistenceTransform(BaseEstimator, TransformerMixin):
-    """
-    This is a class for the affine transformation (x,y) -> (x,y-x) to be applied on persistence diagrams.
-    """
-    def __init__(self):
-        """
-        Constructor for BirthPersistenceTransform class.
-        """
-        return None
-
-    def fit(self, X, y=None):
-        """
-        Fit the BirthPersistenceTransform class on a list of persistence diagrams (this function actually does nothing but is useful when BirthPersistenceTransform is included in a scikit-learn Pipeline).
-
-        Parameters:
-            X (list of n x 2 numpy array): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        return self
-
-    def transform(self, X):
-        """
-        Apply the BirthPersistenceTransform function on the persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy array): input persistence diagrams.
-
-        Returns:
-            list of n x 2 numpy array: transformed persistence diagrams.
-        """
-        Xfit = []
-        for diag in X:
-            #new_diag = np.empty(diag.shape)
-            #np.copyto(new_diag, diag)
-            new_diag = np.copy(diag)
-            new_diag[:,1] = new_diag[:,1] - new_diag[:,0]
-            Xfit.append(new_diag)
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply BirthPersistenceTransform on a single persistence diagram and outputs the result.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            n x 2 numpy array: transformed persistence diagram.
-        """
-        return self.transform([diag])[0]
-
-class DiagramScaler(BaseEstimator, TransformerMixin):
-    """
-    This is a class for preprocessing persistence diagrams with a given list of scalers, such as those included in scikit-learn.
-    """
-    def __init__(self, use=False, scalers=[]):
-        """
-        Constructor for the DiagramScaler class.
-
-        Parameters:
-            use (bool): whether to use the class or not (default False).
-            scalers (list of classes): list of scalers to be fit on the persistence diagrams (default []). Each element of the list is a tuple with two elements: the first one is a list of coordinates, and the second one is a scaler (i.e. a class with fit() and transform() methods) that is going to be applied to these coordinates. Common scalers can be found in the scikit-learn library (such as MinMaxScaler for instance).
-        """
-        self.scalers  = scalers
-        self.use      = use
-
-    def fit(self, X, y=None):
-        """
-        Fit the DiagramScaler class on a list of persistence diagrams: persistence diagrams are concatenated in a big numpy array, and scalers are fit (by calling their fit() method) on their corresponding coordinates in this big array.
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.is_fitted_ = True
-        if self.use:
-            if len(X) == 1:
-                P = X[0]
-            else:
-                P = np.concatenate(X,0)
-            for (indices, scaler) in self.scalers:
-                scaler.fit(np.reshape(P[:,indices], [-1, 1]))
-        return self
-
-    def transform(self, X):
-        """
-        Apply the DiagramScaler function on the persistence diagrams. The fitted scalers are applied (by calling their transform() method) to their corresponding coordinates in each persistence diagram individually.  
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-
-        Returns:
-            list of n x 2 or n x 1 numpy arrays: transformed persistence diagrams.
-        """
-        Xfit = [np.copy(d) for d in X]
-        if self.use:
-            for i in range(len(Xfit)):
-                if Xfit[i].shape[0] > 0:
-                    for (indices, scaler) in self.scalers:
-                        for I in indices:
-                            Xfit[i][:,I] = np.squeeze(scaler.transform(np.reshape(Xfit[i][:,I], [-1,1])))
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply DiagramScaler on a single persistence diagram and outputs the result.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            n x 2 numpy array: transformed persistence diagram.
-        """
-        return _maybe_fit_transform(self, 'is_fitted_', diag)
-
-class Padding(BaseEstimator, TransformerMixin):
-    """
-    This is a class for padding a list of persistence diagrams with dummy points, so that all persistence diagrams end up with the same number of points.
-    """
-    def __init__(self, use=False):
-        """
-        Constructor for the Padding class.
-
-        Parameters:
-            use (bool): whether to use the class or not (default False).
-        """
-        self.use = use
-
-    def fit(self, X, y=None):
-        """
-        Fit the Padding class on a list of persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        self.max_pts_ = max(len(diag) for diag in X)
-        return self
-
-    def transform(self, X):
-        """
-        Add dummy points to each persistence diagram so that they all have the same cardinality. All points are given an additional coordinate indicating if the point was added after padding (0) or already present before (1).  
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-
-        Returns:
-            list of n x 3 or n x 2 numpy arrays: padded persistence diagrams.
-        """
-        if self.use:
-            Xfit, num_diag = [], len(X)
-            for diag in X:
-                diag_pad = np.pad(diag, ((0,max(0, self.max_pts_ - diag.shape[0])), (0,1)), "constant", constant_values=((0,0),(0,0)))
-                diag_pad[:diag.shape[0],2] = np.ones(diag.shape[0])
-                Xfit.append(diag_pad)                    
-        else:
-            Xfit = X
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply Padding on a single persistence diagram and outputs the result.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            n x 2 numpy array: padded persistence diagram.
-        """
-        return _maybe_fit_transform(self, 'max_pts_', diag)
-
-class ProminentPoints(BaseEstimator, TransformerMixin):
-    """
-    This is a class for removing points that are close or far from the diagonal in persistence diagrams.  If persistence diagrams are n x 2 numpy arrays (i.e. persistence diagrams with ordinary features), points are ordered and thresholded by distance-to-diagonal. If persistence diagrams are n x 1 numpy arrays (i.e. persistence diagrams with essential features), points are not ordered and thresholded by first coordinate.
-    """
-    def __init__(self, use=False, num_pts=10, threshold=-1, location="upper"):
-        """
-        Constructor for the ProminentPoints class.
-     
-        Parameters:
-            use (bool): whether to use the class or not (default False).
-            location (string): either "upper" or "lower" (default "upper"). Whether to keep the points that are far away ("upper") or close ("lower") to the diagonal.
-            num_pts (int): cardinality threshold (default 10). If location == "upper", keep the top **num_pts** points that are the farthest away from the diagonal. If location == "lower", keep the top **num_pts** points that are the closest to the diagonal. 
-            threshold (float): distance-to-diagonal threshold (default -1). If location == "upper", keep the points that are at least at a distance **threshold** from the diagonal. If location == "lower", keep the points that are at most at a distance **threshold** from the diagonal. 
-        """
-        self.num_pts    = num_pts
-        self.threshold  = threshold
-        self.use        = use
-        self.location   = location
-
-    def fit(self, X, y=None):
-        """
-        Fit the ProminentPoints class on a list of persistence diagrams (this function actually does nothing but is useful when ProminentPoints is included in a scikit-learn Pipeline).
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        return self
-
-    def transform(self, X):
-        """
-        If location == "upper", first select the top **num_pts** points that are the farthest away from the diagonal, then select and return from these points the ones that are at least at distance **threshold** from the diagonal for each persistence diagram individually. If location == "lower", first select the top **num_pts** points that are the closest to the diagonal, then select and return from these points the ones that are at most at distance **threshold** from the diagonal for each persistence diagram individually.
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-
-        Returns:
-            list of n x 2 or n x 1 numpy arrays: thresholded persistence diagrams.
-        """
-        if self.use:
-            Xfit, num_diag = [], len(X)
-            for i in range(num_diag):
-                diag = X[i]
-                if diag.shape[1] >= 2:
-                    if diag.shape[0] > 0:
-                        pers       = np.abs(diag[:,1] - diag[:,0])
-                        idx_thresh = pers >= self.threshold
-                        thresh_diag, thresh_pers  = diag[idx_thresh], pers[idx_thresh]
-                        sort_index  = np.flip(np.argsort(thresh_pers, axis=None), 0)
-                        if self.location == "upper":
-                            new_diag = thresh_diag[sort_index[:min(self.num_pts, thresh_diag.shape[0])],:]
-                        if self.location == "lower":
-                            new_diag = np.concatenate( [ thresh_diag[sort_index[min(self.num_pts, thresh_diag.shape[0]):],:], diag[~idx_thresh] ], axis=0)
-                    else:
-                        new_diag = diag
-
-                else:
-                    if diag.shape[0] > 0:
-                        birth      = diag[:,:1]
-                        idx_thresh = birth >= self.threshold
-                        thresh_diag, thresh_birth  = diag[idx_thresh], birth[idx_thresh]
-                        if self.location == "upper":
-                            new_diag = thresh_diag[:min(self.num_pts, thresh_diag.shape[0]),:]
-                        if self.location == "lower":
-                            new_diag = np.concatenate( [ thresh_diag[min(self.num_pts, thresh_diag.shape[0]):,:], diag[~idx_thresh] ], axis=0)
-                    else:
-                        new_diag = diag
-
-                Xfit.append(new_diag)                    
-        else:
-            Xfit = X
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply ProminentPoints on a single persistence diagram and outputs the result.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            n x 2 numpy array: thresholded persistence diagram.
-        """
-        return self.transform([diag])[0]
-
-class DiagramSelector(BaseEstimator, TransformerMixin):
-    """
-    This is a class for extracting finite or essential points in persistence diagrams.
-    """
-    def __init__(self, use=False, limit=np.inf, point_type="finite"):
-        """
-        Constructor for the DiagramSelector class.
-
-        Parameters:
-            use (bool): whether to use the class or not (default False).
-            limit (float): second coordinate value that is the criterion for being an essential point (default numpy.inf).
-            point_type (string): either "finite" or "essential". The type of the points that are going to be extracted.
-        """
-        self.use, self.limit, self.point_type = use, limit, point_type
-
-    def fit(self, X, y=None):
-        """
-        Fit the DiagramSelector class on a list of persistence diagrams (this function actually does nothing but is useful when DiagramSelector is included in a scikit-learn Pipeline).
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        return self
-
-    def transform(self, X):
-        """
-        Extract and return the finite or essential points of each persistence diagram individually.
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-
-        Returns:
-            list of n x 2 or n x 1 numpy arrays: extracted persistence diagrams.
-        """
-        if self.use:
-            Xfit, num_diag = [], len(X)
-            if self.point_type == "finite":
-                Xfit = [ diag[diag[:,1] < self.limit] if diag.shape[0] != 0 else diag for diag in X]
-            else:
-                Xfit = [ diag[diag[:,1] >= self.limit, 0:1] if diag.shape[0] != 0 else diag for diag in X]
-        else:
-            Xfit = X
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply DiagramSelector on a single persistence diagram and outputs the result.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            n x 2 numpy array: extracted persistence diagram.
-        """
-        return self.transform([diag])[0]
-
-
-# Mermaid sequence diagram - https://mermaid-js.github.io/mermaid-live-editor/
-# sequenceDiagram
-#     USER->>DimensionSelector: fit_transform(<br/>[[array( Hi(X0) ), array( Hj(X0) ), ...],<br/> [array( Hi(X1) ), array( Hj(X1) ), ...],<br/> ...])
-#     DimensionSelector->>thread1: _transform([array( Hi(X0) ), array( Hj(X0) )], ...)
-#     DimensionSelector->>thread2: _transform([array( Hi(X1) ), array( Hj(X1) )], ...)
-#     Note right of DimensionSelector: ...
-#     thread1->>DimensionSelector: array( Hn(X0) )
-#     thread2->>DimensionSelector: array( Hn(X1) )
-#     Note right of DimensionSelector: ...
-#     DimensionSelector->>USER: [array( Hn(X0) ), <br/> array( Hn(X1) ), <br/> ...]
-
-class DimensionSelector(BaseEstimator, TransformerMixin):
-    """
-    This is a class to select persistence diagrams in a specific dimension from its index.
-    """
-
-    def __init__(self, index=0):
-        """
-        Constructor for the DimensionSelector class.
-
-        Parameters:
-            index (int): The returned persistence diagrams dimension index. Default value is `0`.
-        """
-        self.index = index
-
-    def fit(self, X, Y=None):
-        """
-        Nothing to be done, but useful when included in a scikit-learn Pipeline.
-        """
-        return self
-
-    def transform(self, X, Y=None):
-        """
-        Select persistence diagrams from its dimension.
-
-        Parameters:
-            X (list of list of tuple): List of list of persistence pairs, i.e.
-                `[[array( Hi(X0) ), array( Hj(X0) ), ...], [array( Hi(X1) ), array( Hj(X1) ), ...], ...]` 
-
-        Returns:
-            list of tuple:
-            Persistence diagrams in a specific dimension. i.e. if `index` was set to `m` and `Hn` is at index `m` of
-            the input, it returns `[array( Hn(X0) ), array( Hn(X1), ...]`
-        """
-
-        return [persistence[self.index] for persistence in X]
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Mathieu Carri√®re, Vincent Rouvreau
+#
+# Copyright (C) 2018-2019 Inria
+#
+# Modification(s):
+#   - 2021/10 Vincent Rouvreau: Add DimensionSelector
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+from sklearn.base          import BaseEstimator, TransformerMixin
+from sklearn.preprocessing import StandardScaler
+
+
+#############################################
+# Utils #####################################
+#############################################
+
+def _maybe_fit_transform(obj, attr, diag):
+    """
+    In __call__, use transform on the object itself if it has been fitted,
+    otherwise fit_transform on a clone of the object so it doesn't affect future calls.
+    """
+    if hasattr(obj, attr):
+        result = obj.transform([diag])
+    else:
+        result = obj.__class__(**obj.get_params()).fit_transform([diag])
+    return result[0]
+
+class Clamping(BaseEstimator, TransformerMixin):
+    """
+    This is a class for clamping a list of values. It is not meant to be called directly on (a list of) persistence diagrams, but it is rather meant to be used as a parameter for the DiagramScaler class. As such it has the same methods and purpose as common scalers from sklearn.preprocessing such as MinMaxScaler, RobustScaler, StandardScaler, etc. A typical use would be for instance if you want to clamp abscissae or ordinates (or both) of persistence diagrams within a pre-defined interval.
+    """
+    def __init__(self, minimum=-np.inf, maximum=np.inf):
+        """
+        Constructor for the Clamping class.
+
+        Parameters:
+            limit (float): clamping value (default np.inf).
+        """
+        self.minimum = minimum
+        self.maximum = maximum
+
+    def fit(self, X, y=None):
+        """
+        Fit the Clamping class on a list of values (this function actually does nothing but is useful when Clamping is included in a scikit-learn Pipeline).
+
+        Parameters:
+            X (numpy array of size n): input values.
+            y (n x 1 array): value labels (unused).
+        """
+        return self
+
+    def transform(self, X):
+        """
+        Clamp list of values.
+
+        Parameters:
+            X (numpy array of size n): input list of values.
+
+        Returns:
+            numpy array of size n: output list of values.
+        """
+        Xfit = np.clip(X, self.minimum, self.maximum)
+        return Xfit
+
+
+#############################################
+# Preprocessing #############################
+#############################################
+
+class BirthPersistenceTransform(BaseEstimator, TransformerMixin):
+    """
+    This is a class for the affine transformation (x,y) -> (x,y-x) to be applied on persistence diagrams.
+    """
+    def __init__(self):
+        """
+        Constructor for BirthPersistenceTransform class.
+        """
+        return None
+
+    def fit(self, X, y=None):
+        """
+        Fit the BirthPersistenceTransform class on a list of persistence diagrams (this function actually does nothing but is useful when BirthPersistenceTransform is included in a scikit-learn Pipeline).
+
+        Parameters:
+            X (list of n x 2 numpy array): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        return self
+
+    def transform(self, X):
+        """
+        Apply the BirthPersistenceTransform function on the persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy array): input persistence diagrams.
+
+        Returns:
+            list of n x 2 numpy array: transformed persistence diagrams.
+        """
+        Xfit = []
+        for diag in X:
+            #new_diag = np.empty(diag.shape)
+            #np.copyto(new_diag, diag)
+            new_diag = np.copy(diag)
+            new_diag[:,1] = new_diag[:,1] - new_diag[:,0]
+            Xfit.append(new_diag)
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply BirthPersistenceTransform on a single persistence diagram and outputs the result.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            n x 2 numpy array: transformed persistence diagram.
+        """
+        return self.transform([diag])[0]
+
+class DiagramScaler(BaseEstimator, TransformerMixin):
+    """
+    This is a class for preprocessing persistence diagrams with a given list of scalers, such as those included in scikit-learn.
+    """
+    def __init__(self, use=False, scalers=[]):
+        """
+        Constructor for the DiagramScaler class.
+
+        Parameters:
+            use (bool): whether to use the class or not (default False).
+            scalers (list of classes): list of scalers to be fit on the persistence diagrams (default []). Each element of the list is a tuple with two elements: the first one is a list of coordinates, and the second one is a scaler (i.e. a class with fit() and transform() methods) that is going to be applied to these coordinates. Common scalers can be found in the scikit-learn library (such as MinMaxScaler for instance).
+        """
+        self.scalers  = scalers
+        self.use      = use
+
+    def fit(self, X, y=None):
+        """
+        Fit the DiagramScaler class on a list of persistence diagrams: persistence diagrams are concatenated in a big numpy array, and scalers are fit (by calling their fit() method) on their corresponding coordinates in this big array.
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.is_fitted_ = True
+        if self.use:
+            if len(X) == 1:
+                P = X[0]
+            else:
+                P = np.concatenate(X,0)
+            for (indices, scaler) in self.scalers:
+                scaler.fit(np.reshape(P[:,indices], [-1, 1]))
+        return self
+
+    def transform(self, X):
+        """
+        Apply the DiagramScaler function on the persistence diagrams. The fitted scalers are applied (by calling their transform() method) to their corresponding coordinates in each persistence diagram individually.  
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+
+        Returns:
+            list of n x 2 or n x 1 numpy arrays: transformed persistence diagrams.
+        """
+        Xfit = [np.copy(d) for d in X]
+        if self.use:
+            for i in range(len(Xfit)):
+                if Xfit[i].shape[0] > 0:
+                    for (indices, scaler) in self.scalers:
+                        for I in indices:
+                            Xfit[i][:,I] = np.squeeze(scaler.transform(np.reshape(Xfit[i][:,I], [-1,1])))
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply DiagramScaler on a single persistence diagram and outputs the result.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            n x 2 numpy array: transformed persistence diagram.
+        """
+        return _maybe_fit_transform(self, 'is_fitted_', diag)
+
+class Padding(BaseEstimator, TransformerMixin):
+    """
+    This is a class for padding a list of persistence diagrams with dummy points, so that all persistence diagrams end up with the same number of points.
+    """
+    def __init__(self, use=False):
+        """
+        Constructor for the Padding class.
+
+        Parameters:
+            use (bool): whether to use the class or not (default False).
+        """
+        self.use = use
+
+    def fit(self, X, y=None):
+        """
+        Fit the Padding class on a list of persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        self.max_pts_ = max(len(diag) for diag in X)
+        return self
+
+    def transform(self, X):
+        """
+        Add dummy points to each persistence diagram so that they all have the same cardinality. All points are given an additional coordinate indicating if the point was added after padding (0) or already present before (1).  
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+
+        Returns:
+            list of n x 3 or n x 2 numpy arrays: padded persistence diagrams.
+        """
+        if self.use:
+            Xfit, num_diag = [], len(X)
+            for diag in X:
+                diag_pad = np.pad(diag, ((0,max(0, self.max_pts_ - diag.shape[0])), (0,1)), "constant", constant_values=((0,0),(0,0)))
+                diag_pad[:diag.shape[0],2] = np.ones(diag.shape[0])
+                Xfit.append(diag_pad)                    
+        else:
+            Xfit = X
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply Padding on a single persistence diagram and outputs the result.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            n x 2 numpy array: padded persistence diagram.
+        """
+        return _maybe_fit_transform(self, 'max_pts_', diag)
+
+class ProminentPoints(BaseEstimator, TransformerMixin):
+    """
+    This is a class for removing points that are close or far from the diagonal in persistence diagrams.  If persistence diagrams are n x 2 numpy arrays (i.e. persistence diagrams with ordinary features), points are ordered and thresholded by distance-to-diagonal. If persistence diagrams are n x 1 numpy arrays (i.e. persistence diagrams with essential features), points are not ordered and thresholded by first coordinate.
+    """
+    def __init__(self, use=False, num_pts=10, threshold=-1, location="upper"):
+        """
+        Constructor for the ProminentPoints class.
+     
+        Parameters:
+            use (bool): whether to use the class or not (default False).
+            location (string): either "upper" or "lower" (default "upper"). Whether to keep the points that are far away ("upper") or close ("lower") to the diagonal.
+            num_pts (int): cardinality threshold (default 10). If location == "upper", keep the top **num_pts** points that are the farthest away from the diagonal. If location == "lower", keep the top **num_pts** points that are the closest to the diagonal. 
+            threshold (float): distance-to-diagonal threshold (default -1). If location == "upper", keep the points that are at least at a distance **threshold** from the diagonal. If location == "lower", keep the points that are at most at a distance **threshold** from the diagonal. 
+        """
+        self.num_pts    = num_pts
+        self.threshold  = threshold
+        self.use        = use
+        self.location   = location
+
+    def fit(self, X, y=None):
+        """
+        Fit the ProminentPoints class on a list of persistence diagrams (this function actually does nothing but is useful when ProminentPoints is included in a scikit-learn Pipeline).
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        return self
+
+    def transform(self, X):
+        """
+        If location == "upper", first select the top **num_pts** points that are the farthest away from the diagonal, then select and return from these points the ones that are at least at distance **threshold** from the diagonal for each persistence diagram individually. If location == "lower", first select the top **num_pts** points that are the closest to the diagonal, then select and return from these points the ones that are at most at distance **threshold** from the diagonal for each persistence diagram individually.
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+
+        Returns:
+            list of n x 2 or n x 1 numpy arrays: thresholded persistence diagrams.
+        """
+        if self.use:
+            Xfit, num_diag = [], len(X)
+            for i in range(num_diag):
+                diag = X[i]
+                if diag.shape[1] >= 2:
+                    if diag.shape[0] > 0:
+                        pers       = np.abs(diag[:,1] - diag[:,0])
+                        idx_thresh = pers >= self.threshold
+                        thresh_diag, thresh_pers  = diag[idx_thresh], pers[idx_thresh]
+                        sort_index  = np.flip(np.argsort(thresh_pers, axis=None), 0)
+                        if self.location == "upper":
+                            new_diag = thresh_diag[sort_index[:min(self.num_pts, thresh_diag.shape[0])],:]
+                        if self.location == "lower":
+                            new_diag = np.concatenate( [ thresh_diag[sort_index[min(self.num_pts, thresh_diag.shape[0]):],:], diag[~idx_thresh] ], axis=0)
+                    else:
+                        new_diag = diag
+
+                else:
+                    if diag.shape[0] > 0:
+                        birth      = diag[:,:1]
+                        idx_thresh = birth >= self.threshold
+                        thresh_diag, thresh_birth  = diag[idx_thresh], birth[idx_thresh]
+                        if self.location == "upper":
+                            new_diag = thresh_diag[:min(self.num_pts, thresh_diag.shape[0]),:]
+                        if self.location == "lower":
+                            new_diag = np.concatenate( [ thresh_diag[min(self.num_pts, thresh_diag.shape[0]):,:], diag[~idx_thresh] ], axis=0)
+                    else:
+                        new_diag = diag
+
+                Xfit.append(new_diag)                    
+        else:
+            Xfit = X
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply ProminentPoints on a single persistence diagram and outputs the result.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            n x 2 numpy array: thresholded persistence diagram.
+        """
+        return self.transform([diag])[0]
+
+class DiagramSelector(BaseEstimator, TransformerMixin):
+    """
+    This is a class for extracting finite or essential points in persistence diagrams.
+    """
+    def __init__(self, use=False, limit=np.inf, point_type="finite"):
+        """
+        Constructor for the DiagramSelector class.
+
+        Parameters:
+            use (bool): whether to use the class or not (default False).
+            limit (float): second coordinate value that is the criterion for being an essential point (default numpy.inf).
+            point_type (string): either "finite" or "essential". The type of the points that are going to be extracted.
+        """
+        self.use, self.limit, self.point_type = use, limit, point_type
+
+    def fit(self, X, y=None):
+        """
+        Fit the DiagramSelector class on a list of persistence diagrams (this function actually does nothing but is useful when DiagramSelector is included in a scikit-learn Pipeline).
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        return self
+
+    def transform(self, X):
+        """
+        Extract and return the finite or essential points of each persistence diagram individually.
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+
+        Returns:
+            list of n x 2 or n x 1 numpy arrays: extracted persistence diagrams.
+        """
+        if self.use:
+            Xfit, num_diag = [], len(X)
+            if self.point_type == "finite":
+                Xfit = [ diag[diag[:,1] < self.limit] if diag.shape[0] != 0 else diag for diag in X]
+            else:
+                Xfit = [ diag[diag[:,1] >= self.limit, 0:1] if diag.shape[0] != 0 else diag for diag in X]
+        else:
+            Xfit = X
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply DiagramSelector on a single persistence diagram and outputs the result.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            n x 2 numpy array: extracted persistence diagram.
+        """
+        return self.transform([diag])[0]
+
+
+# Mermaid sequence diagram - https://mermaid-js.github.io/mermaid-live-editor/
+# sequenceDiagram
+#     USER->>DimensionSelector: fit_transform(<br/>[[array( Hi(X0) ), array( Hj(X0) ), ...],<br/> [array( Hi(X1) ), array( Hj(X1) ), ...],<br/> ...])
+#     DimensionSelector->>thread1: _transform([array( Hi(X0) ), array( Hj(X0) )], ...)
+#     DimensionSelector->>thread2: _transform([array( Hi(X1) ), array( Hj(X1) )], ...)
+#     Note right of DimensionSelector: ...
+#     thread1->>DimensionSelector: array( Hn(X0) )
+#     thread2->>DimensionSelector: array( Hn(X1) )
+#     Note right of DimensionSelector: ...
+#     DimensionSelector->>USER: [array( Hn(X0) ), <br/> array( Hn(X1) ), <br/> ...]
+
+class DimensionSelector(BaseEstimator, TransformerMixin):
+    """
+    This is a class to select persistence diagrams in a specific dimension from its index.
+    """
+
+    def __init__(self, index=0):
+        """
+        Constructor for the DimensionSelector class.
+
+        Parameters:
+            index (int): The returned persistence diagrams dimension index. Default value is `0`.
+        """
+        self.index = index
+
+    def fit(self, X, Y=None):
+        """
+        Nothing to be done, but useful when included in a scikit-learn Pipeline.
+        """
+        return self
+
+    def transform(self, X, Y=None):
+        """
+        Select persistence diagrams from its dimension.
+
+        Parameters:
+            X (list of list of tuple): List of list of persistence pairs, i.e.
+                `[[array( Hi(X0) ), array( Hj(X0) ), ...], [array( Hi(X1) ), array( Hj(X1) ), ...], ...]` 
+
+        Returns:
+            list of tuple:
+            Persistence diagrams in a specific dimension. i.e. if `index` was set to `m` and `Hn` is at index `m` of
+            the input, it returns `[array( Hn(X0) ), array( Hn(X1), ...]`
+        """
+
+        return [persistence[self.index] for persistence in X]
```

## gudhi/representations/vector_methods.py

 * *Ordering differences only*

```diff
@@ -1,821 +1,821 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Mathieu Carri√®re, Martin Royer, Gard Spreemann
-#
-# Copyright (C) 2018-2020 Inria
-#
-# Modification(s):
-#   - 2020/06 Martin: ATOL integration
-#   - 2020/12 Gard: A more flexible Betti curve class capable of computing exact curves.
-#   - 2021/11 Vincent Rouvreau: factorize _automatic_sample_range
-
-import numpy as np
-from sklearn.base          import BaseEstimator, TransformerMixin
-from sklearn.exceptions    import NotFittedError
-from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler
-from sklearn.metrics       import pairwise
-try:
-    # New location since 1.0
-    from sklearn.metrics     import DistanceMetric
-except ImportError:
-    # Will be removed in 1.3
-    from sklearn.neighbors     import DistanceMetric
-
-from .preprocessing import DiagramScaler, BirthPersistenceTransform, _maybe_fit_transform
-
-#############################################
-# Finite Vectorization methods ##############
-#############################################
-
-class PersistenceImage(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing persistence images from a list of persistence diagrams. A persistence image is a 2D function computed from a persistence diagram by convolving the diagram points with a weighted Gaussian kernel. The plane is then discretized into an image with pixels, which is flattened and returned as a vector. See http://jmlr.org/papers/v18/16-337.html for more details.
-    """
-    def __init__(self, bandwidth=1., weight=lambda x: 1, resolution=[20,20], im_range=[np.nan, np.nan, np.nan, np.nan]):
-        """
-        Constructor for the PersistenceImage class.
-
-        Parameters:
-            bandwidth (double): bandwidth of the Gaussian kernel (default 1.).
-            weight (function): weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie lists or numpy arrays of the form [p_x,p_y].
-            resolution ([int,int]): size (in pixels) of the persistence image (default [20,20]).
-            im_range ([double,double,double,double]): minimum and maximum of each axis of the persistence image, of the form [x_min, x_max, y_min, y_max] (default [numpy.nan, numpy.nan, numpy.nan, numpyp.nan]). If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
-        """
-        self.bandwidth, self.weight = bandwidth, weight
-        self.resolution, self.im_range = resolution, im_range
-
-    def fit(self, X, y=None):
-        """
-        Fit the PersistenceImage class on a list of persistence diagrams: if any of the values in **im_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        if np.isnan(np.array(self.im_range)).any():
-            if all(len(d) == 0 for d in X):
-                self.im_range_fixed_ = self.im_range
-            else:
-                new_X = BirthPersistenceTransform().fit_transform(X)
-                pre = DiagramScaler(use=True, scalers=[([0], MinMaxScaler()), ([1], MinMaxScaler())]).fit(new_X,y)
-                [mx,my],[Mx,My] = [pre.scalers[0][1].data_min_[0], pre.scalers[1][1].data_min_[0]], [pre.scalers[0][1].data_max_[0], pre.scalers[1][1].data_max_[0]]
-                self.im_range_fixed_ = np.where(np.isnan(np.array(self.im_range)), np.array([mx, Mx, my, My]), np.array(self.im_range))
-        else:
-            self.im_range_fixed_ = self.im_range
-        return self
-
-    def transform(self, X):
-        """
-        Compute the persistence image for each persistence diagram individually and store the results in a single numpy array.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-    
-        Returns:
-            numpy array with shape (number of diagrams) x (number of pixels = **resolution[0]** x **resolution[1]**): output persistence images.
-        """
-        num_diag, Xfit = len(X), []
-        new_X = BirthPersistenceTransform().fit_transform(X)
-
-        for i in range(num_diag):
-
-            diagram, num_pts_in_diag = new_X[i], X[i].shape[0]
-
-            w = np.empty(num_pts_in_diag)
-            for j in range(num_pts_in_diag):
-                w[j] = self.weight(diagram[j,:])
-
-            x_values, y_values = np.linspace(self.im_range_fixed_[0], self.im_range_fixed_[1], self.resolution[0]), np.linspace(self.im_range_fixed_[2], self.im_range_fixed_[3], self.resolution[1])
-            Xs, Ys = np.tile((diagram[:,0][:,np.newaxis,np.newaxis]-x_values[np.newaxis,np.newaxis,:]),[1,self.resolution[1],1]), np.tile(diagram[:,1][:,np.newaxis,np.newaxis]-y_values[np.newaxis,:,np.newaxis],[1,1,self.resolution[0]])
-            image = np.tensordot(w, np.exp((-np.square(Xs)-np.square(Ys))/(2*np.square(self.bandwidth)))/(np.square(self.bandwidth)*2*np.pi), 1)
-
-            Xfit.append(image.flatten()[np.newaxis,:])
-
-        Xfit = np.concatenate(Xfit, 0)
-
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply PersistenceImage on a single persistence diagram and outputs the result.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            numpy array with shape (number of pixels = **resolution[0]** x **resolution[1]**):: output persistence image.
-        """
-        return _maybe_fit_transform(self, 'im_range_fixed_', diag)
-
-def _automatic_sample_range(sample_range, X):
-        """
-        Compute and returns sample range from the persistence diagrams if one of the sample_range values is numpy.nan.
-
-        Parameters:
-            sample_range (a numpy array of 2 float): minimum and maximum of all piecewise-linear function domains, of
-                the form [x_min, x_max].
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        nan_in_range = np.isnan(sample_range)
-        if nan_in_range.any():
-            try:
-                pre = DiagramScaler(use=True, scalers=[([0], MinMaxScaler()), ([1], MinMaxScaler())]).fit(X)
-                [mx,my] = [pre.scalers[0][1].data_min_[0], pre.scalers[1][1].data_min_[0]]
-                [Mx,My] = [pre.scalers[0][1].data_max_[0], pre.scalers[1][1].data_max_[0]]
-                return np.where(nan_in_range, np.array([mx, My]), sample_range)
-            except ValueError:
-                # Empty persistence diagram case - https://github.com/GUDHI/gudhi-devel/issues/507
-                pass
-        return sample_range
-
-
-def _trim_endpoints(x, are_endpoints_nan):
-    if are_endpoints_nan[0]:
-        x = x[1:]
-    if are_endpoints_nan[1]:
-        x = x[:-1]
-    return x
-
-
-def _grid_from_sample_range(self, X):
-    sample_range = np.array(self.sample_range)
-    self.nan_in_range_ = np.isnan(sample_range)
-    self.new_resolution_ = self.resolution
-    if not self.keep_endpoints:
-        self.new_resolution_ += self.nan_in_range_.sum()
-    self.sample_range_fixed_ = _automatic_sample_range(sample_range, X)
-    self.grid_ = np.linspace(self.sample_range_fixed_[0], self.sample_range_fixed_[1], self.new_resolution_)
-    if not self.keep_endpoints:
-        self.grid_ = _trim_endpoints(self.grid_, self.nan_in_range_)
-
-
-class Landscape(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing persistence landscapes from a list of persistence diagrams. A persistence landscape is a collection of 1D piecewise-linear functions computed from the rank function associated to the persistence diagram. These piecewise-linear functions are then sampled evenly on a given range and the corresponding vectors of samples are concatenated and returned. See http://jmlr.org/papers/v16/bubenik15a.html for more details.
-
-    Attributes:
-        grid_ (1d array): The grid on which the landscapes are computed.
-    """
-    def __init__(self, num_landscapes=5, resolution=100, sample_range=[np.nan, np.nan], *, keep_endpoints=False):
-        """
-        Constructor for the Landscape class.
-
-        Parameters:
-            num_landscapes (int): number of piecewise-linear functions to output (default 5).
-            resolution (int): number of sample for all piecewise-linear functions (default 100).
-            sample_range ([double, double]): minimum and maximum of all piecewise-linear function domains, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
-            keep_endpoints (bool): when computing `sample_range`, use the exact extremities (where the value is always 0). This is mostly useful for plotting, the default is to use a slightly smaller range.
-        """
-        self.num_landscapes, self.resolution, self.sample_range = num_landscapes, resolution, sample_range
-        self.keep_endpoints = keep_endpoints
-
-    def fit(self, X, y=None):
-        """
-        Fit the Landscape class on a list of persistence diagrams: if any of the values in **sample_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        _grid_from_sample_range(self, X)
-        return self
-
-    def transform(self, X):
-        """
-        Compute the persistence landscape for each persistence diagram individually and concatenate the results.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-    
-        Returns:
-            numpy array with shape (number of diagrams) x (number of samples = **num_landscapes** x **resolution**): output persistence landscapes.
-        """
-
-        Xfit = []
-        x_values = self.grid_
-        for diag in X:
-            midpoints, heights = (diag[:, 0] + diag[:, 1]) / 2., (diag[:, 1] - diag[:, 0]) / 2.
-            tent_functions = np.maximum(heights[None, :] - np.abs(x_values[:, None] - midpoints[None, :]), 0)
-            n_points = diag.shape[0]
-            # Complete the array with zeros to get the right number of landscapes
-            if self.num_landscapes > n_points:
-                tent_functions = np.concatenate(
-                    [tent_functions, np.zeros((tent_functions.shape[0], self.num_landscapes-n_points))],
-                    axis=1
-                )
-            tent_functions.partition(tent_functions.shape[1]-self.num_landscapes, axis=1)
-            landscapes = np.sort(tent_functions[:, -self.num_landscapes:], axis=1)[:, ::-1].T
-
-            landscapes = np.sqrt(2) * np.ravel(landscapes)
-            Xfit.append(landscapes)
-
-        return np.stack(Xfit, axis=0)
-
-    def __call__(self, diag):
-        """
-        Apply Landscape on a single persistence diagram and outputs the result.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            numpy array with shape (number of samples = **num_landscapes** x **resolution**): output persistence landscape.
-        """
-        return _maybe_fit_transform(self, 'grid_', diag)
-
-class Silhouette(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing persistence silhouettes from a list of persistence diagrams. A persistence silhouette is computed by taking a weighted average of the collection of 1D piecewise-linear functions given by the persistence landscapes, and then by evenly sampling this average on a given range. Finally, the corresponding vector of samples is returned. See https://arxiv.org/abs/1312.0308 for more details.
-
-    Attributes:
-        grid_ (1d array): The grid on which the silhouette is computed.
-    """
-    def __init__(self, weight=lambda x: 1, resolution=100, sample_range=[np.nan, np.nan], *, keep_endpoints=False):
-        """
-        Constructor for the Silhouette class.
-
-        Parameters:
-            weight (function): weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie on lists or numpy arrays of the form [p_x,p_y].
-            resolution (int): number of samples for the weighted average (default 100).
-            sample_range ([double, double]): minimum and maximum for the weighted average domain, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
-            keep_endpoints (bool): when computing `sample_range`, use the exact extremities (where the value is always 0). This is mostly useful for plotting, the default is to use a slightly smaller range.
-        """
-        self.weight, self.resolution, self.sample_range = weight, resolution, sample_range
-        self.keep_endpoints = keep_endpoints
-
-    def fit(self, X, y=None):
-        """
-        Fit the Silhouette class on a list of persistence diagrams: if any of the values in **sample_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        _grid_from_sample_range(self, X)
-        return self
-
-    def transform(self, X):
-        """
-        Compute the persistence silhouette for each persistence diagram individually and concatenate the results.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-    
-        Returns:
-            numpy array with shape (number of diagrams) x (**resolution**): output persistence silhouettes.
-        """
-        Xfit = []
-        x_values = self.grid_
-
-        for diag in X:
-            midpoints, heights = (diag[:, 0] + diag[:, 1]) / 2., (diag[:, 1] - diag[:, 0]) / 2.
-            weights = np.array([self.weight(pt) for pt in diag])
-            total_weight = np.sum(weights)
-
-            tent_functions = np.maximum(heights[None, :] - np.abs(x_values[:, None] - midpoints[None, :]), 0)
-            silhouette = np.sum(weights[None, :] / total_weight * tent_functions, axis=1)
-            Xfit.append(silhouette * np.sqrt(2))
-
-        return np.stack(Xfit, axis=0)
-
-    def __call__(self, diag):
-        """
-        Apply Silhouette on a single persistence diagram and outputs the result.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            numpy array with shape (**resolution**): output persistence silhouette.
-        """
-        return _maybe_fit_transform(self, 'grid_', diag)
-
-
-class BettiCurve(BaseEstimator, TransformerMixin):
-    """
-    Compute Betti curves from persistence diagrams. There are several modes of operation: with a given resolution (with or without a sample_range), with a predefined grid, and with none of the previous. With a predefined grid, the class computes the Betti numbers at those grid points. Without a predefined grid, if the resolution is set to None, it can be fit to a list of persistence diagrams and produce a grid that consists of (at least) the filtration values at which at least one of those persistence diagrams changes Betti numbers, and then compute the Betti numbers at those grid points. In the latter mode, the exact Betti curve is computed for the entire real line. Otherwise, if the resolution is given, the Betti curve is obtained by sampling evenly using either the given sample_range or based on the persistence diagrams.
-
-    Examples
-    --------
-    If pd is a persistence diagram and xs is a nonempty grid of finite values such that xs[0] >= pd.min(), then the results of:
-
-    >>> bc = BettiCurve(predefined_grid=xs) # doctest: +SKIP
-    >>> result = bc(pd) # doctest: +SKIP
-
-    and
-
-    >>> from scipy.interpolate import interp1d # doctest: +SKIP
-    >>> bc = BettiCurve(resolution=None, predefined_grid=None) # doctest: +SKIP
-    >>> bettis = bc.fit_transform([pd]) # doctest: +SKIP
-    >>> interp = interp1d(bc.grid_, bettis[0, :], kind="previous", fill_value="extrapolate") # doctest: +SKIP
-    >>> result = np.array(interp(xs), dtype=int) # doctest: +SKIP
-
-    are the same.
-
-    Attributes
-    ----------
-    grid_ : 1d array
-        The grid on which the Betti numbers are computed. If predefined_grid was specified, `grid_` will always be that grid, independently of data. If not and resolution is None, the grid is fitted to capture all filtration values at which the Betti numbers change.
-    """
-
-    def __init__(self, resolution=100, sample_range=[np.nan, np.nan], predefined_grid=None, *, keep_endpoints=False):
-        """
-        Constructor for the BettiCurve class.
-
-        Parameters:
-            resolution (int): number of samples for the piecewise-constant function (default 100), or None for the exact curve.
-            sample_range ([double, double]): minimum and maximum of the piecewise-constant function domain, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
-            predefined_grid (1d array or None, default=None): Predefined filtration grid points at which to compute the Betti curves. Must be strictly ordered. Infinities are ok. If None (default), and resolution is given, the grid will be uniform from x_min to x_max in 'resolution' steps, otherwise a grid will be computed that captures all changes in Betti numbers in the provided data.
-            keep_endpoints (bool): when computing `sample_range` (fixed `resolution`, no `predefined_grid`), use the exact extremities. This is mostly useful for plotting, the default is to use a slightly smaller range.
-        """
-
-        if (predefined_grid is not None) and (not isinstance(predefined_grid, np.ndarray)):
-            raise ValueError("Expected predefined_grid as array or None.")
-
-        self.predefined_grid = predefined_grid
-        self.resolution = resolution
-        self.sample_range = sample_range
-        self.keep_endpoints = keep_endpoints
-
-    def is_fitted(self):
-        return hasattr(self, "grid_")
-
-    def fit(self, X, y = None):
-        """
-        Fit the BettiCurve class on a list of persistence diagrams: if any of the values in **sample_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams. When no predefined grid is provided and resolution set to None, compute a filtration grid that captures all changes in Betti numbers for all the given persistence diagrams.
-
-        Parameters:
-            X (list of 2d arrays): Persistence diagrams.
-            y (None): Ignored.
-        """
-
-        if self.predefined_grid is None:
-            if self.resolution is None: # Flexible/exact version
-                events = np.unique(np.concatenate([pd.flatten() for pd in X] + [[-np.inf]], axis=0))
-                self.grid_ = np.array(events)
-            else:
-                _grid_from_sample_range(self, X)
-        else:
-            self.grid_ = self.predefined_grid # Get the predefined grid from user
-
-        return self
-
-    def transform(self, X):
-        """
-        Compute Betti curves.
-
-        Parameters:
-            X (list of 2d arrays): Persistence diagrams.
-
-        Returns:
-            `len(X).len(self.grid_)` array of ints: Betti numbers of the given persistence diagrams at the grid points given in `self.grid_`
-        """
-
-        if not self.is_fitted():
-            raise NotFittedError("Not fitted.")
-
-        if not X:
-            X = [np.zeros((0, 2))]
-        
-        N = len(X)
-
-        events = np.concatenate([pd.flatten(order="F") for pd in X], axis=0)
-        sorting = np.argsort(events)
-        offsets = np.zeros(1 + N, dtype=int)
-        for i in range(0, N):
-            offsets[i+1] = offsets[i] + 2*X[i].shape[0]
-        starts = offsets[0:N]
-        ends = offsets[1:N + 1] - 1
-
-        bettis = [[0] for i in range(0, N)]
-
-        i = 0
-        for x in self.grid_:
-            while i < len(sorting) and events[sorting[i]] <= x:
-                j = np.searchsorted(ends, sorting[i])
-                delta = 1 if sorting[i] - starts[j] < len(X[j]) else -1
-                bettis[j][-1] += delta
-                i += 1
-            for k in range(0, N):
-                bettis[k].append(bettis[k][-1])
-
-        return np.array(bettis, dtype=int)[:, 0:-1]
-
-    def fit_transform(self, X):
-        """
-        The result is the same as fit(X) followed by transform(X), but potentially faster.
-        """
-
-        if self.predefined_grid is None and self.resolution is None:
-            if not X:
-                X = [np.zeros((0, 2))]
-
-            N = len(X)
-
-            events = np.concatenate([pd.flatten(order="F") for pd in X], axis=0)
-            sorting = np.argsort(events)
-            offsets = np.zeros(1 + N, dtype=int)
-            for i in range(0, N):
-                offsets[i+1] = offsets[i] + 2*X[i].shape[0]
-            starts = offsets[0:N]
-            ends = offsets[1:N + 1] - 1
-
-            xs = [-np.inf]
-            bettis = [[0] for i in range(0, N)]
-
-            for i in sorting:
-                j = np.searchsorted(ends, i)
-                delta = 1 if i - starts[j] < len(X[j]) else -1
-                if events[i] == xs[-1]:
-                    bettis[j][-1] += delta
-                else:
-                    xs.append(events[i])
-                    for k in range(0, j):
-                        bettis[k].append(bettis[k][-1])
-                    bettis[j].append(bettis[j][-1] + delta)
-                    for k in range(j+1, N):
-                        bettis[k].append(bettis[k][-1])
-
-            self.grid_ = np.array(xs)
-            return np.array(bettis, dtype=int)
-
-        else:
-            return self.fit(X).transform(X)
-
-    def __call__(self, diag):
-        """
-        Shorthand for transform on a single persistence diagram.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-        """
-        return _maybe_fit_transform(self, 'grid_', diag)
-
-
-
-class Entropy(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing persistence entropy. Persistence entropy is a statistic for persistence diagrams inspired from Shannon entropy. This statistic can also be used to compute a feature vector, called the entropy summary function. See https://arxiv.org/pdf/1803.08304.pdf for more details. Note that a previous implementation was contributed by Manuel Soriano-Trigueros.
-
-    Attributes:
-        grid_ (1d array): In vector mode, the grid on which the entropy summary function is computed.
-    """
-    def __init__(self, mode="scalar", normalized=True, resolution=100, sample_range=[np.nan, np.nan], *, keep_endpoints=False):
-        """
-        Constructor for the Entropy class.
-
-        Parameters:
-            mode (string): what entropy to compute: either "scalar" for computing the entropy statistics, or "vector" for computing the entropy summary functions (default "scalar").
-            normalized (bool): whether to normalize the entropy summary function (default True). Used only if **mode** = "vector". 
-            resolution (int): number of sample for the entropy summary function (default 100). Used only if **mode** = "vector".
-            sample_range ([double, double]): minimum and maximum of the entropy summary function domain, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method. Used only if **mode** = "vector".
-            keep_endpoints (bool): when computing `sample_range`, use the exact extremities. This is mostly useful for plotting, the default is to use a slightly smaller range.
-        """
-        self.mode, self.normalized, self.resolution, self.sample_range = mode, normalized, resolution, sample_range
-        self.keep_endpoints = keep_endpoints
-
-    def fit(self, X, y=None):
-        """
-        Fit the Entropy class on a list of persistence diagrams.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        if self.mode == "vector":
-            _grid_from_sample_range(self, X)
-            self.step_ = self.grid_[1] - self.grid_[0]
-        return self
-
-    def transform(self, X):
-        """
-        Compute the entropy for each persistence diagram individually and concatenate the results.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-    
-        Returns:
-            numpy array with shape (number of diagrams) x (1 if **mode** = "scalar" else **resolution**): output entropy.
-        """
-        num_diag, Xfit = len(X), []
-        new_X = BirthPersistenceTransform().fit_transform(X)        
-
-        for i in range(num_diag):
-            orig_diagram, new_diagram, num_pts_in_diag = X[i], new_X[i], X[i].shape[0]
-                
-            p = new_diagram[:,1]
-            p = p/np.sum(p)
-            if self.mode == "scalar":
-                ent = -np.dot(p, np.log(p))
-                Xfit.append(np.array([[ent]]))
-            else:
-                ent = np.zeros(self.resolution)
-                for j in range(num_pts_in_diag):
-                    [px,py] = orig_diagram[j,:2]
-                    min_idx = np.clip(np.ceil((px - self.sample_range_fixed_[0]) / self.step_).astype(int), 0, self.resolution)
-                    max_idx = np.clip(np.ceil((py - self.sample_range_fixed_[0]) / self.step_).astype(int), 0, self.resolution)
-                    ent[min_idx:max_idx]-=p[j]*np.log(p[j])
-                if self.normalized:
-                    ent = ent / np.linalg.norm(ent, ord=1)
-                Xfit.append(np.reshape(ent,[1,-1]))
-
-        Xfit = np.concatenate(Xfit, axis=0)
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply Entropy on a single persistence diagram and outputs the result.
-        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            numpy array with shape (1 if **mode** = "scalar" else **resolution**): output entropy.
-        """
-        return _maybe_fit_transform(self, 'grid_', diag)
-
-class TopologicalVector(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing topological vectors from a list of persistence diagrams. The topological vector associated to a persistence diagram is the sorted vector of a slight modification of the pairwise distances between the persistence diagram points. See https://diglib.eg.org/handle/10.1111/cgf12692 for more details.
-    """
-    def __init__(self, threshold=10):
-        """
-        Constructor for the TopologicalVector class.
-
-        Parameters:
-            threshold (int): number of distances to keep (default 10). This is the dimension of the topological vector. If -1, this threshold is computed from the list of persistence diagrams by considering the one with the largest number of points and using the dimension of its corresponding topological vector as threshold. 
-        """
-        self.threshold = threshold
-
-    def fit(self, X, y=None):
-        """
-        Fit the TopologicalVector class on a list of persistence diagrams (this function actually does nothing but is useful when TopologicalVector is included in a scikit-learn Pipeline).
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        return self
-
-    def transform(self, X):
-        """
-        Compute the topological vector for each persistence diagram individually and concatenate the results.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-    
-        Returns:
-            numpy array with shape (number of diagrams) x (**threshold**): output topological vectors.
-        """
-        if self.threshold == -1:
-            thresh = np.array([X[i].shape[0] for i in range(len(X))]).max()
-        else:
-            thresh = self.threshold
-
-        num_diag = len(X)
-        Xfit = np.zeros([num_diag, thresh])
-
-        for i in range(num_diag):
-
-            diagram, num_pts_in_diag = X[i], X[i].shape[0]
-            pers = 0.5 * (diagram[:,1]-diagram[:,0])
-            min_pers = np.minimum(pers,np.transpose(pers))
-            # Works fine with sklearn 1.0, but an ValueError exception is thrown on past versions
-            try:
-                distances = DistanceMetric.get_metric("chebyshev").pairwise(diagram)
-            except ValueError:
-                # Empty persistence diagram case - https://github.com/GUDHI/gudhi-devel/issues/507
-                assert len(diagram) == 0
-                distances = np.empty(shape = [0, 0])
-            vect = np.flip(np.sort(np.triu(np.minimum(distances, min_pers)), axis=None), 0)
-            dim = min(len(vect), thresh)
-            Xfit[i, :dim] = vect[:dim]
-
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply TopologicalVector on a single persistence diagram and outputs the result.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            numpy array with shape (**threshold**): output topological vector.
-        """
-        return self.transform([diag])[0,:]
-
-class ComplexPolynomial(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing complex polynomials from a list of persistence diagrams. The persistence diagram points are seen as the roots of some complex polynomial, whose coefficients are returned in a complex vector. See https://link.springer.com/chapter/10.1007%2F978-3-319-23231-7_27 for more details.
-    """
-    def __init__(self, polynomial_type="R", threshold=10):
-        """
-        Constructor for the ComplexPolynomial class.
-
-        Parameters:
-           polynomial_type (char): either "R", "S" or "T" (default "R"). Type of complex polynomial that is going to be computed (explained in https://link.springer.com/chapter/10.1007%2F978-3-319-23231-7_27).
-           threshold (int): number of coefficients (default 10). This is the dimension of the complex vector of coefficients, i.e. the number of coefficients corresponding to the largest degree terms of the polynomial. If -1, this threshold is computed from the list of persistence diagrams by considering the one with the largest number of points and using the dimension of its corresponding complex vector of coefficients as threshold. 
-        """
-        self.threshold, self.polynomial_type = threshold, polynomial_type
-
-    def fit(self, X, y=None):
-        """
-        Fit the ComplexPolynomial class on a list of persistence diagrams (this function actually does nothing but is useful when ComplexPolynomial is included in a scikit-learn Pipeline).
-
-        Parameters:
-            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
-            y (n x 1 array): persistence diagram labels (unused).
-        """
-        return self
-
-    def transform(self, X):
-        """
-        Compute the complex vector of coefficients for each persistence diagram individually and concatenate the results.
-
-        Parameters:
-            X (list of n x 2 numpy arrays): input persistence diagrams.
-    
-        Returns:
-            numpy array with shape (number of diagrams) x (**threshold**): output complex vectors of coefficients.
-        """
-        if self.threshold == -1:
-            thresh = np.array([X[i].shape[0] for i in range(len(X))]).max()
-        else:
-            thresh = self.threshold
-
-        Xfit = np.zeros([len(X), thresh]) + 1j * np.zeros([len(X), thresh])
-        for d in range(len(X)):
-            D, N = X[d], X[d].shape[0]
-            if self.polynomial_type == "R":
-                roots = D[:,0] + 1j * D[:,1]
-            elif self.polynomial_type == "S":
-                alpha = np.linalg.norm(D, axis=1)
-                alpha = np.where(alpha==0, np.ones(N), alpha)
-                roots = np.multiply( np.multiply(  (D[:,0]+1j*D[:,1]), (D[:,1]-D[:,0])  ), 1./(np.sqrt(2)*alpha) )
-            elif self.polynomial_type == "T":
-                alpha = np.linalg.norm(D, axis=1)
-                roots = np.multiply(  (D[:,1]-D[:,0])/2, np.cos(alpha) - np.sin(alpha) + 1j * (np.cos(alpha) + np.sin(alpha))  )
-            coeff = [0] * (N+1)
-            coeff[N] = 1
-            for i in range(1, N+1): 
-                for j in range(N-i-1, N): 
-                    coeff[j] += ((-1) * roots[i-1] * coeff[j+1])   
-            coeff = np.array(coeff[::-1])[1:]
-            Xfit[d, :min(thresh, coeff.shape[0])] = coeff[:min(thresh, coeff.shape[0])]
-        return Xfit
-
-    def __call__(self, diag):
-        """
-        Apply ComplexPolynomial on a single persistence diagram and outputs the result.
-
-        Parameters:
-            diag (n x 2 numpy array): input persistence diagram.
-
-        Returns:
-            numpy array with shape (**threshold**): output complex vector of coefficients.
-        """
-        return self.transform([diag])[0,:]
-
-def _lapl_contrast(measure, centers, inertias):
-    """contrast function for vectorising `measure` in ATOL"""
-    return np.exp(-pairwise.pairwise_distances(measure, Y=centers) / inertias)
-
-def _gaus_contrast(measure, centers, inertias):
-    """contrast function for vectorising `measure` in ATOL"""
-    return np.exp(-pairwise.pairwise_distances(measure, Y=centers, squared=True) / inertias**2)
-
-def _indicator_contrast(diags, centers, inertias):
-    """contrast function for vectorising `measure` in ATOL"""
-    robe_curve = np.clip(2-pairwise.pairwise_distances(diags, Y=centers)/inertias, 0, 1)
-    return robe_curve
-
-def _cloud_weighting(measure):
-    """automatic uniform weighting with mass 1 for `measure` in ATOL"""
-    return np.ones(shape=measure.shape[0])
-
-def _iidproba_weighting(measure):
-    """automatic uniform weighting with mass 1/N for `measure` in ATOL"""
-    return np.ones(shape=measure.shape[0]) / measure.shape[0]
-
-class Atol(BaseEstimator, TransformerMixin):
-    """
-    This class allows to vectorise measures (e.g. point clouds, persistence diagrams, etc) after a quantisation step.
-
-    ATOL paper: :cite:`royer2019atol`
-
-    Example
-    --------
-    >>> from sklearn.cluster import KMeans
-    >>> from gudhi.representations.vector_methods import Atol
-    >>> import numpy as np
-    >>> a = np.array([[1, 2, 4], [1, 4, 0], [1, 0, 4]])
-    >>> b = np.array([[4, 2, 0], [4, 4, 0], [4, 0, 2]])
-    >>> c = np.array([[3, 2, -1], [1, 2, -1]])
-    >>> atol_vectoriser = Atol(quantiser=KMeans(n_clusters=2, random_state=202006))
-    >>> atol_vectoriser.fit(X=[a, b, c]).centers
-    array([[ 2.6       ,  2.8       , -0.4       ],
-           [ 2.        ,  0.66666667,  3.33333333]])
-    >>> atol_vectoriser(a)
-    array([0.42375966, 1.18168665])
-    >>> atol_vectoriser(c)
-    array([1.25157463, 0.02062512])
-    >>> atol_vectoriser.transform(X=[a, b, c])
-    array([[0.42375966, 1.18168665],
-           [1.06330156, 0.29861028],
-           [1.25157463, 0.02062512]])
-    """
-    # Note the example above must be up to date with the one in tests called test_atol_doc
-    def __init__(self, quantiser, weighting_method="cloud", contrast="gaussian"):
-        """
-        Constructor for the Atol measure vectorisation class.
-
-        Parameters:
-            quantiser (Object): Object with `fit` (sklearn API consistent) and `cluster_centers` and `n_clusters`
-                attributes, e.g. sklearn.cluster.KMeans. It will be fitted when the Atol object function `fit` is called.
-            weighting_method (string): constant generic function for weighting the measure points
-                choose from {"cloud", "iidproba"}
-                (default: constant function, i.e. the measure is seen as a point cloud by default).
-                This will have no impact if weights are provided along with measures all the way: `fit` and `transform`.
-            contrast (string): constant function for evaluating proximity of a measure with respect to centers
-                choose from {"gaussian", "laplacian", "indicator"}
-                (default: gaussian contrast function, see page 3 in the ATOL paper).
-        """
-        self.quantiser = quantiser
-        self.contrast = {
-            "gaussian": _gaus_contrast,
-            "laplacian": _lapl_contrast,
-            "indicator": _indicator_contrast,
-        }.get(contrast, _gaus_contrast)
-        self.weighting_method = {
-            "cloud"   : _cloud_weighting,
-            "iidproba": _iidproba_weighting,
-        }.get(weighting_method, _cloud_weighting)
-
-    def fit(self, X, y=None, sample_weight=None):
-        """
-        Calibration step: fit centers to the sample measures and derive inertias between centers.
-
-        Parameters:
-            X (list N x d numpy arrays): input measures in R^d from which to learn center locations and inertias
-                (measures can have different N).
-            y: Ignored, present for API consistency by convention.
-            sample_weight (list of numpy arrays): weights for each measure point in X, optional.
-                If None, the object's weighting_method will be used.
-
-        Returns:
-            self
-        """
-        if not hasattr(self.quantiser, 'fit'):
-            raise TypeError("quantiser %s has no `fit` attribute." % (self.quantiser))
-        if sample_weight is None:
-            sample_weight = np.concatenate([self.weighting_method(measure) for measure in X])
-
-        measures_concat = np.concatenate(X)
-        self.quantiser.fit(X=measures_concat, sample_weight=sample_weight)
-        self.centers = self.quantiser.cluster_centers_
-        # Hack, but some people are unhappy if the order depends on the version of sklearn
-        self.centers = self.centers[np.lexsort(self.centers.T)]
-        if self.quantiser.n_clusters == 1:
-            dist_centers = pairwise.pairwise_distances(measures_concat)
-            np.fill_diagonal(dist_centers, 0)
-            self.inertias = np.array([np.max(dist_centers)/2])
-        else:
-            dist_centers = pairwise.pairwise_distances(self.centers)
-            dist_centers[dist_centers == 0] = np.inf
-            self.inertias = np.min(dist_centers, axis=0)/2
-        return self
-
-    def __call__(self, measure, sample_weight=None):
-        """
-        Apply measure vectorisation on a single measure. Only available after `fit` has been called.
-
-        Parameters:
-            measure (n x d numpy array): input measure in R^d.
-
-        Returns:
-            numpy array in R^self.quantiser.n_clusters.
-        """
-        if sample_weight is None:
-            sample_weight = self.weighting_method(measure)
-        return np.sum(sample_weight * self.contrast(measure, self.centers, self.inertias.T).T, axis=1)
-
-    def transform(self, X, sample_weight=None):
-        """
-        Apply measure vectorisation on a list of measures.
-
-        Parameters:
-            X (list N x d numpy arrays): input measures in R^d from which to learn center locations and inertias
-                (measures can have different N).
-            sample_weight (list of numpy arrays): weights for each measure point in X, optional.
-                If None, the object's weighting_method will be used.
-
-        Returns:
-            numpy array with shape (number of measures) x (self.quantiser.n_clusters).
-        """
-        if sample_weight is None:
-            sample_weight = [self.weighting_method(measure) for measure in X]
-        return np.stack([self(measure, sample_weight=weight) for measure, weight in zip(X, sample_weight)])
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Mathieu Carri√®re, Martin Royer, Gard Spreemann
+#
+# Copyright (C) 2018-2020 Inria
+#
+# Modification(s):
+#   - 2020/06 Martin: ATOL integration
+#   - 2020/12 Gard: A more flexible Betti curve class capable of computing exact curves.
+#   - 2021/11 Vincent Rouvreau: factorize _automatic_sample_range
+
+import numpy as np
+from sklearn.base          import BaseEstimator, TransformerMixin
+from sklearn.exceptions    import NotFittedError
+from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler
+from sklearn.metrics       import pairwise
+try:
+    # New location since 1.0
+    from sklearn.metrics     import DistanceMetric
+except ImportError:
+    # Will be removed in 1.3
+    from sklearn.neighbors     import DistanceMetric
+
+from .preprocessing import DiagramScaler, BirthPersistenceTransform, _maybe_fit_transform
+
+#############################################
+# Finite Vectorization methods ##############
+#############################################
+
+class PersistenceImage(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing persistence images from a list of persistence diagrams. A persistence image is a 2D function computed from a persistence diagram by convolving the diagram points with a weighted Gaussian kernel. The plane is then discretized into an image with pixels, which is flattened and returned as a vector. See http://jmlr.org/papers/v18/16-337.html for more details.
+    """
+    def __init__(self, bandwidth=1., weight=lambda x: 1, resolution=[20,20], im_range=[np.nan, np.nan, np.nan, np.nan]):
+        """
+        Constructor for the PersistenceImage class.
+
+        Parameters:
+            bandwidth (double): bandwidth of the Gaussian kernel (default 1.).
+            weight (function): weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie lists or numpy arrays of the form [p_x,p_y].
+            resolution ([int,int]): size (in pixels) of the persistence image (default [20,20]).
+            im_range ([double,double,double,double]): minimum and maximum of each axis of the persistence image, of the form [x_min, x_max, y_min, y_max] (default [numpy.nan, numpy.nan, numpy.nan, numpyp.nan]). If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
+        """
+        self.bandwidth, self.weight = bandwidth, weight
+        self.resolution, self.im_range = resolution, im_range
+
+    def fit(self, X, y=None):
+        """
+        Fit the PersistenceImage class on a list of persistence diagrams: if any of the values in **im_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        if np.isnan(np.array(self.im_range)).any():
+            if all(len(d) == 0 for d in X):
+                self.im_range_fixed_ = self.im_range
+            else:
+                new_X = BirthPersistenceTransform().fit_transform(X)
+                pre = DiagramScaler(use=True, scalers=[([0], MinMaxScaler()), ([1], MinMaxScaler())]).fit(new_X,y)
+                [mx,my],[Mx,My] = [pre.scalers[0][1].data_min_[0], pre.scalers[1][1].data_min_[0]], [pre.scalers[0][1].data_max_[0], pre.scalers[1][1].data_max_[0]]
+                self.im_range_fixed_ = np.where(np.isnan(np.array(self.im_range)), np.array([mx, Mx, my, My]), np.array(self.im_range))
+        else:
+            self.im_range_fixed_ = self.im_range
+        return self
+
+    def transform(self, X):
+        """
+        Compute the persistence image for each persistence diagram individually and store the results in a single numpy array.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+    
+        Returns:
+            numpy array with shape (number of diagrams) x (number of pixels = **resolution[0]** x **resolution[1]**): output persistence images.
+        """
+        num_diag, Xfit = len(X), []
+        new_X = BirthPersistenceTransform().fit_transform(X)
+
+        for i in range(num_diag):
+
+            diagram, num_pts_in_diag = new_X[i], X[i].shape[0]
+
+            w = np.empty(num_pts_in_diag)
+            for j in range(num_pts_in_diag):
+                w[j] = self.weight(diagram[j,:])
+
+            x_values, y_values = np.linspace(self.im_range_fixed_[0], self.im_range_fixed_[1], self.resolution[0]), np.linspace(self.im_range_fixed_[2], self.im_range_fixed_[3], self.resolution[1])
+            Xs, Ys = np.tile((diagram[:,0][:,np.newaxis,np.newaxis]-x_values[np.newaxis,np.newaxis,:]),[1,self.resolution[1],1]), np.tile(diagram[:,1][:,np.newaxis,np.newaxis]-y_values[np.newaxis,:,np.newaxis],[1,1,self.resolution[0]])
+            image = np.tensordot(w, np.exp((-np.square(Xs)-np.square(Ys))/(2*np.square(self.bandwidth)))/(np.square(self.bandwidth)*2*np.pi), 1)
+
+            Xfit.append(image.flatten()[np.newaxis,:])
+
+        Xfit = np.concatenate(Xfit, 0)
+
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply PersistenceImage on a single persistence diagram and outputs the result.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            numpy array with shape (number of pixels = **resolution[0]** x **resolution[1]**):: output persistence image.
+        """
+        return _maybe_fit_transform(self, 'im_range_fixed_', diag)
+
+def _automatic_sample_range(sample_range, X):
+        """
+        Compute and returns sample range from the persistence diagrams if one of the sample_range values is numpy.nan.
+
+        Parameters:
+            sample_range (a numpy array of 2 float): minimum and maximum of all piecewise-linear function domains, of
+                the form [x_min, x_max].
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        nan_in_range = np.isnan(sample_range)
+        if nan_in_range.any():
+            try:
+                pre = DiagramScaler(use=True, scalers=[([0], MinMaxScaler()), ([1], MinMaxScaler())]).fit(X)
+                [mx,my] = [pre.scalers[0][1].data_min_[0], pre.scalers[1][1].data_min_[0]]
+                [Mx,My] = [pre.scalers[0][1].data_max_[0], pre.scalers[1][1].data_max_[0]]
+                return np.where(nan_in_range, np.array([mx, My]), sample_range)
+            except ValueError:
+                # Empty persistence diagram case - https://github.com/GUDHI/gudhi-devel/issues/507
+                pass
+        return sample_range
+
+
+def _trim_endpoints(x, are_endpoints_nan):
+    if are_endpoints_nan[0]:
+        x = x[1:]
+    if are_endpoints_nan[1]:
+        x = x[:-1]
+    return x
+
+
+def _grid_from_sample_range(self, X):
+    sample_range = np.array(self.sample_range)
+    self.nan_in_range_ = np.isnan(sample_range)
+    self.new_resolution_ = self.resolution
+    if not self.keep_endpoints:
+        self.new_resolution_ += self.nan_in_range_.sum()
+    self.sample_range_fixed_ = _automatic_sample_range(sample_range, X)
+    self.grid_ = np.linspace(self.sample_range_fixed_[0], self.sample_range_fixed_[1], self.new_resolution_)
+    if not self.keep_endpoints:
+        self.grid_ = _trim_endpoints(self.grid_, self.nan_in_range_)
+
+
+class Landscape(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing persistence landscapes from a list of persistence diagrams. A persistence landscape is a collection of 1D piecewise-linear functions computed from the rank function associated to the persistence diagram. These piecewise-linear functions are then sampled evenly on a given range and the corresponding vectors of samples are concatenated and returned. See http://jmlr.org/papers/v16/bubenik15a.html for more details.
+
+    Attributes:
+        grid_ (1d array): The grid on which the landscapes are computed.
+    """
+    def __init__(self, num_landscapes=5, resolution=100, sample_range=[np.nan, np.nan], *, keep_endpoints=False):
+        """
+        Constructor for the Landscape class.
+
+        Parameters:
+            num_landscapes (int): number of piecewise-linear functions to output (default 5).
+            resolution (int): number of sample for all piecewise-linear functions (default 100).
+            sample_range ([double, double]): minimum and maximum of all piecewise-linear function domains, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
+            keep_endpoints (bool): when computing `sample_range`, use the exact extremities (where the value is always 0). This is mostly useful for plotting, the default is to use a slightly smaller range.
+        """
+        self.num_landscapes, self.resolution, self.sample_range = num_landscapes, resolution, sample_range
+        self.keep_endpoints = keep_endpoints
+
+    def fit(self, X, y=None):
+        """
+        Fit the Landscape class on a list of persistence diagrams: if any of the values in **sample_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        _grid_from_sample_range(self, X)
+        return self
+
+    def transform(self, X):
+        """
+        Compute the persistence landscape for each persistence diagram individually and concatenate the results.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+    
+        Returns:
+            numpy array with shape (number of diagrams) x (number of samples = **num_landscapes** x **resolution**): output persistence landscapes.
+        """
+
+        Xfit = []
+        x_values = self.grid_
+        for diag in X:
+            midpoints, heights = (diag[:, 0] + diag[:, 1]) / 2., (diag[:, 1] - diag[:, 0]) / 2.
+            tent_functions = np.maximum(heights[None, :] - np.abs(x_values[:, None] - midpoints[None, :]), 0)
+            n_points = diag.shape[0]
+            # Complete the array with zeros to get the right number of landscapes
+            if self.num_landscapes > n_points:
+                tent_functions = np.concatenate(
+                    [tent_functions, np.zeros((tent_functions.shape[0], self.num_landscapes-n_points))],
+                    axis=1
+                )
+            tent_functions.partition(tent_functions.shape[1]-self.num_landscapes, axis=1)
+            landscapes = np.sort(tent_functions[:, -self.num_landscapes:], axis=1)[:, ::-1].T
+
+            landscapes = np.sqrt(2) * np.ravel(landscapes)
+            Xfit.append(landscapes)
+
+        return np.stack(Xfit, axis=0)
+
+    def __call__(self, diag):
+        """
+        Apply Landscape on a single persistence diagram and outputs the result.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            numpy array with shape (number of samples = **num_landscapes** x **resolution**): output persistence landscape.
+        """
+        return _maybe_fit_transform(self, 'grid_', diag)
+
+class Silhouette(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing persistence silhouettes from a list of persistence diagrams. A persistence silhouette is computed by taking a weighted average of the collection of 1D piecewise-linear functions given by the persistence landscapes, and then by evenly sampling this average on a given range. Finally, the corresponding vector of samples is returned. See https://arxiv.org/abs/1312.0308 for more details.
+
+    Attributes:
+        grid_ (1d array): The grid on which the silhouette is computed.
+    """
+    def __init__(self, weight=lambda x: 1, resolution=100, sample_range=[np.nan, np.nan], *, keep_endpoints=False):
+        """
+        Constructor for the Silhouette class.
+
+        Parameters:
+            weight (function): weight function for the persistence diagram points (default constant function, ie lambda x: 1). This function must be defined on 2D points, ie on lists or numpy arrays of the form [p_x,p_y].
+            resolution (int): number of samples for the weighted average (default 100).
+            sample_range ([double, double]): minimum and maximum for the weighted average domain, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
+            keep_endpoints (bool): when computing `sample_range`, use the exact extremities (where the value is always 0). This is mostly useful for plotting, the default is to use a slightly smaller range.
+        """
+        self.weight, self.resolution, self.sample_range = weight, resolution, sample_range
+        self.keep_endpoints = keep_endpoints
+
+    def fit(self, X, y=None):
+        """
+        Fit the Silhouette class on a list of persistence diagrams: if any of the values in **sample_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        _grid_from_sample_range(self, X)
+        return self
+
+    def transform(self, X):
+        """
+        Compute the persistence silhouette for each persistence diagram individually and concatenate the results.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+    
+        Returns:
+            numpy array with shape (number of diagrams) x (**resolution**): output persistence silhouettes.
+        """
+        Xfit = []
+        x_values = self.grid_
+
+        for diag in X:
+            midpoints, heights = (diag[:, 0] + diag[:, 1]) / 2., (diag[:, 1] - diag[:, 0]) / 2.
+            weights = np.array([self.weight(pt) for pt in diag])
+            total_weight = np.sum(weights)
+
+            tent_functions = np.maximum(heights[None, :] - np.abs(x_values[:, None] - midpoints[None, :]), 0)
+            silhouette = np.sum(weights[None, :] / total_weight * tent_functions, axis=1)
+            Xfit.append(silhouette * np.sqrt(2))
+
+        return np.stack(Xfit, axis=0)
+
+    def __call__(self, diag):
+        """
+        Apply Silhouette on a single persistence diagram and outputs the result.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            numpy array with shape (**resolution**): output persistence silhouette.
+        """
+        return _maybe_fit_transform(self, 'grid_', diag)
+
+
+class BettiCurve(BaseEstimator, TransformerMixin):
+    """
+    Compute Betti curves from persistence diagrams. There are several modes of operation: with a given resolution (with or without a sample_range), with a predefined grid, and with none of the previous. With a predefined grid, the class computes the Betti numbers at those grid points. Without a predefined grid, if the resolution is set to None, it can be fit to a list of persistence diagrams and produce a grid that consists of (at least) the filtration values at which at least one of those persistence diagrams changes Betti numbers, and then compute the Betti numbers at those grid points. In the latter mode, the exact Betti curve is computed for the entire real line. Otherwise, if the resolution is given, the Betti curve is obtained by sampling evenly using either the given sample_range or based on the persistence diagrams.
+
+    Examples
+    --------
+    If pd is a persistence diagram and xs is a nonempty grid of finite values such that xs[0] >= pd.min(), then the results of:
+
+    >>> bc = BettiCurve(predefined_grid=xs) # doctest: +SKIP
+    >>> result = bc(pd) # doctest: +SKIP
+
+    and
+
+    >>> from scipy.interpolate import interp1d # doctest: +SKIP
+    >>> bc = BettiCurve(resolution=None, predefined_grid=None) # doctest: +SKIP
+    >>> bettis = bc.fit_transform([pd]) # doctest: +SKIP
+    >>> interp = interp1d(bc.grid_, bettis[0, :], kind="previous", fill_value="extrapolate") # doctest: +SKIP
+    >>> result = np.array(interp(xs), dtype=int) # doctest: +SKIP
+
+    are the same.
+
+    Attributes
+    ----------
+    grid_ : 1d array
+        The grid on which the Betti numbers are computed. If predefined_grid was specified, `grid_` will always be that grid, independently of data. If not and resolution is None, the grid is fitted to capture all filtration values at which the Betti numbers change.
+    """
+
+    def __init__(self, resolution=100, sample_range=[np.nan, np.nan], predefined_grid=None, *, keep_endpoints=False):
+        """
+        Constructor for the BettiCurve class.
+
+        Parameters:
+            resolution (int): number of samples for the piecewise-constant function (default 100), or None for the exact curve.
+            sample_range ([double, double]): minimum and maximum of the piecewise-constant function domain, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method.
+            predefined_grid (1d array or None, default=None): Predefined filtration grid points at which to compute the Betti curves. Must be strictly ordered. Infinities are ok. If None (default), and resolution is given, the grid will be uniform from x_min to x_max in 'resolution' steps, otherwise a grid will be computed that captures all changes in Betti numbers in the provided data.
+            keep_endpoints (bool): when computing `sample_range` (fixed `resolution`, no `predefined_grid`), use the exact extremities. This is mostly useful for plotting, the default is to use a slightly smaller range.
+        """
+
+        if (predefined_grid is not None) and (not isinstance(predefined_grid, np.ndarray)):
+            raise ValueError("Expected predefined_grid as array or None.")
+
+        self.predefined_grid = predefined_grid
+        self.resolution = resolution
+        self.sample_range = sample_range
+        self.keep_endpoints = keep_endpoints
+
+    def is_fitted(self):
+        return hasattr(self, "grid_")
+
+    def fit(self, X, y = None):
+        """
+        Fit the BettiCurve class on a list of persistence diagrams: if any of the values in **sample_range** is numpy.nan, replace it with the corresponding value computed on the given list of persistence diagrams. When no predefined grid is provided and resolution set to None, compute a filtration grid that captures all changes in Betti numbers for all the given persistence diagrams.
+
+        Parameters:
+            X (list of 2d arrays): Persistence diagrams.
+            y (None): Ignored.
+        """
+
+        if self.predefined_grid is None:
+            if self.resolution is None: # Flexible/exact version
+                events = np.unique(np.concatenate([pd.flatten() for pd in X] + [[-np.inf]], axis=0))
+                self.grid_ = np.array(events)
+            else:
+                _grid_from_sample_range(self, X)
+        else:
+            self.grid_ = self.predefined_grid # Get the predefined grid from user
+
+        return self
+
+    def transform(self, X):
+        """
+        Compute Betti curves.
+
+        Parameters:
+            X (list of 2d arrays): Persistence diagrams.
+
+        Returns:
+            `len(X).len(self.grid_)` array of ints: Betti numbers of the given persistence diagrams at the grid points given in `self.grid_`
+        """
+
+        if not self.is_fitted():
+            raise NotFittedError("Not fitted.")
+
+        if not X:
+            X = [np.zeros((0, 2))]
+        
+        N = len(X)
+
+        events = np.concatenate([pd.flatten(order="F") for pd in X], axis=0)
+        sorting = np.argsort(events)
+        offsets = np.zeros(1 + N, dtype=int)
+        for i in range(0, N):
+            offsets[i+1] = offsets[i] + 2*X[i].shape[0]
+        starts = offsets[0:N]
+        ends = offsets[1:N + 1] - 1
+
+        bettis = [[0] for i in range(0, N)]
+
+        i = 0
+        for x in self.grid_:
+            while i < len(sorting) and events[sorting[i]] <= x:
+                j = np.searchsorted(ends, sorting[i])
+                delta = 1 if sorting[i] - starts[j] < len(X[j]) else -1
+                bettis[j][-1] += delta
+                i += 1
+            for k in range(0, N):
+                bettis[k].append(bettis[k][-1])
+
+        return np.array(bettis, dtype=int)[:, 0:-1]
+
+    def fit_transform(self, X):
+        """
+        The result is the same as fit(X) followed by transform(X), but potentially faster.
+        """
+
+        if self.predefined_grid is None and self.resolution is None:
+            if not X:
+                X = [np.zeros((0, 2))]
+
+            N = len(X)
+
+            events = np.concatenate([pd.flatten(order="F") for pd in X], axis=0)
+            sorting = np.argsort(events)
+            offsets = np.zeros(1 + N, dtype=int)
+            for i in range(0, N):
+                offsets[i+1] = offsets[i] + 2*X[i].shape[0]
+            starts = offsets[0:N]
+            ends = offsets[1:N + 1] - 1
+
+            xs = [-np.inf]
+            bettis = [[0] for i in range(0, N)]
+
+            for i in sorting:
+                j = np.searchsorted(ends, i)
+                delta = 1 if i - starts[j] < len(X[j]) else -1
+                if events[i] == xs[-1]:
+                    bettis[j][-1] += delta
+                else:
+                    xs.append(events[i])
+                    for k in range(0, j):
+                        bettis[k].append(bettis[k][-1])
+                    bettis[j].append(bettis[j][-1] + delta)
+                    for k in range(j+1, N):
+                        bettis[k].append(bettis[k][-1])
+
+            self.grid_ = np.array(xs)
+            return np.array(bettis, dtype=int)
+
+        else:
+            return self.fit(X).transform(X)
+
+    def __call__(self, diag):
+        """
+        Shorthand for transform on a single persistence diagram.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+        """
+        return _maybe_fit_transform(self, 'grid_', diag)
+
+
+
+class Entropy(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing persistence entropy. Persistence entropy is a statistic for persistence diagrams inspired from Shannon entropy. This statistic can also be used to compute a feature vector, called the entropy summary function. See https://arxiv.org/pdf/1803.08304.pdf for more details. Note that a previous implementation was contributed by Manuel Soriano-Trigueros.
+
+    Attributes:
+        grid_ (1d array): In vector mode, the grid on which the entropy summary function is computed.
+    """
+    def __init__(self, mode="scalar", normalized=True, resolution=100, sample_range=[np.nan, np.nan], *, keep_endpoints=False):
+        """
+        Constructor for the Entropy class.
+
+        Parameters:
+            mode (string): what entropy to compute: either "scalar" for computing the entropy statistics, or "vector" for computing the entropy summary functions (default "scalar").
+            normalized (bool): whether to normalize the entropy summary function (default True). Used only if **mode** = "vector". 
+            resolution (int): number of sample for the entropy summary function (default 100). Used only if **mode** = "vector".
+            sample_range ([double, double]): minimum and maximum of the entropy summary function domain, of the form [x_min, x_max] (default [numpy.nan, numpy.nan]). It is the interval on which samples will be drawn evenly. If one of the values is numpy.nan, it can be computed from the persistence diagrams with the fit() method. Used only if **mode** = "vector".
+            keep_endpoints (bool): when computing `sample_range`, use the exact extremities. This is mostly useful for plotting, the default is to use a slightly smaller range.
+        """
+        self.mode, self.normalized, self.resolution, self.sample_range = mode, normalized, resolution, sample_range
+        self.keep_endpoints = keep_endpoints
+
+    def fit(self, X, y=None):
+        """
+        Fit the Entropy class on a list of persistence diagrams.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        if self.mode == "vector":
+            _grid_from_sample_range(self, X)
+            self.step_ = self.grid_[1] - self.grid_[0]
+        return self
+
+    def transform(self, X):
+        """
+        Compute the entropy for each persistence diagram individually and concatenate the results.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+    
+        Returns:
+            numpy array with shape (number of diagrams) x (1 if **mode** = "scalar" else **resolution**): output entropy.
+        """
+        num_diag, Xfit = len(X), []
+        new_X = BirthPersistenceTransform().fit_transform(X)        
+
+        for i in range(num_diag):
+            orig_diagram, new_diagram, num_pts_in_diag = X[i], new_X[i], X[i].shape[0]
+                
+            p = new_diagram[:,1]
+            p = p/np.sum(p)
+            if self.mode == "scalar":
+                ent = -np.dot(p, np.log(p))
+                Xfit.append(np.array([[ent]]))
+            else:
+                ent = np.zeros(self.resolution)
+                for j in range(num_pts_in_diag):
+                    [px,py] = orig_diagram[j,:2]
+                    min_idx = np.clip(np.ceil((px - self.sample_range_fixed_[0]) / self.step_).astype(int), 0, self.resolution)
+                    max_idx = np.clip(np.ceil((py - self.sample_range_fixed_[0]) / self.step_).astype(int), 0, self.resolution)
+                    ent[min_idx:max_idx]-=p[j]*np.log(p[j])
+                if self.normalized:
+                    ent = ent / np.linalg.norm(ent, ord=1)
+                Xfit.append(np.reshape(ent,[1,-1]))
+
+        Xfit = np.concatenate(Xfit, axis=0)
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply Entropy on a single persistence diagram and outputs the result.
+        If :func:`fit` hasn't been run, this uses `fit_transform` on a clone of the object and thus does not affect later calls.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            numpy array with shape (1 if **mode** = "scalar" else **resolution**): output entropy.
+        """
+        return _maybe_fit_transform(self, 'grid_', diag)
+
+class TopologicalVector(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing topological vectors from a list of persistence diagrams. The topological vector associated to a persistence diagram is the sorted vector of a slight modification of the pairwise distances between the persistence diagram points. See https://diglib.eg.org/handle/10.1111/cgf12692 for more details.
+    """
+    def __init__(self, threshold=10):
+        """
+        Constructor for the TopologicalVector class.
+
+        Parameters:
+            threshold (int): number of distances to keep (default 10). This is the dimension of the topological vector. If -1, this threshold is computed from the list of persistence diagrams by considering the one with the largest number of points and using the dimension of its corresponding topological vector as threshold. 
+        """
+        self.threshold = threshold
+
+    def fit(self, X, y=None):
+        """
+        Fit the TopologicalVector class on a list of persistence diagrams (this function actually does nothing but is useful when TopologicalVector is included in a scikit-learn Pipeline).
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        return self
+
+    def transform(self, X):
+        """
+        Compute the topological vector for each persistence diagram individually and concatenate the results.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+    
+        Returns:
+            numpy array with shape (number of diagrams) x (**threshold**): output topological vectors.
+        """
+        if self.threshold == -1:
+            thresh = np.array([X[i].shape[0] for i in range(len(X))]).max()
+        else:
+            thresh = self.threshold
+
+        num_diag = len(X)
+        Xfit = np.zeros([num_diag, thresh])
+
+        for i in range(num_diag):
+
+            diagram, num_pts_in_diag = X[i], X[i].shape[0]
+            pers = 0.5 * (diagram[:,1]-diagram[:,0])
+            min_pers = np.minimum(pers,np.transpose(pers))
+            # Works fine with sklearn 1.0, but an ValueError exception is thrown on past versions
+            try:
+                distances = DistanceMetric.get_metric("chebyshev").pairwise(diagram)
+            except ValueError:
+                # Empty persistence diagram case - https://github.com/GUDHI/gudhi-devel/issues/507
+                assert len(diagram) == 0
+                distances = np.empty(shape = [0, 0])
+            vect = np.flip(np.sort(np.triu(np.minimum(distances, min_pers)), axis=None), 0)
+            dim = min(len(vect), thresh)
+            Xfit[i, :dim] = vect[:dim]
+
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply TopologicalVector on a single persistence diagram and outputs the result.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            numpy array with shape (**threshold**): output topological vector.
+        """
+        return self.transform([diag])[0,:]
+
+class ComplexPolynomial(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing complex polynomials from a list of persistence diagrams. The persistence diagram points are seen as the roots of some complex polynomial, whose coefficients are returned in a complex vector. See https://link.springer.com/chapter/10.1007%2F978-3-319-23231-7_27 for more details.
+    """
+    def __init__(self, polynomial_type="R", threshold=10):
+        """
+        Constructor for the ComplexPolynomial class.
+
+        Parameters:
+           polynomial_type (char): either "R", "S" or "T" (default "R"). Type of complex polynomial that is going to be computed (explained in https://link.springer.com/chapter/10.1007%2F978-3-319-23231-7_27).
+           threshold (int): number of coefficients (default 10). This is the dimension of the complex vector of coefficients, i.e. the number of coefficients corresponding to the largest degree terms of the polynomial. If -1, this threshold is computed from the list of persistence diagrams by considering the one with the largest number of points and using the dimension of its corresponding complex vector of coefficients as threshold. 
+        """
+        self.threshold, self.polynomial_type = threshold, polynomial_type
+
+    def fit(self, X, y=None):
+        """
+        Fit the ComplexPolynomial class on a list of persistence diagrams (this function actually does nothing but is useful when ComplexPolynomial is included in a scikit-learn Pipeline).
+
+        Parameters:
+            X (list of n x 2 or n x 1 numpy arrays): input persistence diagrams.
+            y (n x 1 array): persistence diagram labels (unused).
+        """
+        return self
+
+    def transform(self, X):
+        """
+        Compute the complex vector of coefficients for each persistence diagram individually and concatenate the results.
+
+        Parameters:
+            X (list of n x 2 numpy arrays): input persistence diagrams.
+    
+        Returns:
+            numpy array with shape (number of diagrams) x (**threshold**): output complex vectors of coefficients.
+        """
+        if self.threshold == -1:
+            thresh = np.array([X[i].shape[0] for i in range(len(X))]).max()
+        else:
+            thresh = self.threshold
+
+        Xfit = np.zeros([len(X), thresh]) + 1j * np.zeros([len(X), thresh])
+        for d in range(len(X)):
+            D, N = X[d], X[d].shape[0]
+            if self.polynomial_type == "R":
+                roots = D[:,0] + 1j * D[:,1]
+            elif self.polynomial_type == "S":
+                alpha = np.linalg.norm(D, axis=1)
+                alpha = np.where(alpha==0, np.ones(N), alpha)
+                roots = np.multiply( np.multiply(  (D[:,0]+1j*D[:,1]), (D[:,1]-D[:,0])  ), 1./(np.sqrt(2)*alpha) )
+            elif self.polynomial_type == "T":
+                alpha = np.linalg.norm(D, axis=1)
+                roots = np.multiply(  (D[:,1]-D[:,0])/2, np.cos(alpha) - np.sin(alpha) + 1j * (np.cos(alpha) + np.sin(alpha))  )
+            coeff = [0] * (N+1)
+            coeff[N] = 1
+            for i in range(1, N+1): 
+                for j in range(N-i-1, N): 
+                    coeff[j] += ((-1) * roots[i-1] * coeff[j+1])   
+            coeff = np.array(coeff[::-1])[1:]
+            Xfit[d, :min(thresh, coeff.shape[0])] = coeff[:min(thresh, coeff.shape[0])]
+        return Xfit
+
+    def __call__(self, diag):
+        """
+        Apply ComplexPolynomial on a single persistence diagram and outputs the result.
+
+        Parameters:
+            diag (n x 2 numpy array): input persistence diagram.
+
+        Returns:
+            numpy array with shape (**threshold**): output complex vector of coefficients.
+        """
+        return self.transform([diag])[0,:]
+
+def _lapl_contrast(measure, centers, inertias):
+    """contrast function for vectorising `measure` in ATOL"""
+    return np.exp(-pairwise.pairwise_distances(measure, Y=centers) / inertias)
+
+def _gaus_contrast(measure, centers, inertias):
+    """contrast function for vectorising `measure` in ATOL"""
+    return np.exp(-pairwise.pairwise_distances(measure, Y=centers, squared=True) / inertias**2)
+
+def _indicator_contrast(diags, centers, inertias):
+    """contrast function for vectorising `measure` in ATOL"""
+    robe_curve = np.clip(2-pairwise.pairwise_distances(diags, Y=centers)/inertias, 0, 1)
+    return robe_curve
+
+def _cloud_weighting(measure):
+    """automatic uniform weighting with mass 1 for `measure` in ATOL"""
+    return np.ones(shape=measure.shape[0])
+
+def _iidproba_weighting(measure):
+    """automatic uniform weighting with mass 1/N for `measure` in ATOL"""
+    return np.ones(shape=measure.shape[0]) / measure.shape[0]
+
+class Atol(BaseEstimator, TransformerMixin):
+    """
+    This class allows to vectorise measures (e.g. point clouds, persistence diagrams, etc) after a quantisation step.
+
+    ATOL paper: :cite:`royer2019atol`
+
+    Example
+    --------
+    >>> from sklearn.cluster import KMeans
+    >>> from gudhi.representations.vector_methods import Atol
+    >>> import numpy as np
+    >>> a = np.array([[1, 2, 4], [1, 4, 0], [1, 0, 4]])
+    >>> b = np.array([[4, 2, 0], [4, 4, 0], [4, 0, 2]])
+    >>> c = np.array([[3, 2, -1], [1, 2, -1]])
+    >>> atol_vectoriser = Atol(quantiser=KMeans(n_clusters=2, random_state=202006))
+    >>> atol_vectoriser.fit(X=[a, b, c]).centers
+    array([[ 2.6       ,  2.8       , -0.4       ],
+           [ 2.        ,  0.66666667,  3.33333333]])
+    >>> atol_vectoriser(a)
+    array([0.42375966, 1.18168665])
+    >>> atol_vectoriser(c)
+    array([1.25157463, 0.02062512])
+    >>> atol_vectoriser.transform(X=[a, b, c])
+    array([[0.42375966, 1.18168665],
+           [1.06330156, 0.29861028],
+           [1.25157463, 0.02062512]])
+    """
+    # Note the example above must be up to date with the one in tests called test_atol_doc
+    def __init__(self, quantiser, weighting_method="cloud", contrast="gaussian"):
+        """
+        Constructor for the Atol measure vectorisation class.
+
+        Parameters:
+            quantiser (Object): Object with `fit` (sklearn API consistent) and `cluster_centers` and `n_clusters`
+                attributes, e.g. sklearn.cluster.KMeans. It will be fitted when the Atol object function `fit` is called.
+            weighting_method (string): constant generic function for weighting the measure points
+                choose from {"cloud", "iidproba"}
+                (default: constant function, i.e. the measure is seen as a point cloud by default).
+                This will have no impact if weights are provided along with measures all the way: `fit` and `transform`.
+            contrast (string): constant function for evaluating proximity of a measure with respect to centers
+                choose from {"gaussian", "laplacian", "indicator"}
+                (default: gaussian contrast function, see page 3 in the ATOL paper).
+        """
+        self.quantiser = quantiser
+        self.contrast = {
+            "gaussian": _gaus_contrast,
+            "laplacian": _lapl_contrast,
+            "indicator": _indicator_contrast,
+        }.get(contrast, _gaus_contrast)
+        self.weighting_method = {
+            "cloud"   : _cloud_weighting,
+            "iidproba": _iidproba_weighting,
+        }.get(weighting_method, _cloud_weighting)
+
+    def fit(self, X, y=None, sample_weight=None):
+        """
+        Calibration step: fit centers to the sample measures and derive inertias between centers.
+
+        Parameters:
+            X (list N x d numpy arrays): input measures in R^d from which to learn center locations and inertias
+                (measures can have different N).
+            y: Ignored, present for API consistency by convention.
+            sample_weight (list of numpy arrays): weights for each measure point in X, optional.
+                If None, the object's weighting_method will be used.
+
+        Returns:
+            self
+        """
+        if not hasattr(self.quantiser, 'fit'):
+            raise TypeError("quantiser %s has no `fit` attribute." % (self.quantiser))
+        if sample_weight is None:
+            sample_weight = np.concatenate([self.weighting_method(measure) for measure in X])
+
+        measures_concat = np.concatenate(X)
+        self.quantiser.fit(X=measures_concat, sample_weight=sample_weight)
+        self.centers = self.quantiser.cluster_centers_
+        # Hack, but some people are unhappy if the order depends on the version of sklearn
+        self.centers = self.centers[np.lexsort(self.centers.T)]
+        if self.quantiser.n_clusters == 1:
+            dist_centers = pairwise.pairwise_distances(measures_concat)
+            np.fill_diagonal(dist_centers, 0)
+            self.inertias = np.array([np.max(dist_centers)/2])
+        else:
+            dist_centers = pairwise.pairwise_distances(self.centers)
+            dist_centers[dist_centers == 0] = np.inf
+            self.inertias = np.min(dist_centers, axis=0)/2
+        return self
+
+    def __call__(self, measure, sample_weight=None):
+        """
+        Apply measure vectorisation on a single measure. Only available after `fit` has been called.
+
+        Parameters:
+            measure (n x d numpy array): input measure in R^d.
+
+        Returns:
+            numpy array in R^self.quantiser.n_clusters.
+        """
+        if sample_weight is None:
+            sample_weight = self.weighting_method(measure)
+        return np.sum(sample_weight * self.contrast(measure, self.centers, self.inertias.T).T, axis=1)
+
+    def transform(self, X, sample_weight=None):
+        """
+        Apply measure vectorisation on a list of measures.
+
+        Parameters:
+            X (list N x d numpy arrays): input measures in R^d from which to learn center locations and inertias
+                (measures can have different N).
+            sample_weight (list of numpy arrays): weights for each measure point in X, optional.
+                If None, the object's weighting_method will be used.
+
+        Returns:
+            numpy array with shape (number of measures) x (self.quantiser.n_clusters).
+        """
+        if sample_weight is None:
+            sample_weight = [self.weighting_method(measure) for measure in X]
+        return np.stack([self(measure, sample_weight=weight) for measure, weight in zip(X, sample_weight)])
```

## gudhi/sklearn/cubical_persistence.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Vincent Rouvreau
-#
-# Copyright (C) 2021 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-from .. import CubicalComplex
-from .._persline import persistence_on_a_line
-from sklearn.base import BaseEstimator, TransformerMixin
-
-import numpy as np
-# joblib is required by scikit-learn
-from joblib import Parallel, delayed
-
-# Mermaid sequence diagram - https://mermaid-js.github.io/mermaid-live-editor/
-# sequenceDiagram
-#     USER->>CubicalPersistence: fit_transform(X)
-#     CubicalPersistence->>thread1: _tranform(X[0])
-#     CubicalPersistence->>thread2: _tranform(X[1])
-#     Note right of CubicalPersistence: ...
-#     thread1->>CubicalPersistence: [array( H0(X[0]) ), array( H1(X[0]) )]
-#     thread2->>CubicalPersistence: [array( H0(X[1]) ), array( H1(X[1]) )]
-#     Note right of CubicalPersistence: ...
-#     CubicalPersistence->>USER: [[array( H0(X[0]) ), array( H1(X[0]) )],<br/> [array( H0(X[1]) ), array( H1(X[1]) )],<br/> ...]
-
-
-class CubicalPersistence(BaseEstimator, TransformerMixin):
-    """
-    This is a class for computing the persistence diagrams from a cubical complex.
-    """
-
-    def __init__(
-        self,
-        homology_dimensions,
-        input_type='top_dimensional_cells',
-        homology_coeff_field=11,
-        min_persistence=0.0,
-        n_jobs=None,
-    ):
-        """
-        Constructor for the CubicalPersistence class.
-
-        Parameters:
-            homology_dimensions (int or list of int): The returned persistence diagrams dimension(s).
-                Short circuit the use of :class:`~gudhi.representations.preprocessing.DimensionSelector` when only one
-                dimension matters (in other words, when `homology_dimensions` is an int).
-            input_type (str): 'top_dimensional_cells' if the filtration values passed to `transform()` are those of the
-                top-dimensional cells, 'vertices' if they correspond to the vertices.
-            homology_coeff_field (int): The homology coefficient field. Must be a prime number. Default value is 11.
-            min_persistence (float): The minimum persistence value to take into account (strictly greater than
-                `min_persistence`). Default value is `0.0`. Set `min_persistence` to `-1.0` to see all values.
-            n_jobs (int): cf. https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html
-        """
-        self.homology_dimensions = homology_dimensions
-        self.input_type = input_type
-        self.homology_coeff_field = homology_coeff_field
-        self.min_persistence = min_persistence
-        self.n_jobs = n_jobs
-
-    def fit(self, X, Y=None):
-        """
-        Nothing to be done, but useful when included in a scikit-learn Pipeline.
-        """
-        return self
-
-    def __transform(self, cells):
-        cells = np.asarray(cells)
-        if len(cells.shape) == 1 and self.min_persistence >= 0:
-            res = persistence_on_a_line(cells)
-            if self.min_persistence > 0:
-                # It would be more efficient inside persistence_on_a_line, but not worth it?
-                res = res[res[:, 1] - res[:, 0] > self.min_persistence]
-            # Wasteful if dim_list_ does not contain 0, but that seems unlikely.
-            return [res if i == 0 else np.empty((0,2)) for i in self.dim_list_]
-
-        if self.input_type == 'top_dimensional_cells':
-            cubical_complex = CubicalComplex(top_dimensional_cells=cells)
-        elif self.input_type == 'vertices':
-            cubical_complex = CubicalComplex(vertices=cells)
-        else:
-            raise ValueError("input_type can only be 'top_dimensional_cells' or 'vertices'")
-        cubical_complex.compute_persistence(
-            homology_coeff_field=self.homology_coeff_field, min_persistence=self.min_persistence
-        )
-        return [
-            cubical_complex.persistence_intervals_in_dimension(dim) for dim in self.dim_list_
-        ]
-
-    def transform(self, X, Y=None):
-        """Compute all the cubical complexes and their associated persistence diagrams.
-
-        :param X: Filtration values of the top-dimensional cells or vertices for each complex.
-        :type X: list of array-like
-
-        :return: Persistence diagrams in the format:
-
-              - If `homology_dimensions` was set to `n`: `[array( Hn(X[0]) ), array( Hn(X[1]) ), ...]` 
-              - If `homology_dimensions` was set to `[i, j]`: `[[array( Hi(X[0]) ), array( Hj(X[0]) )], [array( Hi(X[1]) ), array( Hj(X[1]) )], ...]`
-        :rtype: list of (,2) array_like or list of list of (,2) array_like
-        """
-        # Depends on homology_dimensions is an integer or a list of integer (else case)
-        if isinstance(self.homology_dimensions, int):
-            unwrap = True
-            self.dim_list_ = [ self.homology_dimensions ]
-        else:
-            unwrap = False
-            self.dim_list_ = self.homology_dimensions
-
-        # threads is preferred as cubical construction and persistence computation releases the GIL
-        res = Parallel(n_jobs=self.n_jobs, prefer="threads")(delayed(self.__transform)(cells) for cells in X)
-        if unwrap:
-            res = [d[0] for d in res]
-        return res
-
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Vincent Rouvreau
+#
+# Copyright (C) 2021 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+from .. import CubicalComplex
+from .._persline import persistence_on_a_line
+from sklearn.base import BaseEstimator, TransformerMixin
+
+import numpy as np
+# joblib is required by scikit-learn
+from joblib import Parallel, delayed
+
+# Mermaid sequence diagram - https://mermaid-js.github.io/mermaid-live-editor/
+# sequenceDiagram
+#     USER->>CubicalPersistence: fit_transform(X)
+#     CubicalPersistence->>thread1: _tranform(X[0])
+#     CubicalPersistence->>thread2: _tranform(X[1])
+#     Note right of CubicalPersistence: ...
+#     thread1->>CubicalPersistence: [array( H0(X[0]) ), array( H1(X[0]) )]
+#     thread2->>CubicalPersistence: [array( H0(X[1]) ), array( H1(X[1]) )]
+#     Note right of CubicalPersistence: ...
+#     CubicalPersistence->>USER: [[array( H0(X[0]) ), array( H1(X[0]) )],<br/> [array( H0(X[1]) ), array( H1(X[1]) )],<br/> ...]
+
+
+class CubicalPersistence(BaseEstimator, TransformerMixin):
+    """
+    This is a class for computing the persistence diagrams from a cubical complex.
+    """
+
+    def __init__(
+        self,
+        homology_dimensions,
+        input_type='top_dimensional_cells',
+        homology_coeff_field=11,
+        min_persistence=0.0,
+        n_jobs=None,
+    ):
+        """
+        Constructor for the CubicalPersistence class.
+
+        Parameters:
+            homology_dimensions (int or list of int): The returned persistence diagrams dimension(s).
+                Short circuit the use of :class:`~gudhi.representations.preprocessing.DimensionSelector` when only one
+                dimension matters (in other words, when `homology_dimensions` is an int).
+            input_type (str): 'top_dimensional_cells' if the filtration values passed to `transform()` are those of the
+                top-dimensional cells, 'vertices' if they correspond to the vertices.
+            homology_coeff_field (int): The homology coefficient field. Must be a prime number. Default value is 11.
+            min_persistence (float): The minimum persistence value to take into account (strictly greater than
+                `min_persistence`). Default value is `0.0`. Set `min_persistence` to `-1.0` to see all values.
+            n_jobs (int): cf. https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html
+        """
+        self.homology_dimensions = homology_dimensions
+        self.input_type = input_type
+        self.homology_coeff_field = homology_coeff_field
+        self.min_persistence = min_persistence
+        self.n_jobs = n_jobs
+
+    def fit(self, X, Y=None):
+        """
+        Nothing to be done, but useful when included in a scikit-learn Pipeline.
+        """
+        return self
+
+    def __transform(self, cells):
+        cells = np.asarray(cells)
+        if len(cells.shape) == 1 and self.min_persistence >= 0:
+            res = persistence_on_a_line(cells)
+            if self.min_persistence > 0:
+                # It would be more efficient inside persistence_on_a_line, but not worth it?
+                res = res[res[:, 1] - res[:, 0] > self.min_persistence]
+            # Wasteful if dim_list_ does not contain 0, but that seems unlikely.
+            return [res if i == 0 else np.empty((0,2)) for i in self.dim_list_]
+
+        if self.input_type == 'top_dimensional_cells':
+            cubical_complex = CubicalComplex(top_dimensional_cells=cells)
+        elif self.input_type == 'vertices':
+            cubical_complex = CubicalComplex(vertices=cells)
+        else:
+            raise ValueError("input_type can only be 'top_dimensional_cells' or 'vertices'")
+        cubical_complex.compute_persistence(
+            homology_coeff_field=self.homology_coeff_field, min_persistence=self.min_persistence
+        )
+        return [
+            cubical_complex.persistence_intervals_in_dimension(dim) for dim in self.dim_list_
+        ]
+
+    def transform(self, X, Y=None):
+        """Compute all the cubical complexes and their associated persistence diagrams.
+
+        :param X: Filtration values of the top-dimensional cells or vertices for each complex.
+        :type X: list of array-like
+
+        :return: Persistence diagrams in the format:
+
+              - If `homology_dimensions` was set to `n`: `[array( Hn(X[0]) ), array( Hn(X[1]) ), ...]` 
+              - If `homology_dimensions` was set to `[i, j]`: `[[array( Hi(X[0]) ), array( Hj(X[0]) )], [array( Hi(X[1]) ), array( Hj(X[1]) )], ...]`
+        :rtype: list of (,2) array_like or list of list of (,2) array_like
+        """
+        # Depends on homology_dimensions is an integer or a list of integer (else case)
+        if isinstance(self.homology_dimensions, int):
+            unwrap = True
+            self.dim_list_ = [ self.homology_dimensions ]
+        else:
+            unwrap = False
+            self.dim_list_ = self.homology_dimensions
+
+        # threads is preferred as cubical construction and persistence computation releases the GIL
+        res = Parallel(n_jobs=self.n_jobs, prefer="threads")(delayed(self.__transform)(cells) for cells in X)
+        if unwrap:
+            res = [d[0] for d in res]
+        return res
+
```

## gudhi/tensorflow/__init__.py

 * *Ordering differences only*

```diff
@@ -1,5 +1,5 @@
-from .cubical_layer import CubicalLayer
-from .lower_star_simplex_tree_layer import LowerStarSimplexTreeLayer
-from .rips_layer import RipsLayer
-
-__all__ = ["LowerStarSimplexTreeLayer", "RipsLayer", "CubicalLayer"]
+from .cubical_layer import CubicalLayer
+from .lower_star_simplex_tree_layer import LowerStarSimplexTreeLayer
+from .rips_layer import RipsLayer
+
+__all__ = ["LowerStarSimplexTreeLayer", "RipsLayer", "CubicalLayer"]
```

## gudhi/tensorflow/cubical_layer.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-import numpy               as np
-import tensorflow          as tf
-from ..cubical_complex  import CubicalComplex
-
-######################
-# Cubical filtration #
-######################
-
-# The parameters of the model are the pixel values.
-
-def _Cubical(Xflat, Xdim, dimensions, homology_coeff_field):
-    # Parameters: Xflat (flattened image),
-    #             Xdim (shape of non-flattened image)
-    #             dimensions (homology dimensions)
-
-    # Compute the persistence pairs with Gudhi
-    # We reverse the dimensions because CubicalComplex uses Fortran ordering
-    cc = CubicalComplex(dimensions=Xdim[::-1], top_dimensional_cells=Xflat)
-    cc.compute_persistence(homology_coeff_field=homology_coeff_field)
-
-    # Retrieve and output image indices/pixels corresponding to positive and negative simplices
-    cof_pp = cc.cofaces_of_persistence_pairs()
-    
-    L_cofs = []
-    for dim in dimensions:
-
-        try:
-            cof = cof_pp[0][dim]
-        except IndexError:
-            cof = np.array([])
-
-        L_cofs.append(np.array(cof, dtype=np.int32))
-
-    return L_cofs
-
-class CubicalLayer(tf.keras.layers.Layer):
-    """
-    TensorFlow layer for computing the persistent homology of a cubical complex
-    """
-    def __init__(self, homology_dimensions, min_persistence=None, homology_coeff_field=11, **kwargs):
-        """
-        Constructor for the CubicalLayer class
-
-        Parameters:
-            homology_dimensions (List[int]): list of homology dimensions
-            min_persistence (List[float]): minimum distance-to-diagonal of the points in the output persistence diagrams (default None, in which case 0. is used for all dimensions)
-            homology_coeff_field (int): homology field coefficient. Must be a prime number. Default value is 11. Max is 46337.
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.dimensions = homology_dimensions
-        self.min_persistence = min_persistence if min_persistence is not None else [0.] * len(self.dimensions)
-        self.hcf = homology_coeff_field
-        assert len(self.min_persistence) == len(self.dimensions)
-
-    def call(self, X):
-        """
-        Compute persistence diagram associated to a cubical complex filtered by some pixel values 
-
-        Parameters:
-            X (TensorFlow variable): pixel values of the cubical complex
-
-        Returns:
-            List[Tuple[tf.Tensor,tf.Tensor]]: List of cubical persistence diagrams. The length of this list is the same than that of dimensions, i.e., there is one persistence diagram per homology dimension provided in the input list dimensions. Moreover, the finite and essential parts of the persistence diagrams are provided separately: each element of this list is a tuple of size two that contains the finite and essential parts of the corresponding persistence diagram, of shapes [num_finite_points, 2] and [num_essential_points, 1] respectively. Note that the essential part is always empty in cubical persistence diagrams, except in homology dimension zero, where the essential part always contains a single point, with abscissa equal to the smallest value in the complex, and infinite ordinate
-        """
-        # Compute pixels associated to positive and negative simplices 
-        # Don't compute gradient for this operation
-        Xflat = tf.reshape(X, [-1])
-        Xdim, Xflat_numpy = X.shape, Xflat.numpy()
-        indices_list = _Cubical(Xflat_numpy, Xdim, self.dimensions, self.hcf)
-        index_essential = np.argmin(Xflat_numpy) # index of minimum pixel value for essential persistence diagram
-        # Get persistence diagram by simply picking the corresponding entries in the image
-        self.dgms = []
-        for idx_dim, dimension in enumerate(self.dimensions):
-            finite_dgm = tf.reshape(tf.gather(Xflat, indices_list[idx_dim]), [-1,2])
-            essential_dgm = tf.reshape(tf.gather(Xflat, index_essential), [-1,1]) if dimension == 0 else tf.zeros([0, 1])
-            min_pers = self.min_persistence[idx_dim]
-            if min_pers >= 0:
-                persistent_indices = tf.where(tf.math.abs(finite_dgm[:,1]-finite_dgm[:,0]) > min_pers)
-                self.dgms.append((tf.reshape(tf.gather(finite_dgm, indices=persistent_indices), [-1,2]), essential_dgm))
-            else:
-                self.dgms.append((finite_dgm, essential_dgm))
-        return self.dgms
+import numpy               as np
+import tensorflow          as tf
+from ..cubical_complex  import CubicalComplex
+
+######################
+# Cubical filtration #
+######################
+
+# The parameters of the model are the pixel values.
+
+def _Cubical(Xflat, Xdim, dimensions, homology_coeff_field):
+    # Parameters: Xflat (flattened image),
+    #             Xdim (shape of non-flattened image)
+    #             dimensions (homology dimensions)
+
+    # Compute the persistence pairs with Gudhi
+    # We reverse the dimensions because CubicalComplex uses Fortran ordering
+    cc = CubicalComplex(dimensions=Xdim[::-1], top_dimensional_cells=Xflat)
+    cc.compute_persistence(homology_coeff_field=homology_coeff_field)
+
+    # Retrieve and output image indices/pixels corresponding to positive and negative simplices
+    cof_pp = cc.cofaces_of_persistence_pairs()
+    
+    L_cofs = []
+    for dim in dimensions:
+
+        try:
+            cof = cof_pp[0][dim]
+        except IndexError:
+            cof = np.array([])
+
+        L_cofs.append(np.array(cof, dtype=np.int32))
+
+    return L_cofs
+
+class CubicalLayer(tf.keras.layers.Layer):
+    """
+    TensorFlow layer for computing the persistent homology of a cubical complex
+    """
+    def __init__(self, homology_dimensions, min_persistence=None, homology_coeff_field=11, **kwargs):
+        """
+        Constructor for the CubicalLayer class
+
+        Parameters:
+            homology_dimensions (List[int]): list of homology dimensions
+            min_persistence (List[float]): minimum distance-to-diagonal of the points in the output persistence diagrams (default None, in which case 0. is used for all dimensions)
+            homology_coeff_field (int): homology field coefficient. Must be a prime number. Default value is 11. Max is 46337.
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.dimensions = homology_dimensions
+        self.min_persistence = min_persistence if min_persistence is not None else [0.] * len(self.dimensions)
+        self.hcf = homology_coeff_field
+        assert len(self.min_persistence) == len(self.dimensions)
+
+    def call(self, X):
+        """
+        Compute persistence diagram associated to a cubical complex filtered by some pixel values 
+
+        Parameters:
+            X (TensorFlow variable): pixel values of the cubical complex
+
+        Returns:
+            List[Tuple[tf.Tensor,tf.Tensor]]: List of cubical persistence diagrams. The length of this list is the same than that of dimensions, i.e., there is one persistence diagram per homology dimension provided in the input list dimensions. Moreover, the finite and essential parts of the persistence diagrams are provided separately: each element of this list is a tuple of size two that contains the finite and essential parts of the corresponding persistence diagram, of shapes [num_finite_points, 2] and [num_essential_points, 1] respectively. Note that the essential part is always empty in cubical persistence diagrams, except in homology dimension zero, where the essential part always contains a single point, with abscissa equal to the smallest value in the complex, and infinite ordinate
+        """
+        # Compute pixels associated to positive and negative simplices 
+        # Don't compute gradient for this operation
+        Xflat = tf.reshape(X, [-1])
+        Xdim, Xflat_numpy = X.shape, Xflat.numpy()
+        indices_list = _Cubical(Xflat_numpy, Xdim, self.dimensions, self.hcf)
+        index_essential = np.argmin(Xflat_numpy) # index of minimum pixel value for essential persistence diagram
+        # Get persistence diagram by simply picking the corresponding entries in the image
+        self.dgms = []
+        for idx_dim, dimension in enumerate(self.dimensions):
+            finite_dgm = tf.reshape(tf.gather(Xflat, indices_list[idx_dim]), [-1,2])
+            essential_dgm = tf.reshape(tf.gather(Xflat, index_essential), [-1,1]) if dimension == 0 else tf.zeros([0, 1])
+            min_pers = self.min_persistence[idx_dim]
+            if min_pers >= 0:
+                persistent_indices = tf.where(tf.math.abs(finite_dgm[:,1]-finite_dgm[:,0]) > min_pers)
+                self.dgms.append((tf.reshape(tf.gather(finite_dgm, indices=persistent_indices), [-1,2]), essential_dgm))
+            else:
+                self.dgms.append((finite_dgm, essential_dgm))
+        return self.dgms
```

## gudhi/tensorflow/lower_star_simplex_tree_layer.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-import numpy               as np
-import tensorflow          as tf
-
-#########################################
-# Lower star filtration on simplex tree #
-#########################################
-
-# The parameters of the model are the vertex function values of the simplex tree.
-
-def _LowerStarSimplexTree(simplextree, filtration, dimensions, homology_coeff_field):
-    # Parameters: simplextree (simplex tree on which to compute persistence)
-    #             filtration (function values on the vertices of st),
-    #             dimensions (homology dimensions),
-    #             homology_coeff_field (homology field coefficient)
-    
-    simplextree.reset_filtration(-np.inf, 0)
-
-    # Assign new filtration values
-    for i in range(simplextree.num_vertices()):
-        simplextree.assign_filtration([i], filtration[i])
-    simplextree.make_filtration_non_decreasing()
-    
-    # Compute persistence diagram
-    simplextree.compute_persistence(homology_coeff_field=homology_coeff_field)
-    
-    # Get vertex pairs for optimization. First, get all simplex pairs
-    pairs = simplextree.lower_star_persistence_generators()
-    
-    L_indices = []
-    for dimension in dimensions:
-    
-        finite_pairs = pairs[0][dimension] if len(pairs[0]) >= dimension+1 else np.empty(shape=[0,2])
-        essential_pairs = pairs[1][dimension] if len(pairs[1]) >= dimension+1 else np.empty(shape=[0,1])
-        
-        finite_indices = np.array(finite_pairs.flatten(), dtype=np.int32)
-        essential_indices = np.array(essential_pairs.flatten(), dtype=np.int32)
-
-        L_indices.append((finite_indices, essential_indices))
-
-    return L_indices
-
-class LowerStarSimplexTreeLayer(tf.keras.layers.Layer):
-    """
-    TensorFlow layer for computing lower-star persistence out of a simplex tree
-    """
-    def __init__(self, simplextree, homology_dimensions, min_persistence=None, homology_coeff_field=11, **kwargs):
-        """
-        Constructor for the LowerStarSimplexTreeLayer class
-  
-        Parameters:
-            simplextree (gudhi.SimplexTree): underlying simplex tree. Its vertices MUST be named with integers from 0 to n-1, where n is its number of vertices. Note that its filtration values are modified in each call of the class.
-            homology_dimensions (List[int]): list of homology dimensions
-            min_persistence (List[float]): minimum distance-to-diagonal of the points in the output persistence diagrams (default None, in which case 0. is used for all dimensions)
-            homology_coeff_field (int): homology field coefficient. Must be a prime number. Default value is 11. Max is 46337.
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.dimensions  = homology_dimensions
-        self.simplextree = simplextree
-        self.min_persistence = min_persistence if min_persistence is not None else [0. for _ in range(len(self.dimensions))]
-        self.hcf = homology_coeff_field
-        assert len(self.min_persistence) == len(self.dimensions)
-
-    def call(self, filtration):
-        """
-        Compute lower-star persistence diagram associated to a function defined on the vertices of the simplex tree
-
-        Parameters:
-            F (TensorFlow variable): filter function values over the vertices of the simplex tree. The ith entry of F corresponds to vertex i in self.simplextree
-
-        Returns:
-            List[Tuple[tf.Tensor,tf.Tensor]]: List of lower-star persistence diagrams. The length of this list is the same than that of dimensions, i.e., there is one persistence diagram per homology dimension provided in the input list dimensions. Moreover, the finite and essential parts of the persistence diagrams are provided separately: each element of this list is a tuple of size two that contains the finite and essential parts of the corresponding persistence diagram, of shapes [num_finite_points, 2] and [num_essential_points, 1] respectively
-        """
-        # Don't try to compute gradients for the vertex pairs
-        indices = _LowerStarSimplexTree(self.simplextree, filtration.numpy(), self.dimensions, self.hcf)
-        # Get persistence diagrams
-        self.dgms = []
-        for idx_dim, dimension in enumerate(self.dimensions):
-            finite_dgm = tf.reshape(tf.gather(filtration, indices[idx_dim][0]), [-1,2])
-            essential_dgm = tf.reshape(tf.gather(filtration, indices[idx_dim][1]), [-1,1])
-            min_pers = self.min_persistence[idx_dim]
-            if min_pers >= 0:
-                persistent_indices = tf.where(tf.math.abs(finite_dgm[:,1]-finite_dgm[:,0]) > min_pers)
-                self.dgms.append((tf.reshape(tf.gather(finite_dgm, indices=persistent_indices),[-1,2]), essential_dgm))
-            else:
-                self.dgms.append((finite_dgm, essential_dgm))
-        return self.dgms
-
+import numpy               as np
+import tensorflow          as tf
+
+#########################################
+# Lower star filtration on simplex tree #
+#########################################
+
+# The parameters of the model are the vertex function values of the simplex tree.
+
+def _LowerStarSimplexTree(simplextree, filtration, dimensions, homology_coeff_field):
+    # Parameters: simplextree (simplex tree on which to compute persistence)
+    #             filtration (function values on the vertices of st),
+    #             dimensions (homology dimensions),
+    #             homology_coeff_field (homology field coefficient)
+    
+    simplextree.reset_filtration(-np.inf, 0)
+
+    # Assign new filtration values
+    for i in range(simplextree.num_vertices()):
+        simplextree.assign_filtration([i], filtration[i])
+    simplextree.make_filtration_non_decreasing()
+    
+    # Compute persistence diagram
+    simplextree.compute_persistence(homology_coeff_field=homology_coeff_field)
+    
+    # Get vertex pairs for optimization. First, get all simplex pairs
+    pairs = simplextree.lower_star_persistence_generators()
+    
+    L_indices = []
+    for dimension in dimensions:
+    
+        finite_pairs = pairs[0][dimension] if len(pairs[0]) >= dimension+1 else np.empty(shape=[0,2])
+        essential_pairs = pairs[1][dimension] if len(pairs[1]) >= dimension+1 else np.empty(shape=[0,1])
+        
+        finite_indices = np.array(finite_pairs.flatten(), dtype=np.int32)
+        essential_indices = np.array(essential_pairs.flatten(), dtype=np.int32)
+
+        L_indices.append((finite_indices, essential_indices))
+
+    return L_indices
+
+class LowerStarSimplexTreeLayer(tf.keras.layers.Layer):
+    """
+    TensorFlow layer for computing lower-star persistence out of a simplex tree
+    """
+    def __init__(self, simplextree, homology_dimensions, min_persistence=None, homology_coeff_field=11, **kwargs):
+        """
+        Constructor for the LowerStarSimplexTreeLayer class
+  
+        Parameters:
+            simplextree (gudhi.SimplexTree): underlying simplex tree. Its vertices MUST be named with integers from 0 to n-1, where n is its number of vertices. Note that its filtration values are modified in each call of the class.
+            homology_dimensions (List[int]): list of homology dimensions
+            min_persistence (List[float]): minimum distance-to-diagonal of the points in the output persistence diagrams (default None, in which case 0. is used for all dimensions)
+            homology_coeff_field (int): homology field coefficient. Must be a prime number. Default value is 11. Max is 46337.
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.dimensions  = homology_dimensions
+        self.simplextree = simplextree
+        self.min_persistence = min_persistence if min_persistence is not None else [0. for _ in range(len(self.dimensions))]
+        self.hcf = homology_coeff_field
+        assert len(self.min_persistence) == len(self.dimensions)
+
+    def call(self, filtration):
+        """
+        Compute lower-star persistence diagram associated to a function defined on the vertices of the simplex tree
+
+        Parameters:
+            F (TensorFlow variable): filter function values over the vertices of the simplex tree. The ith entry of F corresponds to vertex i in self.simplextree
+
+        Returns:
+            List[Tuple[tf.Tensor,tf.Tensor]]: List of lower-star persistence diagrams. The length of this list is the same than that of dimensions, i.e., there is one persistence diagram per homology dimension provided in the input list dimensions. Moreover, the finite and essential parts of the persistence diagrams are provided separately: each element of this list is a tuple of size two that contains the finite and essential parts of the corresponding persistence diagram, of shapes [num_finite_points, 2] and [num_essential_points, 1] respectively
+        """
+        # Don't try to compute gradients for the vertex pairs
+        indices = _LowerStarSimplexTree(self.simplextree, filtration.numpy(), self.dimensions, self.hcf)
+        # Get persistence diagrams
+        self.dgms = []
+        for idx_dim, dimension in enumerate(self.dimensions):
+            finite_dgm = tf.reshape(tf.gather(filtration, indices[idx_dim][0]), [-1,2])
+            essential_dgm = tf.reshape(tf.gather(filtration, indices[idx_dim][1]), [-1,1])
+            min_pers = self.min_persistence[idx_dim]
+            if min_pers >= 0:
+                persistent_indices = tf.where(tf.math.abs(finite_dgm[:,1]-finite_dgm[:,0]) > min_pers)
+                self.dgms.append((tf.reshape(tf.gather(finite_dgm, indices=persistent_indices),[-1,2]), essential_dgm))
+            else:
+                self.dgms.append((finite_dgm, essential_dgm))
+        return self.dgms
+
```

## gudhi/tensorflow/perslay.py

 * *Ordering differences only*

```diff
@@ -1,284 +1,284 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Mathieu Carri√®re
-#
-# Copyright (C) 2021 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import tensorflow as tf
-import math
-
-class GridPerslayWeight(tf.keras.layers.Layer):
-    """
-    This is a class for computing a differentiable weight function for persistence diagram points. This function is defined from an array that contains its values on a 2D grid.
-    """
-    def __init__(self, grid, grid_bnds, **kwargs):
-        """
-        Constructor for the GridPerslayWeight class.
-  
-        Parameters:
-            grid (n x n numpy array): grid of values.
-            grid_bnds (2 x 2 numpy array): boundaries of the grid, of the form [[min_x, max_x], [min_y, max_y]].
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.grid = tf.Variable(initial_value=grid, trainable=True)
-        self.grid_bnds = grid_bnds
-    
-    def build(self, input_shape):
-        return self
-
-    def call(self, diagrams):
-        """
-        Apply GridPerslayWeight on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            weight (n x None): ragged tensor containing the weights of the points in the n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-        """
-        grid_shape = self.grid.shape
-        indices = []
-        for dim in range(2):
-            [m,M] = self.grid_bnds[dim]
-            coords = tf.expand_dims(diagrams[:,:,dim],-1)
-            ids = grid_shape[dim]*(coords-m)/(M-m)
-            indices.append(tf.cast(ids, tf.int32))
-        weight = tf.gather_nd(params=self.grid, indices=tf.concat(indices, axis=2))
-        return weight
-    
-class GaussianMixturePerslayWeight(tf.keras.layers.Layer):
-    """
-    This is a class for computing a differentiable weight function for persistence diagram points. This function is defined from a mixture of Gaussian functions.
-    """
-    def __init__(self, gaussians, **kwargs):
-        """
-        Constructor for the GridPerslayWeight class.
-  
-        Parameters:
-            gaussians (4 x n numpy array): parameters of the n Gaussian functions, of the form transpose([[mu_x^1, mu_y^1, sigma_x^1, sigma_y^1], ..., [mu_x^n, mu_y^n, sigma_x^n, sigma_y^n]]). 
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.W = tf.Variable(initial_value=gaussians, trainable=True)
-
-    def build(self, input_shape):
-        return self
-        
-    def call(self, diagrams):
-        """
-        Apply GaussianMixturePerslayWeight on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            weight (n x None): ragged tensor containing the weights of the points in the n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-        """
-        means     = tf.expand_dims(tf.expand_dims(self.W[:2,:],0),0)
-        variances = tf.expand_dims(tf.expand_dims(self.W[2:,:],0),0)
-        diagrams  = tf.expand_dims(diagrams, -1)
-        dists     = tf.math.multiply(tf.math.square(diagrams-means), 1/tf.math.square(variances))
-        weight    = tf.math.reduce_sum(tf.math.exp(tf.math.reduce_sum(-dists, axis=2)), axis=2)
-        return weight
-    
-class PowerPerslayWeight(tf.keras.layers.Layer):
-    """
-    This is a class for computing a differentiable weight function for persistence diagram points. This function is defined as a constant multiplied by the distance to the diagonal of the persistence diagram point raised to some power.
-    """
-    def __init__(self, constant, power, **kwargs):
-        """
-        Constructor for the PowerPerslayWeight class.
-  
-        Parameters:
-            constant (float): constant value.
-            power (float): power applied to the distance to the diagonal. 
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.constant = tf.Variable(initial_value=constant, trainable=True)
-        self.power = power
-        
-    def build(self, input_shape):
-        return self
-    
-    def call(self, diagrams):
-        """
-        Apply PowerPerslayWeight on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            weight (n x None): ragged tensor containing the weights of the points in the n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-        """
-        weight = self.constant * tf.math.pow(tf.math.abs(diagrams[:,:,1]-diagrams[:,:,0]), self.power)
-        return weight
-    
-
-class GaussianPerslayPhi(tf.keras.layers.Layer):
-    """
-    This is a class for computing a transformation function for persistence diagram points. This function turns persistence diagram points into 2D Gaussian functions centered on the points, that are then evaluated on a regular 2D grid.
-    """
-    def __init__(self, image_size, image_bnds, variance, **kwargs):
-        """
-        Constructor for the GaussianPerslayPhi class.
-  
-        Parameters:
-            image_size (int numpy array): number of grid elements on each grid axis, of the form [n_x, n_y].
-            image_bnds (2 x 2 numpy array): boundaries of the grid, of the form [[min_x, max_x], [min_y, max_y]].
-            variance (float): variance of the Gaussian functions. 
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.image_size = image_size
-        self.image_bnds = image_bnds
-        self.variance   = tf.Variable(initial_value=variance, trainable=True)
-        
-    def build(self, input_shape):
-        return self
-        
-    def call(self, diagrams):
-        """
-        Apply GaussianPerslayPhi on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            output (n x None x image_size x image_size x 1): ragged tensor containing the evaluations on the 2D grid of the 2D Gaussian functions corresponding to the persistence diagram points, in the form of a 2D image with 1 channel that can be processed with, e.g., convolutional layers. The second dimension is ragged since persistence diagrams can have different numbers of points.
-            output_shape (int numpy array): shape of the output tensor.
-        """
-        diagrams_d = tf.concat([diagrams[:,:,0:1], diagrams[:,:,1:2]-diagrams[:,:,0:1]], axis=2)
-        step = [(self.image_bnds[i][1]-self.image_bnds[i][0])/self.image_size[i] for i in range(2)]
-        coords = [tf.range(self.image_bnds[i][0], self.image_bnds[i][1], step[i]) for i in range(2)]
-        M = tf.meshgrid(*coords)
-        mu = tf.concat([tf.expand_dims(tens, 0) for tens in M], axis=0)
-        for _ in range(2):
-            diagrams_d = tf.expand_dims(diagrams_d,-1)
-        dists = tf.math.square(diagrams_d-mu) / (2*tf.math.square(self.variance))
-        gauss = tf.math.exp(tf.math.reduce_sum(-dists, axis=2)) / (2*math.pi*tf.math.square(self.variance))
-        output = tf.expand_dims(gauss,-1)
-        output_shape = M[0].shape + tuple([1])
-        return output, output_shape
-     
-class TentPerslayPhi(tf.keras.layers.Layer):
-    """
-    This is a class for computing a transformation function for persistence diagram points. This function turns persistence diagram points into 1D tent functions (linearly increasing on the first half of the bar corresponding to the point from zero to half of the bar length, linearly decreasing on the second half and zero elsewhere) centered on the points, that are then evaluated on a regular 1D grid.
-    """
-    def __init__(self, samples, **kwargs):
-        """
-        Constructor for the GaussianPerslayPhi class.
-  
-        Parameters:
-            samples (float numpy array): grid elements on which to evaluate the tent functions, of the form [x_1, ..., x_n].
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.samples   = tf.Variable(initial_value=samples, trainable=True)
-        
-    def build(self, input_shape):
-        return self
-        
-    def call(self, diagrams):
-        """
-        Apply TentPerslayPhi on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            output (n x None x num_samples): ragged tensor containing the evaluations on the 1D grid of the 1D tent functions corresponding to the persistence diagram points. The second dimension is ragged since persistence diagrams can have different numbers of points.
-            output_shape (int numpy array): shape of the output tensor.
-        """
-        samples_d = tf.expand_dims(tf.expand_dims(self.samples,0),0)
-        xs, ys = diagrams[:,:,0:1], diagrams[:,:,1:2]
-        output = tf.math.maximum(.5*(ys-xs) - tf.math.abs(samples_d-.5*(ys+xs)), tf.constant([0.]))
-        output_shape = self.samples.shape
-        return output, output_shape
-    
-class FlatPerslayPhi(tf.keras.layers.Layer):
-    """
-    This is a class for computing a transformation function for persistence diagram points. This function turns persistence diagram points into 1D constant functions (that evaluate to half of the bar length on the bar corresponding to the point and zero elsewhere), that are then evaluated on a regular 1D grid.
-    """
-    def __init__(self, samples, theta, **kwargs):
-        """
-        Constructor for the FlatPerslayPhi class.
-  
-        Parameters:
-            samples (float numpy array): grid elements on which to evaluate the constant functions, of the form [x_1, ..., x_n].
-            theta (float): sigmoid parameter used to approximate the constant function with a differentiable sigmoid function. The bigger the theta, the closer to a constant function the output will be. 
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.samples = tf.Variable(initial_value=samples, trainable=True)
-        self.theta   = tf.Variable(initial_value=theta,   trainable=True)
-        
-    def build(self, input_shape):
-        return self
-        
-    def call(self, diagrams):
-        """
-        Apply FlatPerslayPhi on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            output (n x None x num_samples): ragged tensor containing the evaluations on the 1D grid of the 1D constant functions corresponding to the persistence diagram points. The second dimension is ragged since persistence diagrams can have different numbers of points.
-            output_shape (int numpy array): shape of the output tensor.
-        """
-        samples_d = tf.expand_dims(tf.expand_dims(self.samples,0),0)
-        xs, ys = diagrams[:,:,0:1], diagrams[:,:,1:2]
-        output = 1./(1.+tf.math.exp(-self.theta*(.5*(ys-xs)-tf.math.abs(samples_d-.5*(ys+xs)))))
-        output_shape = self.samples.shape
-        return output, output_shape
-
-class Perslay(tf.keras.layers.Layer):
-    """
-    This is a TensorFlow layer for vectorizing persistence diagrams in a differentiable way within a neural network. This function implements the PersLay equation, see `the corresponding article <http://proceedings.mlr.press/v108/carriere20a.html>`_.
-    """
-    def __init__(self, weight, phi, perm_op, rho, **kwargs):
-        """
-        Constructor for the Perslay class.
-
-        Parameters:
-            weight (function): weight function for the persistence diagram points. Can be either :class:`~gudhi.tensorflow.perslay.GridPerslayWeight`, :class:`~gudhi.tensorflow.perslay.GaussianMixturePerslayWeight`, :class:`~gudhi.tensorflow.perslay.PowerPerslayWeight`, or a custom TensorFlow function that takes persistence diagrams as argument (represented as an (n x None x 2) ragged tensor, where n is the number of diagrams).
-            phi (function): transformation function for the persistence diagram points. Can be either :class:`~gudhi.tensorflow.perslay.GaussianPerslayPhi`, :class:`~gudhi.tensorflow.perslay.TentPerslayPhi`, :class:`~gudhi.tensorflow.perslay.FlatPerslayPhi`, or a custom TensorFlow class (that can have trainable parameters) with a method `call` that takes persistence diagrams as argument (represented as an (n x None x 2) ragged tensor, where n is the number of diagrams).
-            perm_op (function): permutation invariant function, such as `tf.math.reduce_sum`, `tf.math.reduce_mean`, `tf.math.reduce_max`, `tf.math.reduce_min`, or a custom TensorFlow function that takes two arguments: a tensor and an axis on which to apply the permutation invariant operation. If perm_op is the string "topk" (where k is a number), this function will be computed as `tf.math.top_k` with parameter `int(k)`.
-            rho (function): postprocessing function that is applied after the permutation invariant operation. Can be any TensorFlow layer.
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.weight  = weight
-        self.phi     = phi
-        self.perm_op = perm_op  
-        self.rho     = rho
-
-    def build(self, input_shape):
-        return self
-
-    def call(self, diagrams):
-        """
-        Apply Perslay on a ragged tensor containing a list of persistence diagrams.
-
-        Parameters:
-            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
-
-        Returns:
-            vector (n x output_shape): tensor containing the vectorizations of the persistence diagrams.
-        """
-        vector, dim = self.phi(diagrams)
-        weight = self.weight(diagrams)
-        for _ in range(len(dim)):
-            weight = tf.expand_dims(weight, -1)
-        vector = tf.math.multiply(vector, weight)
-          
-        permop = self.perm_op
-        if type(permop) == str and permop[:3] == 'top':
-            k = int(permop[3:])
-            vector = vector.to_tensor(default_value=-1e10)
-            vector = tf.math.top_k(tf.transpose(vector, perm=[0, 2, 1]), k=k).values
-            vector = tf.reshape(vector, [-1,k*dim[0]])
-        else:
-            vector = permop(vector, axis=1)
-
-        vector = self.rho(vector)
-            
-        return vector
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Mathieu Carri√®re
+#
+# Copyright (C) 2021 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import tensorflow as tf
+import math
+
+class GridPerslayWeight(tf.keras.layers.Layer):
+    """
+    This is a class for computing a differentiable weight function for persistence diagram points. This function is defined from an array that contains its values on a 2D grid.
+    """
+    def __init__(self, grid, grid_bnds, **kwargs):
+        """
+        Constructor for the GridPerslayWeight class.
+  
+        Parameters:
+            grid (n x n numpy array): grid of values.
+            grid_bnds (2 x 2 numpy array): boundaries of the grid, of the form [[min_x, max_x], [min_y, max_y]].
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.grid = tf.Variable(initial_value=grid, trainable=True)
+        self.grid_bnds = grid_bnds
+    
+    def build(self, input_shape):
+        return self
+
+    def call(self, diagrams):
+        """
+        Apply GridPerslayWeight on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            weight (n x None): ragged tensor containing the weights of the points in the n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+        """
+        grid_shape = self.grid.shape
+        indices = []
+        for dim in range(2):
+            [m,M] = self.grid_bnds[dim]
+            coords = tf.expand_dims(diagrams[:,:,dim],-1)
+            ids = grid_shape[dim]*(coords-m)/(M-m)
+            indices.append(tf.cast(ids, tf.int32))
+        weight = tf.gather_nd(params=self.grid, indices=tf.concat(indices, axis=2))
+        return weight
+    
+class GaussianMixturePerslayWeight(tf.keras.layers.Layer):
+    """
+    This is a class for computing a differentiable weight function for persistence diagram points. This function is defined from a mixture of Gaussian functions.
+    """
+    def __init__(self, gaussians, **kwargs):
+        """
+        Constructor for the GridPerslayWeight class.
+  
+        Parameters:
+            gaussians (4 x n numpy array): parameters of the n Gaussian functions, of the form transpose([[mu_x^1, mu_y^1, sigma_x^1, sigma_y^1], ..., [mu_x^n, mu_y^n, sigma_x^n, sigma_y^n]]). 
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.W = tf.Variable(initial_value=gaussians, trainable=True)
+
+    def build(self, input_shape):
+        return self
+        
+    def call(self, diagrams):
+        """
+        Apply GaussianMixturePerslayWeight on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            weight (n x None): ragged tensor containing the weights of the points in the n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+        """
+        means     = tf.expand_dims(tf.expand_dims(self.W[:2,:],0),0)
+        variances = tf.expand_dims(tf.expand_dims(self.W[2:,:],0),0)
+        diagrams  = tf.expand_dims(diagrams, -1)
+        dists     = tf.math.multiply(tf.math.square(diagrams-means), 1/tf.math.square(variances))
+        weight    = tf.math.reduce_sum(tf.math.exp(tf.math.reduce_sum(-dists, axis=2)), axis=2)
+        return weight
+    
+class PowerPerslayWeight(tf.keras.layers.Layer):
+    """
+    This is a class for computing a differentiable weight function for persistence diagram points. This function is defined as a constant multiplied by the distance to the diagonal of the persistence diagram point raised to some power.
+    """
+    def __init__(self, constant, power, **kwargs):
+        """
+        Constructor for the PowerPerslayWeight class.
+  
+        Parameters:
+            constant (float): constant value.
+            power (float): power applied to the distance to the diagonal. 
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.constant = tf.Variable(initial_value=constant, trainable=True)
+        self.power = power
+        
+    def build(self, input_shape):
+        return self
+    
+    def call(self, diagrams):
+        """
+        Apply PowerPerslayWeight on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            weight (n x None): ragged tensor containing the weights of the points in the n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+        """
+        weight = self.constant * tf.math.pow(tf.math.abs(diagrams[:,:,1]-diagrams[:,:,0]), self.power)
+        return weight
+    
+
+class GaussianPerslayPhi(tf.keras.layers.Layer):
+    """
+    This is a class for computing a transformation function for persistence diagram points. This function turns persistence diagram points into 2D Gaussian functions centered on the points, that are then evaluated on a regular 2D grid.
+    """
+    def __init__(self, image_size, image_bnds, variance, **kwargs):
+        """
+        Constructor for the GaussianPerslayPhi class.
+  
+        Parameters:
+            image_size (int numpy array): number of grid elements on each grid axis, of the form [n_x, n_y].
+            image_bnds (2 x 2 numpy array): boundaries of the grid, of the form [[min_x, max_x], [min_y, max_y]].
+            variance (float): variance of the Gaussian functions. 
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.image_size = image_size
+        self.image_bnds = image_bnds
+        self.variance   = tf.Variable(initial_value=variance, trainable=True)
+        
+    def build(self, input_shape):
+        return self
+        
+    def call(self, diagrams):
+        """
+        Apply GaussianPerslayPhi on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            output (n x None x image_size x image_size x 1): ragged tensor containing the evaluations on the 2D grid of the 2D Gaussian functions corresponding to the persistence diagram points, in the form of a 2D image with 1 channel that can be processed with, e.g., convolutional layers. The second dimension is ragged since persistence diagrams can have different numbers of points.
+            output_shape (int numpy array): shape of the output tensor.
+        """
+        diagrams_d = tf.concat([diagrams[:,:,0:1], diagrams[:,:,1:2]-diagrams[:,:,0:1]], axis=2)
+        step = [(self.image_bnds[i][1]-self.image_bnds[i][0])/self.image_size[i] for i in range(2)]
+        coords = [tf.range(self.image_bnds[i][0], self.image_bnds[i][1], step[i]) for i in range(2)]
+        M = tf.meshgrid(*coords)
+        mu = tf.concat([tf.expand_dims(tens, 0) for tens in M], axis=0)
+        for _ in range(2):
+            diagrams_d = tf.expand_dims(diagrams_d,-1)
+        dists = tf.math.square(diagrams_d-mu) / (2*tf.math.square(self.variance))
+        gauss = tf.math.exp(tf.math.reduce_sum(-dists, axis=2)) / (2*math.pi*tf.math.square(self.variance))
+        output = tf.expand_dims(gauss,-1)
+        output_shape = M[0].shape + tuple([1])
+        return output, output_shape
+     
+class TentPerslayPhi(tf.keras.layers.Layer):
+    """
+    This is a class for computing a transformation function for persistence diagram points. This function turns persistence diagram points into 1D tent functions (linearly increasing on the first half of the bar corresponding to the point from zero to half of the bar length, linearly decreasing on the second half and zero elsewhere) centered on the points, that are then evaluated on a regular 1D grid.
+    """
+    def __init__(self, samples, **kwargs):
+        """
+        Constructor for the GaussianPerslayPhi class.
+  
+        Parameters:
+            samples (float numpy array): grid elements on which to evaluate the tent functions, of the form [x_1, ..., x_n].
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.samples   = tf.Variable(initial_value=samples, trainable=True)
+        
+    def build(self, input_shape):
+        return self
+        
+    def call(self, diagrams):
+        """
+        Apply TentPerslayPhi on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            output (n x None x num_samples): ragged tensor containing the evaluations on the 1D grid of the 1D tent functions corresponding to the persistence diagram points. The second dimension is ragged since persistence diagrams can have different numbers of points.
+            output_shape (int numpy array): shape of the output tensor.
+        """
+        samples_d = tf.expand_dims(tf.expand_dims(self.samples,0),0)
+        xs, ys = diagrams[:,:,0:1], diagrams[:,:,1:2]
+        output = tf.math.maximum(.5*(ys-xs) - tf.math.abs(samples_d-.5*(ys+xs)), tf.constant([0.]))
+        output_shape = self.samples.shape
+        return output, output_shape
+    
+class FlatPerslayPhi(tf.keras.layers.Layer):
+    """
+    This is a class for computing a transformation function for persistence diagram points. This function turns persistence diagram points into 1D constant functions (that evaluate to half of the bar length on the bar corresponding to the point and zero elsewhere), that are then evaluated on a regular 1D grid.
+    """
+    def __init__(self, samples, theta, **kwargs):
+        """
+        Constructor for the FlatPerslayPhi class.
+  
+        Parameters:
+            samples (float numpy array): grid elements on which to evaluate the constant functions, of the form [x_1, ..., x_n].
+            theta (float): sigmoid parameter used to approximate the constant function with a differentiable sigmoid function. The bigger the theta, the closer to a constant function the output will be. 
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.samples = tf.Variable(initial_value=samples, trainable=True)
+        self.theta   = tf.Variable(initial_value=theta,   trainable=True)
+        
+    def build(self, input_shape):
+        return self
+        
+    def call(self, diagrams):
+        """
+        Apply FlatPerslayPhi on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            output (n x None x num_samples): ragged tensor containing the evaluations on the 1D grid of the 1D constant functions corresponding to the persistence diagram points. The second dimension is ragged since persistence diagrams can have different numbers of points.
+            output_shape (int numpy array): shape of the output tensor.
+        """
+        samples_d = tf.expand_dims(tf.expand_dims(self.samples,0),0)
+        xs, ys = diagrams[:,:,0:1], diagrams[:,:,1:2]
+        output = 1./(1.+tf.math.exp(-self.theta*(.5*(ys-xs)-tf.math.abs(samples_d-.5*(ys+xs)))))
+        output_shape = self.samples.shape
+        return output, output_shape
+
+class Perslay(tf.keras.layers.Layer):
+    """
+    This is a TensorFlow layer for vectorizing persistence diagrams in a differentiable way within a neural network. This function implements the PersLay equation, see `the corresponding article <http://proceedings.mlr.press/v108/carriere20a.html>`_.
+    """
+    def __init__(self, weight, phi, perm_op, rho, **kwargs):
+        """
+        Constructor for the Perslay class.
+
+        Parameters:
+            weight (function): weight function for the persistence diagram points. Can be either :class:`~gudhi.tensorflow.perslay.GridPerslayWeight`, :class:`~gudhi.tensorflow.perslay.GaussianMixturePerslayWeight`, :class:`~gudhi.tensorflow.perslay.PowerPerslayWeight`, or a custom TensorFlow function that takes persistence diagrams as argument (represented as an (n x None x 2) ragged tensor, where n is the number of diagrams).
+            phi (function): transformation function for the persistence diagram points. Can be either :class:`~gudhi.tensorflow.perslay.GaussianPerslayPhi`, :class:`~gudhi.tensorflow.perslay.TentPerslayPhi`, :class:`~gudhi.tensorflow.perslay.FlatPerslayPhi`, or a custom TensorFlow class (that can have trainable parameters) with a method `call` that takes persistence diagrams as argument (represented as an (n x None x 2) ragged tensor, where n is the number of diagrams).
+            perm_op (function): permutation invariant function, such as `tf.math.reduce_sum`, `tf.math.reduce_mean`, `tf.math.reduce_max`, `tf.math.reduce_min`, or a custom TensorFlow function that takes two arguments: a tensor and an axis on which to apply the permutation invariant operation. If perm_op is the string "topk" (where k is a number), this function will be computed as `tf.math.top_k` with parameter `int(k)`.
+            rho (function): postprocessing function that is applied after the permutation invariant operation. Can be any TensorFlow layer.
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.weight  = weight
+        self.phi     = phi
+        self.perm_op = perm_op  
+        self.rho     = rho
+
+    def build(self, input_shape):
+        return self
+
+    def call(self, diagrams):
+        """
+        Apply Perslay on a ragged tensor containing a list of persistence diagrams.
+
+        Parameters:
+            diagrams (n x None x 2): ragged tensor containing n persistence diagrams. The second dimension is ragged since persistence diagrams can have different numbers of points.
+
+        Returns:
+            vector (n x output_shape): tensor containing the vectorizations of the persistence diagrams.
+        """
+        vector, dim = self.phi(diagrams)
+        weight = self.weight(diagrams)
+        for _ in range(len(dim)):
+            weight = tf.expand_dims(weight, -1)
+        vector = tf.math.multiply(vector, weight)
+          
+        permop = self.perm_op
+        if type(permop) == str and permop[:3] == 'top':
+            k = int(permop[3:])
+            vector = vector.to_tensor(default_value=-1e10)
+            vector = tf.math.top_k(tf.transpose(vector, perm=[0, 2, 1]), k=k).values
+            vector = tf.reshape(vector, [-1,k*dim[0]])
+        else:
+            vector = permop(vector, axis=1)
+
+        vector = self.rho(vector)
+            
+        return vector
```

## gudhi/tensorflow/rips_layer.py

 * *Ordering differences only*

```diff
@@ -1,93 +1,93 @@
-import numpy               as np
-import tensorflow          as tf
-from ..rips_complex     import RipsComplex
-
-############################
-# Vietoris-Rips filtration #
-############################
-
-# The parameters of the model are the point coordinates.
-
-def _Rips(DX, max_edge, dimensions, homology_coeff_field):
-    # Parameters: DX (distance matrix), 
-    #             max_edge (maximum edge length for Rips filtration), 
-    #             dimensions (homology dimensions)
-
-    # Compute the persistence pairs with Gudhi
-    rc = RipsComplex(distance_matrix=DX, max_edge_length=max_edge)
-    st = rc.create_simplex_tree(max_dimension=max(dimensions)+1)
-    st.compute_persistence(homology_coeff_field=homology_coeff_field)
-    pairs = st.flag_persistence_generators()
-
-    L_indices = []
-    for dimension in dimensions:
-
-        if dimension == 0:
-            finite_pairs = pairs[0]
-            essential_pairs = pairs[2]
-        else:
-            finite_pairs = pairs[1][dimension-1] if len(pairs[1]) >= dimension else np.empty(shape=[0,4])
-            essential_pairs = pairs[3][dimension-1] if len(pairs[3]) >= dimension else np.empty(shape=[0,2])
-        
-        finite_indices = np.array(finite_pairs.flatten(), dtype=np.int32)
-        essential_indices = np.array(essential_pairs.flatten(), dtype=np.int32)
-
-        L_indices.append((finite_indices, essential_indices))
-
-    return L_indices
-
-class RipsLayer(tf.keras.layers.Layer):
-    """
-    TensorFlow layer for computing Rips persistence out of a point cloud
-    """
-    def __init__(self, homology_dimensions, maximum_edge_length=np.inf, min_persistence=None, homology_coeff_field=11, **kwargs):
-        """
-        Constructor for the RipsLayer class
-
-        Parameters:
-            maximum_edge_length (float): maximum edge length for the Rips complex 
-            homology_dimensions (List[int]): list of homology dimensions
-            min_persistence (List[float]): minimum distance-to-diagonal of the points in the output persistence diagrams (default None, in which case 0. is used for all dimensions)
-            homology_coeff_field (int): homology field coefficient. Must be a prime number. Default value is 11. Max is 46337.
-        """
-        super().__init__(dynamic=True, **kwargs)
-        self.max_edge = maximum_edge_length
-        self.dimensions = homology_dimensions
-        self.min_persistence = min_persistence if min_persistence is not None else [0. for _ in range(len(self.dimensions))]
-        self.hcf = homology_coeff_field
-        assert len(self.min_persistence) == len(self.dimensions)
-        
-    def call(self, X):
-        """
-        Compute Rips persistence diagram associated to a point cloud
-
-        Parameters:   
-            X (TensorFlow variable): point cloud of shape [number of points, number of dimensions]
-
-        Returns:
-            List[Tuple[tf.Tensor,tf.Tensor]]: List of Rips persistence diagrams. The length of this list is the same than that of dimensions, i.e., there is one persistence diagram per homology dimension provided in the input list dimensions. Moreover, the finite and essential parts of the persistence diagrams are provided separately: each element of this list is a tuple of size two that contains the finite and essential parts of the corresponding persistence diagram, of shapes [num_finite_points, 2] and [num_essential_points, 1] respectively
-        """    
-        # Compute distance matrix
-        DX = tf.norm(tf.expand_dims(X, 1)-tf.expand_dims(X, 0), axis=2)
-        # Compute vertices associated to positive and negative simplices 
-        # Don't compute gradient for this operation
-        indices = _Rips(DX.numpy(), self.max_edge, self.dimensions, self.hcf)
-        # Get persistence diagrams by simply picking the corresponding entries in the distance matrix
-        self.dgms = []
-        for idx_dim, dimension in enumerate(self.dimensions):
-            cur_idx = indices[idx_dim]
-            if dimension > 0:
-                finite_dgm = tf.reshape(tf.gather_nd(DX, tf.reshape(cur_idx[0], [-1,2])), [-1,2])
-                essential_dgm = tf.reshape(tf.gather_nd(DX, tf.reshape(cur_idx[1], [-1,2])), [-1,1])
-            else:
-                reshaped_cur_idx = tf.reshape(cur_idx[0], [-1,3])
-                finite_dgm = tf.concat([tf.zeros([reshaped_cur_idx.shape[0],1]), tf.reshape(tf.gather_nd(DX, reshaped_cur_idx[:,1:]), [-1,1])], axis=1)
-                essential_dgm = tf.zeros([cur_idx[1].shape[0],1])
-            min_pers = self.min_persistence[idx_dim]
-            if min_pers >= 0:
-                persistent_indices = tf.where(tf.math.abs(finite_dgm[:,1]-finite_dgm[:,0]) > min_pers)
-                self.dgms.append((tf.reshape(tf.gather(finite_dgm, indices=persistent_indices),[-1,2]), essential_dgm))
-            else:
-                self.dgms.append((finite_dgm, essential_dgm))
-        return self.dgms
-
+import numpy               as np
+import tensorflow          as tf
+from ..rips_complex     import RipsComplex
+
+############################
+# Vietoris-Rips filtration #
+############################
+
+# The parameters of the model are the point coordinates.
+
+def _Rips(DX, max_edge, dimensions, homology_coeff_field):
+    # Parameters: DX (distance matrix), 
+    #             max_edge (maximum edge length for Rips filtration), 
+    #             dimensions (homology dimensions)
+
+    # Compute the persistence pairs with Gudhi
+    rc = RipsComplex(distance_matrix=DX, max_edge_length=max_edge)
+    st = rc.create_simplex_tree(max_dimension=max(dimensions)+1)
+    st.compute_persistence(homology_coeff_field=homology_coeff_field)
+    pairs = st.flag_persistence_generators()
+
+    L_indices = []
+    for dimension in dimensions:
+
+        if dimension == 0:
+            finite_pairs = pairs[0]
+            essential_pairs = pairs[2]
+        else:
+            finite_pairs = pairs[1][dimension-1] if len(pairs[1]) >= dimension else np.empty(shape=[0,4])
+            essential_pairs = pairs[3][dimension-1] if len(pairs[3]) >= dimension else np.empty(shape=[0,2])
+        
+        finite_indices = np.array(finite_pairs.flatten(), dtype=np.int32)
+        essential_indices = np.array(essential_pairs.flatten(), dtype=np.int32)
+
+        L_indices.append((finite_indices, essential_indices))
+
+    return L_indices
+
+class RipsLayer(tf.keras.layers.Layer):
+    """
+    TensorFlow layer for computing Rips persistence out of a point cloud
+    """
+    def __init__(self, homology_dimensions, maximum_edge_length=np.inf, min_persistence=None, homology_coeff_field=11, **kwargs):
+        """
+        Constructor for the RipsLayer class
+
+        Parameters:
+            maximum_edge_length (float): maximum edge length for the Rips complex 
+            homology_dimensions (List[int]): list of homology dimensions
+            min_persistence (List[float]): minimum distance-to-diagonal of the points in the output persistence diagrams (default None, in which case 0. is used for all dimensions)
+            homology_coeff_field (int): homology field coefficient. Must be a prime number. Default value is 11. Max is 46337.
+        """
+        super().__init__(dynamic=True, **kwargs)
+        self.max_edge = maximum_edge_length
+        self.dimensions = homology_dimensions
+        self.min_persistence = min_persistence if min_persistence is not None else [0. for _ in range(len(self.dimensions))]
+        self.hcf = homology_coeff_field
+        assert len(self.min_persistence) == len(self.dimensions)
+        
+    def call(self, X):
+        """
+        Compute Rips persistence diagram associated to a point cloud
+
+        Parameters:   
+            X (TensorFlow variable): point cloud of shape [number of points, number of dimensions]
+
+        Returns:
+            List[Tuple[tf.Tensor,tf.Tensor]]: List of Rips persistence diagrams. The length of this list is the same than that of dimensions, i.e., there is one persistence diagram per homology dimension provided in the input list dimensions. Moreover, the finite and essential parts of the persistence diagrams are provided separately: each element of this list is a tuple of size two that contains the finite and essential parts of the corresponding persistence diagram, of shapes [num_finite_points, 2] and [num_essential_points, 1] respectively
+        """    
+        # Compute distance matrix
+        DX = tf.norm(tf.expand_dims(X, 1)-tf.expand_dims(X, 0), axis=2)
+        # Compute vertices associated to positive and negative simplices 
+        # Don't compute gradient for this operation
+        indices = _Rips(DX.numpy(), self.max_edge, self.dimensions, self.hcf)
+        # Get persistence diagrams by simply picking the corresponding entries in the distance matrix
+        self.dgms = []
+        for idx_dim, dimension in enumerate(self.dimensions):
+            cur_idx = indices[idx_dim]
+            if dimension > 0:
+                finite_dgm = tf.reshape(tf.gather_nd(DX, tf.reshape(cur_idx[0], [-1,2])), [-1,2])
+                essential_dgm = tf.reshape(tf.gather_nd(DX, tf.reshape(cur_idx[1], [-1,2])), [-1,1])
+            else:
+                reshaped_cur_idx = tf.reshape(cur_idx[0], [-1,3])
+                finite_dgm = tf.concat([tf.zeros([reshaped_cur_idx.shape[0],1]), tf.reshape(tf.gather_nd(DX, reshaped_cur_idx[:,1:]), [-1,1])], axis=1)
+                essential_dgm = tf.zeros([cur_idx[1].shape[0],1])
+            min_pers = self.min_persistence[idx_dim]
+            if min_pers >= 0:
+                persistent_indices = tf.where(tf.math.abs(finite_dgm[:,1]-finite_dgm[:,0]) > min_pers)
+                self.dgms.append((tf.reshape(tf.gather(finite_dgm, indices=persistent_indices),[-1,2]), essential_dgm))
+            else:
+                self.dgms.append((finite_dgm, essential_dgm))
+        return self.dgms
+
```

## gudhi/wasserstein/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .wasserstein import wasserstein_distance
+from .wasserstein import wasserstein_distance
```

## gudhi/wasserstein/barycenter.py

 * *Ordering differences only*

```diff
@@ -1,146 +1,146 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Theo Lacombe
-#
-# Copyright (C) 2019 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-
-import ot
-import numpy as np
-import scipy.spatial.distance as sc
-
-from gudhi.wasserstein import wasserstein_distance
-
-
-def _mean(x, m):
-    '''
-    :param x: a list of 2D-points, off diagonal, x_0... x_{k-1}
-    :param m: total amount of points taken into account, that is we have (m-k) copies of diagonal
-    :returns: the weighted mean of x with (m-k) copies of the diagonal
-    '''
-    k = len(x)
-    if k > 0:
-        w = np.mean(x, axis=0)
-        w_delta = (w[0] + w[1]) / 2 * np.ones(2)
-        return (k * w + (m-k) * w_delta) / m
-    else:
-        return np.array([0, 0])
-
-
-def lagrangian_barycenter(pdiagset, init=None, verbose=False):
-    '''
-    :param pdiagset: a list of ``numpy.array`` of shape `(n x 2)` (`n` can variate), encoding a set of persistence
-        diagrams with only finite coordinates.
-    :param init: The initial value for barycenter estimate.
-        If ``None``, init is made on a random diagram from the dataset.
-        Otherwise, it can be an ``int`` (then initialization is made on ``pdiagset[init]``)
-        or a `(n x 2)` ``numpy.array`` encoding a persistence diagram with `n` points.
-    :type init: ``int``, or (n x 2) ``np.array``
-    :param verbose: if ``True``, returns additional information about the barycenter.
-    :type verbose: boolean
-    :returns: If not verbose (default), a ``numpy.array`` encoding the barycenter estimate of pdiagset
-        (local minimum of the energy function).
-        If ``pdiagset`` is empty, returns ``None``.
-        If verbose, returns a couple ``(Y, log)`` where ``Y`` is the barycenter estimate,
-        and ``log`` is a ``dict`` that contains additional information:
-
-        - `"groupings"`, a list of list of pairs ``(i,j)``. Namely, ``G[k] = [...(i, j)...]``, where ``(i,j)`` indicates that `pdiagset[k][i]`` is matched to ``Y[j]`` if ``i = -1`` or ``j = -1``, it means they represent the diagonal.
-
-        - `"energy"`, ``float`` representing the Frechet energy value obtained. It is the mean of squared distances of observations to the output.
-
-        - `"nb_iter"`, ``int`` number of iterations performed before convergence of the algorithm.
-    '''
-    X = pdiagset  # to shorten notations, not a copy
-    m = len(X)  # number of diagrams we are averaging
-    if m == 0:
-        print("Warning: computing barycenter of empty diag set. Returns None")
-        return None
-
-    # store the number of off-diagonal point for each of the X_i
-    nb_off_diag = np.array([len(X_i) for X_i in X])
-    # Initialisation of barycenter
-    if init is None:
-        i0 = np.random.randint(m)  # Index of first state for the barycenter
-        Y = X[i0].copy()
-    else:
-        if type(init)==int:
-            Y = X[init].copy()
-        else:
-            Y = init.copy()
-
-    nb_iter = 0
-
-    converged = False  # stopping criterion
-    while not converged:
-        nb_iter += 1
-        K = len(Y)  # current nb of points in Y (some might be on diagonal)
-        G = np.full((K, m), -1, dtype=int)  # will store for each j, the (index)
-                              # point matched in each other diagram
-                              #(might be the diagonal).
-                              # that is G[j, i] = k <=> y_j is matched to
-                              # x_k in the diagram i-th diagram X[i]
-        updated_points = np.zeros((K, 2))  # will store the new positions of
-                                           # the points of Y.
-                                           # If points disappear, there thrown
-                                           # on [0,0] by default.
-        new_created_points = []  # will store potential new points.
-
-        # Step 1 : compute optimal matching (Y, X_i) for each X_i
-        #          and create new points in Y if needed
-        for i in range(m):
-            _, indices = wasserstein_distance(Y, X[i], matching=True, order=2., internal_p=2.)
-            for y_j, x_i_j in indices:
-                if y_j >= 0:  # we matched an off diagonal point to x_i_j...
-                    if x_i_j >= 0:  # ...which is also an off-diagonal point.
-                        G[y_j, i] = x_i_j
-                    else:  # ...which is a diagonal point
-                        G[y_j, i] = -1  # -1 stands for the diagonal (mask)
-                else:  # We matched a diagonal point to x_i_j...
-                    if x_i_j >= 0:  # which is a off-diag point !
-                                                # need to create new point in Y
-                        new_y = _mean(np.array([X[i][x_i_j]]), m)
-                        # Average this point with (m-1) copies of Delta
-                        new_created_points.append(new_y)
-
-        # Step 2 : Update current point position thanks to groupings computed
-        to_delete = []
-        for j in range(K):
-            matched_points = [X[i][G[j, i]] for i in range(m) if G[j, i] > -1]
-            new_y_j = _mean(matched_points, m)
-            if not np.array_equal(new_y_j, np.array([0,0])):
-                updated_points[j] = new_y_j
-            else: # this points is no longer of any use.
-                to_delete.append(j)
-        # we remove the point to be deleted now.
-        updated_points = np.delete(updated_points, to_delete, axis=0)
-
-        # we cannot converge if there have been new created points.
-        if new_created_points:
-            Y = np.concatenate((updated_points, new_created_points))
-        else:
-            # Step 3 : we check convergence
-            if np.array_equal(updated_points, Y):
-                converged = True
-            Y = updated_points
-
-
-    if verbose:
-        groupings = []
-        energy = 0
-        log = {}
-        n_y = len(Y)
-        for i in range(m):
-            cost, edges = wasserstein_distance(Y, X[i], matching=True, order=2., internal_p=2.)
-            groupings.append(edges)
-            energy += cost
-            log["groupings"] = groupings
-        energy = energy/m
-        log["energy"] = energy
-        log["nb_iter"] = nb_iter
-
-        return Y, log
-    else:
-        return Y
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Theo Lacombe
+#
+# Copyright (C) 2019 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+
+import ot
+import numpy as np
+import scipy.spatial.distance as sc
+
+from gudhi.wasserstein import wasserstein_distance
+
+
+def _mean(x, m):
+    '''
+    :param x: a list of 2D-points, off diagonal, x_0... x_{k-1}
+    :param m: total amount of points taken into account, that is we have (m-k) copies of diagonal
+    :returns: the weighted mean of x with (m-k) copies of the diagonal
+    '''
+    k = len(x)
+    if k > 0:
+        w = np.mean(x, axis=0)
+        w_delta = (w[0] + w[1]) / 2 * np.ones(2)
+        return (k * w + (m-k) * w_delta) / m
+    else:
+        return np.array([0, 0])
+
+
+def lagrangian_barycenter(pdiagset, init=None, verbose=False):
+    '''
+    :param pdiagset: a list of ``numpy.array`` of shape `(n x 2)` (`n` can variate), encoding a set of persistence
+        diagrams with only finite coordinates.
+    :param init: The initial value for barycenter estimate.
+        If ``None``, init is made on a random diagram from the dataset.
+        Otherwise, it can be an ``int`` (then initialization is made on ``pdiagset[init]``)
+        or a `(n x 2)` ``numpy.array`` encoding a persistence diagram with `n` points.
+    :type init: ``int``, or (n x 2) ``np.array``
+    :param verbose: if ``True``, returns additional information about the barycenter.
+    :type verbose: boolean
+    :returns: If not verbose (default), a ``numpy.array`` encoding the barycenter estimate of pdiagset
+        (local minimum of the energy function).
+        If ``pdiagset`` is empty, returns ``None``.
+        If verbose, returns a couple ``(Y, log)`` where ``Y`` is the barycenter estimate,
+        and ``log`` is a ``dict`` that contains additional information:
+
+        - `"groupings"`, a list of list of pairs ``(i,j)``. Namely, ``G[k] = [...(i, j)...]``, where ``(i,j)`` indicates that `pdiagset[k][i]`` is matched to ``Y[j]`` if ``i = -1`` or ``j = -1``, it means they represent the diagonal.
+
+        - `"energy"`, ``float`` representing the Frechet energy value obtained. It is the mean of squared distances of observations to the output.
+
+        - `"nb_iter"`, ``int`` number of iterations performed before convergence of the algorithm.
+    '''
+    X = pdiagset  # to shorten notations, not a copy
+    m = len(X)  # number of diagrams we are averaging
+    if m == 0:
+        print("Warning: computing barycenter of empty diag set. Returns None")
+        return None
+
+    # store the number of off-diagonal point for each of the X_i
+    nb_off_diag = np.array([len(X_i) for X_i in X])
+    # Initialisation of barycenter
+    if init is None:
+        i0 = np.random.randint(m)  # Index of first state for the barycenter
+        Y = X[i0].copy()
+    else:
+        if type(init)==int:
+            Y = X[init].copy()
+        else:
+            Y = init.copy()
+
+    nb_iter = 0
+
+    converged = False  # stopping criterion
+    while not converged:
+        nb_iter += 1
+        K = len(Y)  # current nb of points in Y (some might be on diagonal)
+        G = np.full((K, m), -1, dtype=int)  # will store for each j, the (index)
+                              # point matched in each other diagram
+                              #(might be the diagonal).
+                              # that is G[j, i] = k <=> y_j is matched to
+                              # x_k in the diagram i-th diagram X[i]
+        updated_points = np.zeros((K, 2))  # will store the new positions of
+                                           # the points of Y.
+                                           # If points disappear, there thrown
+                                           # on [0,0] by default.
+        new_created_points = []  # will store potential new points.
+
+        # Step 1 : compute optimal matching (Y, X_i) for each X_i
+        #          and create new points in Y if needed
+        for i in range(m):
+            _, indices = wasserstein_distance(Y, X[i], matching=True, order=2., internal_p=2.)
+            for y_j, x_i_j in indices:
+                if y_j >= 0:  # we matched an off diagonal point to x_i_j...
+                    if x_i_j >= 0:  # ...which is also an off-diagonal point.
+                        G[y_j, i] = x_i_j
+                    else:  # ...which is a diagonal point
+                        G[y_j, i] = -1  # -1 stands for the diagonal (mask)
+                else:  # We matched a diagonal point to x_i_j...
+                    if x_i_j >= 0:  # which is a off-diag point !
+                                                # need to create new point in Y
+                        new_y = _mean(np.array([X[i][x_i_j]]), m)
+                        # Average this point with (m-1) copies of Delta
+                        new_created_points.append(new_y)
+
+        # Step 2 : Update current point position thanks to groupings computed
+        to_delete = []
+        for j in range(K):
+            matched_points = [X[i][G[j, i]] for i in range(m) if G[j, i] > -1]
+            new_y_j = _mean(matched_points, m)
+            if not np.array_equal(new_y_j, np.array([0,0])):
+                updated_points[j] = new_y_j
+            else: # this points is no longer of any use.
+                to_delete.append(j)
+        # we remove the point to be deleted now.
+        updated_points = np.delete(updated_points, to_delete, axis=0)
+
+        # we cannot converge if there have been new created points.
+        if new_created_points:
+            Y = np.concatenate((updated_points, new_created_points))
+        else:
+            # Step 3 : we check convergence
+            if np.array_equal(updated_points, Y):
+                converged = True
+            Y = updated_points
+
+
+    if verbose:
+        groupings = []
+        energy = 0
+        log = {}
+        n_y = len(Y)
+        for i in range(m):
+            cost, edges = wasserstein_distance(Y, X[i], matching=True, order=2., internal_p=2.)
+            groupings.append(edges)
+            energy += cost
+            log["groupings"] = groupings
+        energy = energy/m
+        log["energy"] = energy
+        log["nb_iter"] = nb_iter
+
+        return Y, log
+    else:
+        return Y
```

## gudhi/wasserstein/wasserstein.py

 * *Ordering differences only*

```diff
@@ -1,355 +1,355 @@
-# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
-# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
-# Author(s):       Theo Lacombe
-#
-# Copyright (C) 2019 Inria
-#
-# Modification(s):
-#   - YYYY/MM Author: Description of the modification
-
-import numpy as np
-import scipy.spatial.distance as sc
-import warnings
-
-try:
-    import ot
-except ImportError:
-    print("POT (Python Optimal Transport) package is not installed. Try to run $ conda install -c conda-forge pot ; or $ pip install POT")
-
-
-# Currently unused, but Th√©o says it is likely to be used again.
-def _proj_on_diag(X):
-    '''
-    :param X: (n x 2) array encoding the points of a persistent diagram.
-    :returns: (n x 2) array encoding the (respective orthogonal) projections of the points onto the diagonal
-    '''
-    Z = (X[:,0] + X[:,1]) / 2.
-    return np.array([Z , Z]).T
-
-
-def _dist_to_diag(X, internal_p):
-    '''
-    :param X: (n x 2) array encoding the points of a persistent diagram.
-    :param internal_p: Ground metric (i.e. norm L^p).
-    :returns: (n) array encoding the (respective orthogonal) distances of the points to the diagonal
-
-    .. note::
-        Assumes that the points are above the diagonal.
-    '''
-    return (X[:, 1] - X[:, 0]) * 2 ** (1.0 / internal_p - 1)
-
-
-def _build_dist_matrix(X, Y, order, internal_p):
-    '''
-    :param X: (n x 2) numpy.array encoding the (points of the) first diagram.
-    :param Y: (m x 2) numpy.array encoding the second diagram.
-    :param order: exponent for the Wasserstein metric.
-    :param internal_p: Ground metric (i.e. norm L^p).
-    :returns: (n+1) x (m+1) np.array encoding the cost matrix C.
-                For 0 <= i < n, 0 <= j < m, C[i,j] encodes the distance between X[i] and Y[j],
-                while C[i, m] (resp. C[n, j]) encodes the distance (to the p) between X[i] (resp Y[j])
-                and its orthogonal projection onto the diagonal.
-                note also that C[n, m] = 0  (it costs nothing to move from the diagonal to the diagonal).
-    '''
-    Cxd = _dist_to_diag(X, internal_p)**order
-    Cdy = _dist_to_diag(Y, internal_p)**order
-    if np.isinf(internal_p):
-        C = sc.cdist(X,Y, metric='chebyshev')**order
-    else:
-        C = sc.cdist(X,Y, metric='minkowski', p=internal_p)**order
-    Cf = np.hstack((C, Cxd[:,None]))
-    Cdy = np.append(Cdy, 0)
-
-    Cf = np.vstack((Cf, Cdy[None,:]))
-
-    return Cf
-
-
-def _perstot_autodiff(X, order, internal_p):
-    '''
-    Version of _perstot that works on eagerpy tensors.
-    '''
-    return _dist_to_diag(X, internal_p).norms.lp(order)
-
-
-def _perstot(X, order, internal_p, enable_autodiff):
-    '''
-    :param X: (n x 2) numpy.array (points of a given diagram).
-    :param order: exponent for Wasserstein.
-    :param internal_p: Ground metric on the (upper-half) plane (i.e. norm L^p in R^2).
-    :param enable_autodiff: If X is torch.tensor, tensorflow.Tensor or jax.numpy.ndarray, make the computation
-        transparent to automatic differentiation.
-    :type enable_autodiff: bool
-    :returns: float, the total persistence of the diagram (that is, its distance to the empty diagram).
-
-    .. note::
-        Can be +inf if the diagram has an essential part (points with infinite coordinates).
-    '''
-    if enable_autodiff:
-        import eagerpy as ep
-
-        return _perstot_autodiff(ep.astensor(X), order, internal_p).raw
-    else:
-        return np.linalg.norm(_dist_to_diag(X, internal_p), ord=order)
-
-
-def _get_essential_parts(a):
-    '''
-    :param a: (n x 2) numpy.array (point of a diagram)
-    :returns: five lists of indices (between 0 and len(a)) accounting for the five types of points with infinite
-    coordinates that can occur in a diagram, namely:
-        type0 : (-inf, finite)
-        type1 : (finite, +inf)
-        type2 : (-inf, +inf)
-        type3 : (-inf, -inf)
-        type4 : (+inf, +inf)
-    .. note::
-        For instance, a[_get_essential_parts(a)[0]] returns the points in a of coordinates (-inf, x) for some finite x.
-        Note also that points with (+inf, -inf) are not handled (points (x,y) in dgm satisfy by assumption (y >= x)).
-
-        Finally, we consider that points with coordinates (-inf,-inf) and (+inf, +inf) belong to the diagonal.
-    '''
-    if len(a):
-        first_coord_finite = np.isfinite(a[:,0])
-        second_coord_finite = np.isfinite(a[:,1])
-        first_coord_infinite_positive = (a[:,0] == np.inf)
-        second_coord_infinite_positive = (a[:,1] == np.inf)
-        first_coord_infinite_negative = (a[:,0] == -np.inf)
-        second_coord_infinite_negative = (a[:,1] == -np.inf)
-
-        ess_first_type  = np.where(second_coord_finite & first_coord_infinite_negative)[0] # coord (-inf, x)
-        ess_second_type = np.where(first_coord_finite & second_coord_infinite_positive)[0]  # coord (x, +inf)
-        ess_third_type  = np.where(first_coord_infinite_negative & second_coord_infinite_positive)[0]  # coord (-inf, +inf)
-
-        ess_fourth_type = np.where(first_coord_infinite_negative & second_coord_infinite_negative)[0] # coord (-inf, -inf)
-        ess_fifth_type  = np.where(first_coord_infinite_positive  & second_coord_infinite_positive)[0]  # coord (+inf, +inf)
-        return ess_first_type, ess_second_type, ess_third_type, ess_fourth_type, ess_fifth_type
-    else:
-        return [], [], [], [], []
-
-
-def _cost_and_match_essential_parts(X, Y, idX, idY, order, axis):
-    '''
-    :param X: (n x 2) numpy.array (dgm points)
-    :param Y: (n x 2) numpy.array (dgm points)
-    :param idX: indices to consider for this one dimensional OT problem (in X)
-    :param idY: indices to consider for this one dimensional OT problem (in Y)
-    :param order: exponent for Wasserstein distance computation
-    :param axis: must be 0 or 1, correspond to the coordinate which is finite.
-    :returns: cost (float) and match for points with *one* infinite coordinate.
-
-    .. note::
-        Assume idX, idY come when calling _handle_essential_parts, thus have same length.
-    '''
-    u = X[idX, axis]
-    v = Y[idY, axis]
-
-    cost = np.sum(np.abs(np.sort(u) - np.sort(v))**(order))  # OT cost in 1D
-
-    sortidX = idX[np.argsort(u)]
-    sortidY = idY[np.argsort(v)]
-    # We return [i,j] sorted per value
-    match = list(zip(sortidX, sortidY))
-
-    return cost, match
-
-
-def _handle_essential_parts(X, Y, order):
-    '''
-    :param X: (n x 2) numpy array, first diagram.
-    :param Y: (n x 2) numpy array, second diagram.
-    :order: Wasserstein order for cost computation.
-    :returns: cost and matching due to essential parts. If cost is +inf, matching will be set to None.
-    '''
-    ess_parts_X = _get_essential_parts(X)
-    ess_parts_Y = _get_essential_parts(Y)
-
-    # Treats the case of infinite cost (cardinalities of essential parts differ).
-    for u, v in list(zip(ess_parts_X, ess_parts_Y))[:3]: # ignore types 4 and 5 as they belong to the diagonal
-        if len(u) != len(v):
-            return np.inf, None
-
-    # Now we know each essential part has the same number of points in both diagrams.
-    # Handle type 0 and type 1 essential parts (those with one finite coordinates)
-    c1, m1 = _cost_and_match_essential_parts(X, Y, ess_parts_X[0], ess_parts_Y[0], axis=1, order=order)
-    c2, m2 = _cost_and_match_essential_parts(X, Y, ess_parts_X[1], ess_parts_Y[1], axis=0, order=order)
-
-    c = c1 + c2
-    m = m1 + m2
-
-    # Handle type3 (coordinates (-inf,+inf), so we just align points)
-    m += list(zip(ess_parts_X[2], ess_parts_Y[2]))
-
-    # Handle type 4 and 5, considered as belonging to the diagonal so matched to (-1) with cost 0.
-    for z in ess_parts_X[3:]:
-        m += [(u, -1) for u in z] # points in X are matched to -1
-    for z in ess_parts_Y[3:]:
-        m += [(-1, v) for v in z] # -1 is match to points in Y
-
-    return c, np.array(m)
-
-
-def _finite_part(X):
-    '''
-    :param X: (n x 2) numpy array encoding a persistence diagram.
-    :returns: The finite part of a diagram `X` (points with finite coordinates).
-    '''
-    return X[np.where(np.isfinite(X[:,0]) & np.isfinite(X[:,1]))]
-
-
-def _warn_infty(matching):
-    '''
-    Handle essential parts with different cardinalities. Warn the user about cost being infinite and (if
-    `matching=True`) about the returned matching being `None`.
-    '''
-    if matching:
-        warnings.warn('Cardinality of essential parts differs. Distance (cost) is +inf, and the returned matching is None.')
-        return np.inf, None
-    else:
-        warnings.warn('Cardinality of essential parts differs. Distance (cost) is +inf.')
-        return np.inf
-
-
-def wasserstein_distance(X, Y, matching=False, order=1., internal_p=np.inf, enable_autodiff=False,
-                         keep_essential_parts=True):
-    '''
-    Compute the Wasserstein distance between persistence diagram using Python Optimal Transport backend.
-    Diagrams can contain points with infinity coordinates (essential parts).
-    Points with (-inf,-inf) and (+inf,+inf) coordinates are considered as belonging to the diagonal.
-    If the distance between two diagrams is +inf (which happens if the cardinalities of essential
-    parts differ) and optimal matching is required, it will be set to ``None``.
-
-    :param X: The first diagram.
-    :type X: n x 2 numpy.array
-    :param Y:  The second diagram.
-    :type Y: m x 2 numpy.array
-    :param matching: if ``True``, computes and returns the optimal matching between X and Y, encoded as
-        a (n x 2) np.array  [...[i,j]...], meaning the i-th point in X is matched to
-        the j-th point in Y, with the convention that (-1) represents the diagonal.
-    :param order: Wasserstein exponent q (1 <= q < infinity).
-    :type order: float
-    :param internal_p: Ground metric on the (upper-half) plane (i.e. norm L^p in R^2).
-    :type internal_p: float
-    :param enable_autodiff: If X and Y are ``torch.tensor`` or ``tensorflow.Tensor``, make the computation
-        transparent to automatic differentiation. This requires the package EagerPy and is currently incompatible
-        with ``matching=True`` and with ``keep_essential_parts=True``.
-
-        .. note:: This considers the function defined on the coordinates of the off-diagonal finite points of X and Y
-            and lets the various frameworks compute its gradient. It never pulls new points from the diagonal.
-    :type enable_autodiff: bool
-    :param keep_essential_parts: If ``False``, only considers the finite points in the diagrams.
-                                 Otherwise, include essential parts in cost and matching computation.
-    :type keep_essential_parts: bool
-    :returns: The Wasserstein distance of order q (1 <= q < infinity) between persistence diagrams with
-              respect to the internal_p-norm as ground metric.
-              If matching is set to True, also returns the optimal matching between X and Y.
-              If cost is +inf, any matching is optimal and thus it returns `None` instead.
-    '''
-
-    # First step: handle empty diagrams
-    n = len(X)
-    m = len(Y)
-
-    if n == 0:
-        if m == 0:
-            if not matching:
-                # What if enable_autodiff?
-                return 0.
-            else:
-                return 0., np.array([])
-        else:
-            cost = _perstot(Y, order, internal_p, enable_autodiff)
-            if cost == np.inf:
-                return _warn_infty(matching)
-            else:
-                if not matching:
-                    return cost
-                else:
-                    return cost, np.array([[-1, j] for j in range(m)])
-    elif m == 0:
-        cost = _perstot(X, order, internal_p, enable_autodiff)
-        if cost == np.inf:
-            return _warn_infty(matching)
-        else:
-            if not matching:
-                return cost
-            else:
-                return cost, np.array([[i, -1] for i in range(n)])
-
-
-    # Check essential part and enable autodiff together
-    if enable_autodiff and keep_essential_parts:
-        warnings.warn('''enable_autodiff=True and keep_essential_parts=True are incompatible together.
-                      keep_essential_parts is set to False: only points with finite coordinates are considered
-                      in the following.
-                      ''')
-        keep_essential_parts = False
-
-    # Second step: handle essential parts if needed.
-    if keep_essential_parts:
-        essential_cost, essential_matching = _handle_essential_parts(X, Y, order=order)
-        if (essential_cost == np.inf):
-            return _warn_infty(matching)  # Tells the user that cost is infty and matching (if True) is None.
-            # avoid computing transport cost between the finite parts if essential parts
-            # cardinalities do not match (saves time)
-    else:
-        essential_cost = 0
-        essential_matching = None
-
-    # Now the standard pipeline for finite parts
-    if enable_autodiff:
-        import eagerpy as ep
-
-        X_orig = ep.astensor(X)
-        Y_orig = ep.astensor(Y)
-        X = X_orig.numpy()
-        Y = Y_orig.numpy()
-
-    # Extract finite points of the diagrams. 
-    X, Y = _finite_part(X), _finite_part(Y)
-    n = len(X)
-    m = len(Y)
-
-    M = _build_dist_matrix(X, Y, order=order, internal_p=internal_p)
-    a = np.ones(n+1) # weight vector of the input diagram. Uniform here.
-    a[-1] = m
-    b = np.ones(m+1) # weight vector of the input diagram. Uniform here.
-    b[-1] = n
-
-    if matching:
-        assert not enable_autodiff, "matching and enable_autodiff are currently incompatible"
-        P = ot.emd(a=a,b=b,M=M, numItermax=2000000)
-        ot_cost = np.sum(np.multiply(P,M))
-        P[-1, -1] = 0  # Remove matching corresponding to the diagonal
-        match = np.argwhere(P)
-        # Now we turn to -1 points encoding the diagonal
-        match[:,0][match[:,0] >= n] = -1
-        match[:,1][match[:,1] >= m] = -1
-        # Finally incorporate the essential part matching
-        if essential_matching is not None:
-            match = np.concatenate([match, essential_matching]) if essential_matching.size else match
-        return (ot_cost + essential_cost) ** (1./order) , match
-
-    if enable_autodiff:
-        P = ot.emd(a=a, b=b, M=M, numItermax=2000000)
-        pairs_X_Y = np.argwhere(P[:-1, :-1])
-        pairs_X_diag = np.nonzero(P[:-1, -1])
-        pairs_Y_diag = np.nonzero(P[-1, :-1])
-        dists = []
-        # empty arrays are not handled properly by the helpers, so we avoid calling them
-        if len(pairs_X_Y):
-            dists.append((Y_orig[pairs_X_Y[:, 1]] - X_orig[pairs_X_Y[:, 0]]).norms.lp(internal_p, axis=-1).norms.lp(order))
-        if len(pairs_X_diag[0]):
-            dists.append(_perstot_autodiff(X_orig[pairs_X_diag], order, internal_p))
-        if len(pairs_Y_diag[0]):
-            dists.append(_perstot_autodiff(Y_orig[pairs_Y_diag], order, internal_p))
-        dists = [dist.reshape(1) for dist in dists]
-        return ep.concatenate(dists).norms.lp(order).raw
-        # We can also concatenate the 3 vectors to compute just one norm.
-
-    # Comptuation of the ot cost using the ot.emd2 library.
-    # Note: it is the Wasserstein distance to the power q.
-    # The default numItermax=100000 is not sufficient for some examples with 5000 points, what is a good value?
-    ot_cost = ot.emd2(a, b, M, numItermax=2000000)
-
-    return (ot_cost + essential_cost) ** (1./order)
+# This file is part of the Gudhi Library - https://gudhi.inria.fr/ - which is released under MIT.
+# See file LICENSE or go to https://gudhi.inria.fr/licensing/ for full license details.
+# Author(s):       Theo Lacombe
+#
+# Copyright (C) 2019 Inria
+#
+# Modification(s):
+#   - YYYY/MM Author: Description of the modification
+
+import numpy as np
+import scipy.spatial.distance as sc
+import warnings
+
+try:
+    import ot
+except ImportError:
+    print("POT (Python Optimal Transport) package is not installed. Try to run $ conda install -c conda-forge pot ; or $ pip install POT")
+
+
+# Currently unused, but Th√©o says it is likely to be used again.
+def _proj_on_diag(X):
+    '''
+    :param X: (n x 2) array encoding the points of a persistent diagram.
+    :returns: (n x 2) array encoding the (respective orthogonal) projections of the points onto the diagonal
+    '''
+    Z = (X[:,0] + X[:,1]) / 2.
+    return np.array([Z , Z]).T
+
+
+def _dist_to_diag(X, internal_p):
+    '''
+    :param X: (n x 2) array encoding the points of a persistent diagram.
+    :param internal_p: Ground metric (i.e. norm L^p).
+    :returns: (n) array encoding the (respective orthogonal) distances of the points to the diagonal
+
+    .. note::
+        Assumes that the points are above the diagonal.
+    '''
+    return (X[:, 1] - X[:, 0]) * 2 ** (1.0 / internal_p - 1)
+
+
+def _build_dist_matrix(X, Y, order, internal_p):
+    '''
+    :param X: (n x 2) numpy.array encoding the (points of the) first diagram.
+    :param Y: (m x 2) numpy.array encoding the second diagram.
+    :param order: exponent for the Wasserstein metric.
+    :param internal_p: Ground metric (i.e. norm L^p).
+    :returns: (n+1) x (m+1) np.array encoding the cost matrix C.
+                For 0 <= i < n, 0 <= j < m, C[i,j] encodes the distance between X[i] and Y[j],
+                while C[i, m] (resp. C[n, j]) encodes the distance (to the p) between X[i] (resp Y[j])
+                and its orthogonal projection onto the diagonal.
+                note also that C[n, m] = 0  (it costs nothing to move from the diagonal to the diagonal).
+    '''
+    Cxd = _dist_to_diag(X, internal_p)**order
+    Cdy = _dist_to_diag(Y, internal_p)**order
+    if np.isinf(internal_p):
+        C = sc.cdist(X,Y, metric='chebyshev')**order
+    else:
+        C = sc.cdist(X,Y, metric='minkowski', p=internal_p)**order
+    Cf = np.hstack((C, Cxd[:,None]))
+    Cdy = np.append(Cdy, 0)
+
+    Cf = np.vstack((Cf, Cdy[None,:]))
+
+    return Cf
+
+
+def _perstot_autodiff(X, order, internal_p):
+    '''
+    Version of _perstot that works on eagerpy tensors.
+    '''
+    return _dist_to_diag(X, internal_p).norms.lp(order)
+
+
+def _perstot(X, order, internal_p, enable_autodiff):
+    '''
+    :param X: (n x 2) numpy.array (points of a given diagram).
+    :param order: exponent for Wasserstein.
+    :param internal_p: Ground metric on the (upper-half) plane (i.e. norm L^p in R^2).
+    :param enable_autodiff: If X is torch.tensor, tensorflow.Tensor or jax.numpy.ndarray, make the computation
+        transparent to automatic differentiation.
+    :type enable_autodiff: bool
+    :returns: float, the total persistence of the diagram (that is, its distance to the empty diagram).
+
+    .. note::
+        Can be +inf if the diagram has an essential part (points with infinite coordinates).
+    '''
+    if enable_autodiff:
+        import eagerpy as ep
+
+        return _perstot_autodiff(ep.astensor(X), order, internal_p).raw
+    else:
+        return np.linalg.norm(_dist_to_diag(X, internal_p), ord=order)
+
+
+def _get_essential_parts(a):
+    '''
+    :param a: (n x 2) numpy.array (point of a diagram)
+    :returns: five lists of indices (between 0 and len(a)) accounting for the five types of points with infinite
+    coordinates that can occur in a diagram, namely:
+        type0 : (-inf, finite)
+        type1 : (finite, +inf)
+        type2 : (-inf, +inf)
+        type3 : (-inf, -inf)
+        type4 : (+inf, +inf)
+    .. note::
+        For instance, a[_get_essential_parts(a)[0]] returns the points in a of coordinates (-inf, x) for some finite x.
+        Note also that points with (+inf, -inf) are not handled (points (x,y) in dgm satisfy by assumption (y >= x)).
+
+        Finally, we consider that points with coordinates (-inf,-inf) and (+inf, +inf) belong to the diagonal.
+    '''
+    if len(a):
+        first_coord_finite = np.isfinite(a[:,0])
+        second_coord_finite = np.isfinite(a[:,1])
+        first_coord_infinite_positive = (a[:,0] == np.inf)
+        second_coord_infinite_positive = (a[:,1] == np.inf)
+        first_coord_infinite_negative = (a[:,0] == -np.inf)
+        second_coord_infinite_negative = (a[:,1] == -np.inf)
+
+        ess_first_type  = np.where(second_coord_finite & first_coord_infinite_negative)[0] # coord (-inf, x)
+        ess_second_type = np.where(first_coord_finite & second_coord_infinite_positive)[0]  # coord (x, +inf)
+        ess_third_type  = np.where(first_coord_infinite_negative & second_coord_infinite_positive)[0]  # coord (-inf, +inf)
+
+        ess_fourth_type = np.where(first_coord_infinite_negative & second_coord_infinite_negative)[0] # coord (-inf, -inf)
+        ess_fifth_type  = np.where(first_coord_infinite_positive  & second_coord_infinite_positive)[0]  # coord (+inf, +inf)
+        return ess_first_type, ess_second_type, ess_third_type, ess_fourth_type, ess_fifth_type
+    else:
+        return [], [], [], [], []
+
+
+def _cost_and_match_essential_parts(X, Y, idX, idY, order, axis):
+    '''
+    :param X: (n x 2) numpy.array (dgm points)
+    :param Y: (n x 2) numpy.array (dgm points)
+    :param idX: indices to consider for this one dimensional OT problem (in X)
+    :param idY: indices to consider for this one dimensional OT problem (in Y)
+    :param order: exponent for Wasserstein distance computation
+    :param axis: must be 0 or 1, correspond to the coordinate which is finite.
+    :returns: cost (float) and match for points with *one* infinite coordinate.
+
+    .. note::
+        Assume idX, idY come when calling _handle_essential_parts, thus have same length.
+    '''
+    u = X[idX, axis]
+    v = Y[idY, axis]
+
+    cost = np.sum(np.abs(np.sort(u) - np.sort(v))**(order))  # OT cost in 1D
+
+    sortidX = idX[np.argsort(u)]
+    sortidY = idY[np.argsort(v)]
+    # We return [i,j] sorted per value
+    match = list(zip(sortidX, sortidY))
+
+    return cost, match
+
+
+def _handle_essential_parts(X, Y, order):
+    '''
+    :param X: (n x 2) numpy array, first diagram.
+    :param Y: (n x 2) numpy array, second diagram.
+    :order: Wasserstein order for cost computation.
+    :returns: cost and matching due to essential parts. If cost is +inf, matching will be set to None.
+    '''
+    ess_parts_X = _get_essential_parts(X)
+    ess_parts_Y = _get_essential_parts(Y)
+
+    # Treats the case of infinite cost (cardinalities of essential parts differ).
+    for u, v in list(zip(ess_parts_X, ess_parts_Y))[:3]: # ignore types 4 and 5 as they belong to the diagonal
+        if len(u) != len(v):
+            return np.inf, None
+
+    # Now we know each essential part has the same number of points in both diagrams.
+    # Handle type 0 and type 1 essential parts (those with one finite coordinates)
+    c1, m1 = _cost_and_match_essential_parts(X, Y, ess_parts_X[0], ess_parts_Y[0], axis=1, order=order)
+    c2, m2 = _cost_and_match_essential_parts(X, Y, ess_parts_X[1], ess_parts_Y[1], axis=0, order=order)
+
+    c = c1 + c2
+    m = m1 + m2
+
+    # Handle type3 (coordinates (-inf,+inf), so we just align points)
+    m += list(zip(ess_parts_X[2], ess_parts_Y[2]))
+
+    # Handle type 4 and 5, considered as belonging to the diagonal so matched to (-1) with cost 0.
+    for z in ess_parts_X[3:]:
+        m += [(u, -1) for u in z] # points in X are matched to -1
+    for z in ess_parts_Y[3:]:
+        m += [(-1, v) for v in z] # -1 is match to points in Y
+
+    return c, np.array(m)
+
+
+def _finite_part(X):
+    '''
+    :param X: (n x 2) numpy array encoding a persistence diagram.
+    :returns: The finite part of a diagram `X` (points with finite coordinates).
+    '''
+    return X[np.where(np.isfinite(X[:,0]) & np.isfinite(X[:,1]))]
+
+
+def _warn_infty(matching):
+    '''
+    Handle essential parts with different cardinalities. Warn the user about cost being infinite and (if
+    `matching=True`) about the returned matching being `None`.
+    '''
+    if matching:
+        warnings.warn('Cardinality of essential parts differs. Distance (cost) is +inf, and the returned matching is None.')
+        return np.inf, None
+    else:
+        warnings.warn('Cardinality of essential parts differs. Distance (cost) is +inf.')
+        return np.inf
+
+
+def wasserstein_distance(X, Y, matching=False, order=1., internal_p=np.inf, enable_autodiff=False,
+                         keep_essential_parts=True):
+    '''
+    Compute the Wasserstein distance between persistence diagram using Python Optimal Transport backend.
+    Diagrams can contain points with infinity coordinates (essential parts).
+    Points with (-inf,-inf) and (+inf,+inf) coordinates are considered as belonging to the diagonal.
+    If the distance between two diagrams is +inf (which happens if the cardinalities of essential
+    parts differ) and optimal matching is required, it will be set to ``None``.
+
+    :param X: The first diagram.
+    :type X: n x 2 numpy.array
+    :param Y:  The second diagram.
+    :type Y: m x 2 numpy.array
+    :param matching: if ``True``, computes and returns the optimal matching between X and Y, encoded as
+        a (n x 2) np.array  [...[i,j]...], meaning the i-th point in X is matched to
+        the j-th point in Y, with the convention that (-1) represents the diagonal.
+    :param order: Wasserstein exponent q (1 <= q < infinity).
+    :type order: float
+    :param internal_p: Ground metric on the (upper-half) plane (i.e. norm L^p in R^2).
+    :type internal_p: float
+    :param enable_autodiff: If X and Y are ``torch.tensor`` or ``tensorflow.Tensor``, make the computation
+        transparent to automatic differentiation. This requires the package EagerPy and is currently incompatible
+        with ``matching=True`` and with ``keep_essential_parts=True``.
+
+        .. note:: This considers the function defined on the coordinates of the off-diagonal finite points of X and Y
+            and lets the various frameworks compute its gradient. It never pulls new points from the diagonal.
+    :type enable_autodiff: bool
+    :param keep_essential_parts: If ``False``, only considers the finite points in the diagrams.
+                                 Otherwise, include essential parts in cost and matching computation.
+    :type keep_essential_parts: bool
+    :returns: The Wasserstein distance of order q (1 <= q < infinity) between persistence diagrams with
+              respect to the internal_p-norm as ground metric.
+              If matching is set to True, also returns the optimal matching between X and Y.
+              If cost is +inf, any matching is optimal and thus it returns `None` instead.
+    '''
+
+    # First step: handle empty diagrams
+    n = len(X)
+    m = len(Y)
+
+    if n == 0:
+        if m == 0:
+            if not matching:
+                # What if enable_autodiff?
+                return 0.
+            else:
+                return 0., np.array([])
+        else:
+            cost = _perstot(Y, order, internal_p, enable_autodiff)
+            if cost == np.inf:
+                return _warn_infty(matching)
+            else:
+                if not matching:
+                    return cost
+                else:
+                    return cost, np.array([[-1, j] for j in range(m)])
+    elif m == 0:
+        cost = _perstot(X, order, internal_p, enable_autodiff)
+        if cost == np.inf:
+            return _warn_infty(matching)
+        else:
+            if not matching:
+                return cost
+            else:
+                return cost, np.array([[i, -1] for i in range(n)])
+
+
+    # Check essential part and enable autodiff together
+    if enable_autodiff and keep_essential_parts:
+        warnings.warn('''enable_autodiff=True and keep_essential_parts=True are incompatible together.
+                      keep_essential_parts is set to False: only points with finite coordinates are considered
+                      in the following.
+                      ''')
+        keep_essential_parts = False
+
+    # Second step: handle essential parts if needed.
+    if keep_essential_parts:
+        essential_cost, essential_matching = _handle_essential_parts(X, Y, order=order)
+        if (essential_cost == np.inf):
+            return _warn_infty(matching)  # Tells the user that cost is infty and matching (if True) is None.
+            # avoid computing transport cost between the finite parts if essential parts
+            # cardinalities do not match (saves time)
+    else:
+        essential_cost = 0
+        essential_matching = None
+
+    # Now the standard pipeline for finite parts
+    if enable_autodiff:
+        import eagerpy as ep
+
+        X_orig = ep.astensor(X)
+        Y_orig = ep.astensor(Y)
+        X = X_orig.numpy()
+        Y = Y_orig.numpy()
+
+    # Extract finite points of the diagrams. 
+    X, Y = _finite_part(X), _finite_part(Y)
+    n = len(X)
+    m = len(Y)
+
+    M = _build_dist_matrix(X, Y, order=order, internal_p=internal_p)
+    a = np.ones(n+1) # weight vector of the input diagram. Uniform here.
+    a[-1] = m
+    b = np.ones(m+1) # weight vector of the input diagram. Uniform here.
+    b[-1] = n
+
+    if matching:
+        assert not enable_autodiff, "matching and enable_autodiff are currently incompatible"
+        P = ot.emd(a=a,b=b,M=M, numItermax=2000000)
+        ot_cost = np.sum(np.multiply(P,M))
+        P[-1, -1] = 0  # Remove matching corresponding to the diagonal
+        match = np.argwhere(P)
+        # Now we turn to -1 points encoding the diagonal
+        match[:,0][match[:,0] >= n] = -1
+        match[:,1][match[:,1] >= m] = -1
+        # Finally incorporate the essential part matching
+        if essential_matching is not None:
+            match = np.concatenate([match, essential_matching]) if essential_matching.size else match
+        return (ot_cost + essential_cost) ** (1./order) , match
+
+    if enable_autodiff:
+        P = ot.emd(a=a, b=b, M=M, numItermax=2000000)
+        pairs_X_Y = np.argwhere(P[:-1, :-1])
+        pairs_X_diag = np.nonzero(P[:-1, -1])
+        pairs_Y_diag = np.nonzero(P[-1, :-1])
+        dists = []
+        # empty arrays are not handled properly by the helpers, so we avoid calling them
+        if len(pairs_X_Y):
+            dists.append((Y_orig[pairs_X_Y[:, 1]] - X_orig[pairs_X_Y[:, 0]]).norms.lp(internal_p, axis=-1).norms.lp(order))
+        if len(pairs_X_diag[0]):
+            dists.append(_perstot_autodiff(X_orig[pairs_X_diag], order, internal_p))
+        if len(pairs_Y_diag[0]):
+            dists.append(_perstot_autodiff(Y_orig[pairs_Y_diag], order, internal_p))
+        dists = [dist.reshape(1) for dist in dists]
+        return ep.concatenate(dists).norms.lp(order).raw
+        # We can also concatenate the 3 vectors to compute just one norm.
+
+    # Comptuation of the ot cost using the ot.emd2 library.
+    # Note: it is the Wasserstein distance to the power q.
+    # The default numItermax=100000 is not sufficient for some examples with 5000 points, what is a good value?
+    ot_cost = ot.emd2(a, b, M, numItermax=2000000)
+
+    return (ot_cost + essential_cost) ** (1./order)
```

## Comparing `gudhi-3.8.0rc2.dist-info/METADATA` & `gudhi-3.8.0rc3.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,18 +1,41 @@
-Metadata-Version: 2.1
-Name: gudhi
-Version: 3.8.0rc2
-Summary: The Gudhi library is an open source library for Computational Topology and Topological Data Analysis (TDA).
-Home-page: https://gudhi.inria.fr/
-Author: GUDHI Editorial Board <https://gudhi.inria.fr/contact/>
-License: UNKNOWN
-Project-URL: Bug Tracker, https://github.com/GUDHI/gudhi-devel/issues
-Project-URL: Documentation, https://gudhi.inria.fr/python/latest/
-Project-URL: Source Code, https://github.com/GUDHI/gudhi-devel
-Project-URL: License, https://gudhi.inria.fr/licensing/
-Platform: UNKNOWN
-Requires-Python: >=3.5.0
-Description-Content-Type: text/x-rst
-Requires-Dist: numpy (>=1.15.0)
-
-The Gudhi library is an open source library for Computational Topology andTopological Data Analysis (TDA). It offers state-of-the-art algorithmsto construct various types of simplicial complexes, data structures torepresent them, and algorithms to compute geometric approximations of shapesand persistent homology.The GUDHI library offers the following interoperable modules:* Complexes:   * Cubical   * Simplicial: Rips, Witness, Alpha and ƒåech complexes   * Cover: Nerve and Graph induced complexes* Data structures and basic operations:   * Simplex tree, Skeleton blockers and Toplex map   * Construction, update, filtration and simplification* Topological descriptors computation* Manifold reconstruction* Topological descriptors tools:   * Bottleneck distance   * Statistical tools   * Persistence diagram and barcodeFor more information about Topological Data Analysis and its workflow, pleaserefer to the `Wikipedia TDA dedicated page <https://en.wikipedia.org/wiki/Topological_data_analysis>`_.
-
+Metadata-Version: 2.1
+Name: gudhi
+Version: 3.8.0rc3
+Summary: The Gudhi library is an open source library for Computational Topology and Topological Data Analysis (TDA).
+Home-page: https://gudhi.inria.fr/
+Author: GUDHI Editorial Board <https://gudhi.inria.fr/contact/>
+License: UNKNOWN
+Project-URL: Bug Tracker, https://github.com/GUDHI/gudhi-devel/issues
+Project-URL: Documentation, https://gudhi.inria.fr/python/latest/
+Project-URL: Source Code, https://github.com/GUDHI/gudhi-devel
+Project-URL: License, https://gudhi.inria.fr/licensing/
+Platform: UNKNOWN
+Requires-Python: >=3.5.0
+Description-Content-Type: text/x-rst
+Requires-Dist: numpy (>=1.15.0)
+
+The Gudhi library is an open source library for Computational Topology and
+Topological Data Analysis (TDA). It offers state-of-the-art algorithms
+to construct various types of simplicial complexes, data structures to
+represent them, and algorithms to compute geometric approximations of shapes
+and persistent homology.
+
+The GUDHI library offers the following interoperable modules:
+
+* Complexes:
+   * Cubical
+   * Simplicial: Rips, Witness, Alpha and ƒåech complexes
+   * Cover: Nerve and Graph induced complexes
+* Data structures and basic operations:
+   * Simplex tree, Skeleton blockers and Toplex map
+   * Construction, update, filtration and simplification
+* Topological descriptors computation
+* Manifold reconstruction
+* Topological descriptors tools:
+   * Bottleneck distance
+   * Statistical tools
+   * Persistence diagram and barcode
+
+For more information about Topological Data Analysis and its workflow, please
+refer to the `Wikipedia TDA dedicated page <https://en.wikipedia.org/wiki/Topological_data_analysis>`_.
+
```

