# Comparing `tmp/openprotein-0.1.0-py3-none-any.whl.zip` & `tmp/openprotein-0.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,30 +1,36 @@
-Zip file size: 29499 bytes, number of entries: 28
--rw-rw-rw-  2.0 fat      213 b- defN 23-Mar-29 03:21 openprotein/__init__.py
--rw-rw-rw-  2.0 fat      177 b- defN 23-Mar-29 03:18 openprotein/core/__init__.py
--rw-rw-rw-  2.0 fat      834 b- defN 23-Mar-29 06:20 openprotein/core/config.py
+Zip file size: 38818 bytes, number of entries: 34
+-rw-rw-rw-  2.0 fat      250 b- defN 23-Apr-14 07:07 openprotein/__init__.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-Apr-12 06:38 openprotein/core/__init__.py
+-rw-rw-rw-  2.0 fat     1512 b- defN 23-Apr-14 07:08 openprotein/core/config.py
 -rw-rw-rw-  2.0 fat       32 b- defN 23-Mar-29 01:54 openprotein/core/rule.py
--rw-rw-rw-  2.0 fat     4294 b- defN 23-Mar-29 01:54 openprotein/core/setup.py
--rw-rw-rw-  2.0 fat      189 b- defN 23-Mar-29 06:19 openprotein/data/__init__.py
+-rw-rw-rw-  2.0 fat     4296 b- defN 23-Apr-14 08:47 openprotein/core/setup.py
+-rw-rw-rw-  2.0 fat      207 b- defN 23-Apr-04 08:26 openprotein/data/__init__.py
 -rw-rw-rw-  2.0 fat     8912 b- defN 23-Mar-29 01:54 openprotein/data/factory.py
--rw-rw-rw-  2.0 fat    12760 b- defN 23-Mar-29 06:20 openprotein/data/process.py
+-rw-rw-rw-  2.0 fat    14192 b- defN 23-Apr-14 07:12 openprotein/data/process.py
 -rw-rw-rw-  2.0 fat      174 b- defN 23-Mar-29 01:54 openprotein/datasets/__init__.py
 -rw-rw-rw-  2.0 fat     1903 b- defN 23-Mar-29 01:54 openprotein/datasets/ec.py
 -rw-rw-rw-  2.0 fat     1479 b- defN 23-Mar-29 01:54 openprotein/datasets/flip.py
 -rw-rw-rw-  2.0 fat     1678 b- defN 23-Mar-29 01:54 openprotein/datasets/go.py
 -rw-rw-rw-  2.0 fat     1726 b- defN 23-Mar-29 01:54 openprotein/datasets/tape.py
 -rw-rw-rw-  2.0 fat     1465 b- defN 23-Mar-29 01:54 openprotein/datasets/uniref.py
--rw-rw-rw-  2.0 fat       76 b- defN 23-Mar-29 01:54 openprotein/models/__init__.py
--rw-rw-rw-  2.0 fat    35125 b- defN 23-Mar-29 01:54 openprotein/models/esm1b.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Mar-29 01:54 openprotein/task/__init__.py
+-rw-rw-rw-  2.0 fat      196 b- defN 23-Apr-14 07:07 openprotein/models/__init__.py
+-rw-rw-rw-  2.0 fat    29900 b- defN 23-Apr-14 07:08 openprotein/models/esm1b.py
+-rw-rw-rw-  2.0 fat    18797 b- defN 23-Apr-14 07:11 openprotein/models/gearnet.py
+-rw-rw-rw-  2.0 fat      320 b- defN 23-Apr-12 06:38 openprotein/models/proteinbert.py
+-rw-rw-rw-  2.0 fat     1389 b- defN 23-Apr-14 02:56 openprotein/piplines/Pipline.py
+-rw-rw-rw-  2.0 fat     2708 b- defN 23-Apr-04 08:12 openprotein/piplines/Trainer.py
+-rw-rw-rw-  2.0 fat      107 b- defN 23-Apr-04 07:33 openprotein/piplines/__init__.py
+-rw-rw-rw-  2.0 fat      400 b- defN 23-Apr-03 08:27 openprotein/task/__init__.py
 -rw-rw-rw-  2.0 fat     1143 b- defN 23-Mar-29 01:54 openprotein/task/ec.py
--rw-rw-rw-  2.0 fat     4037 b- defN 23-Mar-29 01:54 openprotein/task/flip.py
+-rw-rw-rw-  2.0 fat     1070 b- defN 23-Apr-03 08:28 openprotein/task/flip.py
 -rw-rw-rw-  2.0 fat     2658 b- defN 23-Mar-29 01:54 openprotein/task/tape.py
--rw-rw-rw-  2.0 fat      262 b- defN 23-Mar-29 01:54 openprotein/utils/__init__.py
+-rw-rw-rw-  2.0 fat      310 b- defN 23-Apr-04 06:46 openprotein/utils/__init__.py
 -rw-rw-rw-  2.0 fat      369 b- defN 23-Mar-29 01:54 openprotein/utils/activation.py
--rw-rw-rw-  2.0 fat     9273 b- defN 23-Mar-29 01:54 openprotein/utils/metrics.py
+-rw-rw-rw-  2.0 fat    12694 b- defN 23-Apr-04 06:46 openprotein/utils/metrics.py
 -rw-rw-rw-  2.0 fat     1622 b- defN 23-Mar-29 01:54 openprotein/utils/utils.py
--rw-rw-rw-  2.0 fat      893 b- defN 23-Mar-29 07:53 openprotein-0.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Mar-29 07:53 openprotein-0.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       12 b- defN 23-Mar-29 07:53 openprotein-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2316 b- defN 23-Mar-29 07:53 openprotein-0.1.0.dist-info/RECORD
-28 files, 93714 bytes uncompressed, 25767 bytes compressed:  72.5%
+-rw-rw-rw-  2.0 fat    10470 b- defN 23-Apr-14 13:11 openprotein-0.2.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     1046 b- defN 23-Apr-14 13:11 openprotein-0.2.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-14 13:11 openprotein-0.2.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       12 b- defN 23-Apr-14 13:11 openprotein-0.2.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2855 b- defN 23-Apr-14 13:11 openprotein-0.2.0.dist-info/RECORD
+34 files, 126193 bytes uncompressed, 34248 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -42,14 +42,29 @@
 
 Filename: openprotein/models/__init__.py
 Comment: 
 
 Filename: openprotein/models/esm1b.py
 Comment: 
 
+Filename: openprotein/models/gearnet.py
+Comment: 
+
+Filename: openprotein/models/proteinbert.py
+Comment: 
+
+Filename: openprotein/piplines/Pipline.py
+Comment: 
+
+Filename: openprotein/piplines/Trainer.py
+Comment: 
+
+Filename: openprotein/piplines/__init__.py
+Comment: 
+
 Filename: openprotein/task/__init__.py
 Comment: 
 
 Filename: openprotein/task/ec.py
 Comment: 
 
 Filename: openprotein/task/flip.py
@@ -66,20 +81,23 @@
 
 Filename: openprotein/utils/metrics.py
 Comment: 
 
 Filename: openprotein/utils/utils.py
 Comment: 
 
-Filename: openprotein-0.1.0.dist-info/METADATA
+Filename: openprotein-0.2.0.dist-info/LICENSE
+Comment: 
+
+Filename: openprotein-0.2.0.dist-info/METADATA
 Comment: 
 
-Filename: openprotein-0.1.0.dist-info/WHEEL
+Filename: openprotein-0.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: openprotein-0.1.0.dist-info/top_level.txt
+Filename: openprotein-0.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: openprotein-0.1.0.dist-info/RECORD
+Filename: openprotein-0.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## openprotein/__init__.py

```diff
@@ -1,7 +1,7 @@
 __version__ = "0.1.0"
 
 from .data import MaskedConverter, Alphabet
 from .datasets import Uniref
-from .models import Esm1b
+from .models import Esm1b, GearNet, ProteinBert
 from .utils import Accuracy, MeanSquaredError, Spearman
-from .core import Esm1bConfig
+from .core import Esm1bConfig, GearNetConfig
```

## openprotein/core/__init__.py

```diff
@@ -1,7 +1,7 @@
 from .setup import GlobalConfiguration, Tree, Components
-from .config import Esm1bConfig
+from .config import Esm1bConfig, GearNetConfig
 
 __all__ = [
     "GlobalConfiguration", "Tree", "Components", 
-    "Esm1bConfig"
+    "Esm1bConfig", "GearNetConfig"
 ]
```

## openprotein/core/config.py

```diff
@@ -1,36 +1,76 @@
 from dataclasses import dataclass, field
 
+
 @dataclass
 class Esm1bConfig:
     num_layers: int = field(
-        default=33,
-        metadata={"help": ""},
+        default=33
     )
     embed_dim: int = field(
-        default=1280,
-        metadata={"help": ""},
+        default=1280
     )
     logit_bias: bool = field(
-        default=True,
-        metadata={"help": ""},
+        default=True
     )
     ffn_embed_dim: int = field(
-        default=5120,
-        metadata={"help": ""},
+        default=5120
     )
     attention_heads: int = field(
-        default=20,
-        metadata={"help": ""},
+        default=20
     )
     max_positions: int = field(
-        default=1024,
-        metadata={"help": ""},
+        default=1024
     )
     emb_layer_norm_before: bool = field(
-        default=True,
-        metadata={"help": ""},
+        default=True
     )
     checkpoint_path: str = field(
-        default=None,
-        metadata={"help": ""},
+        default=None
+    )
+
+
+@dataclass
+class GearNetConfig:
+    input_dim: int = field(
+        default=21
+    )
+    embedding_dim: int = field(
+        default=512
+    )
+    hidden_dims: list = field(
+        default_factory=list,
+        metadata={"help": "[512, 512, 512, 512, 512, 512]"},
+    )
+    num_relation: int = field(
+        default=7
+    )
+    edge_input_dim: int = field(
+        default=59
+    )
+    batch_norm: int = field(
+        default=True
+    )
+    activation: bool = field(
+        default='relu'
+    )
+    concat_hidden: bool = field(
+        default=True
+    )
+    short_cut: bool = field(
+        default=True
+    )
+    readout: str = field(
+        default="sum"
+    )
+    dropout: int = field(
+        default=0.2
+    )
+    num_angle_bin: int = field(
+        default=8
+    )
+    layer_norm: bool = field(
+        default=True
+    )
+    use_ieconv: bool = field(
+        default=True
     )
```

## openprotein/core/setup.py

```diff
@@ -1,12 +1,13 @@
 from typing import *
 import weakref
 import threading
 from collections import UserDict
 
+
 class Components(type):
     def __new__(cls, name, bases, attrs):
         obj = type.__new__(cls, name, bases, attrs)
         conf = GlobalConfiguration()
         conf.register_component(name, bases, attrs, obj)
         return obj
```

## openprotein/data/__init__.py

```diff
@@ -1,7 +1,7 @@
-from .process import MaskedConverter, Alphabet, PROTEINSEQ_TOKS
+from .process import MaskedConverter, Alphabet, PROTEINSEQ, TaskConvert
 from .factory import DataFactory
 
 __all__ = [
-    "MaskedConverter", "Alphabet", "PROTEINSEQ_TOKS",
+    "MaskedConverter", "Alphabet", "PROTEINSEQ", "TaskConvert",
     "DataFactory"
 ]
```

## openprotein/data/process.py

```diff
@@ -1,16 +1,16 @@
 from typing import *
 import itertools
 import torch
 import numpy as np
 
-PROTEINSEQ_TOKS = {
-    'toks': ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C',
+
+PROTEINSEQ = ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C',
                 'X', 'B', 'U', 'Z', 'O', '.', '-']
-}
+
 
 class MaskedConverter(object):
     def __init__(self, standard_toks: Sequence[str],
                  prepend_toks: Sequence[str] = ("<null_0>", "<pad>", "<eos>", "<unk>"),
                  append_toks: Sequence[str] = ("<cls>", "<mask>", "<sep>"),
                  prepend_bos: bool = True,
                  append_eos: bool = False):
@@ -126,16 +126,16 @@
             masked_tokens[i, len(masked_sequence) + 1] = self.eos_idx
 
             target_tokens[i, 1:len(mask) + 1][mask | unmask] = encoded_sequence[mask | unmask]
 
         return origin_tokens, masked_tokens, target_tokens
 
     @classmethod
-    def build_convert(cls, proteinseq_toks: dict = PROTEINSEQ_TOKS) -> "MaskedConverter":
-        standard_toks = proteinseq_toks["toks"]
+    def build_convert(cls, proteinseq: dict=None) -> "MaskedConverter":
+        standard_toks = proteinseq if proteinseq else PROTEINSEQ
         prepend_toks = ("<cls>", "<pad>", "<eos>", "<unk>")
         append_toks = ("<mask>",)
         prepend_bos = True
         append_eos = True
         return cls(standard_toks, prepend_toks, append_toks, prepend_bos, append_eos)
 
     def _tokenize(self, text: str) -> str:
@@ -206,14 +206,60 @@
         tokenized_text = split_on_tokens(no_split_token, text)
         return tokenized_text
 
     def encode(self, text):
         return [self.tok_to_idx[tok] for tok in self.tokenize(text)]
 
 
+class TaskConvert(object):
+    """
+    Converter for tokenizing protein sequence.
+
+    Args:
+        alphabet: Dictionary from amino acids to tokens
+    """
+
+    def __init__(self, alphabet):
+        self.alphabet = alphabet
+        self.pad_idx = alphabet.padding_idx
+        self.cls_idx = alphabet.cls_idx
+        self.eos_idx = alphabet.eos_idx
+
+    def __call__(self, seqences: Sequence[Tuple[str, str]]):
+        """
+        Convert Batch to Downstream Task Needed Format.
+
+        Args:
+            a batch of proteins [sequence1, sequence2, ...]
+
+        Returns:
+            aligned tokens
+        """
+        batch_size = len(seqences)
+
+        encoded_sequences = [self.alphabet.encode(sequence) for sequence in seqences]
+        max_encoded_sequences_length = max(len(encoded_sequence) for encoded_sequence in encoded_sequences)
+        tokens = torch.empty(
+            (
+                batch_size,
+                max_encoded_sequences_length + 2,
+            ),
+            dtype=torch.int64
+        )
+        tokens.fill_(self.pad_idx)
+
+        for i, encoded_sequence in enumerate(encoded_sequences):
+            encoded_sequence = torch.tensor(encoded_sequence, dtype=torch.int64)
+            tokens[i, 0] = self.cls_idx
+            tokens[i, 1:len(encoded_sequence) + 1] = encoded_sequence
+            tokens[i, len(encoded_sequence) + 1] = self.eos_idx
+
+        return tokens
+
+
 class Alphabet(object):
     def __init__(
         self,
         standard_toks: Sequence[str],
         prepend_toks: Sequence[str] = ("<null_0>", "<pad>", "<eos>", "<unk>"),
         append_toks: Sequence[str] = ("<cls>", "<mask>", "<sep>"),
         prepend_bos: bool = True,
@@ -255,16 +301,16 @@
     def to_dict(self):
         return self.tok_to_idx.copy()
 
     def pad(self):
         return self.padding_idx
 
     @classmethod
-    def build_alphabet(cls, proteinseq_toks: dict = PROTEINSEQ_TOKS) -> "Alphabet":
-        standard_toks = proteinseq_toks["toks"]
+    def build_alphabet(cls, proteinseq: dict = PROTEINSEQ) -> "Alphabet":
+        standard_toks = proteinseq if proteinseq else PROTEINSEQ
         prepend_toks = ("<cls>", "<pad>", "<eos>", "<unk>")
         append_toks = ("<mask>",)
         prepend_bos = True
         append_eos = True
         return cls(standard_toks, prepend_toks, append_toks, prepend_bos, append_eos)
 
     def _tokenize(self, text) -> str:
```

## openprotein/models/__init__.py

```diff
@@ -1,5 +1,9 @@
 from .esm1b import ProteinBertModel as Esm1b
+from .gearnet import GearNetIEConv as GearNet
+from .proteinbert import ProteinBert
 
 __all__ = [
     "Esm1b",
+    "GearNet",
+    "ProteinBert"
 ]
```

## openprotein/models/esm1b.py

```diff
@@ -1,24 +1,18 @@
 import math
 import uuid
-from typing import Optional, Dict, Tuple, Sequence, List, OrderedDict
-import itertools
+from typing import Optional, Dict, Tuple, OrderedDict
 
 import torch
 import torch.nn.functional as F
 from torch import Tensor
 from torch import nn
 from torch.nn import Parameter
 from torch.nn import LayerNorm as ESM1bLayerNorm
 
-proteinseq_toks = {
-    'toks': ['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X',
-             'B', 'U', 'Z', 'O', '.', '-']
-}
-
 
 def gelu(x):
     """Implementation of the gelu activation function.
 
     For information: OpenAI GPT's gelu is slightly different
     (and gives slightly different results):
     0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))
@@ -731,143 +725,7 @@
                 for k, v in pretrain_decoder_dict['model'].items():
                     if 'sentence_encoder.' in k:
                         k = k.replace('encoder.sentence_encoder.', '')
                         new_state_dict[k] = v
             model.load_state_dict(new_state_dict)
 
         return model
-
-
-class Alphabet(object):
-
-    def __init__(
-            self,
-            standard_toks: Sequence[str],
-            prepend_toks: Sequence[str] = ("<null_0>", "<pad>", "<eos>", "<unk>"),
-            append_toks: Sequence[str] = ("<cls>", "<mask>", "<sep>"),
-            prepend_bos: bool = True,
-            append_eos: bool = False,
-    ):
-        self.standard_toks = list(standard_toks)
-        self.prepend_toks = list(prepend_toks)
-        self.append_toks = list(append_toks)
-        self.prepend_bos = prepend_bos
-        self.append_eos = append_eos
-
-        self.all_toks = list(self.prepend_toks)
-        self.protein_tok_begin = len(self.all_toks)
-        self.all_toks.extend(self.standard_toks)
-        self.protein_tok_end = len(self.all_toks)
-        for i in range((8 - (len(self.all_toks) % 8)) % 8):
-            self.all_toks.append(f"<null_{i + 1}>")
-        self.all_toks.extend(self.append_toks)
-
-        self.tok_to_idx = {tok: i for i, tok in enumerate(self.all_toks)}
-
-        self.unk_idx = self.tok_to_idx["<unk>"]
-        self.padding_idx = self.get_idx("<pad>")
-        self.cls_idx = self.get_idx("<cls>")
-        self.mask_idx = self.get_idx("<mask>")
-        self.eos_idx = self.get_idx("<eos>")
-        self.ppi_idx = self.get_idx("<ppi>")
-        self.all_special_tokens = ['<eos>', '<unk>', '<pad>', '<cls>', '<mask>', '<ppi>']
-        self.unique_no_split_tokens = self.all_toks
-
-    def __len__(self):
-        return len(self.all_toks)
-
-    def get_idx(self, tok):
-        return self.tok_to_idx.get(tok, self.unk_idx)
-
-    def get_tok(self, ind):
-        return self.all_toks[ind]
-
-    def to_dict(self):
-        return self.tok_to_idx.copy()
-
-    def pad(self):
-        return self.padding_idx
-
-    @classmethod
-    def build_alphabet(cls) -> "Alphabet":
-        standard_toks = proteinseq_toks["toks"]
-        prepend_toks = ("<cls>", "<pad>", "<eos>", "<unk>")
-        append_toks = ("<mask>",)
-        prepend_bos = True
-        append_eos = True
-        return cls(standard_toks, prepend_toks, append_toks, prepend_bos, append_eos)
-
-    def _tokenize(self, text) -> str:
-        return text.split()
-
-    def tokenize(self, text, **kwargs) -> List[str]:
-        """
-        Inspired by https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils.py
-        Converts a string in a sequence of tokens, using the tokenizer.
-
-        Args:
-            text (:obj:`str`):
-                The sequence to be encoded.
-
-        Returns:
-            :obj:`List[str]`: The list of tokens.
-        """
-
-        def split_on_token(tok, text):
-            result = []
-            split_text = text.split(tok)
-            for i, sub_text in enumerate(split_text):
-                # AddedToken can control whitespace stripping around them.
-                # We use them for GPT2 and Roberta to have different behavior depending on the special token
-                # Cf. https://github.com/huggingface/transformers/pull/2778
-                # and https://github.com/huggingface/transformers/issues/3788
-                # We strip left and right by default
-                if i < len(split_text) - 1:
-                    sub_text = sub_text.rstrip()
-                if i > 0:
-                    sub_text = sub_text.lstrip()
-
-                if i == 0 and not sub_text:
-                    result.append(tok)
-                elif i == len(split_text) - 1:
-                    if sub_text:
-                        result.append(sub_text)
-                    else:
-                        pass
-                else:
-                    if sub_text:
-                        result.append(sub_text)
-                    result.append(tok)
-            return result
-
-        def split_on_tokens(tok_list, text):
-            if not text.strip():
-                return []
-
-            tokenized_text = []
-            text_list = [text]
-            for tok in tok_list:
-                tokenized_text = []
-                for sub_text in text_list:
-                    if sub_text not in self.unique_no_split_tokens:
-                        tokenized_text.extend(split_on_token(tok, sub_text))
-                    else:
-                        tokenized_text.append(sub_text)
-                text_list = tokenized_text
-
-            return list(
-                itertools.chain.from_iterable(
-                    (
-                        self._tokenize(token)
-                        if token not in self.unique_no_split_tokens
-                        else [token]
-                        for token in tokenized_text
-                    )
-                )
-            )
-
-        no_split_token = self.unique_no_split_tokens
-        tokenized_text = split_on_tokens(no_split_token, text)
-        return tokenized_text
-
-    def encode(self, text):
-        return [self.tok_to_idx[tok] for tok in self.tokenize(text)]
```

## openprotein/task/__init__.py

```diff
@@ -0,0 +1,25 @@
+00000000: 6672 6f6d 202e 6563 2069 6d70 6f72 7420  from .ec import 
+00000010: 5072 6f74 6569 6e46 756e 6374 696f 6e44  ProteinFunctionD
+00000020: 6563 6f64 6572 0d0a 6672 6f6d 202e 666c  ecoder..from .fl
+00000030: 6970 2069 6d70 6f72 7420 5365 7175 656e  ip import Sequen
+00000040: 6365 5265 6772 6573 7369 6f6e 4465 636f  ceRegressionDeco
+00000050: 6465 720d 0a66 726f 6d20 2e74 6170 6520  der..from .tape 
+00000060: 696d 706f 7274 2053 6571 7565 6e63 6543  import SequenceC
+00000070: 6c61 7373 6966 6963 6169 746f 6e44 6563  lassificaitonDec
+00000080: 6f64 6572 2c20 5365 7175 656e 6365 546f  oder, SequenceTo
+00000090: 5365 7175 656e 6365 436c 6173 7369 6669  SequenceClassifi
+000000a0: 6361 6974 6f6e 4465 636f 6465 722c 2050  caitonDecoder, P
+000000b0: 726f 7465 696e 436f 6e74 6163 744d 6170  roteinContactMap
+000000c0: 4465 636f 6465 720d 0a0d 0a5f 5f61 6c6c  Decoder....__all
+000000d0: 5f5f 203d 205b 0d0a 2020 2020 2250 726f  __ = [..    "Pro
+000000e0: 7465 696e 4675 6e63 7469 6f6e 4465 636f  teinFunctionDeco
+000000f0: 6465 7222 2c0d 0a20 2020 2022 5365 7175  der",..    "Sequ
+00000100: 656e 6365 5265 6772 6573 7369 6f6e 4465  enceRegressionDe
+00000110: 636f 6465 7222 2c0d 0a20 2020 2022 5365  coder",..    "Se
+00000120: 7175 656e 6365 436c 6173 7369 6669 6361  quenceClassifica
+00000130: 6974 6f6e 4465 636f 6465 7222 2c0d 0a20  itonDecoder",.. 
+00000140: 2020 2022 5365 7175 656e 6365 546f 5365     "SequenceToSe
+00000150: 7175 656e 6365 436c 6173 7369 6669 6361  quenceClassifica
+00000160: 6974 6f6e 4465 636f 6465 7222 2c0d 0a20  itonDecoder",.. 
+00000170: 2020 2022 5072 6f74 6569 6e43 6f6e 7461     "ProteinConta
+00000180: 6374 4d61 7044 6563 6f64 6572 220d 0a5d  ctMapDecoder"..]
```

## openprotein/task/flip.py

```diff
@@ -2,97 +2,14 @@
 import argparse
 
 import torch
 from torch import nn
 import torch.nn.functional as F
 
 
-class Args(object):
-    def __init__(self):
-        self.parser = argparse.ArgumentParser()
-
-    def args_parser(self):
-        self.parser.add_argument(
-            "--num_layers", default=33, type=int, metavar="N", help="number of layers"
-        )
-        self.parser.add_argument(
-            "--embed_dim", default=1280, type=int, metavar="N", help="embedding dimension"
-        )
-        self.parser.add_argument(
-            "--ffn_embed_dim",
-            default=5120,
-            type=int,
-            metavar="N",
-            help="embedding dimension for FFN",
-        )
-        self.parser.add_argument(
-            "--attention_heads",
-            default=20,
-            type=int,
-            metavar="N",
-            help="number of attention heads",
-        )
-        self.parser.add_argument("--max_positions", default=1024, type=int,
-                                 help="number of positional embeddings to learn")
-        self.parser.add_argument("--emb_layer_norm_before", default=True, type=bool)
-        self.parser.add_argument('--num-sequences', type=int, help='Number of sequences to analyze', default=5000)
-        self.parser.add_argument('--logit_bias', type=bool, default=True)
-        self.parser.add_argument('--checkpoint_path', type=str, help='the path of load models',
-                                 default="E:/esm1b_t33_650M_UR50S.pt")
-        args = self.parser.parse_args()
-        return args
-
-
-class Convert(object):
-    """
-    Converter for tokenizing protein sequence.
-
-    Args:
-        alphabet: Dictionary from amino acids to tokens
-    """
-
-    def __init__(self, alphabet):
-        self.alphabet = alphabet
-        self.pad_idx = alphabet.padding_idx
-        self.cls_idx = alphabet.cls_idx
-        self.eos_idx = alphabet.eos_idx
-
-    def __call__(self, seqences: Sequence[Tuple[str, str]]):
-        """
-        Convert Batch to Downstream Task Needed Format.
-
-        Args:
-            a batch of proteins [sequence1, sequence2, ...]
-
-        Returns:
-            aligned tokens
-        """
-        batch_size = len(seqences)
-
-        encoded_sequences = [self.alphabet.encode(sequence) for sequence in seqences]
-        max_encoded_sequences_length = max(len(encoded_sequence) for encoded_sequence in encoded_sequences)
-        tokens = torch.empty(
-            (
-                batch_size,
-                max_encoded_sequences_length + 2,
-            ),
-            dtype=torch.int64
-        )
-        tokens.fill_(self.pad_idx)
-
-        for i, encoded_sequence in enumerate(encoded_sequences):
-            sequence_length = len(encoded_sequence)
-            encoded_sequence = torch.tensor(encoded_sequence, dtype=torch.int64)
-            tokens[i, 0] = self.cls_idx
-            tokens[i, 1:len(encoded_sequence) + 1] = encoded_sequence
-            tokens[i, len(encoded_sequence) + 1] = self.eos_idx
-
-        return tokens
-
-
 class SequenceRegressionDecoder(nn.Module):
     """
         Process the embedding of the model into the format required by the downstream task called flip, stability or fluorescence
 
         Args:
             embed_dim (int, default=1280): embedding dimension
     """
```

## openprotein/utils/__init__.py

```diff
@@ -1,9 +1,9 @@
 from .activation import gelu
-from .metrics import Accuracy, MeanSquaredError, Spearman
+from .metrics import Accuracy, MeanSquaredError, Spearman, AveragePrecisionScore
 from .utils import convert_to_bytes, convert_to_str
 
 __all__ = [
     "gelu",
-    "Accuracy", "MeanSquaredError", "Spearman",
+    "Accuracy", "MeanSquaredError", "Spearman", "AveragePrecisionScore",
     "convert_to_bytes", "convert_to_str"
 ]
```

## openprotein/utils/metrics.py

```diff
@@ -1,11 +1,9 @@
 from typing import *
-from functools import partial
-
-from sklearn.metrics import accuracy_score, f1_score, mean_squared_error
+from sklearn.metrics import accuracy_score, mean_squared_error, average_precision_score
 from scipy.stats import spearmanr
 
 from openprotein.core import Components
 
 
 class Metric(metaclass=Components):
     """
@@ -17,19 +15,14 @@
         self.preds = []
         self.trues = []
         self.score = 0
         if self._backend == "pt":
             import torch
             self.is_tensor = torch.is_tensor
             self._conver_to_list = lambda x: x.tolist()
-        elif self._backend == "ms":
-            import mindspore
-            self.is_tensor = partial(isinstance, A_tuple=mindspore.Tensor)
-            self._conver_to_list = lambda x: x.asnumpy().tolist()
-
 
     def compute_once(self, true, pred) -> float:
         """
         Abstract methods, requiring subclasses to implement different evaluation metrics calculations
 
         Args:
             true : True values or ground-truth.
@@ -252,7 +245,93 @@
         rho, _ = spearmanr(true, pred, axis=self.axis,
                            nan_policy=self.nan_policy, alternative=self.alternative)
 
         return rho
 
     def __repr__(self):
         return "Spm"
+
+
+
+class AveragePrecisionScore(Metric):
+    """
+    A class to calculate average precision score.
+
+    Args:
+        y_true : ndarray of shape (n_samples,) or (n_samples, n_classes)
+            True binary labels or binary label indicators.
+        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)
+            Target scores, can either be probability estimates of the positive
+            class, confidence values, or non-thresholded measure of decisions
+            (as returned by :term:`decision_function` on some classifiers).
+        average : {'micro', 'samples', 'weighted', 'macro'} or None, \
+                default='macro'
+            If ``None``, the scores for each class are returned. Otherwise,
+            this determines the type of averaging performed on the data:
+            ``'micro'``:
+                Calculate metrics globally by considering each element of the label
+                indicator matrix as a label.
+            ``'macro'``:
+                Calculate metrics for each label, and find their unweighted
+                mean.  This does not take label imbalance into account.
+            ``'weighted'``:
+                Calculate metrics for each label, and find their average, weighted
+                by support (the number of true instances for each label).
+            ``'samples'``:
+                Calculate metrics for each instance, and find their average.
+            Will be ignored when ``y_true`` is binary.
+        pos_label : int or str, default=1
+            The label of the positive class. Only applied to binary ``y_true``.
+            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights.
+
+    Returns:
+        average_precision : float
+
+    References:
+        [1] `Wikipedia entry for the Average precision
+           <https://en.wikipedia.org/w/index.php?title=Information_retrieval&
+           oldid=793358396#Average_precision>`_
+
+    Examples:
+        >>> from openprotein.utils import AveragePrecisionScore
+        >>> aps = AveragePrecisionScore()
+        >>> y_true = [1, 1, 0, 1]
+        >>> y_score = [0.3, 0.4, 0.2, 0.1]
+        >>> aps.(y_true, y_score)
+        0.9166666666666665
+    """
+
+    def __init__(self, average="macro", pos_label=1, sample_weight=None):
+        super().__init__()
+        self.average = average
+        self.pos_label = pos_label
+        self.sample_weight = sample_weight
+
+    def compute_once(self, true, score) -> float:
+        """
+        Calculate the average precision score for current pred and true.
+
+        Args:
+            y_true : ndarray of shape (n_samples,) or (n_samples, n_classes)
+                True binary labels or binary label indicators.
+            y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)
+                Target scores, can either be probability estimates of the positive
+                class, confidence values, or non-thresholded measure of decisions
+                (as returned by :term:`decision_function` on some classifiers).
+
+        Examples:
+            >>> from openprotein.utils import AveragePrecisionScore
+            >>> aps = AveragePrecisionScore()
+            >>> y_true = [1, 1, 0, 1]
+            >>> y_score = [0.3, 0.4, 0.2, 0.1]
+            >>> aps.compute_once(y_true, y_score)
+            0.9166666666666665
+
+            """
+
+        y_score = average_precision_score(true, score)
+        return y_score
+
+    def __repr__(self):
+        return "AveragePrecisionScore"
```

## Comparing `openprotein-0.1.0.dist-info/METADATA` & `openprotein-0.2.0.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,32 @@
 Metadata-Version: 2.1
 Name: openprotein
-Version: 0.1.0
+Version: 0.2.0
 Summary: Open-Protein is an open source pre-training platform that supports multiple protein pre-training models and downstream tasks.
-Home-page: 
+Home-page: UNKNOWN
 Author: HIC-AI
 Author-email: hic_ai@163.com
 License: Apache-2.0
 Keywords: open,protein,pre-training,downstream task
 Platform: UNKNOWN
 Requires-Python: >=3.6.0,<3.10.0
+License-File: LICENSE
 Requires-Dist: numpy (>=1.11)
 Requires-Dist: scikit-learn
+Requires-Dist: transformers
 Requires-Dist: scipy
 Requires-Dist: lmdb
 Provides-Extra: docs
 Requires-Dist: recommonmark ; extra == 'docs'
 Requires-Dist: sphinx ; extra == 'docs'
 Requires-Dist: sphinx-markdown-tables ; extra == 'docs'
 Requires-Dist: sphinx-rtd-theme (==0.4.3) ; extra == 'docs'
 Requires-Dist: sphinx-copybutton ; extra == 'docs'
 Provides-Extra: testing
 Requires-Dist: unittest ; extra == 'testing'
 Provides-Extra: torch
 Requires-Dist: torch (>=1.5.0) ; extra == 'torch'
+Provides-Extra: torch_scatter
+Requires-Dist: torch-scatter (>=2.0.8) ; extra == 'torch_scatter'
 
 UNKNOWN
```

## Comparing `openprotein-0.1.0.dist-info/RECORD` & `openprotein-0.2.0.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,34 @@
-openprotein/__init__.py,sha256=ogHPHYCP0I1MWpPdgNk9h7YZvng9sMcnLnFQFhCPGkU,213
-openprotein/core/__init__.py,sha256=gmmc8kRXWI_Y5xWu7G7hk-MxAHC5xF1BSHRQ6zFbojw,177
-openprotein/core/config.py,sha256=N6XBjf4OwVxwPGlpevNZM5i8I7mhqjgRCW2CnEI8q6Y,834
+openprotein/__init__.py,sha256=r5ZJdhoh3nWi7qUaYO1fGNT_Nri33HeYAts8DIHzaFM,250
+openprotein/core/__init__.py,sha256=Ru0xb6Unt9WcqOaNHzP3ylIhSW7MqCJPM2_sKyn5C88,209
+openprotein/core/config.py,sha256=KtRYmfm3SF1XnAdPUZWBZLuWJWErDYeSRwyDi-FidqA,1512
 openprotein/core/rule.py,sha256=HhRqIz0NYQni4w-hhPOwhANq1GZ0ArpdKWzvhexhoks,32
-openprotein/core/setup.py,sha256=Mw4XrP2qiwndhFey8FgzbyrL3JFVRpHBKbpluBM7TBA,4294
-openprotein/data/__init__.py,sha256=67vtOYLNhVEmPerqA3P-_NeP2dgacRkyoHakLlN3dn0,189
+openprotein/core/setup.py,sha256=Iaa-kny1pSFeuP80VeFMZf_LbxeiLoeBlvUKXIHoMa8,4296
+openprotein/data/__init__.py,sha256=Yq_4WKLAQoassFhCIDTsugNGtLCONu3FpysLx61Qdv8,207
 openprotein/data/factory.py,sha256=L7r6NavDi4FeFncUt1DdtqbgPws77J5dEZTgaSLe7Hw,8912
-openprotein/data/process.py,sha256=j5GEiYB35vkVi0035Ew0HvTvZ3B4CVvvrX_aWxw7mxI,12760
+openprotein/data/process.py,sha256=XwWXzxICNkksLt2obp57vmTovpV_iddoI0_0bYp1eT0,14192
 openprotein/datasets/__init__.py,sha256=FR5tVCt5FtKC7GUbfO7NZKpVcxK2EWHVJKYd__X0MNg,174
 openprotein/datasets/ec.py,sha256=zHqrfUAinb0pEDqCrGGEiM0BvFYBxGo2k_fmJ9juLy0,1903
 openprotein/datasets/flip.py,sha256=4q-kLItCdVYJIVuhy7xUgWtviezhWD0lXJY6pWw1fCY,1479
 openprotein/datasets/go.py,sha256=f6Izw5hSHobzyUFn60YXvc_fGKXm29lemeDGRefwN78,1678
 openprotein/datasets/tape.py,sha256=HeeLL0O1Zp3Rrzmys3adFAi3afrm3EkRvANYM_w4vcc,1726
 openprotein/datasets/uniref.py,sha256=n6xWhJjzpjwtzzKUu5aqSZLvtZiJk7aZO1J3mCwxfeE,1465
-openprotein/models/__init__.py,sha256=lgKVFZRYHvzwPd4Wrf78FHUQxhqDXB0JWcm23QvLHY4,76
-openprotein/models/esm1b.py,sha256=zhchDJR52nH5FGqTVImrZmSYbx_MmvFR7GdfPJw3me8,35125
-openprotein/task/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+openprotein/models/__init__.py,sha256=UDgQBxQJFQuFyQXs61i_QQufb1ZdrL1vJ-r0bVeNAD0,196
+openprotein/models/esm1b.py,sha256=InIeYZnZ1I6_D-6LNoEABUHsjRjQRjLyTkVjo7IOwpk,29900
+openprotein/models/gearnet.py,sha256=o2IGdTUVpERLj5Qv7dXjEQ9dADD2_h8MgAd3tMBEKV8,18797
+openprotein/models/proteinbert.py,sha256=UP8UfuGfqSbebUXjdradfMdvykRbdEISGorFrR77HqA,320
+openprotein/piplines/Pipline.py,sha256=L5iyx4sab0Aha3wCjxpXOF3fYzjnPFPvXGEASCGOWUc,1389
+openprotein/piplines/Trainer.py,sha256=yRdnHveQy69O5exKj5NyqVhb7_x1QGMDNI3FfeC8hQA,2708
+openprotein/piplines/__init__.py,sha256=0Am2JfiBEGMwCwGkOTu_R9kw11b_2CpzNskp5hhnBlA,107
+openprotein/task/__init__.py,sha256=MC2L_Cc8ikSFUlrVVOe50st9tKC_44hDWt4AlmnHlVI,400
 openprotein/task/ec.py,sha256=ZaHHPJjBITPdVMZUepoBzhcub4qkS9M66X89NOrHsKw,1143
-openprotein/task/flip.py,sha256=PQljoHuZkzKyav18s6pnnZ7RhJY4XW6qR4pHpmQfMow,4037
+openprotein/task/flip.py,sha256=mFVDovmoFJ6tUj7G5_fGVys6mQChxWYBvKZy7PLK3nE,1070
 openprotein/task/tape.py,sha256=rZ48Kppu-4rZmcbt_Z307KleAH031YZoL259mONbfXo,2658
-openprotein/utils/__init__.py,sha256=QNpdk9HOazniK8CUycodDX9_g5hHvilfTsI_IbXtqFw,262
+openprotein/utils/__init__.py,sha256=d9qdmY8wyfT_bGEwdrlbnmB4idQqOtQ58Y__RV9HVGg,310
 openprotein/utils/activation.py,sha256=bGRsmDPVb4s6hQCGKOVFwmbTOHRmHWikvgz9kLxOWrM,369
-openprotein/utils/metrics.py,sha256=qPnxCFWIwIxJw8CqHgPp8qy4Y87DKEsB2dycSjvlmZg,9273
+openprotein/utils/metrics.py,sha256=SQUCJiNI7II53uOzXQHes_IBVyBmc55u9c8a91QELA4,12694
 openprotein/utils/utils.py,sha256=dkZO5D94DxLTR_kRcKToAjtOQgdC3D_3JO63Xw5xsJQ,1622
-openprotein-0.1.0.dist-info/METADATA,sha256=VNjA0K1O_Z0XYXT7uybzps-jn_Ckff1HuqfXCc7Xtto,893
-openprotein-0.1.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-openprotein-0.1.0.dist-info/top_level.txt,sha256=Lmds6LNypPrUN3oRAqp7RvaAE3vXvV1RwUTXpCxanic,12
-openprotein-0.1.0.dist-info/RECORD,,
+openprotein-0.2.0.dist-info/LICENSE,sha256=kVwFYEce7Od2QAaAGdUZ9vXLNFh3_Et7IAaizFgnfd8,10470
+openprotein-0.2.0.dist-info/METADATA,sha256=ikjVaH5sEh7AzTVkOqoA0v-6LKdevk6cqS9TZfb7LbA,1046
+openprotein-0.2.0.dist-info/WHEEL,sha256=ewwEueio1C2XeHTvT17n8dZUJgOvyCWCt0WVNLClP9o,92
+openprotein-0.2.0.dist-info/top_level.txt,sha256=Lmds6LNypPrUN3oRAqp7RvaAE3vXvV1RwUTXpCxanic,12
+openprotein-0.2.0.dist-info/RECORD,,
```

