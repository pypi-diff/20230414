# Comparing `tmp/optilogic-2.3.0-py3-none-any.whl.zip` & `tmp/optilogic-2.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,19 +1,19 @@
-Zip file size: 35348 bytes, number of entries: 17
--rw-r--r--  2.0 unx       22 b- defN 23-Mar-03 15:39 optilogic/__init__.py
--rw-r--r--  2.0 unx       48 b- defN 23-Mar-03 15:39 optilogic/pioneer/__init__.py
--rw-r--r--  2.0 unx       20 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/__init__.py
--rw-r--r--  2.0 unx    60336 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/api.py
--rw-r--r--  2.0 unx    90132 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/api_tests.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/quick_tests/__init__.py
--rw-r--r--  2.0 unx     5111 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py
--rw-r--r--  2.0 unx      317 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/quick_tests/bash.py
--rw-r--r--  2.0 unx      209 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/quick_tests/quick.py
--rw-r--r--  2.0 unx      577 b- defN 23-Mar-03 15:39 optilogic/pioneer/api/quick_tests/sleep.py
--rw-r--r--  2.0 unx      131 b- defN 23-Mar-03 15:39 optilogic/pioneer/job_utils/__init__.py
--rw-r--r--  2.0 unx     3553 b- defN 23-Mar-03 15:39 optilogic/pioneer/job_utils/job_utils.py
--rw-r--r--  2.0 unx     1070 b- defN 23-Mar-03 15:40 optilogic-2.3.0.dist-info/LICENSE
--rw-r--r--  2.0 unx      759 b- defN 23-Mar-03 15:40 optilogic-2.3.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-03 15:40 optilogic-2.3.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-Mar-03 15:40 optilogic-2.3.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1523 b- defN 23-Mar-03 15:40 optilogic-2.3.0.dist-info/RECORD
-17 files, 163910 bytes uncompressed, 32786 bytes compressed:  80.0%
+Zip file size: 37596 bytes, number of entries: 17
+-rw-r--r--  2.0 unx       22 b- defN 23-Apr-13 23:40 optilogic/__init__.py
+-rw-r--r--  2.0 unx       48 b- defN 23-Apr-13 23:40 optilogic/pioneer/__init__.py
+-rw-r--r--  2.0 unx       20 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/__init__.py
+-rw-r--r--  2.0 unx    64383 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/api.py
+-rw-r--r--  2.0 unx    95676 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/api_tests.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/quick_tests/__init__.py
+-rw-r--r--  2.0 unx     5111 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py
+-rw-r--r--  2.0 unx      317 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/quick_tests/bash.py
+-rw-r--r--  2.0 unx      209 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/quick_tests/quick.py
+-rw-r--r--  2.0 unx      577 b- defN 23-Apr-13 23:40 optilogic/pioneer/api/quick_tests/sleep.py
+-rw-r--r--  2.0 unx      131 b- defN 23-Apr-13 23:40 optilogic/pioneer/job_utils/__init__.py
+-rw-r--r--  2.0 unx     3553 b- defN 23-Apr-13 23:40 optilogic/pioneer/job_utils/job_utils.py
+-rw-r--r--  2.0 unx     1070 b- defN 23-Apr-13 23:41 optilogic-2.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx      759 b- defN 23-Apr-13 23:41 optilogic-2.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-13 23:41 optilogic-2.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-Apr-13 23:41 optilogic-2.4.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1523 b- defN 23-Apr-13 23:41 optilogic-2.4.0.dist-info/RECORD
+17 files, 173501 bytes uncompressed, 35034 bytes compressed:  79.8%
```

## zipnote {}

```diff
@@ -30,23 +30,23 @@
 
 Filename: optilogic/pioneer/job_utils/__init__.py
 Comment: 
 
 Filename: optilogic/pioneer/job_utils/job_utils.py
 Comment: 
 
-Filename: optilogic-2.3.0.dist-info/LICENSE
+Filename: optilogic-2.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: optilogic-2.3.0.dist-info/METADATA
+Filename: optilogic-2.4.0.dist-info/METADATA
 Comment: 
 
-Filename: optilogic-2.3.0.dist-info/WHEEL
+Filename: optilogic-2.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: optilogic-2.3.0.dist-info/top_level.txt
+Filename: optilogic-2.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: optilogic-2.3.0.dist-info/RECORD
+Filename: optilogic-2.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## optilogic/pioneer/api/api.py

```diff
@@ -27,28 +27,30 @@
 stack: software environment configuration used to build a a workspace from ie linux based pythonic world with specific configuration
 ws,wksp,workspace: software environment instance built from a stack and has associated file storage
 workspace file storage: standard file storage that is bound to specific workspace instance
 '''
 
 import os
 import pickle
-import requests
 import time
 from cachetools import cached, TTLCache
 from datetime import datetime
 from getpass import getpass
-from json import loads
+from json import dumps, loads
 from os import getenv
+from re import fullmatch
+from requests import ConnectionError, delete, get, HTTPError, Response, request
 from sys import platform
 from tempfile import gettempdir
 from types import FrameType
-from typing import Any, Dict, List, Literal, Tuple, Optional
+from typing import Any, cast, Dict, List, Literal, Tuple, Optional
 from warnings import warn, warn_explicit
 
 if platform == 'linux':
+    from inspect import currentframe
     from signal import alarm, signal, SIGALRM
 
 
 class Api:
     '''Optilogic API is to pilot Andromeda system
 
     EXAMPLE CONFIG
@@ -65,21 +67,24 @@
 
     DATABASE_TEMPLATES: Tuple[str, ...] = (
         'empty',
         'anura_2_4_clean',
         'anura_2_5_clean',
         'anura_2_4_blast_off_to_space',
         'anura_2_5_blast_off_to_space',
-        '12d0e352-8f06-45aa-b3ba-58d6d93d3186',
-        '37392edb-d582-4ce8-9f75-024651aa8592',
-        '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
-        '71214744-f90a-4dcc-8008-bb9dab9493be',
-        '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
-        'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
-        'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
+        'anura_2_6_clean',
+        '12d0e352-8f06-45aa-b3ba-58d6d93d3186',  # Blast Off To Space (BOTS)
+        '37392edb-d582-4ce8-9f75-024651aa8592',  # Tactical Capacity Optimization
+        '67a881b3-b6ab-4e95-9452-6dae8e831a6b',  # Detailed Facility Selection
+        '71214744-f90a-4dcc-8008-bb9dab9493be',  # Greenfield Facility Selection
+        '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',  # China Exit Strategy in Asia
+        'b30d9156-d00b-11ed-b72f-9fd8ea75300c',  # Fleet Size Optimization - US Geo
+        'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',  # Out of China
+        'd3cba0d2-d00a-11ed-b72f-9fd8ea75300c',  # Fleet Size Optimization - EMEA Geo
+        'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',  # Multi-Year Capacity Planning
     )
     GEO_PROVIDERS: Tuple[str, ...] = ('bing', 'google', 'mapbox', 'pcmiler', 'ptv')
     JOBSTATES: Tuple[str, ...] = (
         'submitted',
         'starting',
         'started',
         'running',
@@ -111,29 +116,30 @@
     def __init__(
         self,
         auth_legacy=True,
         version: int = 0,
         appkey: Optional[str] = None,
         un: Optional[str] = None,
         pw: Optional[str] = None,
+        ut=False,
         **cfg,
     ):
         env_domain: Optional[str] = getenv('OPTILOGIC_API_URL')
         env_username: Optional[str] = getenv('OPTILOGIC_USERNAME')
         self._domain: str = 'https://api.optilogic.app'
         self.api_crash_count: int = 0
         self.auth_apikey_expiry: int = 0
         self.auth_apikey_mins_left: float = 0.0
         self.auth_apikey: Optional[str] = None
         self.auth_appkey: Optional[str] = appkey
         self.auth_method_legacy: bool = auth_legacy
         self.auth_req_header: dict = {
             'x-api-key': self.auth_apikey,
             'x-app-key': self.auth_appkey,
-            'x-opti-client': 'optipy',
+            'optilogic-client': 'optipy',
         }
         self.auth_username: Optional[str] = un
 
         if env_domain and env_domain != self._domain:
             self._domain = env_domain
         elif cfg.get('test_system'):
             self._domain = 'https://dev-api.optilogic.app'
@@ -143,22 +149,22 @@
                 f'API version {version} not supported. Defaulting to version zero',
                 UserWarning,
                 stacklevel=1,
             )
             version = 0
 
         self.api_version: str = f'{self._domain}/v{version}/'
-
+        self.unit_test: bool = ut
         if self.auth_method_legacy:
             self.auth_req_header.pop('x-app-key')
             warn_explicit(
                 'legacy userpass authentication, please use modern appkey auth https://optilogic.app/#/user-account?tab=appkey',
                 FutureWarning,
                 __file__,
-                73,
+                153,
             )
 
             if self.auth_username is None:
                 self.auth_username = (
                     env_username if env_username else self.__input_timed('REQUIRED API Username')
                 )
             assert len(self.auth_username) > 0
@@ -181,17 +187,21 @@
             if len(self.auth_appkey) != 51 or self.auth_appkey.startswith('op_') is False:
                 raise ValueError(
                     'Valid appkey is required: https://optilogic.app/#/user-account?tab=appkey'
                 )
 
             self.auth_req_header['x-app-key'] = self.auth_appkey
 
-        self.debug_requests: bool = True
+        self.system: Literal['prod', 'test'] = (
+            'test' if self._domain.find('dev-api') > -1 else 'prod'
+        )
         self.dir_tmp: str = os.path.join(gettempdir(), 'optilogic')
         self.file_cache_auth: str = os.path.join(self.dir_tmp, f'auth_{self.auth_username}.pkl')
+        self.file_log_http: str = os.path.join(self.dir_tmp, f'calls_{self.system}.csv')
+        self._log_active: bool = False
 
         if os.path.exists(self.dir_tmp) is False:
             os.makedirs(self.dir_tmp)
         if os.path.exists(self.file_cache_auth) is False and self.auth_method_legacy:
             self.authenticate_legacy()
 
         self._job_start_recent_key: str = self.__job_key_recent()
@@ -199,17 +209,16 @@
         if cfg.get('user_info'):
             self.__account_summary_info()
 
     def __account_summary_info(self) -> None:
         '''display account information'''
 
         print('\nDATETIME'.ljust(25), datetime.now())
-        info: dict = self._account_summary()
-        for k, v in info.items():
-            print(f'{k.ljust(25)}{v}')
+        resp = self.account_info()
+        print(dumps(resp, sort_keys=True, indent=2))
 
     def __alarm_handler(self, signum: int, frame: Optional[FrameType]) -> None:
         '''timeout expired'''
         raise TimeoutError('timeout expired waiting for user input')
 
     def __batch_input_validation(self, batch: dict, find=False) -> None:
         '''assert batch input structure'''
@@ -222,26 +231,67 @@
 
         for item in batch['batchItems']:
             if find and item.get('pySearchTerm') is None:
                 raise KeyError('pySearchTerm key is missing')
             if find is False and item.get('pyModulePath') is None:
                 raise KeyError('pyModulePath key is missing')
 
-    def __does_storage_exist(self, type: str, name: Optional[str] = None) -> bool:
+    def _database_templates_by_name(self, name: str, wildcard=False) -> List[str]:
+        '''find all database template ids by case-insensitive template name
+
+        :param name: str: template name
+        :param wildcard: str: substr matching, default to False
+        '''
+
+        # template[name, id]
+        template_map: Dict[str, str] = {
+            'empty': 'Empty Database',
+            'Anura - Blast off to Space': 'anura_2_5_blast_off_to_space',
+            'Anura - New Model': 'anura_2_5_clean',
+            'Anura - New Model (2.6)': 'anura_2_6_clean',
+            'Blast Off To Space (BOTS)': '12d0e352-8f06-45aa-b3ba-58d6d93d3186',
+            'China Exit Strategy in Asia': '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
+            'Detailed Facility Selection': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
+            'Fleet Size Optimization - EMEA Geo': 'd3cba0d2-d00a-11ed-b72f-9fd8ea75300c',
+            'Fleet Size Optimization - US Geo': 'b30d9156-d00b-11ed-b72f-9fd8ea75300c',
+            'Greenfield Facility Selection': '71214744-f90a-4dcc-8008-bb9dab9493be',
+            'Multi-Year Capacity Planning': 'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
+            'Out of China': 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
+            'Tactical Capacity Optimization': '37392edb-d582-4ce8-9f75-024651aa8592',
+        }
+
+        matches: List[str] = []
+        name = name.lower()
+
+        for t in template_map.items():
+            if wildcard is False:
+                # case-insensitive
+                if name == t[0].lower():
+                    matches.append(t[1])
+            else:
+                # substring case-insensitive
+                if t[0].lower().find(name) > -1:
+                    matches.append(t[1])
+
+        return matches
+
+    def __does_storage_exist(
+        self,
+        type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db'],
+        name: Optional[str] = None,
+    ) -> bool:
         '''does the account have a specific storage type and optionally by name
 
         :param type: str: storage type
         :param name: str: storage name of type, defaults to None
         '''
 
         assert type in self.STORAGE_DEVICE_TYPES
         exists: List[bool]
         devices = self.account_storage_devices()
-        # BUG storage name is not exclusive to type
-        # was to allow users to not have to know the type
 
         if name is None:
             exists = [True for d in devices['storages'] if d['type'] == type]
         else:
             exists = [True for d in devices['storages'] if d['type'] == type and d['name'] == name]
 
         return True if len(exists) >= 1 else False
@@ -255,33 +305,58 @@
     def __input_timed(self, question: str, max_secs: int = 30, pw: bool = False) -> str:
         '''linux only user input prompt with max time in seconds to complete'''
 
         answer: str = ''
         question = f'\n{question.strip()} -> '
 
         if platform == 'linux':
+            if self.unit_test:
+                max_secs = 1
             signal(SIGALRM, self.__alarm_handler)
             alarm(max_secs)
             try:
                 answer = getpass(question) if pw else input(question)
             finally:
                 alarm(0)
         else:
             answer = getpass(question) if pw else input(question)
 
         return answer
 
+    def __log_http_request(self, rsp: Response) -> None:
+        '''log http requests to disk
+
+        :param: resp Response: result from a http request
+        '''
+
+        if platform != 'linux':
+            # stack frame is not guaranteed
+            return
+
+        # linter friendly even though we guarded against stack frame being None
+        fn: str = cast(
+            FrameType, cast(FrameType, cast(FrameType, currentframe()).f_back).f_back
+        ).f_code.co_name
+
+        secs: float = round(rsp.elapsed.total_seconds(), 4)
+        verb: str = rsp.request.method.upper() if rsp.request.method else ''
+        now: datetime = datetime.utcnow()
+        with open(self.file_log_http, 'a+') as f:
+            f.write(f'{now},{secs},{fn.upper()},{rsp.status_code},{verb},{rsp.url}\n')
+
     def __schema_anura_versions(self) -> List[str]:
         '''list of anura schemas tha major.minor.build'''
 
         resp = self.database_schemas()
         versions: List[str] = [s['schemaVersion'] for s in resp['schemas']['anura']]
         return sorted(versions, reverse=True)
 
-    def __storage_type(self, type: str) -> list:
+    def __storage_type(
+        self, type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db']
+    ) -> list:
         '''specific storage type list
 
         :param type: str: storage type
         '''
 
         assert type in self.STORAGE_DEVICE_TYPES
         resp = self.account_storage_devices()
@@ -299,36 +374,14 @@
     @property
     def _account_jobs_count(self) -> int:
         '''all-time user jobs ran'''
 
         resp = self._account_jobs(1)
         return resp.get('totalCount', 0)
 
-    def _account_summary(self) -> dict:
-        '''aggregated account information'''
-
-        resp = self.account_info()
-        d = {
-            'API DOMAIN': self._domain,
-            'API USERNAME': self.auth_username,
-            'API AUTH METHOD LEGACY': self.auth_method_legacy,
-            'JOB MAX CONCURRENCY': resp.get('apiConcurrentSolvesMax'),
-            'JOB HISTORY TOTAL': self._account_jobs_count,
-            'JOB ACTIVE': self._jobs_active,
-            'DATABASE': self.database_count,
-            'WORKSPACE': self.account_workspace_count,
-            'DISK SSD': self.storagetype_count('azure_afs'),
-            'DISK ONEDRIVE': self.storagetype_count('onedrive'),
-        }
-
-        if self.auth_method_legacy:
-            d['API KEY MINS LEFT'] = self.auth_apikey_mins_left
-
-        return d
-
     @cached(cache=TTLCache(maxsize=1, ttl=60))
     def _account_usage(self):
         '''GET /v0/usage/stats - Atlas and Andromeda information'''
 
         warn('account_usage_stats method is experimental', UserWarning, stacklevel=2)
         url = f'{self.api_version}usage/stats'
         resp = self._fetch_json('get', url)
@@ -399,36 +452,32 @@
 
         if extra_headers:
             extra_headers.update(self.auth_req_header)
             headers = extra_headers
         else:
             headers = self.auth_req_header
 
-        resp = requests.Response()
+        resp: Response = Response()
         ret: dict = {}
         try:
-            resp = requests.request(method, url, headers=headers, json=json, data=data)
+            resp = request(method, url, headers=headers, json=json, data=data)
+            if self._log_active:
+                self.__log_http_request(resp)
             resp.raise_for_status()
             ret = resp.json()
-            if self.debug_requests and resp.status_code == 200 and ret.get('result') == 'error':
-                print(url, ret)
-        except requests.HTTPError as he:
+        except HTTPError as he:
             self.api_crash_count += 1
-            if self.debug_requests:
-                print(
-                    f"\nException: {resp.status_code} {method} {url}\n\nResponse Body Text:\n\n{resp.text}\n\n{he}\n"
-                )
             ret = {
                 'crash': True,
                 'url': url,
                 'resp': resp,
                 'response_body': resp.text,
                 'exception': he,
             }
-        except requests.ConnectionError as ce:
+        except ConnectionError as ce:
             if self._domain != 'https://api.optilogic.app':
                 print(f'{self._domain} != https://api.optilogic.app')
                 ret = {'crash': True, 'url': url, 'exception': ce}
         except ValueError as ve:
             # JSONDecodeError
             ret = {'crash': True, 'url': url, 'exception': ve, 'text': resp.text}
 
@@ -524,46 +573,50 @@
                     matches.append(s)
             elif params == 1:
                 if (n == name) or (t and t == category) or (d and desc and d.find(desc) > -1):
                     matches.append(s)
 
         return matches
 
-    @cached(cache=TTLCache(maxsize=1, ttl=60))
-    def account_info(self) -> dict:
+    @cached(cache=TTLCache(maxsize=1, ttl=86400))
+    def account_info(self) -> Dict[str, Any]:
         '''GET v0/account - Get wksp and user information about the account'''
 
         url: str = self.api_version + 'account'
         resp = self._fetch_json('get', url)
         if self.auth_username is None:
             self.auth_username = resp['username']
         return resp
 
-    def account_storage_device(self, type: str, name: Optional[str] = None) -> dict:
+    def account_storage_device(
+        self,
+        type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db'],
+        name: Optional[str] = None,
+    ) -> dict:
         '''get the first storage device by type or with optional name
 
         :param type: str: storage device type
         :param name: str: storage device name, defaults to None
         '''
 
         device: dict = {}
-        if self.__does_storage_exist(type, name) is True:
+        if self.__does_storage_exist(type=type, name=name) is True:
             devices = self.account_storage_devices()
             for d in devices['storages']:
                 if d['type'] == type and name is None:
                     device = d
                     break
                 elif d['type'] == type and d['name'] == name:
                     device = d
                     break
 
         return device
 
-    @cached(cache=TTLCache(maxsize=1, ttl=60))
-    def account_storage_devices(self) -> dict:
+    @cached(cache=TTLCache(maxsize=1, ttl=10))
+    def account_storage_devices(self) -> Dict[str, Any]:
         '''GET ​/v0​​/storage - Get a list of available storage devices in an account'''
 
         url: str = self.api_version + 'storage'
         resp = self._fetch_json('get', url)
         return resp
 
     def account_workspace_create(self, name: str, stack: str = 'Gurobi') -> dict:
@@ -596,30 +649,30 @@
         resp = self._fetch_json('get', url)
         return resp
 
     @property
     def api_server_online(self) -> bool:
         '''check if API service is up and running'''
 
-        response = requests.request('get', f'{self.api_version}ping')
+        response: Response = request('get', f'{self.api_version}ping')
         return True if response.status_code == 200 else False
 
     def authenticate_legacy(self) -> None:
         '''POST ​/refreshApiKey - Legacy auth method that will create an Api Key via username and password
 
         App Key is the preferred auth method for API usage
         https://atlas.optilogic.app/dashboard/#/user-account?tab=appkey
 
         '''
 
         # do a fetch, process response, then set
         url: str = self.api_version + 'refreshApiKey'
         headers: dict = {'x-user-id': self.auth_username, 'x-user-password': self.auth_userpass}
 
-        response = requests.request('post', url, headers=headers)
+        response: Response = request('post', url, headers=headers)
         response.raise_for_status()
         resp: dict = response.json()
 
         # set instance members
         self.auth_apikey = resp['apiKey']
         self.auth_apikey_expiry = int(resp['expirationTime'])
         self.auth_apikey_mins_left = round(
@@ -633,56 +686,50 @@
             'expiration_time': self.auth_apikey_expiry,
             'user': self.auth_username,
         }
 
         with open(self.file_cache_auth, 'wb') as f:
             pickle.dump(auth_pickle, f)
 
+    def database(self, name: str) -> Dict[str, Any]:
+        '''wrapper for get storage device
+
+        :param name: str: postgres database name
+        '''
+
+        return self.storage(name)
+
     @property
     def database_count(self) -> int:
         ''' 'quantity of a postgres databases in account'''
         return self.storagetype_count('postgres_db')
 
     def database_create(
-        self, name: str, desc: Optional[str] = None, template='anura_2_5_clean'
+        self, name: str, desc: Optional[str] = None, template='anura_2_6_clean'
     ) -> dict:
         '''POST /storage/{database_name} - create a postgres database
 
         :param name: str: name of postgres database to create
         :param desc: str describe the new database use case
         :param template: str database template id
             empty,
-            anura_2_4_clean,
             anura_2_5_clean,
-            anura_2_4_blast_off_to_space,
-            anura_2_5_blast_off_to_space
+            anura_2_5_blast_off_to_space,
+            anura_2_6_blast_off_to_space,
         '''
 
-        # template[name, id]
-        template_map: Dict[str, str] = {
-            'empty': 'Empty Database',
-            'Anura - Blast off to Space (2.4)': 'anura_2_4_blast_off_to_space',
-            'Anura - New Model (2.4)': 'anura_2_4_clean',
-            'Anura - Blast off to Space': 'anura_2_5_blast_off_to_space',
-            'Anura - New Model': 'anura_2_5_clean',
-            'Blast Off To Space (BOTS)': '12d0e352-8f06-45aa-b3ba-58d6d93d3186',
-            'China Exit Strategy in Asia': '812d6fae-9fe7-4541-bee5-3cdb78e03eeb',
-            'Detailed Facility Selection': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
-            'Greenfield Facility Selection': '71214744-f90a-4dcc-8008-bb9dab9493be',
-            'Multi-Year Capacity Planning': 'd74fd9cc-e829-4ad3-9f33-f4ef0f3ddae1',
-            'Out of China': 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6',
-            'Tactical Capacity Optimization': '37392edb-d582-4ce8-9f75-024651aa8592',
-        }
-        template_names: List[str] = [name for name in template_map.keys()]
-
         if template not in self.DATABASE_TEMPLATES:
-            if template in template_names:
-                template = template_map[template]
+            # try to find id by looking up by name
+            template_ids: List[str] = self._database_templates_by_name(name)
+            if len(template_ids) > 0:
+                raise ValueError(
+                    f'invalid database template id {template}, possible matches {template_ids}'
+                )
             else:
-                raise AssertionError(f'invalid database template id {template}')
+                raise ValueError(f'invalid database template id {template}')
 
         if self.storagename_database_exists(name):
             raise AssertionError(f'storage of name {name} already exists')
 
         db_count: int = self.database_count
         emp: bool = True if self.account_info()['email'].find('@optilogic.com') > 0 else False
         if emp and db_count > 85:
@@ -697,28 +744,50 @@
         resp = self._fetch_json('post', url, json=d)
         return resp
 
     def database_delete(self, name: str) -> dict:
         '''wrapper for storage_delete'''
         return self.storage_delete(name)
 
+    def database_objects(
+        self, name: str, tables: bool = True, views: bool = True
+    ) -> Dict[str, Any]:
+        '''GET /storage/{database_name}/objects - returns list of schemas with relative tables and views
+
+        :param name: str: postgres database name
+        :param tables: bool: include list of tables, defaults to True
+        :param views: bool: include list views tables, defaults to True
+        '''
+
+        url: str = f'{self.api_version}storage/{name}/db-objects'
+        if not (tables and views) or (tables is False and views is False):
+            query: str = '?type='
+            query += 'tables,' if tables else ''
+            query += 'views' if views else ''
+            query = query.rstrip(',')
+            url += query
+
+        resp: Dict[str, Any] = self._fetch_json('get', url)
+        return resp
+
     @cached(cache=TTLCache(maxsize=1, ttl=86400))
     def database_schemas(self) -> dict:
         '''postgres database schemas and versions'''
 
         url: str = f'{self.api_version}storage/db-versions'
         resp = self._fetch_json('get', url)
         return resp
 
     def database_tables(self, name: str) -> dict:
         '''GET /storage/{database_name}/tables - returns list of schemas and tables
 
         :param name: str: postgres database name
         '''
 
+        warn('Deprecated, please use database_objects method', FutureWarning, stacklevel=2)
         url: str = f'{self.api_version}storage/{name}/tables'
         resp = self._fetch_json('get', url)
         return resp
 
     def database_tables_empty(self, name: str, tables: List[str], dry_run: bool = False) -> dict:
         '''POST /storage/{database_name}/empty-tables - remove all data from specified tables
 
@@ -744,16 +813,15 @@
 
         :param database_name: str: postgres database name
         :param ip: str: ip address to allow through the firewall, defaults to None
 
         omitted ip address will use the client's ip address that is making the request
         '''
 
-        exists: bool = self.storagename_database_exists(database_name)
-        if exists is False:
+        if self.storagename_database_exists(database_name) is False:
             raise AssertionError(f'postgres database {database_name} does not exist')
 
         url: str = f'{self.api_version}storage/{database_name}/ip-in-firewall'
         if ip:
             url += f'?ipAddress={ip}'
 
         time.sleep(1)
@@ -765,16 +833,15 @@
 
         :param database_name: str: postgres database name
         :param ip: str: ip address to allow through the firewall, defaults to None
 
         omitted ip address will use the client's ip address that is making the request
         '''
 
-        exists: bool = self.storagename_database_exists(database_name)
-        if exists is False:
+        if self.storagename_database_exists(database_name) is False:
             raise AssertionError(f'postgres database {database_name} does not exist')
 
         url: str = f'{self.api_version}storage/{database_name}/ip-in-firewall'
         if ip:
             url += f'?ipAddress={ip}'
 
         time.sleep(1)
@@ -886,15 +953,15 @@
             f'{self.api_version}secrets?type={category}'
             if category
             else f'{self.api_version}secrets'
         )
         resp = self._fetch_json('get', url)
         return resp
 
-    def storage(self, name: str) -> dict:
+    def storage(self, name: str) -> Dict[str, Any]:
         '''GET /v0/storage/{storageName} - storage device info
 
         :param name: str: storage device name
         '''
 
         url: str = f'{self.api_version}storage/{name}'
         resp = self._fetch_json('get', url)
@@ -902,19 +969,18 @@
 
     def storage_delete(self, name: str) -> dict:
         '''DELETE /v0/storage/{storageName}
 
         :param name: str: storage device name
         '''
 
-        warn('storage_delete method is experimental', UserWarning, stacklevel=2)
         url: str = f'{self.api_version}storage/{name}'
-        response = requests.delete(url=url, headers=self.auth_req_header)
+        response: Response = delete(url=url, headers=self.auth_req_header)
         response.raise_for_status()
-        # TODO currently always returns 200 status code and empty response body
+        # returns 204 status code which has no content
         return {'result': 'success'}
 
     def storage_disk_create(self, name: str, type: str = 'hdd', size_gb: int = 10) -> dict:
         '''POST /v0/storage/{storageName} - create a new file storage device
 
         :param name: str: name of new file storage device
         :param type: str: hdd or ssd
@@ -940,15 +1006,17 @@
         '''does the account have OneDrive file storage by name'''
         return self.__does_storage_exist('onedrive', name)
 
     def storagename_wkspfiles_exists(self, name: str) -> bool:
         '''does the account have workspace file storage by name'''
         return self.__does_storage_exist('azure_workspace', name)
 
-    def storagetype_count(self, type: str) -> int:
+    def storagetype_count(
+        self, type: Literal['azure_afs', 'azure_workspace', 'onedrive', 'postgres_db']
+    ) -> int:
         '''quantity of a storage type'''
         count: int = len(self.__storage_type(type))
         return count
 
     @property
     def storagetype_accountfiles_exists(self) -> bool:
         '''does the account have account file storage'''
@@ -993,37 +1061,71 @@
         d: Dict[str, str] = {'query': query}
 
         url: str = f'{self.api_version}storage/{database_name}/query-sql'
         resp = self._fetch_json('post', url, json=d)
         return resp
 
     def util_job_monitor(
-        self, wksp: str, job_key: str, stop_when: str = 'running', secs_max: int = 120
+        self,
+        wksp: str,
+        job_key: str,
+        stop_when: Literal[
+            'submitted',
+            'starting',
+            'started',
+            'running',
+            'done',
+            'stopping',
+            'stopped',
+            'canceling',
+            'cancelled',
+            'error',
+        ] = 'running',
+        secs_max: int = 180,
     ) -> bool:
         '''poll a job for status'
 
         :param wksp: str: workspace where relevant jobs belong to
         :param job_key: str: which job to monitor
         :param stop_when: str: return when job state matches, defaults to running
-        :param secs_max: int: hard stop then return when wait time is exhausted, defaults to 100
+        :param secs_max: int: hard stop when wait time is exhausted, defaults to 180
 
         STOP WHEN STATES
         'submitted','starting','started','running','done','stopping','stopped','canceling','cancelled','error'
 
         fail-safe stop will occur when job is in a terminal state
         '''
 
-        stop_when = stop_when.lower()
-        assert stop_when in self.JOBSTATES
+        if stop_when not in self.JOBSTATES:
+            raise ValueError(f'expecting job state in {str(self.JOBSTATES)}')
+
+        # uuid4 pattern
+        pat = '(?:[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}|00000000-0000-0000-0000-000000000000)'
+
+        # validate uuid4 job key case-insensitive
+        valid_job_key = bool(fullmatch(pat, job_key, flags=2))
+        if valid_job_key is False:
+            raise ValueError(f'job_key is invalid uuid4 {job_key}')
 
         time_start: float = time.time()
+        secs: float = 2.5
+        retry: bool = True
         while True:
-            time.sleep(5)
+            time.sleep(secs)
             resp = self.wksp_job_status(wksp, job_key)
+            if retry and resp.get('crash'):
+                retry = False
+                continue
+            elif retry is False and resp.get('crash'):
+                return False
             time_delta: float = time.time() - time_start
+            if 17 < time_delta < 200:
+                secs = round(time_delta * 0.15, 2)
+            elif time_delta > 200:
+                secs = 30
             if time_delta >= secs_max:
                 return False
             elif resp['status'] in self.JOBSTATES_TERMINAL or resp['status'] == stop_when:
                 return True
 
     def wksp_file_copy(
         self, wksp: str, file_path_src: str, file_path_dest: str, overwrite: bool = False
@@ -1064,15 +1166,15 @@
 
         :param wksp: str: workspace to download from
         :param file_path: str: file to download
         '''
 
         dir_path, file_name = os.path.split(file_path)
         url: str = f'{self.api_version}{wksp}/file/{dir_path}/{file_name}?op=download'
-        response = requests.get(url=url, headers=self.auth_req_header)
+        response: Response = get(url=url, headers=self.auth_req_header)
         return response.text
 
     def wksp_file_download_status(self, wksp: str, file_path: str) -> dict:
         '''GET ​/v0​/{workspace}​/file​/{directoryPath}​/{filename} - Get a file metadata
 
         :param wksp: str: workspace to download from
         :param file_path: str: file to download
@@ -1249,26 +1351,26 @@
         '''GET ​/v0​/{workspace}​/job​/{jobKey} - Get job error file
 
         :param wksp: str: workspace where job exists
         :param jobkey: str: unique id
         '''
 
         url: str = f'{self.api_version}{wksp}/job/{jobkey}?op=error'
-        resp = requests.request('get', url, headers=self.auth_req_header)
+        resp: Response = request('get', url, headers=self.auth_req_header)
         return resp.content.decode('utf-8')
 
     def wksp_job_file_result(self, wksp: str, jobkey: str) -> str:
         '''GET ​/v0​/{workspace}​/job​/{jobKey} - Get job result file
 
         :param wksp: str: workspace where job exists
         :param jobkey: str: unique id
         '''
 
         url: str = f'{self.api_version }{wksp}/job/{jobkey}?op=result'
-        resp = requests.request('get', url, headers=self.auth_req_header)
+        resp: Response = request('get', url, headers=self.auth_req_header)
         return resp.content.decode('utf-8')
 
     def wksp_job_ledger(self, wksp: str, jobkey: str, keys: Optional[str] = None) -> dict:
         '''GET /{workspace}/job/{jobKey}/ledger - Get records from a specified job
 
         :param wksp: str: workspace scope
         :param jobkey: str: unique id
@@ -1341,15 +1443,14 @@
             url += f'&directoryPath={dir_path}&filename={file_name}'
 
         url += f'&commandArgs={commandArgs}' if commandArgs else ''
         url += f'&tags={tags}' if tags else ''
         url += f'&timeout={timeout}' if timeout else ''
         url += f'&resourceConfig={resourceConfig}' if resourceConfig else ''
 
-        now: datetime = datetime.utcnow()
         resp = self._fetch_json('post', url)
         self._job_start_recent_key = resp['jobKey'] if resp.get('jobKey') else ''
 
         return resp
 
     def wksp_job_status(self, wksp: str, jobkey: str) -> dict:
         '''GET ​/v0​/{workspace}​/job​/{jobKey} - Get job status
@@ -1467,15 +1568,15 @@
         :param wksp: str: where your files live
 
         QUERYSTRING PARAMETERS
         :param command: str: run, presolve, run_custom, run_default, supplychain-3echelon, estimate, accelerate, supplychain-2echelon, defaults to None
         :param history: str: all, or n days ago, defaults to None
         :param runSecsMax: int: maximum runtime in secs, defaults to None
         :param runSecsMin: int: minimum runtime in secs, defaults to None
-        :param status: str: done, error, submitted, starting, running, cancelled, stopped, cancelling, stopping, defaults to None
+        :param status: str: done, error, submitted, starting, running, cancelled, stopped, canceling, stopping, defaults to None
         :param tags: str: filter jobs where csv string matches, defaults to None
         '''
 
         url: str = f'{self.api_version}{wksp}/jobs'
         query: str = ''
         query += f'&command={command}' if command else ''
         query += f'&history={history}' if history else ''
@@ -1490,29 +1591,42 @@
         return resp
 
     def wksp_jobs_stats(
         self,
         wksp: str,
         command: Optional[str] = None,
         history: Optional[str] = None,
-        status: Optional[str] = None,
+        status: Optional[
+            Literal[
+                'submitted',
+                'starting',
+                'started',
+                'running',
+                'done',
+                'stopping',
+                'stopped',
+                'canceling',
+                'cancelled',
+                'error',
+            ]
+        ] = None,
         runSecsMin: Optional[int] = None,
         runSecsMax: Optional[str] = None,
         tags: Optional[str] = None,
     ) -> dict:
         '''GET ​/v0​/{workspace}​/jobs/stats - Get the stats for jobs for a specific workspace
 
         :param wksp: str:  where your files live
 
         QUERYSTRING PARAMETERS
         :param command: str: run, presolve, run_custom, run_default, supplychain-3echelon, estimate, accelerate, supplychain-2echelon, defaults to None
         :param history: str: all, n days ago, None is 7 days ago, defaults to None
         :param runSecsMax: int: maximum runtime in secs, defaults to None
         :param runSecsMin: int: minimum runtime in secs, defaults to None
-        :param status: str: done, error, submitted, starting, running, cancelled, stopped, cancelling, stopping, defaults to None
+        :param status: str: done, error, submitted, starting, running, cancelled, stopped, canceling, stopping, defaults to None
         :param tags: str: filter jobs where csv string matches, defaults to None
         '''
 
         url: str = f'{self.api_version}{wksp}/jobs'
         query: str = ''
         query += f'&command={command}' if command else ''
         query += f'&history={history}' if history else ''
```

## optilogic/pioneer/api/api_tests.py

```diff
@@ -26,35 +26,36 @@
 from dateutil.parser import parse
 from datetime import date, datetime, timedelta
 from docopt import docopt
 from io import StringIO
 from json import dumps, loads
 from random import randint
 from sys import platform
-from typing import Tuple, Optional
+from typing import Any, Dict, List, Tuple, Optional
+from warnings import warn
 from uuid import uuid4
 
 
 class TestApi(unittest.TestCase):
     '''A series of Pioneer REST API unit tests
 
     OVERRIDE
     docopt configuration passed into the module will override the default static members
 
     STATIC MEMBERS
     USERNAME    required to be issued api key
     USERPASS    required to be issued api key
-    WORKSPACE   defines what storage account to use for IO operations
+    WKSP        file storage to use for IO operations
     APPKEY      non expiring authentication key
     AUTH_LEGACY true for legacy username password authentication method
     '''
 
     USERNAME: Optional[str] = None
     USERPASS: Optional[str] = None
-    WORKSPACE: str = 'Studio'
+    WKSP: str = 'Studio'
     APPKEY: Optional[str] = None
     AUTH_LEGACY: bool = False
 
     # method execution order - unittest.TestLoader.sortTestMethodsUsing
     # default string sort of class method names startswith test_{method-name}, ie dir(self)
 
     @classmethod
@@ -65,29 +66,30 @@
 
         cls.API = api.Api(
             auth_legacy=TestApi.AUTH_LEGACY,
             appkey=TestApi.APPKEY,
             un=TestApi.USERNAME,
             pw=TestApi.USERPASS,
         )
-        cls.API.debug_requests = False
+        cls.__jobkey_quick: str = ''
+        cls.API._log_active = True
 
         # directory references
         cls.dir_local_current: str = os.path.dirname(__file__)
         cls.dir_testdata_local: str = os.path.join(cls.dir_local_current, 'quick_tests')
         assert os.path.exists(cls.dir_testdata_local)
         assert len(os.listdir(cls.dir_testdata_local)) >= 1
         cls.dir_testdata_remote: str = 'quick_tests'
         cls.files_testdata_local: list[str] = []
         cls.files_testdata_remote: list[str] = []
         cls.py_run_me: str = ''
         cls.py_run_me_quick: str = ''
 
         # get all directories from wksp
-        resp = cls.API.wksp_files(cls.WORKSPACE, '/quick_tests/')
+        resp = cls.API.wksp_files(cls.WKSP, '/quick_tests/')
         files_remote: list[str] = [f['filePath'] for f in resp['files']]
 
         # comb over local test data and map to destination file structure
         for f in os.listdir(cls.dir_testdata_local):
             local: str = os.path.join(cls.dir_testdata_local, f)
             if os.path.isfile(local) is False:
                 continue
@@ -104,66 +106,185 @@
             # upload local test data to destination
             for idx, local in enumerate(cls.files_testdata_local):
                 dest = cls.files_testdata_remote[idx]
             res: list[str] = [f for f in files_remote if dest in f]
             if len(res) == 0:
                 print(f'uploading {dest}')
                 resp = cls.API.wksp_file_upload(
-                    cls.WORKSPACE, file_path_dest=dest, file_path_local=local
+                    cls.WKSP, file_path_dest=dest, file_path_local=local
                 )
 
+    def database_ensure_exist(self) -> None:
+        '''database must exist for db unit tests'''
+
+        db: str = 'pg_unittest'
+        exists: bool = self.API.storagename_database_exists(db)
+        if exists:
+            resp = self.API.storage(db)
+            assert resp.get('crash') is None
+        else:
+            self.API.database_create(name=db, desc='common db for unit tests')
+
     def date_isoformat(self, date_str: str) -> bool:
         '''is date string isoformat'''
 
         d: date
         try:
             d = date.fromisoformat(date_str)
             return isinstance(d, date)
         except ValueError:
             return False
 
+    def job_prereq(self) -> None:
+        '''for running test methods in isolation'''
+
+        resp = self.API.wksp_job_start(self.WKSP, self.py_run_me_quick, tags='unittest_prereq')
+        self.assertEqual(resp['result'], 'success')
+        self.__jobkey_quick = resp['jobKey']
+        # BUG ledger and metrics should be immediately available when job is running
+        res: bool = self.API.util_job_monitor(self.WKSP, resp['jobKey'], stop_when='done')
+        self.assertTrue(res)
+
+    def storage_common(self, d: dict) -> None:
+        ''''''
+        pass
+
+    def storage_azure_afs(self, d: dict) -> None:
+        '''common ssd response for get device and devices'''
+
+        self.assertIsInstance(d['bytesUsed'], int)
+        self.assertIsInstance(d['capacity'], int)
+        self.assertIsInstance(d['created'], int)
+        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
+        self.assertIsInstance(d['id'], str)
+        self.assertIsInstance(d['internal'], bool)
+        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
+        self.assertIsInstance(d['name'], str)
+        self.assertIsInstance(d['tier'], str)
+        self.assertIsInstance(d['type'], str)
+        self.assertIsInstance(d['updated'], int)
+
+    def storage_azure_workspace(self, d: dict) -> None:
+        '''common wksp response for get device and devices'''
+
+        self.assertIsInstance(d['bytesUsed'], int)
+        self.assertIsInstance(d['capacity'], int)
+        self.assertIsInstance(d['created'], int)
+        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
+        self.assertIsInstance(d['id'], str)
+        self.assertIsInstance(d['internal'], bool)
+        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
+        self.assertIsInstance(d['name'], str)
+        self.assertIsInstance(d['tier'], str)
+        self.assertIsInstance(d['type'], str)
+        self.assertIsInstance(d['updated'], int)
+        self.assertIsInstance(d['workspaceKey'], str)
+
+    def storage_database(self, d: dict) -> None:
+        '''common db response for get device and devices'''
+
+        self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
+        self.assertTrue(
+            d['bytesUsedLastUpdated'] is None or isinstance(d['bytesUsedLastUpdated'], int)
+        )
+        self.assertIsInstance(d['created'], int)
+        self.assertIsInstance(d['dbname'], str)
+        self.assertIsInstance(d['defaultSchema'], str)
+        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
+        self.assertIsInstance(d['host'], str)
+        self.assertIsInstance(d['id'], str)
+        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
+        self.assertIsInstance(d['name'], str)
+        self.assertIsInstance(d['port'], int)
+        self.assertIsInstance(d['schemaStatus'], str)
+        self.assertTrue(
+            d['schemaStatus'] in ('error', 'invalid', 'valid') or len(d['schemaStatus']) == 0
+        )
+        self.assertTrue(
+            d['schemaStatusLastUpdated'] is None or isinstance(d['schemaStatusLastUpdated'], float)
+        )
+        self.assertIsInstance(d['schemaVersion'], str)
+        self.assertIsInstance(d['type'], str)
+        self.assertIsInstance(d['updated'], int)
+        self.assertIsInstance(d['user'], str)
+
+        # empty pg datase vs anura schema
+        if d['defaultSchema'].startswith('anura_2_'):
+            self.assertRegex(d['schemaVersion'], r'2\.[4-9]\.\d+')
+            # BUG 7867 self.assertIsInstance(d['schemaStatusLastUpdated'], float)
+            self.assertIsInstance(d['schemaStatusLastValidated'], float)
+        else:
+            self.assertEqual(d['defaultSchema'], '"$user"')
+            # BUG 7867 self.assertIsNone(d['schemaStatusLastUpdated'])
+            # BUG 7867 self.assertIsNone(d['schemaStatusLastValidated'])
+            self.assertTrue(len(d['schemaVersion']) == 0)
+
+    def storage_onedrive(self, d: dict) -> None:
+        '''common onedrive storage response for get device and devices'''
+
+        self.assertIsInstance(d['accountName'], str)
+        self.assertIsInstance(d['authenticated'], int)
+        self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
+        self.assertIsInstance(d['capacity'], int)
+        self.assertIsInstance(d['created'], int)
+        self.assertTrue(d['description'] is None or isinstance(d['description'], str))
+        self.assertIsInstance(d['endpointSuffix'], str)
+        self.assertIsInstance(d['homeAccountId'], str)
+        self.assertIsInstance(d['id'], str)
+        self.assertIsInstance(d['internal'], bool)
+        self.assertTrue(d['lockoutReason'] is None or isinstance(d['lockoutReason'], str))
+        self.assertIsInstance(d['name'], str)
+        self.assertIsInstance(d['protocol'], str)
+        self.assertIsInstance(d['tier'], str)
+        self.assertIsInstance(d['type'], str)
+        self.assertIsInstance(d['updated'], int)
+        self.assertIsInstance(d['userId'], str)
+        self.assertIsInstance(d['username'], str)
+
     def test_000_init_api_version_bad(self) -> None:
         '''recover from a bad api version provided'''
 
         bad_version: int = 99
 
         with redirect_stderr(StringIO()) as err:
-            a = api.Api(auth_legacy=TestApi.AUTH_LEGACY, version=bad_version)
+            a = api.Api(auth_legacy=TestApi.AUTH_LEGACY, version=bad_version, ut=True)
             output: str = err.getvalue().strip()
 
         self.assertGreater(output.find(f'API version {bad_version} not supported'), -1)
         self.assertRegex(a.api_version, r'app/v0/')
 
     def test_000_init_password_missing(self) -> None:
         '''get password uses a secret stream and input will not echo'''
 
         if platform != 'linux':
             self.skipTest('only linux has timed inputs')
 
         with redirect_stdout(StringIO()) as out:
             try:
-                a = api.Api(auth_legacy=True, un=TestApi.USERNAME)
+                a = api.Api(auth_legacy=True, un=TestApi.USERNAME, ut=True)
             except (EOFError, TimeoutError):
                 pass
             output: str = out.getvalue().strip()
 
         self.assertEqual(output.find('REQUIRED API User Password'), -1)
 
     def test_000_prereqs(self) -> None:
         '''ensure job data is available to test against'''
 
-        self.API.wksp_job_start(self.WORKSPACE, self.py_run_me_quick, tags='unittest_preseed')
+        resp = self.API.wksp_job_start(self.WKSP, self.py_run_me_quick, tags='unittest_preseed')
+        self.assertEqual(resp['result'], 'success')
+        self.__jobkey_quick = resp['jobKey']
         stime: float = time.time()
         print('Pre-seeding by running a new job')
         res: bool = self.API.util_job_monitor(
-            self.WORKSPACE, self.API._job_start_recent_key, stop_when='done', secs_max=300
+            self.WKSP, resp['jobKey'], stop_when='done', secs_max=300
         )
         delta: float = time.time() - stime
         print(f'Job completed {res}, time spent {delta}')
-        self.assertLessEqual(delta, 180.0)
+        self.assertLessEqual(delta, 240.0)
 
     def test_auth_apikey(self) -> None:
         '''api key is required for all api calls with legacy authentication'''
 
         self.assertIsNotNone(
             self.API.auth_apikey
         ) if self.API.auth_method_legacy else self.assertIsNone(self.API.auth_apikey)
@@ -175,46 +296,60 @@
             self.assertGreater(self.API.auth_apikey_expiry, datetime.now().timestamp())
         else:
             self.assertEqual(self.API.auth_apikey_expiry, 0)
 
     def test_auth_header(self) -> None:
         '''request header must have valid apikey or appkey'''
 
-        if self.AUTH_LEGACY:
+        if self.API.auth_method_legacy:
             self.assertEqual(self.API.auth_req_header['x-api-key'], self.API.auth_apikey)
         else:
             self.assertEqual(self.API.auth_req_header['x-app-key'], self.API.auth_appkey)
 
     def test_account_info(self) -> None:
         '''account properties'''
 
         resp = self.API.account_info()
         self.assertEqual(resp['result'], 'success')
+        self.assertEqual(len(resp), 7)
+        self.assertIsInstance(resp['email'], str)
+        self.assertGreater(resp['email'].find('@optilogic.com'), -1)  # assume employee user
+        self.assertIsInstance(resp['name'], str)
+        self.assertGreaterEqual(len(resp['name']), 3)
+        # TODO regex name validation
+        self.assertIsInstance(resp['subscriptionName'], str)
+        self.assertEqual(resp['subscriptionName'], 'empcustom')  # assume employee user
+
+        self.assertIsInstance(resp['limits'], dict)
+        self.assertEqual(len(resp['limits']), 3)
+        self.assertIsInstance(resp['limits']['concurrentJobs'], int)
+        self.assertIsInstance(resp['limits']['databaseCount'], int)
+        self.assertIsInstance(resp['limits']['fileStorageGb'], int)
+        self.assertEqual(resp['limits']['concurrentJobs'], 50)
+        self.assertEqual(resp['limits']['databaseCount'], 100)
+        self.assertEqual(resp['limits']['fileStorageGb'], 500)  # max possible
+
+        self.assertIsInstance(resp['usage'], dict)
+        self.assertEqual(len(resp['usage']), 4)
+        self.assertIsInstance(resp['usage']['databaseCount'], int)
+        self.assertIsInstance(resp['usage']['fileStorageCount'], int)
+        self.assertIsInstance(resp['usage']['fileStorageGb'], int)
+        self.assertIsInstance(resp['usage']['workspaceCount'], int)
+        self.assertGreaterEqual(resp['usage']['databaseCount'], 0)
+        self.assertLessEqual(resp['usage']['databaseCount'], resp['limits']['databaseCount'])
+        self.assertTrue(0 < resp['usage']['fileStorageCount'] < 10)
+        self.assertTrue(0 < resp['usage']['workspaceCount'] < 10)
+        self.assertLessEqual(
+            resp['usage']['fileStorageGb'],
+            resp['limits']['fileStorageGb'] * resp['usage']['fileStorageCount'],
+        )
+
+        self.assertIsInstance(resp['username'], str)
         if self.API.auth_username:
             self.assertEqual(resp['username'], self.API.auth_username)
-        else:
-            self.assertIsInstance(resp['username'], str)
-        self.assertGreaterEqual(resp['apiConcurrentSolvesMax'], 1)
-        self.assertGreaterEqual(resp['workspaceCount'], 1)
-
-    def test_account_summary(self) -> None:
-        '''aggregated account information'''
-
-        d: dict = self.API._account_summary()
-        self.assertIsInstance(d['API DOMAIN'], str)
-        self.assertIsInstance(d['API USERNAME'], str)
-        self.assertIsInstance(d['API AUTH METHOD LEGACY'], bool)
-        self.assertIsInstance(d['JOB MAX CONCURRENCY'], int)
-        self.assertIsInstance(d['JOB HISTORY TOTAL'], int)
-        self.assertIsInstance(d['JOB ACTIVE'], int)
-        self.assertIsInstance(d['DATABASE'], int)
-        self.assertIsInstance(d['WORKSPACE'], int)
-        self.assertIsInstance(d['DISK SSD'], int)
-        if self.API.auth_method_legacy:
-            self.assertIsInstance(d['API KEY MINS LEFT'], float)
 
     def test_account_jobs(self) -> None:
         ''' 'any user job from any workspace'''
 
         job_count: int = 50
         resp = self.API._account_jobs(max_jobs=job_count)
         self.assertIsInstance(resp['jobs'], list)
@@ -261,15 +396,15 @@
                 pass
 
     def test_account_jobs_active(self) -> None:
         '''compare active account jobs count to all active wksp jobs'''
 
         start_new_job: bool = bool(randint(0, 1))
         if start_new_job:
-            self.API.wksp_job_start(self.WORKSPACE, self.py_run_me, tags='unittest_jobs_active')
+            self.API.wksp_job_start(self.WKSP, self.py_run_me, tags='unittest_jobs_active')
 
         active_account: int = 0
         resp = self.API._account_jobs(
             max_jobs=200
         )  # BUG submitted counts as active therefore must account for excessive submitted jobs
         for job in resp['jobs']:
             if job['status'] in self.API.JOBSTATES_ACTIVE:
@@ -289,102 +424,24 @@
         self.assertIsInstance(resp['count'], int)
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['storages'], list)
         with self.subTest():
             for d in resp['storages']:
                 if d['type'] == 'azure_afs':
                     self.assertEqual(len(d), 11)
-                    self.assertIsInstance(d['bytesUsed'], int)
-                    self.assertIsInstance(d['capacity'], int)
-                    self.assertIsInstance(d['created'], int)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['id'], str)
-                    self.assertIsInstance(d['internal'], bool)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['tier'], str)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
+                    self.storage_azure_afs(d)
                 if d['type'] == 'azure_workspace':
                     self.assertEqual(len(d), 12)
-                    self.assertIsInstance(d['bytesUsed'], int)
-                    self.assertIsInstance(d['capacity'], int)
-                    self.assertIsInstance(d['created'], int)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['id'], str)
-                    self.assertIsInstance(d['internal'], bool)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['tier'], str)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
-                    self.assertIsInstance(d['workspaceKey'], str)
+                    self.storage_azure_workspace(d)
                 elif d['type'] == 'onedrive':
                     self.assertEqual(len(d), 18)
-                    self.assertIsInstance(d['accountName'], str)
-                    self.assertIsInstance(d['authenticated'], int)
-                    self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
-                    self.assertIsInstance(d['capacity'], int)
-                    self.assertIsInstance(d['created'], int)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['endpointSuffix'], str)
-                    self.assertIsInstance(d['homeAccountId'], str)
-                    self.assertIsInstance(d['id'], str)
-                    self.assertIsInstance(d['internal'], bool)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['protocol'], str)
-                    self.assertIsInstance(d['tier'], str)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
-                    self.assertIsInstance(d['username'], str)
+                    self.storage_onedrive(d)
                 elif d['type'] == 'postgres_db':
-                    self.assertEqual(len(d), 17)
-                    self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
-                    self.assertTrue(
-                        d['bytesUsedLastUpdated'] is None
-                        or isinstance(d['bytesUsedLastUpdated'], int)
-                    )
-                    self.assertIsInstance(d['created'], int)
-                    self.assertIsInstance(d['dbname'], str)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['host'], str)
-                    self.assertIsInstance(d['id'], str)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['port'], int)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
-                    self.assertIsInstance(d['user'], str)
-                    self.assertIsInstance(d['defaultSchema'], str)
-                    if d['defaultSchema'].startswith('anura_2_'):
-                        self.assertIsInstance(d['schemaStatus'], str)
-                        self.assertTrue(
-                            d['schemaStatus'] in ('error', 'invalid', 'valid')
-                            or len(d['schemaStatus']) == 0
-                        )
-                        self.assertTrue(
-                            d['schemaStatusLastUpdated'] is None
-                            or isinstance(d['schemaStatusLastUpdated'], float)
-                        )
-                        self.assertIsInstance(d['schemaVersion'], str)
-                        self.assertRegex(d['schemaVersion'], r'2\.[4-9]\.\d+')
-                    else:
-                        self.assertEqual(d['defaultSchema'], '"$user"')
-                        self.assertEqual(d['schemaStatus'], 'valid')
-                        self.assertIsNone(d['schemaStatusLastUpdated'])
-                        self.assertTrue(len(d['schemaVersion']) == 0)
+                    self.assertEqual(len(d), 18)
+                    self.storage_database(d)
 
     def test_account_usage(self) -> None:
         '''atlas and andromeda information'''
 
         resp = self.API._account_usage()
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['andromeda'], dict)
@@ -402,28 +459,34 @@
         dt: datetime = datetime.fromtimestamp(resp['andromeda']['jobsMostRecent'] / 1000)
         now: datetime = datetime.utcnow()
         self.assertEqual(dt.year, now.year)
         self.assertEqual(dt.month, now.month)
 
         self.assertIsInstance(resp['atlas'], dict)
         self.assertIsInstance(resp['atlas']['lastLogin'], int)
-        self.assertIsInstance(resp['atlas']['periodHours'], float)
+        if resp['atlas']['periodHours'] == 0:
+            self.assertIsInstance(resp['atlas']['periodHours'], int)
+        else:
+            self.assertIsInstance(resp['atlas']['periodHours'], float)
         self.assertIsInstance(resp['atlas']['task'], dict)
         self.assertIsInstance(resp['atlas']['workspaceCount'], int)
         self.assertEqual(len(resp['atlas']), 4)
         self.assertEqual(len(str(resp['atlas']['lastLogin'])), 13)
         dt: datetime = datetime.fromtimestamp(resp['atlas']['lastLogin'] / 1000)
         self.assertIsInstance(dt, datetime)
 
         self.assertIsInstance(resp['atlas']['task'], dict)
         if resp['atlas']['task']['durationCurrentWeek'] == 0:
             self.assertIsInstance(resp['atlas']['task']['durationCurrentWeek'], int)
         else:
             self.assertIsInstance(resp['atlas']['task']['durationCurrentWeek'], float)
-        self.assertIsInstance(resp['atlas']['task']['durationLastThirty'], float)
+        if resp['atlas']['task']['durationLastThirty'] == 0:
+            self.assertIsInstance(resp['atlas']['task']['durationLastThirty'], int)
+        else:
+            self.assertIsInstance(resp['atlas']['task']['durationLastThirty'], float)
         self.assertIsInstance(resp['atlas']['task']['durationTotal'], float)
         self.assertIsInstance(resp['atlas']['task']['lastDuration'], float)
         self.assertIsInstance(resp['atlas']['task']['lastRunStart'], int)
         self.assertIsInstance(resp['atlas']['task']['runCurrentWeek'], int)
         self.assertIsInstance(resp['atlas']['task']['runlastThirty'], int)
         self.assertIsInstance(resp['atlas']['task']['runTotal'], int)
         self.assertEqual(len(resp['atlas']['task']), 8)
@@ -446,25 +509,25 @@
             self.assertIn(wksp['status'], ['STARTING', 'RUNNING', 'STOPPING', 'STOPPED'])
             self.assertRegex(wksp['status'], '\\w{3,}')
 
             # https://en.wikipedia.org/wiki/ISO_8601
             dt_wksp_creation: datetime = parse(wksp['createdon'])
             self.assertGreaterEqual(dt_wksp_creation.year, 2020)
 
-            if wksp['name'] == self.WORKSPACE:
+            if wksp['name'] == self.WKSP:
                 wksp_exists = True
 
         self.assertTrue(wksp_exists)
 
     def test_account_workspace_count(self) -> None:
         '''account info and workspaces both return wksp count'''
 
         resp = self.API.account_info()
         ws_count: int = self.API.account_workspace_count
-        self.assertEqual(resp['workspaceCount'], ws_count)
+        self.assertEqual(resp['usage']['workspaceCount'], ws_count)
 
     @unittest.skip('cant delete a wksp atm')
     def test_account_workspace_create(self) -> None:
         '''creating a new workspace'''
 
         resp = self.API.account_workspace_create('delme')
         self.assertEqual(resp['result'], 'success')
@@ -493,61 +556,130 @@
         '''only version zero is supported'''
 
         self.assertTrue(self.API.api_version.endswith('v0/'))
 
     def test_database_create_delete(self) -> None:
         '''create a postgres database then delete'''
 
-        # get the latest BOTS template, anura_2_{{n}}_blast_off_to_space
-        bots: list[str] = [t for t in self.API.DATABASE_TEMPLATES if t.find('blast') > -1]
-        bots = sorted(bots, reverse=True)
-        dbname: str = f'pg_{time.perf_counter_ns()}'
+        bots: List[str] = self.API._database_templates_by_name('blast', wildcard=True)
+        self.assertGreaterEqual(len(bots), 1)
+        dbname: str = f'pg_unittest_{time.perf_counter_ns()}'
 
-        resp = self.API.database_create(dbname, desc=f'unittest {dbname}', template=bots[0])
+        # create database
+        resp: dict = self.API.database_create(dbname, desc=f'unittest {dbname}', template=bots[0])
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['storageId'], str)
         self.assertEqual(len(resp['storageId']), 36)
 
+        # verify database was created
+        db_created: bool = False
+        start: float = time.perf_counter()
+        wait_until: float = 60.0
+        while db_created is False and (time.perf_counter() - start) < wait_until:
+            resp = self.API.database(dbname)
+            if resp.get('crash'):
+                continue
+            if resp.get('result') == 'success':
+                db_created = True
+                break
+            time.sleep(2)
+
+        with self.subTest():
+            self.assertTrue(db_created)
+
+        if db_created is False:
+            warn(f'failed to create db within {wait_until} seconds)', UserWarning, stacklevel=2)
+
+        # delete database
         resp = self.API.storage_delete(dbname)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
 
     def test_database_create_delete_wrong_id(self) -> None:
         '''create a postgres database with a template name instead of id'''
 
         tname: str = 'Out of China'
         self.assertFalse(tname in self.API.DATABASE_TEMPLATES)
-        dbname: str = f'pg_{time.perf_counter_ns()}'
+        with self.assertRaises(ValueError):
+            self.API.database_create(name='cant', template=tname)
 
-        # create using an invalid template id
-        resp = self.API.database_create(dbname, desc=f'unittest {dbname} {tname}', template=tname)
+        # get template id
+        tids: List[str] = self.API._database_templates_by_name(tname)
+        self.assertEqual(len(tids), 1)
+        self.assertEqual(tids[0], 'b69f11eb-ed38-4b72-a43d-d59f7ab2cfa6')
+        dbname: str = f'pg_unittest_{time.perf_counter_ns()}'
+
+        # create database with matching template id
+        resp = self.API.database_create(dbname, desc=f'unittest {dbname} {tname}', template=tids[0])
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['storageId'], str)
         self.assertEqual(len(resp['storageId']), 36)
 
-        # verify template name fallback
-        db = self.API.storage(dbname)
+        # verify database creation
+        db = self.API.database(dbname)
         self.assertEqual(db['result'], 'success')
         self.assertEqual(db['name'], dbname)
         self.assertEqual(db['type'], 'postgres_db')
         self.assertEqual(db['id'], resp['storageId'])
         self.assertGreater(db['description'].find(tname), -1)
 
+        # delete database
         resp = self.API.storage_delete(dbname)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
 
-    def test_database_create_failure(self) -> None:
+    def test_database_create_failure_template_bad(self) -> None:
         '''attempt creating a postgres database with bad template id'''
 
-        with self.assertRaises(AssertionError):
+        with self.assertRaises(ValueError):
             self.API.database_create(name='cant', template='does_not_exist')
 
+    def test_database_create_failure_template_swap(self) -> None:
+        '''attempt creating a postgres database with template name instead of id'''
+
+        with self.assertRaises(ValueError):
+            self.API.database_create(name='cant', template='Out of China')
+
+    def test_database_objects(self) -> None:
+        '''tables and views with stats'''
+
+        self.database_ensure_exist()
+        resp: Dict[str, Any] = self.API.database_objects('pg_unittest')
+        self.assertEqual(resp['result'], 'success')
+        self.assertIsInstance(resp['schemas'], list)
+        for schema in resp['schemas']:
+            self.assertEqual(len(schema.keys()), 6)
+            self.assertIsInstance(schema['isDefault'], bool)
+            self.assertIsInstance(schema['name'], str)
+            self.assertIsInstance(schema['tableCount'], int)
+            self.assertIsInstance(schema['tables'], list)
+            self.assertIsInstance(schema['viewCount'], int)
+            self.assertIsInstance(schema['views'], list)
+            self.assertEqual(len(schema['tables']), schema['tableCount'])
+            self.assertEqual(len(schema['views']), schema['viewCount'])
+            for t in schema['tables']:
+                self.assertIsInstance(t['name'], str)
+                self.assertIsInstance(t['rows'], int)
+                self.assertGreaterEqual(len(t['name']), 1)
+                self.assertGreaterEqual(t['rows'], 0)
+
+        # tables only
+        resp: Dict[str, Any] = self.API.database_objects('pg_unittest', views=False)
+        self.assertEqual(resp['result'], 'success')
+        for schema in resp['schemas']:
+            self.assertIsNone(schema.get('views'))
+
+        # views only
+        resp: Dict[str, Any] = self.API.database_objects('pg_unittest', tables=False)
+        self.assertEqual(resp['result'], 'success')
+        for schema in resp['schemas']:
+            self.assertIsNone(schema.get('tables'))
+
     def test_database_schemas(self) -> None:
         '''currently only anura schemas'''
 
         ANURA_STATUS: Tuple[str, ...] = ('current', 'deprecated', 'preview', 'retired')
         resp = self.API.database_schemas()
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['result'], 'success')
@@ -580,17 +712,15 @@
                 if v.get('techPreviewDate'):
                     self.assertIsInstance(v['techPreviewDate'], str)
                     self.assertTrue(self.date_isoformat(v['techPreviewDate']))
 
     def test_database_tables(self) -> None:
         '''list of schemas and tables'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have a postgres database')
-
+        self.database_ensure_exist()
         db = self.API.account_storage_device(type='postgres_db')
         resp = self.API.database_tables(db['name'])
         self.assertIsInstance(resp['result'], str)
         for schema in resp['schemas']:
             self.assertIsInstance(schema['name'], str)
             self.assertIsInstance(schema['tables'], int)
             self.assertIsInstance(schema['is_default_schema'], bool)
@@ -600,28 +730,28 @@
             self.assertIsInstance(resp['tables'][0]['name'], str)
             self.assertIsInstance(resp['tables'][0]['rows'], int)
             self.assertIsInstance(resp['tables'][0]['schema'], str)
 
     def test_database_tables_empty(self) -> None:
         '''clear the data in specified tables'''
 
-        db: str = 'unittest_empty_bom_table'
+        db: str = 'pg_unittest_empty_bom_table'
         tbl: str = 'billsofmaterials'
         anura_versions: list[str] = sorted(
             {t[0:9] for t in self.API.DATABASE_TEMPLATES if t.find('anura') > -1}, reverse=True
         )
         schema: str = anura_versions[0]
 
         # assert db exists
         exists: bool = self.API.storagename_database_exists(db)
         if exists:
             resp = self.API.storage(db)
             schema = resp['defaultSchema']
         else:
-            self.API.database_create(name=db, template=f'{schema}_blast_off_to_space')
+            self.API.database_create(name=db, template=f'{schema}_clean')
 
         # assert db has data
         QUERY_ROWS: str = f'SELECT COUNT(*) FROM {schema}.{tbl}'
         resp = self.API.sql_query(db, QUERY_ROWS)
         rows: int = int(resp['queryResults'][0]['count'])
         if rows == 0:
             query_insert = f'INSERT INTO {schema}.{tbl}\n'
@@ -664,19 +794,21 @@
 
     def test_database_templates(self) -> None:
         '''empty db or anura schemas'''
 
         resp = self.API.database_templates()
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
-        self.assertEqual(resp['count'], 12)
+        self.assertEqual(resp['count'], 15)
         self.assertIsInstance(resp['templates'], list)
-        self.assertEqual(len(resp['templates']), 12)
+        tcount: int = len(resp['templates'])
+        self.assertEqual(tcount, 15)
+        self.assertEqual(tcount, resp['count'])
+        self.assertEqual(tcount, len(self.API.DATABASE_TEMPLATES))
 
-        # self.assertTrue(t['id'] in self.API.DATABASE_TEMPLATES)
         TEMPLATE_KEYS: Tuple[str, ...] = ('id', 'name', 'schema', 'serverName')
         for t in resp['templates']:
             self.assertIsInstance(t, dict)
             for k in TEMPLATE_KEYS:
                 self.assertIsInstance(t[k], str)
 
         TEMPLATES: list[dict] = [
@@ -733,14 +865,28 @@
                 'id': '67a881b3-b6ab-4e95-9452-6dae8e831a6b',
                 'name': 'Detailed Facility Selection',
                 'role': 'hidden',
                 'schema': 'anura_2_5',
                 'serverName': 'default',
             },
             {
+                "id": "d3cba0d2-d00a-11ed-b72f-9fd8ea75300c",
+                "name": "Fleet Size Optimization - EMEA Geo",
+                "role": "hidden",
+                "schema": "anura_2_5",
+                "serverName": "default",
+            },
+            {
+                "id": "b30d9156-d00b-11ed-b72f-9fd8ea75300c",
+                "name": "Fleet Size Optimization - US Geo",
+                "role": "hidden",
+                "schema": "anura_2_5",
+                "serverName": "default",
+            },
+            {
                 'id': '71214744-f90a-4dcc-8008-bb9dab9493be',
                 'name': 'Greenfield Facility Selection',
                 'role': 'hidden',
                 'schema': 'anura_2_5',
                 'serverName': 'default',
             },
             {
@@ -760,70 +906,92 @@
             {
                 'id': '37392edb-d582-4ce8-9f75-024651aa8592',
                 'name': 'Tactical Capacity Optimization',
                 'role': 'hidden',
                 'schema': 'anura_2_5',
                 'serverName': 'default',
             },
+            {
+                'id': 'anura_2_6_clean',
+                'name': 'Anura - New Model (2.6)',
+                'role': 'preview-schema',
+                'schema': 'anura_2_6',
+                'serverName': 'default',
+            },
         ]
 
         self.assertEqual(resp['templates'], TEMPLATES)
 
+    def test_database_templates_by_name(self) -> None:
+        '''look up the database template id by case-insensitive template name'''
+
+        template_names: List[str] = [
+            'empty',
+            'Anura - Blast off to Space',
+            'Anura - New Model',
+            'Anura - New Model (2.6)',
+            'Blast Off To Space (BOTS)',
+            'China Exit Strategy in Asia',
+            'Detailed Facility Selection',
+            'Fleet Size Optimization - EMEA Geo',
+            'Fleet Size Optimization - US Geo',
+            'Greenfield Facility Selection',
+            'Multi-Year Capacity Planning',
+            'Out of China',
+            'Tactical Capacity Optimization',
+        ]
+
+        for name in template_names:
+            tids = self.API._database_templates_by_name(name)
+            self.assertEqual(len(tids), 1)
+
     def test_ip_address_allow(self) -> None:
         '''whitelist ip address'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have a postgres database')
-
+        self.database_ensure_exist()
         db = self.API.account_storage_device(type='postgres_db')
         resp = self.API.ip_address_allow(database_name=db['name'], ip='127.0.0.0')
 
         self.assertIsInstance(resp['ip'], str)
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['ip'], '127.0.0.0')
         self.assertEqual(resp['result'], 'accepted')
         self.assertIn('five-minute delay', resp['message'])
 
     def test_ip_address_allow_invalid(self) -> None:
         '''unable to whitelist, ip address is invalid'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have a postgres database')
-
+        self.database_ensure_exist()
         db = self.API.account_storage_device(type='postgres_db')
         r = self.API.ip_address_allow(database_name=db['name'], ip='alpha.0.0.0')
         resp: dict = r['resp'].json()
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['message'], 'ipAddress is missing or invalid')
 
     def test_ip_address_allowed(self) -> None:
         '''ip address is whitelisted'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have a postgres database')
-
+        self.database_ensure_exist()
         db = self.API.account_storage_device(type='postgres_db')
         resp = self.API.ip_address_allowed(database_name=db['name'], ip='127.0.0.0')
         self.assertIsInstance(resp['allowed'], bool)
         self.assertIsInstance(resp['ip'], str)
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['allowed'], True)
         self.assertEqual(resp['ip'], '127.0.0.0')
         self.assertEqual(resp['result'], 'success')
         self.assertIn('is in the firewall', resp['message'])
 
     def test_ip_address_allowed_invalid(self) -> None:
         '''ip address is invalid'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have a postgres database')
-
+        self.database_ensure_exist()
         db = self.API.account_storage_device(type='postgres_db')
         r = self.API.ip_address_allowed(database_name=db['name'], ip='alpha.0.0.0')
         resp: dict = r['resp'].json()
 
         self.assertIsInstance(resp['message'], str)
         self.assertIsInstance(resp['result'], str)
         self.assertEqual(resp['message'], 'ipAddress is missing or invalid')
@@ -1021,17 +1189,15 @@
             self.assertEqual(resp['result'], 'success')
             self.assertEqual(resp['id'], s['id'])
             self.assertEqual(resp['name'], s['name'])
 
     def test_sql_connect_info(self) -> None:
         '''get the connection information for a sql storage item'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have a postgres database')
-
+        self.database_ensure_exist()
         pg = self.API.account_storage_device('postgres_db')
         resp = self.API.sql_connection_info(pg['name'])
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp['raw']), 6)
         self.assertIsInstance(resp['raw']['host'], str)
         self.assertTrue(resp['raw']['host'].endswith('postgres.database.azure.com'))
         self.assertIsInstance(resp['raw']['dbname'], str)
@@ -1049,109 +1215,32 @@
 
     def test_storage(self) -> None:
         '''perform storage device info on every device from storage device list'''
 
         devices = self.API.account_storage_devices()
         self.assertEqual(devices['result'], 'success')
         with self.subTest():
-            for d in devices['storages']:
-                d = self.API.storage(d['name'])
+            for device in devices['storages']:
+                d: Dict[str, Any] = self.API.storage(device['name'])
                 self.assertEqual(d['result'], 'success')
                 if d['type'] == 'azure_afs':
                     self.assertEqual(len(d), 12)
-                    self.assertIsInstance(d['bytesUsed'], int)
-                    self.assertIsInstance(d['capacity'], int)
-                    self.assertIsInstance(d['created'], int)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['id'], str)
-                    self.assertIsInstance(d['internal'], bool)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['tier'], str)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
+                    self.storage_azure_afs(d)
                 if d['type'] == 'azure_workspace':
                     self.assertEqual(len(d), 13)
-                    self.assertIsInstance(d['bytesUsed'], int)
-                    self.assertIsInstance(d['capacity'], int)
-                    self.assertIsInstance(d['created'], int)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['id'], str)
-                    self.assertIsInstance(d['internal'], bool)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['tier'], str)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
-                    self.assertIsInstance(d['workspaceKey'], str)
+                    self.storage_azure_workspace(d)
                 elif d['type'] == 'onedrive':
                     self.assertEqual(len(d), 20)
-                    self.assertIsInstance(d['accountName'], str)
-                    self.assertIsInstance(d['authenticated'], int)
-                    self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
-                    self.assertIsInstance(d['capacity'], int)
-                    self.assertIsInstance(d['created'], int)
+                    self.storage_onedrive(d)
+                    # connect is not in get devices call due to real time performance
                     self.assertIsInstance(d['connected'], bool)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['endpointSuffix'], str)
-                    self.assertIsInstance(d['homeAccountId'], str)
-                    self.assertIsInstance(d['id'], str)
-                    self.assertIsInstance(d['internal'], bool)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['protocol'], str)
-                    self.assertIsInstance(d['tier'], str)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
-                    self.assertIsInstance(d['userId'], str)
-                    self.assertIsInstance(d['username'], str)
                 elif d['type'] == 'postgres_db':
-                    self.assertEqual(len(d), 18)
-                    self.assertTrue(d['bytesUsed'] is None or isinstance(d['bytesUsed'], int))
-                    self.assertTrue(
-                        d['bytesUsedLastUpdated'] is None
-                        or isinstance(d['bytesUsedLastUpdated'], int)
-                    )
-                    self.assertIsInstance(d['created'], int)
-                    self.assertIsInstance(d['dbname'], str)
-                    self.assertTrue(d['description'] is None or isinstance(d['description'], str))
-                    self.assertIsInstance(d['host'], str)
-                    self.assertIsInstance(d['id'], str)
-                    self.assertTrue(
-                        d['lockoutReason'] is None or isinstance(d['lockoutReason'], str)
-                    )
-                    self.assertIsInstance(d['name'], str)
-                    self.assertIsInstance(d['port'], int)
-                    self.assertIsInstance(d['type'], str)
-                    self.assertIsInstance(d['updated'], int)
-                    self.assertIsInstance(d['user'], str)
-                    self.assertIsInstance(d['defaultSchema'], str)
-                    if d['defaultSchema'].startswith('anura_2_'):
-                        self.assertIsInstance(d['schemaStatus'], str)
-                        self.assertTrue(
-                            d['schemaStatus'] in ('error', 'invalid', 'valid')
-                            or len(d['schemaStatus']) == 0
-                        )
-                        self.assertTrue(
-                            d['schemaStatusLastUpdated'] is None
-                            or isinstance(d['schemaStatusLastUpdated'], float)
-                        )
-                        self.assertIsInstance(d['schemaVersion'], str)
-                        self.assertRegex(d['schemaVersion'], r'2\.[4-9]\.\d+')
-                    else:
-                        self.assertEqual(d['defaultSchema'], '"$user"')
-                        self.assertEqual(d['schemaStatus'], 'valid')
-                        self.assertIsNone(d['schemaStatusLastUpdated'])
-                        self.assertTrue(len(d['schemaVersion']) == 0)
+                    # storage item contains an additional result key
+                    self.assertEqual(len(d), 19)
+                    self.storage_database(d)
 
     @unittest.skip('api has not implemented')
     def test_storage_disk_create(self):
         '''create a new file storage device'''
 
         raise NotImplementedError
 
@@ -1160,34 +1249,49 @@
         '''delete storage device'''
 
         raise NotImplementedError
 
     def test_sql_query(self) -> None:
         '''test sql statement execution'''
 
-        if self.API.storagetype_database_exists is False:
-            self.skipTest('account does not have postgres database')
-
+        self.database_ensure_exist()
         pg = self.API.account_storage_device(type='postgres_db')
         resp = self.API.sql_query(
             database_name=pg['name'], query='SELECT datname FROM pg_database;'
         )
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['rowCount'], int)
         self.assertGreaterEqual(resp['rowCount'], 1)
         self.assertIsInstance(resp['queryResults'], list)
         self.assertGreaterEqual(len(resp['queryResults']), 1)
 
+    def test_util_job_monitor_bad(self) -> None:
+        '''job monitor check invalid job key or job state'''
+
+        # invalid job state
+        with self.assertRaises(ValueError):
+            self.API.util_job_monitor(self.WKSP, '5633e372-337a-454c-aae4-10084ea5bac6', 'invalid')  # type: ignore
+        # invalid job key
+        with self.assertRaises(ValueError):
+            self.API.util_job_monitor(self.WKSP, '')
+        with self.assertRaises(ValueError):
+            self.API.util_job_monitor(self.WKSP, 'invalid')
+        with self.assertRaises(ValueError):
+            self.API.util_job_monitor(self.WKSP, '633e372-337a-454c-aae4-10084ea5bac6')
+        # valid but job key does not exist
+        resp: bool = self.API.util_job_monitor(self.WKSP, '00000000-0000-0000-0000-000000000000')
+        self.assertFalse(resp)
+
     def test_wksp_file_copy(self) -> None:
         '''make a copy of a file within a workspace'''
 
         src: str = self.py_run_me
         dest: str = f'{self.dir_testdata_remote}/cp_test.txt'
         resp = self.API.wksp_file_copy(
-            self.WORKSPACE, file_path_src=src, file_path_dest=dest, overwrite=True
+            self.WKSP, file_path_src=src, file_path_dest=dest, overwrite=True
         )
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['copyStatus'], 'success')
         self.assertEqual(resp['message'], 'Copy complete')
         src_result: str = (
             f"{resp['sourceFileInfo']['directoryPath']}/{resp['sourceFileInfo']['filename']}"
         )
@@ -1197,41 +1301,41 @@
         self.assertEqual(src, src_result)
         self.assertEqual(dest, dest_result)
 
     def test_wksp_file_delete(self) -> None:
         '''delete a copied file with a workspace'''
 
         f: str = f'{self.dir_testdata_remote}/cp_test.txt'
-        resp = self.API.wksp_file_delete(self.WORKSPACE, file_path=f)
+        resp = self.API.wksp_file_delete(self.WKSP, file_path=f)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['message'], 'File deleted')
         file_result = f"{resp['fileInfo']['directoryPath']}/{resp['fileInfo']['filename']}"
         self.assertEqual(f, file_result)
 
     def test_wksp_file_download(self) -> None:
         '''download a file from a given workspace'''
 
-        download = self.API.wksp_file_download(self.WORKSPACE, file_path=self.py_run_me)
+        download = self.API.wksp_file_download(self.WKSP, file_path=self.py_run_me)
         self.assertGreaterEqual(len(download), 1)
         self.assertIsInstance(download, str)
 
     def test_wksp_file_download_crash(self) -> None:
         '''download a file from a given workspace'''
 
-        resp: str = self.API.wksp_file_download(self.WORKSPACE, file_path='does_not_exist')
+        resp: str = self.API.wksp_file_download(self.WKSP, file_path='does_not_exist')
         self.assertIsInstance(resp, str)
         r: dict = loads(resp)
         self.assertEqual(r['result'], 'error')
         self.assertIsInstance(r['error'], str)
         self.assertEqual(len(r['correlationId']), 36)
 
     def test_wksp_file_download_meta(self) -> None:
         '''file metadata'''
 
-        resp = self.API.wksp_file_download_status(self.WORKSPACE, file_path=self.py_run_me)
+        resp = self.API.wksp_file_download_status(self.WKSP, file_path=self.py_run_me)
         self.assertEqual(resp['result'], 'success')
         keys: Tuple[str, ...] = (
             'result',
             'workspace',
             'filename',
             'directoryPath',
             'filePath',
@@ -1241,59 +1345,59 @@
             'fileCreatedOn',
             'fileLastWriteOn',
             'fileChangeOn',
         )
         for key in resp.keys():
             self.assertIn(key, keys)
         self.assertEqual(resp['filePath'], self.py_run_me)
-        self.assertEqual(resp['workspace'], self.WORKSPACE)
+        self.assertEqual(resp['workspace'], self.WKSP)
         self.assertIsInstance(resp['contentLength'], int)
         dt: datetime = parse(resp['lastModified'])
         self.assertEqual(dt.tzname(), 'UTC')
 
     def test_wksp_file_upload(self) -> None:
         '''upload a file to a workspace'''
 
         dest: str = f'{self.dir_testdata_remote}/str2file.txt'
         resp = self.API.wksp_file_upload(
-            self.WORKSPACE, file_path_dest=dest, overwrite=True, filestr='test'
+            self.WKSP, file_path_dest=dest, overwrite=True, filestr='test'
         )
         self.assertEqual(resp['result'], 'success')
         self.assertIn(resp['message'], ['File created', 'File replaced'])
 
     def test_wksp_files(self) -> None:
         '''file structure from a given workspace and must have at least one file'''
 
-        resp = self.API.wksp_files(self.WORKSPACE)
+        resp = self.API.wksp_files(self.WKSP)
         self.assertEqual(resp['result'], 'success')
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['files'], list)
         self.assertGreaterEqual(len(resp['files']), 1)
         self.assertTrue(resp['files'][0].get('filename'))
         self.assertTrue(resp['files'][0].get('directoryPath'))
         self.assertTrue(resp['files'][0].get('filePath'))
         self.assertTrue(resp['files'][0].get('contentLength'))
 
     def test_wksp_folder_delete(self) -> None:
         '''delete a folder from a workspace'''
 
         folder: str = 'delmenow'
         fp: str = os.path.join(folder, 'delme.txt')
-        self.API.wksp_file_upload(self.WORKSPACE, fp, filestr='first file line')
-        resp = self.API.wksp_folder_delete(self.WORKSPACE, dir_path=folder, force=True)
+        self.API.wksp_file_upload(self.WKSP, fp, filestr='first file line')
+        resp = self.API.wksp_folder_delete(self.WKSP, dir_path=folder, force=True)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['message'], 'Directory and all contents deleted')
         self.assertEqual(resp['directoryPath'], folder)
 
     def test_wksp_info(self) -> None:
         '''properties of a given workspace'''
 
-        resp = self.API.wksp_info(self.WORKSPACE)
+        resp = self.API.wksp_info(self.WKSP)
         self.assertEqual(resp['result'], 'success')
-        self.assertEqual(resp['name'], self.WORKSPACE)
+        self.assertEqual(resp['name'], self.WKSP)
         self.assertEqual(len(resp['key']), 25)
         self.assertRegex(resp['key'], '^workspace')
         self.assertIn(resp['stack'], ['Optilogic', 'Simulation', 'Gurobi'])
         self.assertTrue(resp['status'].isupper())
 
     def test_wksp_job_back2back(self) -> None:
         '''one job to run many python modules in a row'''
@@ -1306,17 +1410,15 @@
         item_two: dict = {
             'pyModulePath': '/projects/quick_tests/airline_hub_location_cbc.py',
             'timeout': 30,
         }
         batch = {'batchItems': [item_one, item_two]}
 
         tag: str = 'unittest_batch_back2back'
-        resp = self.API.wksp_job_back2back(
-            self.WORKSPACE, batch=batch, verboseOutput=True, tags=tag
-        )
+        resp = self.API.wksp_job_back2back(self.WKSP, batch=batch, verboseOutput=True, tags=tag)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp), 5)
         self.assertEqual(resp['message'], 'Job submitted')
         self.assertIsInstance(resp['jobKey'], str)
         self.assertEqual(len(resp['jobKey']), 36)
         self.assertIsInstance(resp['batch'], dict)
         self.assertEqual(len(resp['batch']), 1)
@@ -1336,27 +1438,27 @@
         self.assertEqual(resp['batch']['batchItems'][1][0], item_two['pyModulePath'])
         self.assertIsNone(resp['batch']['batchItems'][1][1])
         self.assertEqual(resp['batch']['batchItems'][1][2], item_two['timeout'])
 
         # jobInfo
         self.assertIsInstance(resp['jobInfo'], dict)
         self.assertEqual(len(resp['jobInfo']), 4)
-        self.assertEqual(resp['jobInfo']['workspace'], self.WORKSPACE)
+        self.assertEqual(resp['jobInfo']['workspace'], self.WKSP)
         self.assertEqual(resp['jobInfo']['tags'], tag)
         self.assertEqual(resp['jobInfo']['timeout'], -1)
         self.assertIsInstance(resp['jobInfo']['resourceConfig'], dict)
         self.assertEqual(len(resp['jobInfo']['resourceConfig']), 4)
         self.assertEqual(resp['jobInfo']['resourceConfig']['cpu'], '1vCore')
         self.assertEqual(resp['jobInfo']['resourceConfig']['name'], '3XS')
         self.assertEqual(resp['jobInfo']['resourceConfig']['ram'], '2Gb')
         self.assertEqual(resp['jobInfo']['resourceConfig']['run_rate'], 2)
 
         # verify new batch job
-        job = self.API.wksp_job_status(self.WORKSPACE, resp['jobKey'])
-        self.assertEqual(job['jobInfo']['workspace'], self.WORKSPACE)
+        job = self.API.wksp_job_status(self.WKSP, resp['jobKey'])
+        self.assertEqual(job['jobInfo']['workspace'], self.WKSP)
         self.assertEqual(job['jobInfo']['directoryPath'], '/usr/bin')
         self.assertEqual(job['jobInfo']['filename'], 'batch_run.py')
         self.assertEqual(job['jobInfo']['command'], 'run')
         self.assertIsInstance(job['jobInfo']['commandArgs'], str)
         args: dict = loads(job['jobInfo']['commandArgs'][1:-1])
         self.assertIsInstance(args, dict)
         self.assertIsInstance(args['batchItems'], list)
@@ -1383,15 +1485,15 @@
             'pySearchTerm': '/projects/quick_tests/airline_hub_location_cbc.py',
             'timeout': 30,
         }
         batch = {'batchItems': [item_one, item_two]}
 
         tag: str = 'unittest_batch_back2back_find'
         resp = self.API.wksp_job_back2back_findnrun(
-            self.WORKSPACE, batch=batch, verboseOutput=True, tags=tag
+            self.WKSP, batch=batch, verboseOutput=True, tags=tag
         )
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp), 5)
         self.assertEqual(resp['message'], 'Job submitted')
         self.assertIsInstance(resp['jobKey'], str)
         self.assertEqual(len(resp['jobKey']), 36)
         self.assertIsInstance(resp['batch'], dict)
@@ -1413,27 +1515,27 @@
         self.assertEqual(resp['batch']['batchItems'][1][0], item_two['pySearchTerm'])
         self.assertIsNone(resp['batch']['batchItems'][1][1])
         self.assertEqual(resp['batch']['batchItems'][1][2], item_two['timeout'])
 
         # jobInfo
         self.assertIsInstance(resp['jobInfo'], dict)
         self.assertEqual(len(resp['jobInfo']), 4)
-        self.assertEqual(resp['jobInfo']['workspace'], self.WORKSPACE)
+        self.assertEqual(resp['jobInfo']['workspace'], self.WKSP)
         self.assertEqual(resp['jobInfo']['tags'], tag)
         self.assertEqual(resp['jobInfo']['timeout'], -1)
         self.assertIsInstance(resp['jobInfo']['resourceConfig'], dict)
         self.assertEqual(len(resp['jobInfo']['resourceConfig']), 4)
         self.assertEqual(resp['jobInfo']['resourceConfig']['cpu'], '1vCore')
         self.assertEqual(resp['jobInfo']['resourceConfig']['name'], '3XS')
         self.assertEqual(resp['jobInfo']['resourceConfig']['ram'], '2Gb')
         self.assertEqual(resp['jobInfo']['resourceConfig']['run_rate'], 2)
 
         # verify new batch job
-        job = self.API.wksp_job_status(self.WORKSPACE, resp['jobKey'])
-        self.assertEqual(job['jobInfo']['workspace'], self.WORKSPACE)
+        job = self.API.wksp_job_status(self.WKSP, resp['jobKey'])
+        self.assertEqual(job['jobInfo']['workspace'], self.WKSP)
         self.assertEqual(job['jobInfo']['directoryPath'], '/usr/bin')
         self.assertEqual(job['jobInfo']['filename'], 'batch_search_n_run.py')
         self.assertEqual(job['jobInfo']['command'], 'run')
         self.assertIsInstance(job['jobInfo']['commandArgs'], str)
         args: dict = loads(job['jobInfo']['commandArgs'][1:-1])
         self.assertIsInstance(args, dict)
         self.assertIsInstance(args['batchItems'], list)
@@ -1447,43 +1549,45 @@
         self.assertEqual(job['jobInfo']['resourceConfig']['name'], '3XS')
         self.assertEqual(job['jobInfo']['resourceConfig']['ram'], '2Gb')
         self.assertEqual(job['jobInfo']['resourceConfig']['run_rate'], 2)
 
     def test_wksp_job_file_error(self) -> None:
         '''get job error file'''
 
-        resp: str = self.API.wksp_job_file_error(self.WORKSPACE, self.API._job_start_recent_key)
+        resp: str = self.API.wksp_job_file_error(self.WKSP, self.API._job_start_recent_key)
         self.assertIsInstance(resp, str)
         if resp.startswith('{\"result\":\"error\"'):
             err: dict = loads(resp)
             self.assertEqual(err['result'], 'error')
             self.assertIsInstance(err['error'], str)
             self.assertIsInstance(err['correlationId'], str)
             self.assertEqual(len(err['correlationId']), 36)
         else:
             self.assertGreater(len(resp), 0)
 
     def test_wksp_job_file_result(self) -> None:
         '''get job result file'''
 
-        resp: str = self.API.wksp_job_file_result(self.WORKSPACE, self.API._job_start_recent_key)
+        resp: str = self.API.wksp_job_file_result(self.WKSP, self.API._job_start_recent_key)
         self.assertIsInstance(resp, str)
         if resp.startswith('{\"result\":\"error\"'):
             err: dict = loads(resp)
             self.assertEqual(err['result'], 'error')
             self.assertIsInstance(err['error'], str)
             self.assertIsInstance(err['correlationId'], str)
             self.assertEqual(len(err['correlationId']), 36)
         else:
             self.assertGreater(len(resp), 0)
 
     def test_wksp_job_ledger(self) -> None:
         '''get job ledger that has realtime messages'''
 
-        resp = self.API.wksp_job_ledger(self.WORKSPACE, self.API._job_start_recent_key)
+        if len(self.__jobkey_quick) == 0:
+            self.job_prereq()
+        resp = self.API.wksp_job_ledger(self.WKSP, self.__jobkey_quick)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['records'], list)
         self.assertGreaterEqual(len(resp['records']), 1)
         self.assertIsInstance(resp['records'][0]['timestamp'], int)
         self.assertIsInstance(resp['records'][0]['datetime'], str)
@@ -1495,15 +1599,17 @@
         self.assertEqual(dt.year, now.year)
         self.assertEqual(dt.month, now.month)
         self.assertEqual(dt.day, now.day)
 
     def test_wksp_job_metrics(self) -> None:
         '''get one second cpu and memory sampling of a job'''
 
-        resp = self.API.wksp_job_metrics(self.WORKSPACE, self.API._job_start_recent_key)
+        if len(self.__jobkey_quick) == 0:
+            self.job_prereq()
+        resp = self.API.wksp_job_metrics(self.WKSP, self.__jobkey_quick)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertGreaterEqual(resp['count'], 1)
         self.assertIsInstance(resp['max'], dict)
         self.assertEqual(len(resp['max']), 7)
         self.assertIsInstance(resp['max']['memoryPercent'], float)
         self.assertIsInstance(resp['max']['memoryResident'], float)
@@ -1531,15 +1637,17 @@
         self.assertIsInstance(resp['records'][0]['memoryPercent'], float)
         self.assertIsInstance(resp['records'][0]['memoryResident'], float)
         self.assertIsInstance(resp['records'][0]['processCount'], int)
 
     def test_wksp_job_metrics_max(self) -> None:
         '''get peak cpu and memory stats of a job'''
 
-        resp = self.API.wksp_job_metrics_max(self.WORKSPACE, self.API._job_start_recent_key)
+        if len(self.__jobkey_quick) == 0:
+            self.job_prereq()
+        resp = self.API.wksp_job_metrics_max(self.WKSP, self.__jobkey_quick)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['max'], dict)
         self.assertEqual(len(resp['max']), 7)
         self.assertIsInstance(resp['max']['memoryPercent'], float)
         self.assertIsInstance(resp['max']['memoryResident'], float)
         self.assertIsInstance(resp['max']['memoryAvailable'], int)
         self.assertIsInstance(resp['max']['cpuPercent'], float)
@@ -1555,38 +1663,37 @@
 
         # spin up a few jobs to create load and evaluate all
         jobs_max: int = 9
         tag_time: float = time.time()
         tag: str = f'metrics_{tag_time}'
         secs: int = 15
         for job in range(jobs_max):
-            self.API.wksp_job_start(self.WORKSPACE, self.py_run_me, tags=tag, timeout=secs)
+            self.API.wksp_job_start(self.WKSP, self.py_run_me, tags=tag, timeout=secs)
 
         # check the jobs that are about to run
         d: dict = {}
         check: bool = True
-        loops: int = 0
         while check:
-            jobs = self.API.wksp_jobs(self.WORKSPACE, tags=tag)
+            jobs = self.API.wksp_jobs(self.WKSP, tags=tag)
 
             # jobs all finished?
             terminal: int = 0
             for t in self.API.JOBSTATES_TERMINAL:
                 terminal += jobs['statusCounts'].get(t)
 
             if terminal == jobs_max:
                 check = False
                 break
 
             # check running jobs for metrics
-            active = self.API.wksp_jobs(self.WORKSPACE, status='running', tags=tag)
+            active = self.API.wksp_jobs(self.WKSP, status='running', tags=tag)
 
             if active['statusCounts']['running'] >= 1:
                 for job in active['jobs']:
-                    resp = self.API.wksp_job_metrics(self.WORKSPACE, job['jobKey'])
+                    resp = self.API.wksp_job_metrics(self.WKSP, job['jobKey'])
                     self.assertEqual(resp['result'], 'success')
                     self.assertIsInstance(resp['count'], int)
                     # self.assertGreaterEqual(resp['count'], 1)
 
                     # missing metrics!
                     if resp['count'] == 0:
                         st: str = str(job['startDatetime'])
@@ -1613,35 +1720,118 @@
             print('\n JobKey, LastSeenWithMissingMetricCount_RunDuration')
             for k in d.items():
                 print(k[1], k[0])
 
     def test_wksp_job_start(self) -> None:
         '''creating a job'''
 
-        resp = self.API.wksp_job_start(
-            self.WORKSPACE, file_path=self.py_run_me, tags='unittest_start'
-        )
+        resp = self.API.wksp_job_start(self.WKSP, file_path=self.py_run_me, tags='unittest_start')
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp['jobKey']), 36)
         job_info_keys: Tuple[str, ...] = (
             'workspace',
             'directoryPath',
             'filename',
             'command',
             'resourceConfig',
             'tags',
             'timeout',
         )
         for key in resp['jobInfo'].keys():
             self.assertIn(key, job_info_keys)
 
+    def test_wksp_job_start_sample(self) -> None:
+        '''create job api call reponse time is the slowest and fails often withg 504s'''
+
+        max: int = int(self.API.account_info()['limits']['concurrentJobs'] * 0.5)
+        min: int = 10
+        job_count: int = max if max > min else min
+        jobs: List[str] = []
+        tag: str = 'unittest_job_speed'
+
+        # start jobs
+        for j in range(job_count):
+            with self.subTest():
+                resp = self.API.wksp_job_start(self.WKSP, self.py_run_me_quick, tags=tag)
+                self.assertEqual(resp['result'], 'success')
+            # d = {}
+            # d['key'] = resp['jobKey']
+            # jobs.append(d)
+            # spin up a few jobs to create load and evaluate all
+
+        return
+        # what is current job count
+        jobs_active: int = self.API._jobs_active
+        jobs_max: int = self.API.account_info()['limits']['concurrentJobs']
+        max: int = jobs_max - jobs_active if jobs_max > jobs_active else 0
+
+        jobs_max: int = 9
+        tag_time: float = time.time()
+        tag: str = f'unittest_job_sample_{tag_time}'
+
+        for job in range(jobs_max):
+            self.API.wksp_job_start(self.WKSP, self.py_run_me, tags=tag)
+
+        # check the jobs that are about to run
+        d: dict = {}
+        check: bool = True
+        while check:
+            jobs = self.API.wksp_jobs(self.WKSP, tags=tag)
+
+            # jobs all finished?
+            terminal: int = 0
+            for t in self.API.JOBSTATES_TERMINAL:
+                terminal += jobs['statusCounts'].get(t)
+
+            if terminal == jobs_max:
+                check = False
+                break
+
+            # check running jobs for metrics
+            active = self.API.wksp_jobs(self.WKSP, status='running', tags=tag)
+
+            if active['statusCounts']['running'] >= 1:
+                for job in active['jobs']:
+                    resp = self.API.wksp_job_metrics(self.WKSP, job['jobKey'])
+                    self.assertEqual(resp['result'], 'success')
+                    self.assertIsInstance(resp['count'], int)
+                    # self.assertGreaterEqual(resp['count'], 1)
+
+                    # missing metrics!
+                    if resp['count'] == 0:
+                        st: str = str(job['startDatetime'])
+                        st = st.replace('T', ' ')
+                        st = st.replace('Z', '')
+                        job_start: datetime = datetime.fromisoformat(st)
+                        now: datetime = datetime.utcnow()
+                        delta: timedelta = now - job_start
+                        d[job['jobKey']] = str(
+                            delta
+                        )  # store job key and elapsed time without metrics
+
+                        print(
+                            f"{str(delta)} secs elapsed and metrics missing for job {job['jobKey']}"
+                        )
+                        with self.subTest():
+                            self.assertLess(delta.total_seconds(), 5)
+
+            time.sleep(1)
+
+        # were there any jobs that failed metric check?
+        secs = 0
+        if len(d) >= 1:
+            print(f"\n\nJob Submitted: {jobs_max}, Job Duration: {secs}, Job Tag: {tag}")
+            print('\n JobKey, LastSeenWithMissingMetricCount_RunDuration')
+            for k in d.items():
+                print(k[1], k[0])
+
     def test_wksp_job_status(self) -> None:
         '''get job status for explicit state'''
 
-        resp = self.API.wksp_job_status(self.WORKSPACE, self.API._job_start_recent_key)
+        resp = self.API.wksp_job_status(self.WKSP, self.API._job_start_recent_key)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp['jobKey']), 36)
         self.assertIsInstance(resp['submittedDatetime'], str)
         self.assertTrue(resp['submittedDatetime'].endswith('Z'))
         dt: datetime = parse(resp['submittedDatetime'])
         self.assertTrue(dt.tzname(), 'UTC')
         now: datetime = datetime.utcnow()
@@ -1669,23 +1859,23 @@
         for key in resp['jobInfo']['resourceConfig']:
             self.assertIn(key, resource_keys)
 
     def test_wksp_job_stop(self) -> None:
         '''stop a most recently created job'''
 
         # guarantee a job is currently running
-        resp = self.API.wksp_job_status(self.WORKSPACE, self.API._job_start_recent_key)
+        resp = self.API.wksp_job_status(self.WKSP, self.API._job_start_recent_key)
         if resp['status'] in self.API.JOBSTATES_TERMINAL:
-            resp = self.API.wksp_job_start(self.WORKSPACE, self.py_run_me)
-            success: bool = self.API.util_job_monitor(self.WORKSPACE, resp['jobKey'])
+            resp = self.API.wksp_job_start(self.WKSP, self.py_run_me)
+            success: bool = self.API.util_job_monitor(self.WKSP, resp['jobKey'])
             if success is False:
                 self.skipTest('failed to start job within two minutes')
 
         # stop running job
-        resp = self.API.wksp_job_stop(self.WORKSPACE, self.API._job_start_recent_key)
+        resp = self.API.wksp_job_stop(self.WKSP, self.API._job_start_recent_key)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['jobKey'], self.API._job_start_recent_key)
         keys: Tuple[str, ...] = ('result', 'message', 'jobKey', 'status', 'jobInfo')
         for key in resp.keys():
             self.assertIn(key, keys)
 
     def test_wksp_jobify(self) -> None:
@@ -1698,15 +1888,15 @@
                     'pyModulePath': '/projects/quick_tests/airline_hub_location_cbc.py',
                     'timeout': 30,
                 },
             ]
         }
 
         tag: str = 'unittest_batch_jobify'
-        resp = self.API.wksp_jobify(self.WORKSPACE, batch=batch, tags=tag)
+        resp = self.API.wksp_jobify(self.WKSP, batch=batch, tags=tag)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['message'], 'Jobs submitted')
         self.assertIsInstance(resp['count'], int)
         self.assertEqual(resp['count'], len(resp['jobKeys']))
         for key in resp['jobKeys']:
             self.assertIsInstance(key, str)
             self.assertEqual(len(key), 36)
@@ -1718,28 +1908,28 @@
             'batchItems': [
                 {'pySearchTerm': '^/quick_tests/sleep.py', 'timeout': 90},
                 {'pySearchTerm': '^/quick_tests/airline_hub_location_cbc.py', 'timeout': 30},
             ]
         }
 
         tag: str = 'unittest_batch_jobify_find'
-        resp = self.API.wksp_jobify_findnrun(self.WORKSPACE, batch=batch, tags=tag)
+        resp = self.API.wksp_jobify_findnrun(self.WKSP, batch=batch, tags=tag)
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(resp['message'], 'Jobs submitted')
         self.assertIsInstance(resp['count'], int)
         self.assertEqual(len(batch['batchItems']), resp['count'])
         self.assertEqual(resp['count'], len(resp['jobKeys']))
         for key in resp['jobKeys']:
             self.assertIsInstance(key, str)
             self.assertEqual(len(key), 36)
 
     def test_wksp_jobs(self) -> None:
         '''list the jobs for a specific workspace'''
 
-        resp = self.API.wksp_jobs(self.WORKSPACE)
+        resp = self.API.wksp_jobs(self.WKSP)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertIsInstance(resp['statusCounts'], dict)
 
         status_keys: Tuple[str, ...] = self.API.JOBSTATES
         for status in resp['statusCounts']:
             self.assertIn(status, status_keys)
@@ -1785,15 +1975,15 @@
                         self.assertIsInstance(job[key]['filename'], str)
                     self.assertIsInstance(job[key]['resourceConfig'], dict)
                     self.assertIsInstance(job[key]['workspace'], str)
 
     def test_wksp_jobs_stats(self) -> None:
         '''get the stats for jobs for a specific workspace'''
 
-        resp = self.API.wksp_jobs(self.WORKSPACE)
+        resp = self.API.wksp_jobs(self.WKSP)
         self.assertEqual(resp['result'], 'success')
         self.assertIsInstance(resp['count'], int)
         self.assertIsInstance(resp['statusCounts'], dict)
 
         status_keys: Tuple[str, ...] = self.API.JOBSTATES
         for status in resp['statusCounts']:
             self.assertIn(status, status_keys)
@@ -1814,17 +2004,17 @@
             self.assertIn(filter, filter_keys)
 
     def test_wksp_share_file(self) -> None:
         '''share a file from a workspace to all other workspaces of a user/self'''
 
         if self.API.auth_username is None:
             self.skipTest('test_wksp_share_folder requires a username')
-        
+
         resp = self.API.wksp_share_file(
-            self.WORKSPACE, file_path=self.py_run_me, targetUsers=self.API.auth_username
+            self.WKSP, file_path=self.py_run_me, targetUsers=self.API.auth_username
         )
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp.keys()), 8)
         self.assertIsInstance(resp['errored'], list)
         self.assertEqual(len(resp['errored']), 0)
         self.assertIsInstance(resp['erroredCount'], int)
         self.assertIsInstance(resp['jobs'], list)
@@ -1845,15 +2035,15 @@
         self.assertEqual(resp['targetUsers'], self.API.auth_username)
 
     def test_wksp_share_file_sample(self) -> None:
         '''OE-5840 API Share File/Folder Results in 500 Internal Server Error'''
 
         if self.API.auth_username is None:
             self.skipTest('test_wksp_share_folder requires a username')
-        
+
         if self.API.account_workspace_count < 2:
             self.skipTest('account does not have required multi-workspaces needed for sharing')
 
         test_result: bool = False
 
         # get all not Studio workspaces
         resp: dict = self.API.account_workspaces()
@@ -1947,15 +2137,15 @@
     def test_wksp_share_folder(self) -> None:
         '''share a subtree from a workspace to all other workspaces of a user/self'''
 
         if self.API.auth_username is None:
             self.skipTest('test_wksp_share_folder requires a username')
 
         resp = self.API.wksp_share_folder(
-            self.WORKSPACE, dir_path=self.dir_testdata_remote, targetUsers=self.API.auth_username
+            self.WKSP, dir_path=self.dir_testdata_remote, targetUsers=self.API.auth_username
         )
         self.assertEqual(resp['result'], 'success')
         self.assertEqual(len(resp.keys()), 8)
         self.assertIsInstance(resp['errored'], list)
         self.assertEqual(len(resp['errored']), 0)
         self.assertIsInstance(resp['erroredCount'], int)
         self.assertIsInstance(resp['jobs'], list)
@@ -1980,9 +2170,9 @@
     # appkey replace YOUR_USERNAME, YOUR_APPLICATION_KEY, and set auth_legacy to False
 
     args: dict = docopt(__doc__)
     TestApi.APPKEY = args.get('--appkey')
     TestApi.AUTH_LEGACY = args.get('--authlegacy', '').lower() == 'true'
     TestApi.USERNAME = args.get('--user')
     TestApi.USERPASS = args.get('--pass')
-    TestApi.WORKSPACE = args.get('--wksp', 'Studio')
+    TestApi.WKSP = args.get('--wksp', 'Studio')
     unittest.main(__name__, argv=['main'])
```

## Comparing `optilogic-2.3.0.dist-info/LICENSE` & `optilogic-2.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `optilogic-2.3.0.dist-info/METADATA` & `optilogic-2.4.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: optilogic
-Version: 2.3.0
+Version: 2.4.0
 Summary: Tools for interfacing with Optilogic Jobs and APIs
 Home-page: https://optilogic.com
 Author: Optilogic
 Author-email: support@optilogic.com
 License: MIT
 Project-URL: Documentation, https://api-docs.optilogic.app/documentation
 Keywords: mip,mixed integer programming,network optimization,optimization,risk,simulation,supply chain design
```

## Comparing `optilogic-2.3.0.dist-info/RECORD` & `optilogic-2.4.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 optilogic/__init__.py,sha256=t3hXtM_MyajzEyk-V1xxM1ZzFZCZtADNjJvvShxdl4A,22
 optilogic/pioneer/__init__.py,sha256=ELVJEdWM5Ia28-LjwVes2qvd7nGkORgVOw0g21vsGPU,48
 optilogic/pioneer/api/__init__.py,sha256=kaCK0y1h6dN5NhQnehgpJCEwyihVDPuuQ7xgtexVric,20
-optilogic/pioneer/api/api.py,sha256=vu5mL6BIqx3kTCBtiaax3ukEi2rYsYxqmNC-CHlMfB4,60336
-optilogic/pioneer/api/api_tests.py,sha256=If4WxsQqRbtFcmOMJnjqv4sbJzWfVmYll4G7lcCnz2M,90132
+optilogic/pioneer/api/api.py,sha256=uH2SXWaDKHsTKCBmQvnRrGsU-sQz1d8BdqrRBMfnbsg,64383
+optilogic/pioneer/api/api_tests.py,sha256=_5hhw6eMxPAdInIQbCwJsxjJVCSTdtzH154oYSozvxI,95676
 optilogic/pioneer/api/quick_tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 optilogic/pioneer/api/quick_tests/airline_hub_location_cbc.py,sha256=S6nxwIvWFqBo1tsFRi0D9VBD8ohBDhb42aBzXkKNgw4,5111
 optilogic/pioneer/api/quick_tests/bash.py,sha256=WPnbdoPVGxOwdMlsk-IyxVWeqXae5DbdN2bpMzNgVCQ,317
 optilogic/pioneer/api/quick_tests/quick.py,sha256=Yf_jhZbx5Nx5W3Dc_iJz8t70KvMDBn7xVTMmNvBn5gM,209
 optilogic/pioneer/api/quick_tests/sleep.py,sha256=CSb8T0inq6NAlDqW321vFCBmldPIiiGWfAU98OwUmV4,577
 optilogic/pioneer/job_utils/__init__.py,sha256=ewS513f69vgJB7L8QZIYtwl6umBFDtOqM52Ms0pAzy4,131
 optilogic/pioneer/job_utils/job_utils.py,sha256=nBeFlkmqXjiNQub-i962qk64c-M2QCMjQvLLv5fuv6U,3553
-optilogic-2.3.0.dist-info/LICENSE,sha256=BfGwbupGKO-1uV11X7aBMgE3LCjo5N7OxqZpMFhP7ls,1070
-optilogic-2.3.0.dist-info/METADATA,sha256=mZwm_R6TDgebczmDbTqGLazAmORY_HYQZNsWf9yHaPQ,759
-optilogic-2.3.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-optilogic-2.3.0.dist-info/top_level.txt,sha256=A59vwR2Gu9fxXluO4gG-COTlfSKCJW910611UXOwnuo,10
-optilogic-2.3.0.dist-info/RECORD,,
+optilogic-2.4.0.dist-info/LICENSE,sha256=BfGwbupGKO-1uV11X7aBMgE3LCjo5N7OxqZpMFhP7ls,1070
+optilogic-2.4.0.dist-info/METADATA,sha256=EPPaSGPemrO_vpL5rRYZ--o-D43qGyN4ZX04hD27z-0,759
+optilogic-2.4.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+optilogic-2.4.0.dist-info/top_level.txt,sha256=A59vwR2Gu9fxXluO4gG-COTlfSKCJW910611UXOwnuo,10
+optilogic-2.4.0.dist-info/RECORD,,
```

