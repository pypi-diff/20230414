# Comparing `tmp/openmetadata-ingestion-1.0.0.0.dev0.tar.gz` & `tmp/openmetadata-ingestion-1.0.0.0.dev1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "openmetadata-ingestion-1.0.0.0.dev0.tar", last modified: Thu Mar 30 09:55:06 2023, max compression
+gzip compressed data, was "openmetadata-ingestion-1.0.0.0.dev1.tar", last modified: Fri Apr 14 12:26:33 2023, max compression
```

## Comparing `openmetadata-ingestion-1.0.0.0.dev0.tar` & `openmetadata-ingestion-1.0.0.0.dev1.tar`

### file list

```diff
@@ -1,1294 +1,1318 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.655776 openmetadata-ingestion-1.0.0.0.dev0/
--rw-r--r--   0 runner    (1001) docker     (123)    11356 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     2531 2023-03-30 09:55:06.655776 openmetadata-ingestion-1.0.0.0.dev0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)      636 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/README.md
--rw-r--r--   0 runner    (1001) docker     (123)      809 2023-03-30 09:55:06.659776 openmetadata-ingestion-1.0.0.0.dev0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     9302 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.375774 openmetadata-ingestion-1.0.0.0.dev0/src/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.375774 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/
--rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.375774 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3562 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/hooks/openmetadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.375774 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3273 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/backend.py
--rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/callback.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.375774 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/commons.py
--rw-r--r--   0 runner    (1001) docker     (123)     3938 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     4223 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/providers.py
--rw-r--r--   0 runner    (1001) docker     (123)     2958 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/operator.py
--rw-r--r--   0 runner    (1001) docker     (123)    12899 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     4200 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/status.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.379774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      700 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2710 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/__version__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.379774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/antlr/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/antlr/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1854 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/antlr/split_listener.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.379774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/automations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2138 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/automations/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.379774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6524 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/backup.py
--rw-r--r--   0 runner    (1001) docker     (123)     1695 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/dataquality.py
--rw-r--r--   0 runner    (1001) docker     (123)     6218 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/db_dump.py
--rw-r--r--   0 runner    (1001) docker     (123)    13593 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/docker.py
--rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/ingest.py
--rw-r--r--   0 runner    (1001) docker     (123)     4451 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/openmetadata_dag_config_migration.py
--rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/openmetadata_imports_migration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1675 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/profile.py
--rw-r--r--   0 runner    (1001) docker     (123)     2882 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/restore.py
--rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/clients/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/clients/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6028 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/clients/aws_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     5201 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/clients/domo_client.py
--rw-r--r--   0 runner    (1001) docker     (123)    15860 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cmd.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/config/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/config/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3222 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/config/common.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12327 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/helper/
--rw-r--r--   0 runner    (1001) docker     (123)      930 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/helper/data_insight_es_index.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2289 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/data_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     7948 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/entity_report_data_processor.py
--rw-r--r--   0 runner    (1001) docker     (123)    13395 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/web_analytic_report_data_processor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/runner/
--rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/runner/kpi_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     5130 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/runner/run_result_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.383775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3705 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.311774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.403775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/
--rw-r--r--   0 runner    (1001) docker     (123)      353 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/airbyte.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      568 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/airflow.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/amundsen.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      557 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/athena.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      606 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/atlas.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      601 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/azuresql.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      830 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      321 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1336 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      998 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      553 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/clickhouse.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      325 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/clickhouse_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/clickhouse_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      378 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/dagster.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      531 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/databricks.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      325 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/databricks_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      482 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/databricks_pipeline.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      627 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/databricks_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      607 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/datalake.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/datalake_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      436 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/db2.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/db2_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2087 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/dbt.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/deltalake.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      547 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/domodashboard.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      616 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/dynamodb.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/fivetran.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/glue.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      523 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/gluepipeline.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      367 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/hive.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1192 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/kafka.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      415 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/kinesis.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      306 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/ldap_user_to_catalog.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      471 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/looker.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      455 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mariadb.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      452 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/metabase.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      699 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/migrate_source.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mlflow.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      744 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mode.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mssql.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      315 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mssql_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      594 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mssql_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      507 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mysql.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mysql_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      530 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/openmetadata.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      448 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/oracle.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      491 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/pinotdb.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      443 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/postgres.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      343 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/postgres_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      630 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/postgres_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      790 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/powerbi.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/presto.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      707 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/query_log_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      628 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/quicksight.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redash.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      488 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redpanda.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      319 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift_profiler.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      646 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      481 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/sagemaker.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      464 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/salesforce.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      472 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/singlestore.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      676 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/snowflake.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      317 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/snowflake_lineage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      620 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/snowflake_usage.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      651 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/superset.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      684 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/tableau.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      874 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/test_suite.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      547 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/trino.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      435 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/vertica.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.311774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.407775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/
--rw-r--r--   0 runner    (1001) docker     (123)     8281 2023-03-30 09:54:42.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/EntityLinkLexer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-03-30 09:54:42.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/EntityLinkListener.py
--rw-r--r--   0 runner    (1001) docker     (123)     9288 2023-03-30 09:54:42.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/EntityLinkParser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2606 2023-03-30 09:54:42.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/FqnLexer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1210 2023-03-30 09:54:42.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/FqnListener.py
--rw-r--r--   0 runner    (1001) docker     (123)     6811 2023-03-30 09:54:42.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/FqnParser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.331774 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.407775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      948 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportData.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.407775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1263 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
--rw-r--r--   0 runner    (1001) docker     (123)     1094 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py
--rw-r--r--   0 runner    (1001) docker     (123)      958 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py
--rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEvent.py
--rw-r--r--   0 runner    (1001) docker     (123)      883 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventData.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.407775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      978 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py
--rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.415775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.415775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/analytics/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/analytics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.415775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/automations/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1483 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/automations/createWorkflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.415775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/createClassification.py
--rw-r--r--   0 runner    (1001) docker     (123)     1754 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/createTag.py
--rw-r--r--   0 runner    (1001) docker     (123)      525 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/loadTags.py
--rw-r--r--   0 runner    (1001) docker     (123)      959 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/createBot.py
--rw-r--r--   0 runner    (1001) docker     (123)     1297 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/createEventPublisherJob.py
--rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/createType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.415775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1525 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createChart.py
--rw-r--r--   0 runner    (1001) docker     (123)     2299 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createContainer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDashboard.py
--rw-r--r--   0 runner    (1001) docker     (123)     1817 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDashboardDataModel.py
--rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDatabase.py
--rw-r--r--   0 runner    (1001) docker     (123)     1252 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDatabaseSchema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1541 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createGlossary.py
--rw-r--r--   0 runner    (1001) docker     (123)     2402 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createGlossaryTerm.py
--rw-r--r--   0 runner    (1001) docker     (123)     1245 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createLocation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2446 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createMlModel.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createTable.py
--rw-r--r--   0 runner    (1001) docker     (123)      727 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createTableProfile.py
--rw-r--r--   0 runner    (1001) docker     (123)     2713 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createTopic.py
--rw-r--r--   0 runner    (1001) docker     (123)      424 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/restoreEntity.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.419775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.419775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.419775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      401 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/closeTask.py
--rw-r--r--   0 runner    (1001) docker     (123)      526 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/createPost.py
--rw-r--r--   0 runner    (1001) docker     (123)     1692 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/createThread.py
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/resolveTask.py
--rw-r--r--   0 runner    (1001) docker     (123)      749 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/threadCount.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.419775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      579 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/lineage/addLineage.py
--rw-r--r--   0 runner    (1001) docker     (123)      672 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/openMetadataServerVersion.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.419775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/policies/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1082 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/policies/createPolicy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.423775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createDashboardService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1163 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createDatabaseService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createMessagingService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createMetadataService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1160 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createMlModelService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createObjectStoreService.py
--rw-r--r--   0 runner    (1001) docker     (123)     1171 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createPipelineService.py
--rw-r--r--   0 runner    (1001) docker     (123)      942 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createStorageService.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.423775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)      547 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/setOwner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.423775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      830 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/createRole.py
--rw-r--r--   0 runner    (1001) docker     (123)     2380 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/createTeam.py
--rw-r--r--   0 runner    (1001) docker     (123)     2082 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/createUser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.423775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1227 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createCustomMetric.py
--rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createTestCase.py
--rw-r--r--   0 runner    (1001) docker     (123)     1159 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createTestDefinition.py
--rw-r--r--   0 runner    (1001) docker     (123)      820 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createTestSuite.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/voteRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.435775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      339 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/basicAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      462 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/basicLoginRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      886 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/changePasswordRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      437 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/createPersonalToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      365 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/emailRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/emailVerificationToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      347 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/generateToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      751 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/jwtAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      409 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/loginRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      626 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/logoutRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      683 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/passwordResetRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1027 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/passwordResetToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      956 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/personalAccessToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/refreshToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      623 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/registrationRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      476 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/revokePersonalToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      324 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/revokeToken.py
--rw-r--r--   0 runner    (1001) docker     (123)      288 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/serviceTokenEnum.py
--rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/ssoAuth.py
--rw-r--r--   0 runner    (1001) docker     (123)      378 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/tokenRefreshRequest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.435775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/applicationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/authConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/authenticationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/authorizerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      442 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      357 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/fernetConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      658 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)     1823 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.435775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      894 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      462 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      978 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     1648 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      507 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/sslConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      436 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      885 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.435775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChart.py
--rw-r--r--   0 runner    (1001) docker     (123)     1933 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.435775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     2537 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/kpi.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.439775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py
--rw-r--r--   0 runner    (1001) docker     (123)      791 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py
--rw-r--r--   0 runner    (1001) docker     (123)      635 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py
--rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py
--rw-r--r--   0 runner    (1001) docker     (123)      975 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py
--rw-r--r--   0 runner    (1001) docker     (123)      831 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py
--rw-r--r--   0 runner    (1001) docker     (123)      843 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.439775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/email/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/email/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1250 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/email/emailRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/email/smtpSettings.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.439775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.443775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/automations/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/automations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1676 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/automations/testServiceConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/automations/workflow.py
--rw-r--r--   0 runner    (1001) docker     (123)     2055 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/bot.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.443775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/classification/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2864 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/classification/classification.py
--rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/classification/tag.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.447775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/chart.py
--rw-r--r--   0 runner    (1001) docker     (123)     4468 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/container.py
--rw-r--r--   0 runner    (1001) docker     (123)     3408 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/dashboard.py
--rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/dashboardDataModel.py
--rw-r--r--   0 runner    (1001) docker     (123)     3385 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/database.py
--rw-r--r--   0 runner    (1001) docker     (123)     3096 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/databaseSchema.py
--rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/glossary.py
--rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/glossaryTerm.py
--rw-r--r--   0 runner    (1001) docker     (123)     2929 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/location.py
--rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     6419 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/mlmodel.py
--rw-r--r--   0 runner    (1001) docker     (123)     6226 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/query.py
--rw-r--r--   0 runner    (1001) docker     (123)     2303 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/report.py
--rw-r--r--   0 runner    (1001) docker     (123)    21332 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/table.py
--rw-r--r--   0 runner    (1001) docker     (123)     4555 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/topic.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.447775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/events/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/events/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      900 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/events/webhook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.447775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/feed/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/feed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5227 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/feed/thread.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.447775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.447775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1821 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
--rw-r--r--   0 runner    (1001) docker     (123)     1497 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/rule.py
--rw-r--r--   0 runner    (1001) docker     (123)      681 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/filters.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.451775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      718 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/deleteAction.py
--rw-r--r--   0 runner    (1001) docker     (123)     1436 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/moveAction.py
--rw-r--r--   0 runner    (1001) docker     (123)      814 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/rule.py
--rw-r--r--   0 runner    (1001) docker     (123)     3173 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/policy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.451775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.451775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.455775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      935 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1492 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1856 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1538 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2112 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.459775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2113 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2566 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3367 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3459 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      927 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.459775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      531 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      518 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2272 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3731 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1690 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1351 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3171 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2678 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3326 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3388 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2629 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3077 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2574 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2749 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3937 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2785 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.463775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      935 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2450 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      901 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      729 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2263 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.463775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2251 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5392 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.463775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      920 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      915 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      735 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.463775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1192 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/azureObjectStoreConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1166 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/gcsObjectStoreConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1139 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/s3ObjectStoreConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.467775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1154 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      927 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1412 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1371 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      916 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2241 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1111 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2529 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1342 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py
--rw-r--r--   0 runner    (1001) docker     (123)     3955 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/dashboardService.py
--rw-r--r--   0 runner    (1001) docker     (123)     5782 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/databaseService.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.467775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5915 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     3714 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/messagingService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3548 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/metadataService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/mlmodelService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3050 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/objectstoreService.py
--rw-r--r--   0 runner    (1001) docker     (123)     3973 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/pipelineService.py
--rw-r--r--   0 runner    (1001) docker     (123)      399 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/serviceType.py
--rw-r--r--   0 runner    (1001) docker     (123)     2078 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/storageService.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.467775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2564 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/role.py
--rw-r--r--   0 runner    (1001) docker     (123)     4320 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/team.py
--rw-r--r--   0 runner    (1001) docker     (123)     1755 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/teamHierarchy.py
--rw-r--r--   0 runner    (1001) docker     (123)     4058 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/user.py
--rw-r--r--   0 runner    (1001) docker     (123)     3392 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/type.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.467775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/entitiesCount.py
--rw-r--r--   0 runner    (1001) docker     (123)      964 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/servicesCount.py
--rw-r--r--   0 runner    (1001) docker     (123)      953 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.471775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.471775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/api/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/api/createEventSubscription.py
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/emailAlertConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      476 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/entitySpelFilters.py
--rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/eventFilterRule.py
--rw-r--r--   0 runner    (1001) docker     (123)     5148 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/eventSubscription.py
--rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.475775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1969 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)      469 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dataInsightPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1577 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1208 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.475775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      676 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
--rw-r--r--   0 runner    (1001) docker     (123)     1291 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      600 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      955 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      927 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      597 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1728 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1116 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.475775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/objectstore/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/objectstore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/objectstore/containerMetadataConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      829 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/objectstoreServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     1366 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)      457 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)     5141 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.475775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/monitoring/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/monitoring/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      282 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/monitoring/eventMonitorProvider.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.475775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.479775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      579 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      700 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      636 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      669 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      757 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      494 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.479775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/awsCredentials.py
--rw-r--r--   0 runner    (1001) docker     (123)      894 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/azureCredentials.py
--rw-r--r--   0 runner    (1001) docker     (123)      822 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/gcsCredentials.py
--rw-r--r--   0 runner    (1001) docker     (123)     2136 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/gcsValues.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.479775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/secrets/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/secrets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      938 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
--rw-r--r--   0 runner    (1001) docker     (123)      560 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/securityConfiguration.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.479775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/ssl/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/ssl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      520 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)      727 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.479775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/settings/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/settings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2213 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/settings/eventPublisherJob.py
--rw-r--r--   0 runner    (1001) docker     (123)     2063 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/settings/settings.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.483775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/customMetric.py
--rw-r--r--   0 runner    (1001) docker     (123)     2617 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/testCase.py
--rw-r--r--   0 runner    (1001) docker     (123)     3317 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/testDefinition.py
--rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/testSuite.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.491775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/
--rw-r--r--   0 runner    (1001) docker     (123)       97 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/auditLog.py
--rw-r--r--   0 runner    (1001) docker     (123)     4211 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/basic.py
--rw-r--r--   0 runner    (1001) docker     (123)     4078 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/changeEvent.py
--rw-r--r--   0 runner    (1001) docker     (123)      870 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/collectionDescriptor.py
--rw-r--r--   0 runner    (1001) docker     (123)      526 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/csvDocumentation.py
--rw-r--r--   0 runner    (1001) docker     (123)      422 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/csvErrorType.py
--rw-r--r--   0 runner    (1001) docker     (123)      942 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/csvFile.py
--rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/csvImportResult.py
--rw-r--r--   0 runner    (1001) docker     (123)      453 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/dailyCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1887 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/databaseConnectionConfig.py
--rw-r--r--   0 runner    (1001) docker     (123)     2457 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityHistory.py
--rw-r--r--   0 runner    (1001) docker     (123)     2938 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityLineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     1560 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityReference.py
--rw-r--r--   0 runner    (1001) docker     (123)     2337 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityRelationship.py
--rw-r--r--   0 runner    (1001) docker     (123)      579 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityUsage.py
--rw-r--r--   0 runner    (1001) docker     (123)      707 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/filterPattern.py
--rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/function.py
--rw-r--r--   0 runner    (1001) docker     (123)      262 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/include.py
--rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/jdbcConnection.py
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/paging.py
--rw-r--r--   0 runner    (1001) docker     (123)      672 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/profile.py
--rw-r--r--   0 runner    (1001) docker     (123)     1362 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/queryParserData.py
--rw-r--r--   0 runner    (1001) docker     (123)      739 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/reaction.py
--rw-r--r--   0 runner    (1001) docker     (123)      604 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/schedule.py
--rw-r--r--   0 runner    (1001) docker     (123)     2333 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/storage.py
--rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/tableQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/tableUsageCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/tagLabel.py
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/usageDetails.py
--rw-r--r--   0 runner    (1001) docker     (123)      413 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/usageRequest.py
--rw-r--r--   0 runner    (1001) docker     (123)      892 2023-03-30 09:54:39.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/votes.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.491775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13738 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/action.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.491775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/utils/ometa_config_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.491775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.495775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/bulk_sink.py
--rw-r--r--   0 runner    (1001) docker     (123)      742 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/closeable.py
--rw-r--r--   0 runner    (1001) docker     (123)     1149 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    16412 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2219 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/processor.py
--rw-r--r--   0 runner    (1001) docker     (123)     1938 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/sink.py
--rw-r--r--   0 runner    (1001) docker     (123)     2902 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/stage.py
--rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/status.py
--rw-r--r--   0 runner    (1001) docker     (123)     9297 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/topology_runner.py
--rw-r--r--   0 runner    (1001) docker     (123)    12770 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.495775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/bulksink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/bulksink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12958 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/bulksink/metadata_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.495775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5040 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/builders.py
--rw-r--r--   0 runner    (1001) docker     (123)     2440 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/headers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1920 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/secrets.py
--rw-r--r--   0 runner    (1001) docker     (123)     1229 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/session.py
--rw-r--r--   0 runner    (1001) docker     (123)    13068 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/test_connections.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.495775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4077 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    16471 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    15098 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/sql_lineage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.499775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/custom_pydantic.py
--rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/custom_types.py
--rw-r--r--   0 runner    (1001) docker     (123)     1843 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/delete_entity.py
--rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/encoders.py
--rw-r--r--   0 runner    (1001) docker     (123)     7769 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/es_documents.py
--rw-r--r--   0 runner    (1001) docker     (123)     1136 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/ometa_classification.py
--rw-r--r--   0 runner    (1001) docker     (123)      825 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/ometa_topic_data.py
--rw-r--r--   0 runner    (1001) docker     (123)      850 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/pipeline_status.py
--rw-r--r--   0 runner    (1001) docker     (123)      911 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/profile_data.py
--rw-r--r--   0 runner    (1001) docker     (123)      976 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/table_metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/tests_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     5299 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/topology.py
--rw-r--r--   0 runner    (1001) docker     (123)     1078 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/user.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.503775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14255 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/auth_provider.py
--rw-r--r--   0 runner    (1001) docker     (123)    10359 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/client_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3964 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/credentials.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.507775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5613 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     3493 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/es_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1841 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/glossary_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     4098 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5479 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)    17437 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/patch_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     4153 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2842 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/query_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)    16292 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2519 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/server_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2772 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/service_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)    10152 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/table_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     7601 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/tests_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/topic_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/user_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/version_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1048 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    26973 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/ometa_api.py
--rw-r--r--   0 runner    (1001) docker     (123)     2588 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/patch.py
--rw-r--r--   0 runner    (1001) docker     (123)     3709 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/provider_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.507775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/processor/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/processor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8406 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/processor/pii.py
--rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/processor/query_parser.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.507775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    35789 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.511775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6646 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     2213 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     5617 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     7996 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     5630 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     9648 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     4394 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     6607 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     4462 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1887 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     1897 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)     2220 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (123)    19889 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.515775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/connections.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.515775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12907 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/dashboard_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.515775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9578 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.515775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    17503 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.515775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2754 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    12565 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.515775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6133 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1780 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9037 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.519775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6930 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1809 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    12063 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.519775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2020 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    12844 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      761 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.519775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2183 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1999 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    11869 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.519775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5676 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/api_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     5011 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     3446 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     5729 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/db_source.py
--rw-r--r--   0 runner    (1001) docker     (123)     1967 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     5887 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.523775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/
--rw-r--r--   0 runner    (1001) docker     (123)      766 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4430 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    16019 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      864 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.523775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.523775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/athena/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/athena/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3243 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/athena/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6344 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/athena/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.527775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/azuresql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/azuresql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/azuresql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3240 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/azuresql/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.527775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5566 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1205 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    16496 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1420 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.527775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2647 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1246 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     8407 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     2301 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     3472 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)      986 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/column_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)    14755 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/column_type_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    21442 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/common_db_source.py
--rw-r--r--   0 runner    (1001) docker     (123)    16780 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/database_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.531775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7189 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     3319 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3321 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    11375 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1006 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2257 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     4821 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.531775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4667 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    24502 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     3732 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.531775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/db2/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/db2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1980 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/db2/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     3210 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/db2/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.531775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dbt/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dbt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7562 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_service.py
--rw-r--r--   0 runner    (1001) docker     (123)    38682 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dbt/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.531775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/deltalake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/deltalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5032 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/deltalake/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    14961 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/deltalake/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.535776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2176 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7737 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.535776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/druid/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/druid/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/druid/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/druid/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.535776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8699 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.535776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/glue/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/glue/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2433 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/glue/connection.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    15828 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/glue/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.535776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3774 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9233 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      736 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     4737 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/lineage_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.535776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mariadb/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mariadb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mariadb/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mariadb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.539775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2487 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)      998 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     1742 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)      988 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     8105 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.539775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2570 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2064 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     5100 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.539775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4073 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7591 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1948 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.539775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.543775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2611 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2256 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    11644 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     5880 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     7381 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1034 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.543775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3402 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6120 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      654 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.543775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1262 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)     1498 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query_parser_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.547775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2657 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1438 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    17991 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     7554 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1290 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.547775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/salesforce/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/salesforce/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1935 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/salesforce/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    10089 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/salesforce/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)    43051 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sample_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3851 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sample_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.547775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/singlestore/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/singlestore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/singlestore/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/singlestore/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.547775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6612 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    13379 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3452 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     4088 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     7694 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    13247 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sql_column_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2970 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlalchemy_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.547775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlite/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlite/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlite/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.551776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9612 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)      980 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     5517 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/usage_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.551776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/lineage.py
--rw-r--r--   0 runner    (1001) docker     (123)    11060 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     4039 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/queries.py
--rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/query_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     1217 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/usage.py
--rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/ldap_users.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.551776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10878 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/common_broker_source.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.551776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4600 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.551776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1838 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    10127 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     2464 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/messaging_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     1627 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3513 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2550 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    20688 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     6272 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2452 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    19301 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     7272 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1792 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/openmetadata/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/openmetadata/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/openmetadata/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.555776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.559775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1841 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8033 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.559775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7459 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     1124 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.559775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/objectstore_service.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.559775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/s3/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/s3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2737 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/s3/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)    17536 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/s3/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.559775 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.563776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3583 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1794 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9291 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.563776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4155 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4002 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)    16659 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.575776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2359 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     9412 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     3549 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/queries.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.575776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8571 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.575776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2086 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6165 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.575776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     1798 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7741 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.575776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1779 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     6767 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.575776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5069 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     2478 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/connection.py
--rw-r--r--   0 runner    (1001) docker     (123)     8776 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)     9064 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/pipeline_service.py
--rw-r--r--   0 runner    (1001) docker     (123)     1139 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/sqa_types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.579776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/stage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/stage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6827 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/stage/table_usage.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.579776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.579776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/datalake_test_suite_interface.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.579776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/mixins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2184 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/mixins/pandas_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.579776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.583776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/mixins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/mixins/sqa_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     6314 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/sqa_test_suite_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     1673 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/test_suite_protocol.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.583776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3972 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/avro_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2730 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/json_schema_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/protobuf_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2304 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/schema_parsers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.583776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.583776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2465 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/api/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    21478 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.587776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.587776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1778 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/distinct_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/duplicate_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1621 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/ilike_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     1860 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/iqr.py
--rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/like_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/non_parametric_skew.py
--rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/null_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/unique_ratio.py
--rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/core.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.587776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/hybrid/
--rw-r--r--   0 runner    (1001) docker     (123)     7342 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/hybrid/histogram.py
--rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.595776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2594 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/column_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2635 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/column_names.py
--rw-r--r--   0 runner    (1001) docker     (123)     1646 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2574 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/count_in_set.py
--rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/distinct_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1580 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/ilike_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1549 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/like_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1683 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/max.py
--rw-r--r--   0 runner    (1001) docker     (123)     2194 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/max_length.py
--rw-r--r--   0 runner    (1001) docker     (123)     2784 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/mean.py
--rw-r--r--   0 runner    (1001) docker     (123)     1683 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/min.py
--rw-r--r--   0 runner    (1001) docker     (123)     2200 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/min_length.py
--rw-r--r--   0 runner    (1001) docker     (123)     1622 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/not_like_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/not_regexp_match_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1549 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/null_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1994 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/regexp_match_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     1424 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/row_count.py
--rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/stddev.py
--rw-r--r--   0 runner    (1001) docker     (123)     1542 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/sum.py
--rw-r--r--   0 runner    (1001) docker     (123)     2533 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/unique_count.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.595776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/system/
--rw-r--r--   0 runner    (1001) docker     (123)    14315 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/system/system.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.595776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2679 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/first_quartile.py
--rw-r--r--   0 runner    (1001) docker     (123)     2703 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/median.py
--rw-r--r--   0 runner    (1001) docker     (123)     2670 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/third_quartile.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.599776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8406 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/converter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.599776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/concat.py
--rw-r--r--   0 runner    (1001) docker     (123)     1510 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/conn_test.py
--rw-r--r--   0 runner    (1001) docker     (123)    10286 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/datetime.py
--rw-r--r--   0 runner    (1001) docker     (123)     2074 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/length.py
--rw-r--r--   0 runner    (1001) docker     (123)     3697 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/median.py
--rw-r--r--   0 runner    (1001) docker     (123)     2887 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/modulo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3140 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/random_num.py
--rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/sum.py
--rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.603776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2119 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/bytea_to_string.py
--rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/hex_byte_string.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/uuid.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.603776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19880 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/core.py
--rw-r--r--   0 runner    (1001) docker     (123)     3478 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/datalake_sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2640 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/default.py
--rw-r--r--   0 runner    (1001) docker     (123)     4135 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/handle_partition.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.603776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.607776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)    12295 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/pandas/pandas_profiler_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     7737 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/profiler_protocol.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.607776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)    18702 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/sqlalchemy/sqa_profiler_interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/models.py
--rw-r--r--   0 runner    (1001) docker     (123)     4724 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/runner.py
--rw-r--r--   0 runner    (1001) docker     (123)     9183 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/sampler.py
--rw-r--r--   0 runner    (1001) docker     (123)     3007 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.607776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/sink/file.py
--rw-r--r--   0 runner    (1001) docker     (123)     3182 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.607776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.607776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/api/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/api/models.py
--rw-r--r--   0 runner    (1001) docker     (123)    20141 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/api/workflow.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.607776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/runner/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/runner/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/runner/core.py
--rw-r--r--   0 runner    (1001) docker     (123)      874 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/runner/models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.611776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/sink/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/sink/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2925 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/sink/metadata_rest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.611776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4843 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/base_test_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.611776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.615776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3052 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2680 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2691 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2713 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2680 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2712 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     3964 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     2678 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     3785 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     3011 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     2532 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (123)     3105 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (123)     2905 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (123)     2954 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.623776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1778 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1761 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1768 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1779 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1761 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1779 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1807 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1767 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1747 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1851 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1789 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1749 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (123)     1936 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (123)     1787 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (123)     1807 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.627776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1757 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1736 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1718 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMinToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1839 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesMissingCount.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1712 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1732 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1788 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeNotNull.py
--rw-r--r--   0 runner    (1001) docker     (123)     2550 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeUnique.py
--rw-r--r--   0 runner    (1001) docker     (123)     2151 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToMatchRegex.py
--rw-r--r--   0 runner    (1001) docker     (123)     2182 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.631776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/mixins/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/mixins/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2218 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/mixins/pandas_validator_mixin.py
--rw-r--r--   0 runner    (1001) docker     (123)     2889 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/mixins/sqa_validator_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.631776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.631776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2311 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     2435 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (123)     3178 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     2614 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     2650 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     2417 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     3541 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableRowInsertedCountToBeBetween.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.635776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1219 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1281 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1151 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1296 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     2950 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableRowInsertedCountToBeBetween.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.639776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1413 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1393 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnNameToExist.py
--rw-r--r--   0 runner    (1001) docker     (123)     1462 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnToMatchSet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1232 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableCustomSQLQuery.py
--rw-r--r--   0 runner    (1001) docker     (123)     1356 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableRowCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableRowCountToEqual.py
--rw-r--r--   0 runner    (1001) docker     (123)     2431 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
--rw-r--r--   0 runner    (1001) docker     (123)      954 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/validator.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.639776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/timer/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/timer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/timer/repeated_timer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2341 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/timer/workflow_reporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.651776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3272 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/azure_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2133 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/class_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1647 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/client_version.py
--rw-r--r--   0 runner    (1001) docker     (123)      955 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     4859 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/credentials.py
--rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/custom_thread_pool.py
--rw-r--r--   0 runner    (1001) docker     (123)    12276 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/dbt_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2962 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/dispatch.py
--rw-r--r--   0 runner    (1001) docker     (123)     2119 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/elasticsearch.py
--rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/entity_link.py
--rw-r--r--   0 runner    (1001) docker     (123)     6904 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/filters.py
--rw-r--r--   0 runner    (1001) docker     (123)    15052 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/fqn.py
--rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/gcs_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9632 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     7309 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/importer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4255 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/lru_cache.py
--rw-r--r--   0 runner    (1001) docker     (123)     2683 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/metadata_service_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)     3559 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/partition.py
--rw-r--r--   0 runner    (1001) docker     (123)     3592 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/s3_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.655776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/aws_based_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     2449 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/aws_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     2379 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1075 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/external_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/noop_secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/secrets_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)     3202 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/secrets_manager_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     1051 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/singleton.py
--rw-r--r--   0 runner    (1001) docker     (123)      992 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/sqa_like_column.py
--rw-r--r--   0 runner    (1001) docker     (123)     5612 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/sqa_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3088 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/sqlalchemy_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1927 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/ssl_registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1945 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/tag_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/test_suite.py
--rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/time_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/timeout.py
--rw-r--r--   0 runner    (1001) docker     (123)      916 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/uuid_encoder.py
--rw-r--r--   0 runner    (1001) docker     (123)    16726 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/workflow_output_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.655776 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/workflow/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/workflow/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3278 2023-03-30 09:52:58.000000 openmetadata-ingestion-1.0.0.0.dev0/src/metadata/workflow/workflow_status_mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-30 09:55:06.655776 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     2531 2023-03-30 09:55:02.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    64138 2023-03-30 09:55:02.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-30 09:55:02.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      144 2023-03-30 09:55:02.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-30 09:54:28.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)     5507 2023-03-30 09:55:02.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       39 2023-03-30 09:55:02.000000 openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.327016 openmetadata-ingestion-1.0.0.0.dev1/
+-rw-r--r--   0 runner    (1001) docker     (123)    11356 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     2531 2023-04-14 12:26:33.327016 openmetadata-ingestion-1.0.0.0.dev1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)      636 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      809 2023-04-14 12:26:33.331016 openmetadata-ingestion-1.0.0.0.dev1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     9378 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.003017 openmetadata-ingestion-1.0.0.0.dev1/src/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.003017 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/
+-rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.007017 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3561 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/hooks/openmetadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.007017 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3273 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/callback.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.007017 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/commons.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3938 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4223 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/providers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2958 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/operator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12899 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4200 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/status.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.007017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2710 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/__version__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.011017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/antlr/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/antlr/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1854 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/antlr/split_listener.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.011017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/automations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2138 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/automations/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.011017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6524 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/backup.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1697 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/dataquality.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6215 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/db_dump.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13593 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/docker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/ingest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1652 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/insight.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4451 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/openmetadata_dag_config_migration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/openmetadata_imports_migration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1675 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2882 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/restore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/clients/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/clients/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6139 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/clients/aws_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5201 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/clients/domo_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16228 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cmd.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/config/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/config/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3222 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/config/common.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13196 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/helper/
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/helper/data_insight_es_index.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2289 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8289 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/entity_report_data_processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13522 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/web_analytic_report_data_processor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/runner/
+-rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/runner/kpi_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5130 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/runner/run_result_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3708 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.015017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1554 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/api/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24326 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3894 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6358 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1673 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/test_suite_protocol.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/runner/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/runner/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1656 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/runner/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)      874 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/runner/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4844 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/base_test_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.019017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.027017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3054 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2682 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2693 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2715 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2682 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2714 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3966 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2680 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4510 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2808 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3013 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2534 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3107 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2907 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2956 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.031017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1782 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1771 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1751 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1855 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1793 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1753 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.035017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1770 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1749 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1731 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1750 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1852 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1737 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1717 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1725 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2195 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.035017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/mixins/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/mixins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2890 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.035017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.039017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2313 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2302 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2437 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3180 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2677 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2419 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3505 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.039017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1226 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1435 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1280 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3095 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.043017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1427 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1475 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1405 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1369 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1289 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2444 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
+-rw-r--r--   0 runner    (1001) docker     (123)      954 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/validator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:32.975017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.067017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/
+-rw-r--r--   0 runner    (1001) docker     (123)      353 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/airbyte.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/airflow.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/amundsen.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      557 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/athena.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      338 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/athena_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/athena_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      606 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/atlas.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      601 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/azuresql.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      894 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1336 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      998 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      553 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/clickhouse.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      325 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/clickhouse_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/clickhouse_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      378 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/dagster.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      413 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/data_insight.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      531 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/databricks.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      325 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/databricks_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      482 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/databricks_pipeline.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      627 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/databricks_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      607 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/datalake.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1207 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/datalake_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      436 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/db2.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/db2_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2087 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/dbt.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/deltalake.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      547 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/domodashboard.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      616 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/dynamodb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/fivetran.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      506 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/glue.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      523 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/gluepipeline.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      367 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/hive.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1192 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/kafka.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      415 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/kinesis.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      306 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/ldap_user_to_catalog.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      471 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/looker.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      455 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mariadb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      452 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/metabase.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      699 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/migrate_source.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1084 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mlflow.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      744 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mode.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mssql.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      315 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mssql_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      594 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mssql_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      507 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mysql.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mysql_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      530 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/openmetadata.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      448 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/oracle.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      491 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/pinotdb.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      443 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/postgres.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      343 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/postgres_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      630 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/postgres_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      826 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/powerbi.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      451 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/presto.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      707 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/query_log_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      628 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/quicksight.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      457 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redash.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      488 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redpanda.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      560 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      319 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1037 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift_profiler.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      646 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      481 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/sagemaker.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      464 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/salesforce.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      472 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/singlestore.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      676 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/snowflake.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      317 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/snowflake_lineage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      620 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/snowflake_usage.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      651 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/superset.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      885 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/tableau.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      874 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/test_suite.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      547 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/trino.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      435 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/vertica.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:32.975017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.075017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/
+-rw-r--r--   0 runner    (1001) docker     (123)     8889 2023-04-14 12:26:08.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/EntityLinkLexer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-04-14 12:26:08.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/EntityLinkListener.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9288 2023-04-14 12:26:08.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/EntityLinkParser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2606 2023-04-14 12:26:08.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/FqnLexer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1210 2023-04-14 12:26:08.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/FqnListener.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6811 2023-04-14 12:26:08.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/FqnParser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:32.983017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.075017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      948 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportData.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.079017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1263 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1094 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py
+-rw-r--r--   0 runner    (1001) docker     (123)      958 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEvent.py
+-rw-r--r--   0 runner    (1001) docker     (123)      883 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventData.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.079017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventType/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventType/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      978 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.083017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.083017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/analytics/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/analytics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.087017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/automations/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1483 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/automations/createWorkflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.091017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/createClassification.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1754 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/createTag.py
+-rw-r--r--   0 runner    (1001) docker     (123)      525 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/loadTags.py
+-rw-r--r--   0 runner    (1001) docker     (123)      959 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/createBot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1240 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/createEventPublisherJob.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/createType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.095017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1350 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createChart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2298 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createContainer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDashboard.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1775 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDashboardDataModel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDatabase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1252 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDatabaseSchema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1541 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createGlossary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2402 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createGlossaryTerm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2446 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createMlModel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createTable.py
+-rw-r--r--   0 runner    (1001) docker     (123)      727 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createTableProfile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2713 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createTopic.py
+-rw-r--r--   0 runner    (1001) docker     (123)      424 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/restoreEntity.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.099017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1294 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.099017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/kpi/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1322 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.099017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      401 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/closeTask.py
+-rw-r--r--   0 runner    (1001) docker     (123)      526 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/createPost.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1692 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/createThread.py
+-rw-r--r--   0 runner    (1001) docker     (123)      421 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/resolveTask.py
+-rw-r--r--   0 runner    (1001) docker     (123)      749 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/threadCount.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.103017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      579 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/lineage/addLineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      672 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/openMetadataServerVersion.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.103017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/policies/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1082 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/policies/createPolicy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.107017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1182 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createDashboardService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1163 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createDatabaseService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createMessagingService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createMetadataService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1160 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createMlModelService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1171 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createPipelineService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1232 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createStorageService.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.107017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/ingestionPipelines/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)      547 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/setOwner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.107017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      830 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/createRole.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2380 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/createTeam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2082 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/createUser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.111017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1227 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createCustomMetric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createTestCase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1159 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createTestDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (123)      820 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createTestSuite.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/voteRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.119017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      339 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      462 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/basicLoginRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      886 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/changePasswordRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      437 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/createPersonalToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/emailRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1120 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/emailVerificationToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      347 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/generateToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      751 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/jwtAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      409 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/loginRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      626 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/logoutRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      683 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/passwordResetRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1027 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/passwordResetToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      956 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/personalAccessToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/refreshToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      623 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/registrationRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      476 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/revokePersonalToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      324 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/revokeToken.py
+-rw-r--r--   0 runner    (1001) docker     (123)      288 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/serviceTokenEnum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1321 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/ssoAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      378 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/tokenRefreshRequest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.123017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1233 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/applicationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/authConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/authenticationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1558 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/authorizerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      363 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/changeEventConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      442 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      357 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/fernetConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      658 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1823 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.127017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      894 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      451 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/jvmDefaultConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      462 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/trustAllConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      978 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1828 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      507 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/sslConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      436 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/taskNotificationConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      885 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.127017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3164 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/dataInsightChart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1933 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.127017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/kpi/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/kpi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1520 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/kpi/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2537 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/kpi/kpi.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.131017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py
+-rw-r--r--   0 runner    (1001) docker     (123)      791 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py
+-rw-r--r--   0 runner    (1001) docker     (123)      635 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py
+-rw-r--r--   0 runner    (1001) docker     (123)      975 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py
+-rw-r--r--   0 runner    (1001) docker     (123)      843 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.131017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/email/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/email/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1250 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/email/emailRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1316 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/email/smtpSettings.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.135017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.135017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/automations/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/automations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/automations/testServiceConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/automations/workflow.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2055 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/bot.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.135017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/classification/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2864 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/classification/classification.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/classification/tag.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.139017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3144 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/chart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4668 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/container.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3408 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/dashboard.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3327 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/dashboardDataModel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3385 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3096 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/databaseSchema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/glossary.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/glossaryTerm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2414 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6419 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/mlmodel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6226 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2303 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/report.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21396 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4555 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/topic.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.139017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/events/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/events/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      900 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/events/webhook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.143017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/feed/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/feed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5227 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/feed/thread.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.143017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.143017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1587 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1821 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1497 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/rule.py
+-rw-r--r--   0 runner    (1001) docker     (123)      681 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/filters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3173 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/policy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.147017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.147017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.147017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      935 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1492 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1700 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2025 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1270 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1538 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1785 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.159017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2250 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2566 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3367 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3459 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.159017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      531 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      518 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1886 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2272 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3731 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1690 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1222 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3359 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2678 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3326 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3652 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2629 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3341 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2574 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2749 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3937 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2785 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.159017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      935 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2450 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      901 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      729 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2263 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.159017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5392 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.163017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      920 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1087 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      915 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      735 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.163017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1154 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1615 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      545 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1412 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1371 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      916 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2241 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1099 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/serviceConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.167017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1172 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1146 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1119 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2930 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1575 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3955 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/dashboardService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5782 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/databaseService.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.167017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/ingestionPipelines/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5984 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3714 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/messagingService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3475 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/metadataService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/mlmodelService.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3973 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/pipelineService.py
+-rw-r--r--   0 runner    (1001) docker     (123)      391 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/serviceType.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2941 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/storageService.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.167017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2564 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/role.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4320 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/team.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1755 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/teamHierarchy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4058 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/user.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3392 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/type.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.167017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1254 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/entitiesCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)      951 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/servicesCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/supersetApiConnection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.171017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.171017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/api/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1620 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/api/createEventSubscription.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/emailAlertConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      476 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/entitySpelFilters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/eventFilterRule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5148 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/eventSubscription.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.175016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)      469 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dataInsightPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1577 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1208 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.175016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      676 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1291 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      600 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      955 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      927 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      597 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1279 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1728 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1116 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1366 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.175016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1361 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      797 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)      457 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5129 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.175016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/monitoring/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/monitoring/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/monitoring/eventMonitorProvider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.179016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.179016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      579 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      700 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/azureSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      636 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      669 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/googleSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      757 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      494 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3242 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/samlSSOClientConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.183017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      677 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/accessTokenAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2001 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/awsCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)      894 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/azureCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)      589 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/basicAuth.py
+-rw-r--r--   0 runner    (1001) docker     (123)      822 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/gcsCredentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2136 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/gcsValues.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1113 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/githubCredentials.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.183017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/secrets/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/secrets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      938 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
+-rw-r--r--   0 runner    (1001) docker     (123)      560 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/securityConfiguration.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.183017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/ssl/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/ssl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      532 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)      727 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/ssl/verifySSLConfig.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.183017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/settings/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/settings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2181 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/settings/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.183017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/system/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/system/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3517 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/system/eventPublisherJob.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.187017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1477 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1323 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/customMetric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2617 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/testCase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3317 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/testDefinition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2725 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/testSuite.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.195016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/auditLog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4211 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/basic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4078 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/changeEvent.py
+-rw-r--r--   0 runner    (1001) docker     (123)      870 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/collectionDescriptor.py
+-rw-r--r--   0 runner    (1001) docker     (123)      526 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvDocumentation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      422 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvErrorType.py
+-rw-r--r--   0 runner    (1001) docker     (123)      942 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvFile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1264 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvImportResult.py
+-rw-r--r--   0 runner    (1001) docker     (123)      453 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/dailyCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1887 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/databaseConnectionConfig.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2457 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityHistory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2938 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityLineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1560 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityReference.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2337 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityRelationship.py
+-rw-r--r--   0 runner    (1001) docker     (123)      579 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityUsage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      707 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/filterPattern.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1349 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/function.py
+-rw-r--r--   0 runner    (1001) docker     (123)      262 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/include.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1022 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/jdbcConnection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/paging.py
+-rw-r--r--   0 runner    (1001) docker     (123)      672 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/profile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1362 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/queryParserData.py
+-rw-r--r--   0 runner    (1001) docker     (123)      739 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/reaction.py
+-rw-r--r--   0 runner    (1001) docker     (123)      604 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/schedule.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2580 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/tableQuery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/tableUsageCount.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/tagLabel.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/usageDetails.py
+-rw-r--r--   0 runner    (1001) docker     (123)      413 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/usageRequest.py
+-rw-r--r--   0 runner    (1001) docker     (123)      892 2023-04-14 12:26:07.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/votes.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.195016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13738 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/action.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.195016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2795 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/utils/ometa_config_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.195016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.199016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/bulk_sink.py
+-rw-r--r--   0 runner    (1001) docker     (123)      742 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/closeable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1149 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16436 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2217 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/processor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1937 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/sink.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2902 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/stage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/status.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9295 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/topology_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12770 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.199016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/bulksink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/bulksink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12958 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/bulksink/metadata_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.203017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5040 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/builders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2440 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/headers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1920 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/secrets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1229 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/session.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13187 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/test_connections.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.203017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4077 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16471 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15543 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/sql_lineage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.207017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3340 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/custom_pydantic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/custom_types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/delete_entity.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/encoders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9159 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/es_documents.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1136 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/ometa_classification.py
+-rw-r--r--   0 runner    (1001) docker     (123)      825 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/ometa_topic_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      850 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/pipeline_status.py
+-rw-r--r--   0 runner    (1001) docker     (123)      911 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/profile_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)      976 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/table_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/tests_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5298 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/topology.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1078 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/user.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.207017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14255 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/auth_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10832 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/client_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3964 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/credentials.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.215016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5607 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3639 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/es_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17727 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/glossary_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4098 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5010 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/lineage_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5479 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18260 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/patch_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4154 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4153 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2842 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/query_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15373 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2519 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/server_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2772 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/service_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9625 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/table_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7614 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/tests_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1472 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/topic_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2307 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/user_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/version_mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1048 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26655 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/ometa_api.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4456 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/provider_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.215016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/processor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8406 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/processor/pii.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4033 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/processor/query_parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.215016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    33846 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.219016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9905 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/container_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8525 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1629 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7376 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8343 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6018 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8548 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/query_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10284 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2474 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4395 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8763 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4517 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1433 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1445 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2219 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18884 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.223016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/connections.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.223016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16246 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/dashboard_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.223016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/domodashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/domodashboard/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9578 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.223016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/columns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26949 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4699 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/parser.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.227017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4365 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1870 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10977 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2096 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.227017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6133 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1780 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9037 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.227017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10656 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1809 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13850 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4151 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.231016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2020 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12844 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      761 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.231016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2183 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1999 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12074 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.231016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5787 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/api_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5011 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3446 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5830 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/db_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1967 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5890 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/mixin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1391 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.235016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/
+-rw-r--r--   0 runner    (1001) docker     (123)      955 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3464 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4498 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19439 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.239016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.239016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3243 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1655 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6344 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1952 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3307 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2299 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.239016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/azuresql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/azuresql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/azuresql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3240 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/azuresql/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.243016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5566 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1205 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16495 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1420 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2536 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1145 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.243016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2647 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1246 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8406 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2301 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3472 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1221 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)      986 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/column_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14811 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/column_type_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21440 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/common_db_source.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14783 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/database_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.247017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7172 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3319 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11373 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1006 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2257 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2434 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.247017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4667 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29662 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3751 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.247017 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/db2/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/db2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1980 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/db2/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3210 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/db2/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.251016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dbt/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dbt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8519 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dbt/dbt_service.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38702 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dbt/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.251016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/deltalake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/deltalake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5032 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/deltalake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14961 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/deltalake/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.251016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/domodatabase/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/domodatabase/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2176 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/domodatabase/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7737 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/domodatabase/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.251016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/druid/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/druid/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2124 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/druid/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1642 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/druid/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.251016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dynamodb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dynamodb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dynamodb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8699 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dynamodb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.251016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/glue/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/glue/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2433 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/glue/connection.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    13029 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/glue/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.255016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3786 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12418 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      736 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4760 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/lineage_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.255016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mariadb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mariadb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mariadb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2079 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mariadb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.255016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2487 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)      997 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5017 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1742 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)      987 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8105 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.259016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2570 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2064 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5100 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.259016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4073 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5084 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2540 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8235 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.259016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/pinotdb/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/pinotdb/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/pinotdb/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/pinotdb/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.263016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2850 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2256 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11644 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5880 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7379 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1034 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.263016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3402 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6120 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      654 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.263016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1262 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1498 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3897 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query_parser_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.267016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1437 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17992 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7554 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2201 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1289 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.267016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/salesforce/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/salesforce/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1935 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/salesforce/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10088 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/salesforce/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)    44528 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sample_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3851 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sample_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.267016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/singlestore/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/singlestore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2038 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/singlestore/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/singlestore/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.271016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6611 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1147 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13378 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3452 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4088 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7693 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13318 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sql_column_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2970 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlalchemy_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.271016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlite/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlite/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2164 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlite/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlite/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.271016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9614 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)      980 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5727 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/usage_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.275016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1181 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/lineage.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11059 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4039 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/queries.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/query_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/usage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/ldap_users.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.275016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10878 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/common_broker_source.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.275016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kafka/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kafka/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4600 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kafka/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kafka/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.279016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1838 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10127 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2464 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/messaging_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.279016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/redpanda/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/redpanda/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/redpanda/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1627 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/redpanda/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.279016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.279016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3513 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2550 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20702 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6272 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.279016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2451 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19299 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8245 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.283016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/metadata_elasticsearch/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/metadata_elasticsearch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1792 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.283016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/openmetadata/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/openmetadata/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/openmetadata/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.283016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.283016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlflow/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1840 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlflow/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8032 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6821 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlmodel_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.283016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/sagemaker/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1764 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7459 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1124 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.283016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.287016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3668 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1793 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9396 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.287016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4155 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4625 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18710 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1743 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/models.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.287016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2455 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9412 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3549 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/queries.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.287016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/databrickspipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1964 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8570 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.287016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/domopipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/domopipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2086 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/domopipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6162 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.291016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1798 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7741 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.291016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/gluepipeline/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1779 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6767 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.291016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5069 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2478 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8799 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9064 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/pipeline_service.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1139 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/sqa_types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.291016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2706 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/connection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16878 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2319 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4854 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/storage_service.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/stage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/stage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6315 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/stage/table_usage.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/pandas/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4718 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/pandas/pandas_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/sqalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/sqalchemy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3408 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/sqalchemy/sqa_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8193 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/avro_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2730 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/json_schema_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7499 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/protobuf_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2304 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/schema_parsers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.295016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.299016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/api/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2466 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/api/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21453 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/api/workflow.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.299016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.299016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/pandas/
+-rw-r--r--   0 runner    (1001) docker     (123)    11888 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/pandas/pandas_profiler_interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7736 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/profiler_protocol.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.299016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/sqlalchemy/
+-rw-r--r--   0 runner    (1001) docker     (123)    18761 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/sqlalchemy/sqa_profiler_interface.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.299016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.299016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1778 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/distinct_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1688 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/duplicate_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1621 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/ilike_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1860 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/iqr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1609 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/like_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/non_parametric_skew.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1745 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/null_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2652 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/unique_ratio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/core.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.303016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/hybrid/
+-rw-r--r--   0 runner    (1001) docker     (123)     7362 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/hybrid/histogram.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4291 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.307016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2587 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/column_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2501 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/column_names.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1530 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/count_in_set.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1713 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/distinct_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1580 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/ilike_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1549 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/like_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/max.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2126 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/max_length.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/mean.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/min.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2127 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/min_length.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1622 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/not_like_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2504 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/not_regexp_match_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1431 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/null_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2396 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/regexp_match_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1306 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/row_count.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3432 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/stddev.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/sum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2654 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/unique_count.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.307016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/system/
+-rw-r--r--   0 runner    (1001) docker     (123)    14936 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/system/system.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.307016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2784 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/first_quartile.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2635 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/median.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2775 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/third_quartile.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.307016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8979 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/converter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.311016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1521 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/concat.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1510 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/conn_test.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10286 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/datetime.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2074 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/length.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5413 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/median.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2887 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/modulo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3140 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/random_num.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2523 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/sum.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4292 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.311016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2119 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/bytea_to_string.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2271 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/hex_byte_string.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/uuid.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.315016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19860 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2293 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/datalake_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2631 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/default.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4135 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/handle_partition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1765 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4725 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9184 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/sampler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3007 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.315016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/sink/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/sink/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2177 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/sink/file.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3182 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/sink/metadata_rest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.315016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      942 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3011 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/github.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1330 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/local.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.315016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/timer/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/timer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1410 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/timer/repeated_timer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2341 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/timer/workflow_reporter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.323016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3272 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/azure_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2132 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/class_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1647 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/client_version.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1115 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4859 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/credentials.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1847 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/custom_thread_pool.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12278 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/dbt_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2962 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/dispatch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/elasticsearch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/entity_link.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7377 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/filters.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15342 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/fqn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/gcs_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9838 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7311 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/importer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4255 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/lru_cache.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2571 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/metadata_service_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3559 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/partition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3592 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/s3_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.327016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1593 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/aws_based_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2449 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/aws_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2379 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/aws_ssm_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1075 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/external_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/noop_secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1288 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/secrets_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3202 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/secrets_manager_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1051 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/singleton.py
+-rw-r--r--   0 runner    (1001) docker     (123)      992 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/sqa_like_column.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5613 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/sqa_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3088 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/sqlalchemy_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1927 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/ssl_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1945 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/tag_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2595 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/test_suite.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3724 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/time_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2300 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/timeout.py
+-rw-r--r--   0 runner    (1001) docker     (123)      916 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/uuid_encoder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16705 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/workflow_output_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.327016 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/workflow/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/workflow/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3277 2023-04-14 12:24:35.000000 openmetadata-ingestion-1.0.0.0.dev1/src/metadata/workflow/workflow_status_mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-14 12:26:33.327016 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     2531 2023-04-14 12:26:28.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    65366 2023-04-14 12:26:28.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-14 12:26:28.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      144 2023-04-14 12:26:28.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-14 12:25:56.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)     5611 2023-04-14 12:26:28.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       39 2023-04-14 12:26:28.000000 openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/top_level.txt
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/LICENSE` & `openmetadata-ingestion-1.0.0.0.dev1/LICENSE`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/PKG-INFO` & `openmetadata-ingestion-1.0.0.0.dev1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: openmetadata-ingestion
-Version: 1.0.0.0.dev0
+Version: 1.0.0.0.dev1
 Summary: Ingestion Framework for OpenMetadata
 Home-page: https://open-metadata.org/
 Author: OpenMetadata Committers
 License: Apache License 2.0
 Project-URL: Documentation, https://docs.open-metadata.org/
 Project-URL: Source, https://github.com/open-metadata/OpenMetadata
 Requires-Python: >=3.7
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/README.md` & `openmetadata-ingestion-1.0.0.0.dev1/README.md`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/setup.cfg` & `openmetadata-ingestion-1.0.0.0.dev1/setup.cfg`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/setup.py` & `openmetadata-ingestion-1.0.0.0.dev1/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,15 +42,20 @@
     "pydomo": "pydomo~=0.3",
     "pymysql": "pymysql>=1.0.2",
     "pyodbc": "pyodbc>=4.0.35,<5",
     "scikit-learn": "scikit-learn~=1.0",  # Python 3.7 only goes up to 1.0.2
 }
 
 COMMONS = {
-    "datalake": {VERSIONS["boto3"], VERSIONS["pandas"], VERSIONS["pyarrow"]},
+    "datalake": {
+        VERSIONS["boto3"],
+        VERSIONS["pandas"],
+        VERSIONS["pyarrow"],
+        "python-snappy~=0.6.1",
+    },
     "hive": {
         "presto-types-parser>=0.0.2",
         "pyhive~=0.6",
     },
     "kafka": {
         VERSIONS["avro"],
         "confluent_kafka==1.8.2",
@@ -94,15 +99,15 @@
     "python-dateutil>=2.8.1",
     "python-jose~=3.3",
     "PyYAML",
     "requests>=2.23",
     "requests-aws4auth~=1.1",  # Only depends on requests as external package. Leaving as base.
     "setuptools~=65.6.3",
     "sqlalchemy>=1.4.0,<2",
-    "openmetadata-sqllineage==1.0.2",
+    "openmetadata-sqllineage>=1.0.3",
     "tabulate==0.9.0",
     "typing-compat~=0.1.0",  # compatibility requirements for 3.7
     "typing-inspect",
     "wheel~=0.38.4",
 }
 
 
@@ -171,15 +176,15 @@
         "sasl~=0.3",
         "thrift-sasl~=0.4",
         "impyla~=0.18.0",
     },
     "kafka": {*COMMONS["kafka"]},
     "kinesis": {VERSIONS["boto3"]},
     "ldap-users": {"ldap3==2.9.1"},
-    "looker": {"looker-sdk>=22.20.0"},
+    "looker": {"looker-sdk>=22.20.0", "lkml~=1.3"},
     "mlflow": {"mlflow-skinny~=1.30", "alembic~=1.10.2"},
     "mssql": {"sqlalchemy-pytds~=0.3"},
     "mssql-odbc": {VERSIONS["pyodbc"]},
     "mysql": {VERSIONS["pymysql"]},
     "nifi": {},  # uses requests
     "okta": {"okta~=2.3"},
     "oracle": {"cx_Oracle>=8.3.0,<9", "oracledb~=1.2"},
@@ -232,15 +237,15 @@
     # install dbt dependency
     "dbt-artifacts-parser",
 }
 
 build_options = {"includes": ["_cffi_backend"]}
 setup(
     name="openmetadata-ingestion",
-    version="1.0.0.0.dev0",
+    version="1.0.0.0.dev1",
     url="https://open-metadata.org/",
     author="OpenMetadata Committers",
     license="Apache License 2.0",
     description="Ingestion Framework for OpenMetadata",
     long_description=get_long_description(),
     long_description_content_type="text/markdown",
     python_requires=">=3.7",
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/__init__.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/__init__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/hooks/openmetadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/hooks/openmetadata.py`

 * *Files 4% similar despite different names*

```diff
@@ -48,15 +48,14 @@
         # Add defaults
         self.default_schema = "http"
         self.default_port = 8585
         self.default_verify_ssl = VerifySSL.no_ssl
         self.default_ssl_config = None
 
     def get_conn(self) -> OpenMetadataConnection:
-
         conn: Connection = self.get_connection(self.openmetadata_conn_id)
         jwt_token = conn.get_password()
         if not jwt_token:
             raise ValueError("JWT Token should be informed.")
 
         if not conn.host:
             raise ValueError("Host should be informed.")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/backend.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/backend.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/callback.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/callback.py`

 * *Files 0% similar despite different names*

```diff
@@ -73,15 +73,14 @@
     Add this function to the args of your DAG or Task
     as the value of `on_success_callback` to track
     task status on task success
 
     :param context: Airflow runtime context
     """
     try:
-
         config = get_lineage_config()
         metadata = OpenMetadata(config.metadata_config)
 
         operator: "BaseOperator" = context["task"]
         dag: "DAG" = context["dag"]
 
         operator.log.info("Updating pipeline status on success...")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/commons.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/commons.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/loader.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/loader.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/config/providers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/config/providers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/operator.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/operator.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/runner.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/airflow_provider_openmetadata/lineage/status.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/airflow_provider_openmetadata/lineage/status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/__main__.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/__main__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/__version__.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/__version__.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/antlr/split_listener.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/antlr/split_listener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/automations/runner.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/automations/runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/backup.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/backup.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/dataquality.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/dataquality.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 Data quality utility for the metadata CLI
 """
 import pathlib
 import sys
 import traceback
 
 from metadata.config.common import load_config_file
-from metadata.test_suite.api.workflow import TestSuiteWorkflow
+from metadata.data_quality.api.workflow import TestSuiteWorkflow
 from metadata.utils.logger import cli_logger
 from metadata.utils.workflow_output_handler import WorkflowType, print_init_error
 
 logger = cli_logger()
 
 
 def run_test(config_path: str) -> None:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/db_dump.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/db_dump.py`

 * *Files 1% similar despite different names*

```diff
@@ -109,15 +109,14 @@
     Dumps JSON data.
 
     Postgres: engine.name == "postgresql"
     MySQL: engine.name == "mysql"
     """
     with open(output, "a", encoding=UTF_8) as file:
         for table in tables:
-
             truncate = STATEMENT_TRUNCATE.format(table=table)
             file.write(truncate)
 
             res = engine.execute(text(STATEMENT_JSON.format(table=table))).all()
             for row in res:
                 insert = f"INSERT INTO {table} (json) VALUES ({clean_col(row.json, engine)});\n"
                 file.write(insert)
@@ -125,15 +124,14 @@
 
 def dump_all(tables: List[str], engine: Engine, output: Path) -> None:
     """
     Dump tables that need to store all data
     """
     with open(output, "a", encoding=UTF_8) as file:
         for table in tables:
-
             truncate = STATEMENT_TRUNCATE.format(table=table)
             file.write(truncate)
 
             res = engine.execute(text(STATEMENT_ALL.format(table=table))).all()
             for row in res:
                 data = ",".join(clean_col(col, engine) for col in row)
 
@@ -143,15 +141,14 @@
 
 def dump_entity_custom(engine: Engine, output: Path, inspector) -> None:
     """
     This function is used to dump entities with custom handling
     """
     with open(output, "a", encoding=UTF_8) as file:
         for table, data in CUSTOM_TABLES.items():
-
             truncate = STATEMENT_TRUNCATE.format(table=table)
             file.write(truncate)
 
             columns = inspector.get_columns(table_name=table)
 
             statement = STATEMENT_ALL_NEW.format(
                 cols=",".join(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/docker.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/docker.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/ingest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/ingest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/openmetadata_dag_config_migration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/openmetadata_dag_config_migration.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/openmetadata_imports_migration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/openmetadata_imports_migration.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/profile.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/profile.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/restore.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/restore.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cli/utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cli/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/clients/aws_client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/clients/aws_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,36 +29,35 @@
     S3 = "s3"
     CLOUDWATCH = "cloudwatch"
     DYNAMO_DB = "dynamodb"
     GLUE = "glue"
     SAGEMAKER = "sagemaker"
     KINESIS = "kinesis"
     QUICKSIGHT = "quicksight"
+    ATHENA = "athena"
 
 
 class AWSAssumeRoleException(Exception):
     """
     Exception class to handle assume role related issues
     """
 
 
 class AWSAssumeRoleCredentialWrapper(BaseModel):
-
     accessKeyId: str
     secretAccessKey: CustomSecretStr
     sessionToken: Optional[str]
 
 
 class AWSClient:
     """
     AWSClient creates a boto3 Session client based on AWSCredentials.
     """
 
     def __init__(self, config: "AWSCredentials"):
-
         self.config = (
             config
             if isinstance(config, AWSCredentials)
             else (AWSCredentials.parse_obj(config) if config else config)
         )
 
     @staticmethod
@@ -175,7 +174,10 @@
         return self.get_client(AWSServices.SAGEMAKER.value)
 
     def get_kinesis_client(self):
         return self.get_client(AWSServices.KINESIS.value)
 
     def get_quicksight_client(self):
         return self.get_client(AWSServices.QUICKSIGHT.value)
+
+    def get_athena_client(self):
+        return self.get_client(AWSServices.ATHENA.value)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/clients/domo_client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/clients/domo_client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/cmd.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/cmd.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from http.server import BaseHTTPRequestHandler, HTTPServer
 
 from metadata.__version__ import get_metadata_version
 from metadata.cli.backup import UploadDestinationType, run_backup
 from metadata.cli.dataquality import run_test
 from metadata.cli.docker import BACKEND_DATABASES, DockerActions, run_docker
 from metadata.cli.ingest import run_ingest
+from metadata.cli.insight import run_insight
 from metadata.cli.openmetadata_dag_config_migration import (
     run_openmetadata_dag_config_migration,
 )
 from metadata.cli.openmetadata_imports_migration import (
     run_openmetadata_imports_migration,
 )
 from metadata.cli.profile import run_profiler
@@ -40,14 +41,15 @@
     INGEST = "ingest"
     PROFILE = "profile"
     TEST = "test"
     DOCKER = "docker"
     BACKUP = "backup"
     RESTORE = "restore"
     WEBHOOK = "webhook"
+    INSIGHT = "insight"
     OPENMETADATA_IMPORTS_MIGRATION = "openmetadata_imports_migration"
     OPENMETADATA_DAG_CONFIG_MIGRATION = "openmetadata_dag_config_migration"
 
 
 OM_IMPORTS_MIGRATION = """
     Update DAG files generated after creating workflow in 0.11 and before.
     In 0.12 the airflow managed API package name changed from `openmetadata` to 
@@ -374,21 +376,26 @@
     )
     webhook_args(
         sub_parser.add_parser(
             MetadataCommands.WEBHOOK.value,
             help="Simple Webserver to test webhook metadata events",
         )
     )
+    create_common_config_parser_args(
+        sub_parser.add_parser(
+            MetadataCommands.INSIGHT.value, help="Data Insigt Workflow"
+        )
+    )
 
     add_metadata_args(parser)
     parser.add_argument("--debug", help="Debug Mode", action="store_true")
     return parser.parse_args(args)
 
 
-def metadata(args=None):
+def metadata(args=None):  # pylint: disable=too-many-branches
     """
     This method implements parsing of the arguments passed from CLI
     """
     contains_args = vars(get_parser(args))
     metadata_workflow = contains_args.get("command")
     config_file = contains_args.get("config")
     if contains_args.get("debug"):
@@ -396,14 +403,16 @@
     elif contains_args.get("log_level"):
         set_loggers_level(contains_args.get("log_level"))
     else:
         set_loggers_level(logging.INFO)
 
     if metadata_workflow == MetadataCommands.INGEST.value:
         run_ingest(config_path=config_file)
+    if metadata_workflow == MetadataCommands.INSIGHT.value:
+        run_insight(config_path=config_file)
     if metadata_workflow == MetadataCommands.PROFILE.value:
         run_profiler(config_path=config_file)
     if metadata_workflow == MetadataCommands.TEST.value:
         run_test(config_path=config_file)
     if metadata_workflow == MetadataCommands.BACKUP.value:
         run_backup(
             common_backup_obj_instance=BackupRestoreArgs(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/config/common.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/config/common.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/api/workflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/api/workflow.py`

 * *Files 6% similar despite different names*

```diff
@@ -48,18 +48,21 @@
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     OpenMetadataWorkflowConfig,
     Sink,
 )
 from metadata.ingestion.api.parser import parse_workflow_config_gracefully
 from metadata.ingestion.api.processor import ProcessorStatus
+from metadata.ingestion.api.workflow import REPORTS_INTERVAL_SECONDS
 from metadata.ingestion.ometa.ometa_api import EntityList, OpenMetadata
 from metadata.ingestion.sink.elasticsearch import ElasticsearchSink
+from metadata.timer.repeated_timer import RepeatedTimer
+from metadata.timer.workflow_reporter import get_ingestion_status_timer
 from metadata.utils.importer import get_sink
-from metadata.utils.logger import data_insight_logger
+from metadata.utils.logger import data_insight_logger, set_loggers_level
 from metadata.utils.time_utils import (
     get_beginning_of_day_timestamp_mill,
     get_end_of_day_timestamp_mill,
 )
 from metadata.utils.workflow_output_handler import print_data_insight_status
 from metadata.workflow.workflow_status_mixin import WorkflowStatusMixin
 
@@ -74,22 +77,26 @@
     Configure and run the Data Insigt workflow
 
     Attributes:
     """
 
     def __init__(self, config: OpenMetadataWorkflowConfig) -> None:
         self.config = config
+        self._timer: Optional[RepeatedTimer] = None
+
+        set_loggers_level(config.workflowConfig.loggerLevel.value)
+
         self.metadata_config: OpenMetadataConnection = (
             self.config.workflowConfig.openMetadataServerConfig
         )
         self.metadata = OpenMetadata(self.metadata_config)
         self.set_ingestion_pipeline_status(state=PipelineState.running)
 
         self.status = ProcessorStatus()
-        self.data_processor: Optional[
+        self.source: Optional[
             Union[
                 DataProcessor,
                 EntityReportDataProcessor,
                 WebAnalyticEntityViewReportDataProcessor,
                 WebAnalyticUserActivityReportDataProcessor,
             ]
         ] = None
@@ -109,14 +116,24 @@
                 sink_config=self.config.sink,
                 metadata_config=self.metadata_config,
                 from_="ingestion",
             )
 
             self.es_sink = cast(ElasticsearchSink, self.es_sink)
 
+    @property
+    def timer(self) -> RepeatedTimer:
+        """Status timer"""
+        if not self._timer:
+            self._timer = get_ingestion_status_timer(
+                interval=REPORTS_INTERVAL_SECONDS, logger=logger, workflow=self
+            )
+
+        return self._timer
+
     @staticmethod
     def _is_kpi_active(entity: Kpi) -> bool:
         """Check if a KPI is active
 
         Args:
             entity (Kpi): KPI entity
 
@@ -193,31 +210,34 @@
 
     def _execute_data_processor(self):
         """Data processor method to refine raw data into report data and ingest it in ES"""
         for report_data_type in ReportDataType:
             has_checked_and_handled_existing_es_data = False
             logger.info(f"Processing data for report type {report_data_type}")
             try:
-                self.data_processor = DataProcessor.create(
+                self.source = DataProcessor.create(
                     _data_processor_type=report_data_type.value, metadata=self.metadata
                 )
-                for record in self.data_processor.process():
+                for record in self.source.process():
                     if hasattr(self, "sink"):
                         self.sink.write_record(record)
                     if hasattr(self, "es_sink"):
                         if not has_checked_and_handled_existing_es_data:
                             self._check_and_handle_existing_es_data(
                                 DataInsightEsIndex[record.data.__class__.__name__].value
                             )
                             has_checked_and_handled_existing_es_data = True
                         self.es_sink.write_record(record)
                     else:
                         logger.warning(
                             "No sink attribute found, skipping ingestion of KPI result"
                         )
+                self.status.records.extend(self.source.processor_status.records)
+                self.status.failures.extend(self.source.processor_status.failures)
+                self.status.warnings.extend(self.source.processor_status.warnings)
 
             except Exception as exc:
                 error = f"Error while executing data insight workflow for report type {report_data_type}: {exc}"
                 logger.error(error)
                 logger.debug(traceback.format_exc())
                 self.status.failed(str(report_data_type), error, traceback.format_exc())
 
@@ -267,14 +287,16 @@
             logger.error(
                 f"Error trying to parse the Profiler Workflow configuration: {err}"
             )
             raise err
 
     def execute(self):
         """Execute workflow"""
+        self.timer.trigger()
+
         try:
             logger.info("Starting data processor execution")
             self._execute_data_processor()
             logger.info("Data processor finished running")
 
             logger.info("Sleeping for 1 second. Waiting for ES data to be indexed.")
             time.sleep(1)
@@ -287,44 +309,47 @@
 
             # At the end of the `execute`, update the associated Ingestion Pipeline status as success
             self.set_ingestion_pipeline_status(PipelineState.success)
         # Any unhandled exception breaking the workflow should update the status
         except Exception as err:
             self.set_ingestion_pipeline_status(PipelineState.failed)
             raise err
+        finally:
+            self.stop()
 
     def _raise_from_status_internal(self, raise_warnings=False):
-        if self.data_processor and self.data_processor.get_status().failures:
+        if self.source and self.source.get_status().failures:
             raise WorkflowExecutionError(
-                "Source reported errors", self.data_processor.get_status()
+                "Source reported errors", self.source.get_status()
             )
         if hasattr(self, "sink") and self.sink.get_status().failures:
             raise WorkflowExecutionError("Sink reported errors", self.sink.get_status())
         if raise_warnings and (
-            (self.data_processor and self.data_processor.get_status().warnings)
+            (self.source and self.source.get_status().warnings)
             or self.sink.get_status().warnings
         ):
             raise WorkflowExecutionError(
                 "Source reported warnings",
-                self.data_processor.get_status() if self.data_processor else None,
+                self.source.get_status() if self.source else None,
             )
 
     def print_status(self) -> None:
         print_data_insight_status(self)
 
     def result_status(self) -> int:
         """
         Returns 1 if status is failed, 0 otherwise.
         """
         if (
-            (self.data_processor and self.data_processor.get_status().failures)
+            (self.source and self.source.get_status().failures)
             or self.status.failures
             or (hasattr(self, "sink") and self.sink.get_status().failures)
         ):
             return 1
         return 0
 
     def stop(self):
         """
         Close all connections
         """
         self.metadata.close()
+        self.timer.stop()
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/helper/data_insight_es_index.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/helper/data_insight_es_index.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/data_processor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/data_processor.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/entity_report_data_processor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/entity_report_data_processor.py`

 * *Files 14% similar despite different names*

```diff
@@ -167,19 +167,27 @@
 
         Returns:
             dict:
         """
         refined_data = defaultdict(lambda: defaultdict(dict))
         for entity in self.fetch_data():
             data_blob_for_entity = {}
-            team = (
-                self._get_team(entity.owner)
-                if not isinstance(entity, User)
-                else self._get_team(entity.teams)
-            )
+            try:
+                team = (
+                    self._get_team(entity.owner)
+                    if not isinstance(entity, User)
+                    else self._get_team(entity.teams)
+                )
+            except Exception:
+                logger.debug(traceback.format_exc())
+                self.processor_status.failed(
+                    entity.name.__root__, "Error retrieving team"
+                )
+                continue
+
             try:
                 entity_tier = get_entity_tier_from_tags(entity.tags)
             except AttributeError:
                 entity_tier = None
                 logger.warning(
                     f"`tags` attribute not supported for entity type {entity.__class__.__name__}"
                 )
@@ -226,14 +234,16 @@
                     str(entity_tier)
                 ] = data_blob_for_entity_counter
             else:
                 refined_data[entity.__class__.__name__][str(team)][
                     str(entity_tier)
                 ].update(data_blob_for_entity_counter)
 
+            self.processor_status.scanned(entity.name.__root__)
+
         return refined_data
 
     def process(self) -> Iterable[ReportData]:
         refined_data: dict = self.refine()
         yield from self._flatten_results(refined_data)
 
     def get_status(self):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/processor/web_analytic_report_data_processor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/processor/web_analytic_report_data_processor.py`

 * *Files 1% similar despite different names*

```diff
@@ -191,14 +191,16 @@
                     "ownerId": owner_id,
                     "views": 1,
                 }
 
             else:
                 refined_data[split_url[1]]["views"] += 1
 
+            self.processor_status.scanned(ENTITIES[entity_type].__name__)
+
     def refine(self):
         """Aggregates data. It will return a dictionary of the following shape
 
         {
             "user_id": {
                 "<session_id>": [
                     {<event_data>},
@@ -321,14 +323,16 @@
                     user_data["totalSessions"] += 1
 
                 user_data["totalPageView"] += 1
 
                 if timestamp > user_data["lastSession"]:
                     user_data["lastSession"] = timestamp
 
+            self.processor_status.scanned(user_id)
+
     def fetch_data(self) -> Iterable[WebAnalyticEventData]:
         if CACHED_EVENTS:
             for event in CACHED_EVENTS:
                 yield event
         else:
             CACHED_EVENTS.extend(
                 self.metadata.list_entities(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/runner/kpi_runner.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/runner/kpi_runner.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/runner/run_result_registry.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/runner/run_result_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/data_insight/sink/metadata_rest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_insight/sink/metadata_rest.py`

 * *Files 3% similar despite different names*

```diff
@@ -62,24 +62,24 @@
     def close(self) -> None:
         self.metadata.close()
 
     def write_record(self, record: Union[ReportData, KpiResult]) -> None:
         try:
             if isinstance(record, ReportData):
                 self.metadata.add_data_insight_report_data(record)
-                logger.info(
-                    "Successfully ingested data insight for"
+                logger.debug(
+                    "Successfully ingested data insight for "
                     f"{record.data.__class__.__name__ if record.data else 'Unknown'}"
                 )
                 self.status.records_written(
                     f"Data Insight: {record.data.__class__.__name__ if record.data else 'Unknown'}"
                 )
             if isinstance(record, KpiResult):
                 self.metadata.add_kpi_result(fqn=record.kpiFqn.__root__, record=record)
-                logger.info(f"Successfully ingested KPI for {record.kpiFqn}")
+                logger.debug(f"Successfully ingested KPI for {record.kpiFqn}")
                 self.status.records_written(f"Data Insight: {record.kpiFqn}")
 
         except APIError as err:
             if isinstance(record, ReportData):
                 name = record.data.__class__.__name__ if record.data else "Unknown"
                 error = f"Failed to sink data insight data for {name} - {err}"
                 logger.debug(traceback.format_exc())
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/airflow.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/airflow.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/athena.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/athena.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/atlas.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/atlas.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/azuresql.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/azuresql.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery_usage.yaml`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 source:
-  type: bigquery
+  type: bigquery-usage
   serviceName: local_bigquery
   serviceConnection:
     config:
       type: BigQuery
       credentials:
         gcsConfig:
           type: service_account
@@ -11,18 +11,27 @@
           privateKeyId: private_key_id
           privateKey: private_key
           clientEmail: gcpuser@project_id.iam.gserviceaccount.com
           clientId: client_id
           authUri: https://accounts.google.com/o/oauth2/auth
           tokenUri: https://oauth2.googleapis.com/token
           authProviderX509CertUrl: https://www.googleapis.com/oauth2/v1/certs
-          clientX509CertUrl: clientX509CertUrl
+          clientX509CertUrl: ''
   sourceConfig:
     config:
-      type: DatabaseMetadata
-sink:
-  type: metadata-rest
+      type: DatabaseUsage
+      queryLogDuration: '1'
+processor:
+  type: query-parser
   config: {}
+stage:
+  type: table-usage
+  config:
+    filename: /tmp/bigquery_usage
+bulkSink:
+  type: metadata-usage
+  config:
+    filename: /tmp/bigquery_usage
 workflowConfig:
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: no-auth
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery_profiler.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery_profiler.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/bigquery_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/bigquery.yaml`

 * *Files 18% similar despite different names*

```diff
@@ -1,37 +1,29 @@
 source:
-  type: bigquery-usage
+  type: bigquery
   serviceName: local_bigquery
   serviceConnection:
     config:
       type: BigQuery
+      taxonomyProjectID: [ project-id-where-policy-tags-exist ]
       credentials:
         gcsConfig:
           type: service_account
           projectId: project_id
           privateKeyId: private_key_id
           privateKey: private_key
           clientEmail: gcpuser@project_id.iam.gserviceaccount.com
           clientId: client_id
           authUri: https://accounts.google.com/o/oauth2/auth
           tokenUri: https://oauth2.googleapis.com/token
           authProviderX509CertUrl: https://www.googleapis.com/oauth2/v1/certs
-          clientX509CertUrl: ''
+          clientX509CertUrl: clientX509CertUrl
   sourceConfig:
     config:
-      type: DatabaseUsage
-      queryLogDuration: '1'
-processor:
-  type: query-parser
+      type: DatabaseMetadata
+sink:
+  type: metadata-rest
   config: {}
-stage:
-  type: table-usage
-  config:
-    filename: /tmp/bigquery_usage
-bulkSink:
-  type: metadata-usage
-  config:
-    filename: /tmp/bigquery_usage
 workflowConfig:
   openMetadataServerConfig:
     hostPort: http://localhost:8585/api
     authProvider: no-auth
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/clickhouse.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/clickhouse.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/clickhouse_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/clickhouse_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/databricks.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/databricks.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/databricks_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/databricks_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/datalake.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/datalake.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/datalake_profiler.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/datalake_profiler.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/db2_profiler.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/db2_profiler.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/dbt.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/dbt.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/domodashboard.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/domodashboard.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/dynamodb.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/dynamodb.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/gluepipeline.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/gluepipeline.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/kafka.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/kafka.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/migrate_source.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/migrate_source.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mlflow.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mlflow.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mode.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mode.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mssql_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mssql_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/mysql_profiler.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/mysql_profiler.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/openmetadata.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/openmetadata.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/postgres_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/postgres_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/powerbi.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/powerbi.yaml`

 * *Files 4% similar despite different names*

```diff
@@ -5,14 +5,15 @@
     config:
       clientId: client_id
       clientSecret: client_secret
       tenantId: tenant_id
       scope:
         - https://analysis.windows.net/powerbi/api/.default
       pagination_entity_per_page: 100
+      # useAdminApis: true or false
       type: PowerBI
   sourceConfig:
     config:
       type: DashboardMetadata
       chartFilterPattern:
         includes:
           - Gross Margin %
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/query_log_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/query_log_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/quicksight.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/quicksight.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift_profiler.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift_profiler.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/redshift_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/redshift_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/snowflake.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/snowflake.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/snowflake_usage.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/snowflake_usage.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/superset.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/superset.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/test_suite.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/test_suite.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/examples/workflows/trino.yaml` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/examples/workflows/trino.yaml`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/EntityLinkLexer.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/EntityLinkLexer.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,102 +8,110 @@
     from typing.io import TextIO
 
 
 
 def serializedATN():
     with StringIO() as buf:
         buf.write("\3\u608b\ua72a\u8133\ub9ed\u417c\u3be7\u7786\u5964\2\t")
-        buf.write("\u00e6\b\1\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7")
+        buf.write("\u00f8\b\1\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7")
         buf.write("\t\7\4\b\t\b\3\2\3\2\3\2\3\2\3\3\3\3\3\4\3\4\3\4\3\4\3")
         buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
         buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3")
         buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
         buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3")
         buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
         buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3")
         buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
         buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3")
         buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
         buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3")
         buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
-        buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\5\4\u00bb\n\4")
+        buf.write("\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3")
+        buf.write("\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4\3\4")
+        buf.write("\3\4\5\4\u00cd\n\4\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
         buf.write("\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3")
-        buf.write("\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5")
-        buf.write("\5\5\u00d8\n\5\3\6\3\6\3\6\3\7\6\7\u00de\n\7\r\7\16\7")
-        buf.write("\u00df\3\b\6\b\u00e3\n\b\r\b\16\b\u00e4\2\2\t\3\3\5\4")
-        buf.write("\7\5\t\6\13\7\r\b\17\t\3\2\4\3\2c|\n\2$$))..\60\60\62")
-        buf.write(";C\\aac|\2\u00fe\2\3\3\2\2\2\2\5\3\2\2\2\2\7\3\2\2\2\2")
-        buf.write("\t\3\2\2\2\2\13\3\2\2\2\2\r\3\2\2\2\2\17\3\2\2\2\3\21")
-        buf.write("\3\2\2\2\5\25\3\2\2\2\7\u00ba\3\2\2\2\t\u00d7\3\2\2\2")
-        buf.write("\13\u00d9\3\2\2\2\r\u00dd\3\2\2\2\17\u00e2\3\2\2\2\21")
-        buf.write("\22\7>\2\2\22\23\7%\2\2\23\24\7G\2\2\24\4\3\2\2\2\25\26")
-        buf.write("\7@\2\2\26\6\3\2\2\2\27\30\7v\2\2\30\31\7c\2\2\31\32\7")
-        buf.write("d\2\2\32\33\7n\2\2\33\u00bb\7g\2\2\34\35\7f\2\2\35\36")
-        buf.write("\7c\2\2\36\37\7v\2\2\37 \7c\2\2 !\7d\2\2!\"\7c\2\2\"#")
-        buf.write("\7u\2\2#\u00bb\7g\2\2$%\7f\2\2%&\7c\2\2&\'\7v\2\2\'(\7")
-        buf.write("c\2\2()\7d\2\2)*\7c\2\2*+\7u\2\2+,\7g\2\2,-\7U\2\2-.\7")
-        buf.write("e\2\2./\7j\2\2/\60\7g\2\2\60\61\7o\2\2\61\u00bb\7c\2\2")
-        buf.write("\62\63\7o\2\2\63\64\7g\2\2\64\65\7v\2\2\65\66\7t\2\2\66")
-        buf.write("\67\7k\2\2\678\7e\2\28\u00bb\7u\2\29:\7f\2\2:;\7c\2\2")
-        buf.write(";<\7u\2\2<=\7j\2\2=>\7d\2\2>?\7q\2\2?@\7c\2\2@A\7t\2\2")
-        buf.write("A\u00bb\7f\2\2BC\7r\2\2CD\7k\2\2DE\7r\2\2EF\7g\2\2FG\7")
-        buf.write("n\2\2GH\7k\2\2HI\7p\2\2I\u00bb\7g\2\2JK\7e\2\2KL\7j\2")
-        buf.write("\2LM\7c\2\2MN\7t\2\2N\u00bb\7v\2\2OP\7t\2\2PQ\7g\2\2Q")
-        buf.write("R\7r\2\2RS\7q\2\2ST\7t\2\2T\u00bb\7v\2\2UV\7v\2\2VW\7")
-        buf.write("q\2\2WX\7r\2\2XY\7k\2\2Y\u00bb\7e\2\2Z[\7o\2\2[\\\7n\2")
-        buf.write("\2\\]\7o\2\2]^\7q\2\2^_\7f\2\2_`\7g\2\2`\u00bb\7n\2\2")
-        buf.write("ab\7d\2\2bc\7q\2\2c\u00bb\7v\2\2de\7V\2\2ef\7J\2\2fg\7")
-        buf.write("T\2\2gh\7G\2\2hi\7C\2\2i\u00bb\7F\2\2jk\7n\2\2kl\7q\2")
-        buf.write("\2lm\7e\2\2mn\7c\2\2no\7v\2\2op\7k\2\2pq\7q\2\2q\u00bb")
-        buf.write("\7p\2\2rs\7i\2\2st\7n\2\2tu\7q\2\2uv\7u\2\2vw\7u\2\2w")
-        buf.write("x\7c\2\2xy\7t\2\2y\u00bb\7{\2\2z{\7i\2\2{|\7n\2\2|}\7")
-        buf.write("q\2\2}~\7u\2\2~\177\7u\2\2\177\u0080\7c\2\2\u0080\u0081")
-        buf.write("\7t\2\2\u0081\u0082\7{\2\2\u0082\u0083\7V\2\2\u0083\u0084")
-        buf.write("\7g\2\2\u0084\u0085\7t\2\2\u0085\u00bb\7o\2\2\u0086\u0087")
-        buf.write("\7v\2\2\u0087\u0088\7c\2\2\u0088\u00bb\7i\2\2\u0089\u008a")
-        buf.write("\7e\2\2\u008a\u008b\7n\2\2\u008b\u008c\7c\2\2\u008c\u008d")
-        buf.write("\7u\2\2\u008d\u008e\7u\2\2\u008e\u008f\7k\2\2\u008f\u0090")
-        buf.write("\7h\2\2\u0090\u0091\7k\2\2\u0091\u0092\7e\2\2\u0092\u0093")
-        buf.write("\7c\2\2\u0093\u0094\7v\2\2\u0094\u0095\7k\2\2\u0095\u0096")
-        buf.write("\7q\2\2\u0096\u00bb\7p\2\2\u0097\u0098\7v\2\2\u0098\u0099")
-        buf.write("\7{\2\2\u0099\u009a\7r\2\2\u009a\u00bb\7g\2\2\u009b\u009c")
-        buf.write("\7v\2\2\u009c\u009d\7g\2\2\u009d\u009e\7u\2\2\u009e\u009f")
-        buf.write("\7v\2\2\u009f\u00a0\7F\2\2\u00a0\u00a1\7g\2\2\u00a1\u00a2")
-        buf.write("\7h\2\2\u00a2\u00a3\7k\2\2\u00a3\u00a4\7p\2\2\u00a4\u00a5")
-        buf.write("\7k\2\2\u00a5\u00a6\7v\2\2\u00a6\u00a7\7k\2\2\u00a7\u00a8")
-        buf.write("\7q\2\2\u00a8\u00bb\7p\2\2\u00a9\u00aa\7v\2\2\u00aa\u00ab")
-        buf.write("\7g\2\2\u00ab\u00ac\7u\2\2\u00ac\u00ad\7v\2\2\u00ad\u00ae")
-        buf.write("\7U\2\2\u00ae\u00af\7w\2\2\u00af\u00b0\7k\2\2\u00b0\u00b1")
-        buf.write("\7v\2\2\u00b1\u00bb\7g\2\2\u00b2\u00b3\7v\2\2\u00b3\u00b4")
-        buf.write("\7g\2\2\u00b4\u00b5\7u\2\2\u00b5\u00b6\7v\2\2\u00b6\u00b7")
-        buf.write("\7E\2\2\u00b7\u00b8\7c\2\2\u00b8\u00b9\7u\2\2\u00b9\u00bb")
-        buf.write("\7g\2\2\u00ba\27\3\2\2\2\u00ba\34\3\2\2\2\u00ba$\3\2\2")
-        buf.write("\2\u00ba\62\3\2\2\2\u00ba9\3\2\2\2\u00baB\3\2\2\2\u00ba")
-        buf.write("J\3\2\2\2\u00baO\3\2\2\2\u00baU\3\2\2\2\u00baZ\3\2\2\2")
-        buf.write("\u00baa\3\2\2\2\u00bad\3\2\2\2\u00baj\3\2\2\2\u00bar\3")
-        buf.write("\2\2\2\u00baz\3\2\2\2\u00ba\u0086\3\2\2\2\u00ba\u0089")
-        buf.write("\3\2\2\2\u00ba\u0097\3\2\2\2\u00ba\u009b\3\2\2\2\u00ba")
-        buf.write("\u00a9\3\2\2\2\u00ba\u00b2\3\2\2\2\u00bb\b\3\2\2\2\u00bc")
-        buf.write("\u00bd\7e\2\2\u00bd\u00be\7q\2\2\u00be\u00bf\7n\2\2\u00bf")
-        buf.write("\u00c0\7w\2\2\u00c0\u00c1\7o\2\2\u00c1\u00c2\7p\2\2\u00c2")
-        buf.write("\u00d8\7u\2\2\u00c3\u00c4\7f\2\2\u00c4\u00c5\7g\2\2\u00c5")
-        buf.write("\u00c6\7u\2\2\u00c6\u00c7\7e\2\2\u00c7\u00c8\7t\2\2\u00c8")
-        buf.write("\u00c9\7k\2\2\u00c9\u00ca\7r\2\2\u00ca\u00cb\7v\2\2\u00cb")
-        buf.write("\u00cc\7k\2\2\u00cc\u00cd\7q\2\2\u00cd\u00d8\7p\2\2\u00ce")
-        buf.write("\u00cf\7v\2\2\u00cf\u00d0\7c\2\2\u00d0\u00d1\7i\2\2\u00d1")
-        buf.write("\u00d8\7u\2\2\u00d2\u00d3\7v\2\2\u00d3\u00d4\7c\2\2\u00d4")
-        buf.write("\u00d5\7u\2\2\u00d5\u00d6\7m\2\2\u00d6\u00d8\7u\2\2\u00d7")
-        buf.write("\u00bc\3\2\2\2\u00d7\u00c3\3\2\2\2\u00d7\u00ce\3\2\2\2")
-        buf.write("\u00d7\u00d2\3\2\2\2\u00d8\n\3\2\2\2\u00d9\u00da\7<\2")
-        buf.write("\2\u00da\u00db\7<\2\2\u00db\f\3\2\2\2\u00dc\u00de\t\2")
-        buf.write("\2\2\u00dd\u00dc\3\2\2\2\u00de\u00df\3\2\2\2\u00df\u00dd")
-        buf.write("\3\2\2\2\u00df\u00e0\3\2\2\2\u00e0\16\3\2\2\2\u00e1\u00e3")
-        buf.write("\t\3\2\2\u00e2\u00e1\3\2\2\2\u00e3\u00e4\3\2\2\2\u00e4")
-        buf.write("\u00e2\3\2\2\2\u00e4\u00e5\3\2\2\2\u00e5\20\3\2\2\2\7")
-        buf.write("\2\u00ba\u00d7\u00df\u00e4\2")
+        buf.write("\5\3\5\3\5\3\5\3\5\5\5\u00ea\n\5\3\6\3\6\3\6\3\7\6\7\u00f0")
+        buf.write("\n\7\r\7\16\7\u00f1\3\b\6\b\u00f5\n\b\r\b\16\b\u00f6\2")
+        buf.write("\2\t\3\3\5\4\7\5\t\6\13\7\r\b\17\t\3\2\4\3\2c|\n\2$$)")
+        buf.write(").\60\62;C\\^^aac|\2\u0111\2\3\3\2\2\2\2\5\3\2\2\2\2\7")
+        buf.write("\3\2\2\2\2\t\3\2\2\2\2\13\3\2\2\2\2\r\3\2\2\2\2\17\3\2")
+        buf.write("\2\2\3\21\3\2\2\2\5\25\3\2\2\2\7\u00cc\3\2\2\2\t\u00e9")
+        buf.write("\3\2\2\2\13\u00eb\3\2\2\2\r\u00ef\3\2\2\2\17\u00f4\3\2")
+        buf.write("\2\2\21\22\7>\2\2\22\23\7%\2\2\23\24\7G\2\2\24\4\3\2\2")
+        buf.write("\2\25\26\7@\2\2\26\6\3\2\2\2\27\30\7v\2\2\30\31\7c\2\2")
+        buf.write("\31\32\7d\2\2\32\33\7n\2\2\33\u00cd\7g\2\2\34\35\7f\2")
+        buf.write("\2\35\36\7c\2\2\36\37\7v\2\2\37 \7c\2\2 !\7d\2\2!\"\7")
+        buf.write("c\2\2\"#\7u\2\2#\u00cd\7g\2\2$%\7f\2\2%&\7c\2\2&\'\7v")
+        buf.write("\2\2\'(\7c\2\2()\7d\2\2)*\7c\2\2*+\7u\2\2+,\7g\2\2,-\7")
+        buf.write("U\2\2-.\7e\2\2./\7j\2\2/\60\7g\2\2\60\61\7o\2\2\61\u00cd")
+        buf.write("\7c\2\2\62\63\7o\2\2\63\64\7g\2\2\64\65\7v\2\2\65\66\7")
+        buf.write("t\2\2\66\67\7k\2\2\678\7e\2\28\u00cd\7u\2\29:\7f\2\2:")
+        buf.write(";\7c\2\2;<\7u\2\2<=\7j\2\2=>\7d\2\2>?\7q\2\2?@\7c\2\2")
+        buf.write("@A\7t\2\2A\u00cd\7f\2\2BC\7r\2\2CD\7k\2\2DE\7r\2\2EF\7")
+        buf.write("g\2\2FG\7n\2\2GH\7k\2\2HI\7p\2\2I\u00cd\7g\2\2JK\7e\2")
+        buf.write("\2KL\7j\2\2LM\7c\2\2MN\7t\2\2N\u00cd\7v\2\2OP\7t\2\2P")
+        buf.write("Q\7g\2\2QR\7r\2\2RS\7q\2\2ST\7t\2\2T\u00cd\7v\2\2UV\7")
+        buf.write("v\2\2VW\7q\2\2WX\7r\2\2XY\7k\2\2Y\u00cd\7e\2\2Z[\7o\2")
+        buf.write("\2[\\\7n\2\2\\]\7o\2\2]^\7q\2\2^_\7f\2\2_`\7g\2\2`\u00cd")
+        buf.write("\7n\2\2ab\7d\2\2bc\7q\2\2c\u00cd\7v\2\2de\7V\2\2ef\7J")
+        buf.write("\2\2fg\7T\2\2gh\7G\2\2hi\7C\2\2i\u00cd\7F\2\2jk\7n\2\2")
+        buf.write("kl\7q\2\2lm\7e\2\2mn\7c\2\2no\7v\2\2op\7k\2\2pq\7q\2\2")
+        buf.write("q\u00cd\7p\2\2rs\7i\2\2st\7n\2\2tu\7q\2\2uv\7u\2\2vw\7")
+        buf.write("u\2\2wx\7c\2\2xy\7t\2\2y\u00cd\7{\2\2z{\7i\2\2{|\7n\2")
+        buf.write("\2|}\7q\2\2}~\7u\2\2~\177\7u\2\2\177\u0080\7c\2\2\u0080")
+        buf.write("\u0081\7t\2\2\u0081\u0082\7{\2\2\u0082\u0083\7V\2\2\u0083")
+        buf.write("\u0084\7g\2\2\u0084\u0085\7t\2\2\u0085\u00cd\7o\2\2\u0086")
+        buf.write("\u0087\7v\2\2\u0087\u0088\7c\2\2\u0088\u00cd\7i\2\2\u0089")
+        buf.write("\u008a\7e\2\2\u008a\u008b\7n\2\2\u008b\u008c\7c\2\2\u008c")
+        buf.write("\u008d\7u\2\2\u008d\u008e\7u\2\2\u008e\u008f\7k\2\2\u008f")
+        buf.write("\u0090\7h\2\2\u0090\u0091\7k\2\2\u0091\u0092\7e\2\2\u0092")
+        buf.write("\u0093\7c\2\2\u0093\u0094\7v\2\2\u0094\u0095\7k\2\2\u0095")
+        buf.write("\u0096\7q\2\2\u0096\u00cd\7p\2\2\u0097\u0098\7v\2\2\u0098")
+        buf.write("\u0099\7{\2\2\u0099\u009a\7r\2\2\u009a\u00cd\7g\2\2\u009b")
+        buf.write("\u009c\7v\2\2\u009c\u009d\7g\2\2\u009d\u009e\7u\2\2\u009e")
+        buf.write("\u009f\7v\2\2\u009f\u00a0\7F\2\2\u00a0\u00a1\7g\2\2\u00a1")
+        buf.write("\u00a2\7h\2\2\u00a2\u00a3\7k\2\2\u00a3\u00a4\7p\2\2\u00a4")
+        buf.write("\u00a5\7k\2\2\u00a5\u00a6\7v\2\2\u00a6\u00a7\7k\2\2\u00a7")
+        buf.write("\u00a8\7q\2\2\u00a8\u00cd\7p\2\2\u00a9\u00aa\7v\2\2\u00aa")
+        buf.write("\u00ab\7g\2\2\u00ab\u00ac\7u\2\2\u00ac\u00ad\7v\2\2\u00ad")
+        buf.write("\u00ae\7U\2\2\u00ae\u00af\7w\2\2\u00af\u00b0\7k\2\2\u00b0")
+        buf.write("\u00b1\7v\2\2\u00b1\u00cd\7g\2\2\u00b2\u00b3\7v\2\2\u00b3")
+        buf.write("\u00b4\7g\2\2\u00b4\u00b5\7u\2\2\u00b5\u00b6\7v\2\2\u00b6")
+        buf.write("\u00b7\7E\2\2\u00b7\u00b8\7c\2\2\u00b8\u00b9\7u\2\2\u00b9")
+        buf.write("\u00cd\7g\2\2\u00ba\u00bb\7f\2\2\u00bb\u00bc\7c\2\2\u00bc")
+        buf.write("\u00bd\7u\2\2\u00bd\u00be\7j\2\2\u00be\u00bf\7d\2\2\u00bf")
+        buf.write("\u00c0\7q\2\2\u00c0\u00c1\7c\2\2\u00c1\u00c2\7t\2\2\u00c2")
+        buf.write("\u00c3\7f\2\2\u00c3\u00c4\7F\2\2\u00c4\u00c5\7c\2\2\u00c5")
+        buf.write("\u00c6\7v\2\2\u00c6\u00c7\7c\2\2\u00c7\u00c8\7O\2\2\u00c8")
+        buf.write("\u00c9\7q\2\2\u00c9\u00ca\7f\2\2\u00ca\u00cb\7g\2\2\u00cb")
+        buf.write("\u00cd\7n\2\2\u00cc\27\3\2\2\2\u00cc\34\3\2\2\2\u00cc")
+        buf.write("$\3\2\2\2\u00cc\62\3\2\2\2\u00cc9\3\2\2\2\u00ccB\3\2\2")
+        buf.write("\2\u00ccJ\3\2\2\2\u00ccO\3\2\2\2\u00ccU\3\2\2\2\u00cc")
+        buf.write("Z\3\2\2\2\u00cca\3\2\2\2\u00ccd\3\2\2\2\u00ccj\3\2\2\2")
+        buf.write("\u00ccr\3\2\2\2\u00ccz\3\2\2\2\u00cc\u0086\3\2\2\2\u00cc")
+        buf.write("\u0089\3\2\2\2\u00cc\u0097\3\2\2\2\u00cc\u009b\3\2\2\2")
+        buf.write("\u00cc\u00a9\3\2\2\2\u00cc\u00b2\3\2\2\2\u00cc\u00ba\3")
+        buf.write("\2\2\2\u00cd\b\3\2\2\2\u00ce\u00cf\7e\2\2\u00cf\u00d0")
+        buf.write("\7q\2\2\u00d0\u00d1\7n\2\2\u00d1\u00d2\7w\2\2\u00d2\u00d3")
+        buf.write("\7o\2\2\u00d3\u00d4\7p\2\2\u00d4\u00ea\7u\2\2\u00d5\u00d6")
+        buf.write("\7f\2\2\u00d6\u00d7\7g\2\2\u00d7\u00d8\7u\2\2\u00d8\u00d9")
+        buf.write("\7e\2\2\u00d9\u00da\7t\2\2\u00da\u00db\7k\2\2\u00db\u00dc")
+        buf.write("\7r\2\2\u00dc\u00dd\7v\2\2\u00dd\u00de\7k\2\2\u00de\u00df")
+        buf.write("\7q\2\2\u00df\u00ea\7p\2\2\u00e0\u00e1\7v\2\2\u00e1\u00e2")
+        buf.write("\7c\2\2\u00e2\u00e3\7i\2\2\u00e3\u00ea\7u\2\2\u00e4\u00e5")
+        buf.write("\7v\2\2\u00e5\u00e6\7c\2\2\u00e6\u00e7\7u\2\2\u00e7\u00e8")
+        buf.write("\7m\2\2\u00e8\u00ea\7u\2\2\u00e9\u00ce\3\2\2\2\u00e9\u00d5")
+        buf.write("\3\2\2\2\u00e9\u00e0\3\2\2\2\u00e9\u00e4\3\2\2\2\u00ea")
+        buf.write("\n\3\2\2\2\u00eb\u00ec\7<\2\2\u00ec\u00ed\7<\2\2\u00ed")
+        buf.write("\f\3\2\2\2\u00ee\u00f0\t\2\2\2\u00ef\u00ee\3\2\2\2\u00f0")
+        buf.write("\u00f1\3\2\2\2\u00f1\u00ef\3\2\2\2\u00f1\u00f2\3\2\2\2")
+        buf.write("\u00f2\16\3\2\2\2\u00f3\u00f5\t\3\2\2\u00f4\u00f3\3\2")
+        buf.write("\2\2\u00f5\u00f6\3\2\2\2\u00f6\u00f4\3\2\2\2\u00f6\u00f7")
+        buf.write("\3\2\2\2\u00f7\20\3\2\2\2\7\2\u00cc\u00e9\u00f1\u00f6")
+        buf.write("\2")
         return buf.getvalue()
 
 
 class EntityLinkLexer(Lexer):
 
     atn = ATNDeserializer().deserialize(serializedATN())
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/EntityLinkListener.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/EntityLinkListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/EntityLinkParser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/EntityLinkParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/FqnLexer.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/FqnLexer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/FqnListener.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/FqnListener.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/antlr/FqnParser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/antlr/FqnParser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/basic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/basic.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/basic.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportData.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportData.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportData.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/entityReportData.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/entityReportData.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticEntityViewReportData.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/webAnalyticEntityViewReportData.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/reportDataType/webAnalyticUserActivityReportData.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/reportDataType/webAnalyticUserActivityReportData.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEvent.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEvent.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEvent.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventData.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventData.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventData.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventType/customEvent.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventType/customEvent.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/analytics/webAnalyticEventType/pageViewEvent.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  analytics/webAnalyticEventType/pageViewEvent.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/analytics/createWebAnalyticEvent.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/analytics/createWebAnalyticEvent.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/automations/createWorkflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/automations/createWorkflow.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/automations/createWorkflow.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/createClassification.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/createClassification.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/createClassification.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/createTag.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/createTag.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/createTag.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/classification/loadTags.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/classification/loadTags.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/classification/loadTags.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/createBot.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/createBot.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createBot.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/createEventPublisherJob.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/createEventPublisherJob.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,39 +1,36 @@
 # generated by datamodel-codegen:
 #   filename:  api/createEventPublisherJob.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ..configuration import elasticSearchConfiguration
-from ..settings import eventPublisherJob
+from ..system import eventPublisherJob
 
 
 class CreateEventPublisherJob(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    publisherType: eventPublisherJob.PublisherType
-    runMode: eventPublisherJob.RunMode
+    name: Optional[str] = Field(None, description='Name of the result')
+    publisherType: Optional[eventPublisherJob.PublisherType] = None
+    runMode: Optional[eventPublisherJob.RunMode] = None
     entities: Optional[List[str]] = Field(
         ['all'], description='List of Entities to Reindex', unique_items=True
     )
     recreateIndex: Optional[bool] = Field(
         False, description='This schema publisher run modes.'
     )
     batchSize: Optional[int] = Field(
         100, description='Maximum number of events sent in a batch (Default 10).'
     )
-    flushIntervalInSec: Optional[int] = Field(
-        30,
-        description='Maximum time to wait before sending request to ES in seconds(Default 30)',
-    )
     searchIndexMappingLanguage: Optional[
         elasticSearchConfiguration.SearchIndexMappingLanguage
     ] = Field(
         elasticSearchConfiguration.SearchIndexMappingLanguage.EN,
         description='Recreate Indexes with updated Language',
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/createType.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/createType.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/createType.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createChart.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,44 +1,35 @@
 # generated by datamodel-codegen:
-#   filename:  api/data/createChart.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  api/dataInsight/kpi/createKpiRequest.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...entity.data import chart
-from ...type import basic, entityReference, tagLabel
+from ....dataInsight.kpi import basic as basic_1
+from ....type import basic, entityReference
 
 
-class CreateChartRequest(BaseModel):
+class CreateKpiRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: basic.EntityName = Field(..., description='Name that identifies this Chart.')
+    name: basic.EntityName = Field(..., description='Name that identifies this Kpi.')
     displayName: Optional[str] = Field(
-        None,
-        description='Display Name that identifies this Chart. It could be title or label from the source services',
-    )
-    description: Optional[basic.Markdown] = Field(
-        None,
-        description='Description of the chart instance. What it has and how to use it.',
-    )
-    chartType: Optional[chart.ChartType] = None
-    chartUrl: Optional[str] = Field(
-        None, description='Chart URL suffix from its service.'
-    )
-    tables: Optional[List[basic.FullyQualifiedEntityName]] = Field(
-        None,
-        description='Link to list of table fully qualified names used in this chart.',
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this chart'
+        None, description='Display Name that identifies this Kpi.'
     )
+    description: basic.Markdown = Field(..., description='Description of the Kpi.')
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this chart'
+        None, description='Owner of this Kpi'
+    )
+    dataInsightChart: basic.FullyQualifiedEntityName = Field(
+        ..., description='Fully qualified name of the Chart this kpi refers to'
     )
-    service: basic.FullyQualifiedEntityName = Field(
-        ..., description='Link to the chart service where this chart is hosted in'
+    startDate: basic.Timestamp = Field(..., description='Start Date for the KPIs')
+    endDate: basic.Timestamp = Field(..., description='End Date for the KPIs')
+    targetDefinition: List[basic_1.KpiTarget] = Field(
+        ..., description='Metrics from the chart and the target to achieve the result.'
     )
+    metricType: basic_1.KpiTargetType
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createContainer.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createContainer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createContainer.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -12,26 +12,26 @@
 from ...type import basic, entityReference, tagLabel
 
 
 class CreateContainerRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: basic.EntityName = Field(
+    name: container.EntityName = Field(
         ..., description='Name that identifies this Container model.'
     )
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this Container model.'
     )
     description: Optional[basic.Markdown] = Field(
         None, description='Description of the Container instance.'
     )
     service: basic.FullyQualifiedEntityName = Field(
         ...,
-        description='Link to the object store service where this container is hosted in.',
+        description='Link to the storage service where this container is hosted in.',
     )
     parent: Optional[entityReference.EntityReference] = Field(
         None, description='Link to the parent container under which this entity sits.'
     )
     dataModel: Optional[container.ContainerDataModel] = Field(
         None,
         description="References to the container's data model, if data is structured, or null otherwise",
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDashboard.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDashboard.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDashboard.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDashboardDataModel.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDashboardDataModel.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDashboardDataModel.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -37,14 +37,12 @@
     service: basic.FullyQualifiedEntityName = Field(
         ...,
         description='Link to the data model service where this data model is hosted in.',
     )
     serviceType: Optional[dashboardService.DashboardServiceType] = Field(
         None, description='Service type where this data model is hosted in.'
     )
-    dataModelType: Optional[dashboardDataModel.DataModelType] = None
+    dataModelType: dashboardDataModel.DataModelType
     sql: Optional[basic.SqlQuery] = Field(
         None, description='In case the Data Model is based on a SQL query.'
     )
-    columns: Optional[List[table.Column]] = Field(
-        None, description='Columns from the data model.'
-    )
+    columns: List[table.Column] = Field(..., description='Columns from the data model.')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDatabase.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDatabase.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDatabase.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createDatabaseSchema.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createDatabaseSchema.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createDatabaseSchema.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createGlossary.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createGlossary.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createGlossary.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createGlossaryTerm.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createGlossaryTerm.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createGlossaryTerm.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createLocation.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/policies/createPolicy.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,37 +1,32 @@
 # generated by datamodel-codegen:
-#   filename:  api/data/createLocation.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  api/policies/createPolicy.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...entity.data import location
-from ...type import basic, entityReference, tagLabel
+from ...entity.policies import policy
+from ...type import basic, entityReference
 
 
-class CreateLocationRequest(BaseModel):
+class CreatePolicyRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: location.EntityName
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this location.'
-    )
-    path: Optional[str] = Field(None, description='Location full path.')
+    name: basic.EntityName = Field(..., description='Name that identifies this Policy.')
+    displayName: Optional[str] = Field(None, description='Title for this Policy.')
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of the location instance.'
-    )
-    locationType: Optional[location.LocationType] = None
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this location'
+        None,
+        description='A short description of the Policy, comprehensible to regular users.',
     )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this Location'
+        None, description='Owner of this Policy.'
     )
-    service: entityReference.EntityReference = Field(
-        ...,
-        description='Link to the service where this location is from. Note this could be of multiple storage service types.',
+    rules: policy.Rules
+    enabled: Optional[bool] = Field(True, description='Is the policy enabled.')
+    location: Optional[basic.Uuid] = Field(
+        None, description='UUID of Location where this policy is applied'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createMlModel.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createMlModel.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createMlModel.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createPipeline.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createQuery.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createQuery.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createQuery.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createTable.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createTable.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTable.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createTableProfile.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createTableProfile.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTableProfile.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/data/createTopic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createTopic.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/data/createTopic.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/dataInsight/createDataInsightChart.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/dataInsight/createDataInsightChart.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/dataInsight/kpi/createKpiRequest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/passwordResetToken.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,35 +1,32 @@
 # generated by datamodel-codegen:
-#   filename:  api/dataInsight/kpi/createKpiRequest.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  auth/passwordResetToken.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ....dataInsight.kpi import basic as basic_1
-from ....type import basic, entityReference
+from ..type import basic
+from . import emailVerificationToken
 
 
-class CreateKpiRequest(BaseModel):
+class PasswordResetToken(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: basic.EntityName = Field(..., description='Name that identifies this Kpi.')
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this Kpi.'
+    token: basic.Uuid = Field(..., description='Unique Refresh Token for user')
+    userId: basic.Uuid = Field(
+        ..., description='User Id of the User this refresh token is given to'
     )
-    description: basic.Markdown = Field(..., description='Description of the Kpi.')
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this Kpi'
+    tokenType: Optional[emailVerificationToken.TokenType] = Field(
+        emailVerificationToken.TokenType.PASSWORD_RESET, description='Token Type'
     )
-    dataInsightChart: basic.FullyQualifiedEntityName = Field(
-        ..., description='Fully qualified name of the Chart this kpi refers to'
+    expiryDate: basic.Timestamp = Field(
+        ..., description='Expiry Date-Time of the token'
     )
-    startDate: basic.Timestamp = Field(..., description='Start Date for the KPIs')
-    endDate: basic.Timestamp = Field(..., description='End Date for the KPIs')
-    targetDefinition: List[basic_1.KpiTarget] = Field(
-        ..., description='Metrics from the chart and the target to achieve the result.'
+    isActive: Optional[bool] = Field(True, description='Expiry Date-Time of the token')
+    isClaimed: Optional[bool] = Field(
+        False, description='Expiry Date-Time of the token'
     )
-    metricType: basic_1.KpiTargetType
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/createPost.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/createPost.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createPost.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
 class CreatePostRequest(BaseModel):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/createThread.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/createThread.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/createThread.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/feed/threadCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/feed/threadCount.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/feed/threadCount.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/lineage/addLineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityUsage.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 # generated by datamodel-codegen:
-#   filename:  api/lineage/addLineage.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/entityUsage.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityLineage
+from . import entityReference, usageDetails
 
 
-class AddLineageRequest(BaseModel):
+class EntityUsage(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    description: Optional[basic.Markdown] = Field(
-        None, description='User provided description of the lineage details.'
+    entity: entityReference.EntityReference = Field(
+        ..., description='Entity for which usage is returned.'
+    )
+    usage: List[usageDetails.UsageDetails] = Field(
+        ..., description='List usage details per day.'
     )
-    edge: entityLineage.EntitiesEdge = Field(..., description='Lineage edge details.')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/openMetadataServerVersion.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/openMetadataServerVersion.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/openMetadataServerVersion.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/policies/createPolicy.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,32 +1,24 @@
 # generated by datamodel-codegen:
-#   filename:  api/policies/createPolicy.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  dataInsight/type/mostViewedEntities.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...entity.policies import policy
-from ...type import basic, entityReference
 
-
-class CreatePolicyRequest(BaseModel):
+class MostViewedEntities(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: basic.EntityName = Field(..., description='Name that identifies this Policy.')
-    displayName: Optional[str] = Field(None, description='Title for this Policy.')
-    description: Optional[basic.Markdown] = Field(
-        None,
-        description='A short description of the Policy, comprehensible to regular users.',
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this Policy.'
+    entityFqn: Optional[str] = Field(None, description='Number of page views')
+    entityHref: Optional[str] = Field(None, description='Entity href link')
+    owner: Optional[str] = Field(None, description='Owner of the entity')
+    entityType: Optional[str] = Field(
+        None, description='Type of entity. Derived from the page URL.'
     )
-    rules: policy.Rules
-    enabled: Optional[bool] = Field(True, description='Is the policy enabled.')
-    location: Optional[basic.Uuid] = Field(
-        None, description='UUID of Location where this policy is applied'
+    pageViews: Optional[float] = Field(
+        None, description='Type of entity. Derived from the page URL.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createDashboardService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createDashboardService.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createDashboardService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createDatabaseService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createDatabaseService.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createDatabaseService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createMessagingService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createMessagingService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMessagingService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createMetadataService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createMetadataService.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMetadataService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createMlModelService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createMlModelService.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createMlModelService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createObjectStoreService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityReference.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,36 +1,43 @@
 # generated by datamodel-codegen:
-#   filename:  api/services/createObjectStoreService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/entityReference.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...entity.services import objectstoreService
-from ...type import basic, entityReference, tagLabel
+from . import basic
 
 
-class CreateObjectStoreServiceRequest(BaseModel):
+class EntityReference(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies the this entity instance uniquely'
+    id: basic.Uuid = Field(
+        ..., description='Unique identifier that identifies an entity instance.'
     )
-    displayName: Optional[str] = Field(
+    type: str = Field(
+        ...,
+        description='Entity type/class name - Examples: `database`, `table`, `metrics`, `databaseService`, `dashboardService`...',
+    )
+    name: Optional[str] = Field(None, description='Name of the entity instance.')
+    fullyQualifiedName: Optional[str] = Field(
         None,
-        description='Display Name that identifies this messaging service. It could be title or label from the source services.',
+        description="Fully qualified name of the entity instance. For entities such as tables, databases fullyQualifiedName is returned in this field. For entities that don't have name hierarchy such as `user` and `team` this will be same as the `name` field.",
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of messaging service entity.'
+        None, description='Optional description of entity.'
     )
-    serviceType: objectstoreService.ObjectStoreServiceType
-    connection: objectstoreService.ObjectStoreConnection
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this Object Store Service.'
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this entity.'
     )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this object store service.'
+    deleted: Optional[bool] = Field(
+        None, description='If true the entity referred to has been soft-deleted.'
     )
+    href: Optional[basic.Href] = Field(None, description='Link to the entity resource.')
+
+
+class EntityReferenceList(BaseModel):
+    __root__: List[EntityReference]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createPipelineService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createPipelineService.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createPipelineService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/createStorageService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/createStorageService.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,30 +1,36 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/createStorageService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityReference, storage
+from ...entity.services import storageService
+from ...type import basic, entityReference, tagLabel
 
 
 class CreateStorageServiceRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
     name: basic.EntityName = Field(
         ..., description='Name that identifies the this entity instance uniquely'
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this storage service.'
+        None,
+        description='Display Name that identifies this storage service. It could be title or label from the source services.',
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of Storage entity.'
+        None, description='Description of storage service entity.'
+    )
+    serviceType: storageService.StorageServiceType
+    connection: storageService.StorageConnection
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this Object Store Service.'
     )
-    serviceType: Optional[storage.StorageServiceType] = None
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this storage service.'
+        None, description='Owner of this object store service.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/services/ingestionPipelines/createIngestionPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/setOwner.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/setOwner.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/setOwner.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/createRole.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/createRole.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createRole.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/createTeam.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/createTeam.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createTeam.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/teams/createUser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/teams/createUser.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/teams/createUser.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createCustomMetric.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createCustomMetric.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createCustomMetric.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createTestCase.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createTestCase.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestCase.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createTestDefinition.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createTestDefinition.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestDefinition.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/api/tests/createTestSuite.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/tests/createTestSuite.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  api/tests/createTestSuite.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/changePasswordRequest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/function.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,30 +1,44 @@
 # generated by datamodel-codegen:
-#   filename:  auth/changePasswordRequest.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/function.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 
-class RequestType(Enum):
-    SELF = 'SELF'
-    USER = 'USER'
+class ParameterType(Enum):
+    NotRequired = 'NotRequired'
+    AllIndexElasticSearch = 'AllIndexElasticSearch'
+    SpecificIndexElasticSearch = 'SpecificIndexElasticSearch'
+    ReadFromParamContext = 'ReadFromParamContext'
 
 
-class ChangePasswordRequest(BaseModel):
+class ParamAdditionalContext(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    username: Optional[str] = Field(None, description='Name of the user')
-    oldPassword: Optional[str] = Field(
-        None, description='Name that identifies this Custom Metric.'
+    data: Optional[List[str]] = Field(
+        None, description='List of Entities', unique_items=True
     )
-    newPassword: str = Field(..., description='Name of the column in a table.')
-    confirmPassword: str = Field(..., description='Name of the column in a table.')
-    requestType: Optional[RequestType] = Field(
-        RequestType.SELF, description='Name of the column in a table.'
+
+
+class Function(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    name: Optional[str] = Field(None, description='Name of the function.')
+    input: Optional[str] = Field(
+        None, description='Description of input taken by the function.'
+    )
+    description: Optional[str] = Field(None, description='Description fo the function.')
+    examples: Optional[List] = Field(
+        None, description='Examples of the function to help users author conditions.'
+    )
+    parameterInputType: Optional[ParameterType] = Field(
+        None, description='List of receivers to send mail to'
     )
+    paramAdditionalContext: Optional[ParamAdditionalContext] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/emailVerificationToken.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/emailVerificationToken.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/emailVerificationToken.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/jwtAuth.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/jwtAuth.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/jwtAuth.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/logoutRequest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/logoutRequest.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/logoutRequest.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/passwordResetRequest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/passwordResetRequest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/passwordResetRequest.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/passwordResetToken.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/refreshToken.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,30 @@
 # generated by datamodel-codegen:
-#   filename:  auth/passwordResetToken.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  auth/refreshToken.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ..type import basic
 from . import emailVerificationToken
 
 
-class PasswordResetToken(BaseModel):
+class RefreshToken(BaseModel):
     class Config:
         extra = Extra.forbid
 
     token: basic.Uuid = Field(..., description='Unique Refresh Token for user')
     userId: basic.Uuid = Field(
         ..., description='User Id of the User this refresh token is given to'
     )
     tokenType: Optional[emailVerificationToken.TokenType] = Field(
-        emailVerificationToken.TokenType.PASSWORD_RESET, description='Token Type'
+        emailVerificationToken.TokenType.REFRESH_TOKEN, description='Token Type'
     )
+    refreshCount: Optional[int] = Field(None, description='Refresh Count')
+    maxRefreshCount: Optional[int] = Field(None, description='Refresh Count')
     expiryDate: basic.Timestamp = Field(
         ..., description='Expiry Date-Time of the token'
     )
-    isActive: Optional[bool] = Field(True, description='Expiry Date-Time of the token')
-    isClaimed: Optional[bool] = Field(
-        False, description='Expiry Date-Time of the token'
-    )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/personalAccessToken.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/personalAccessToken.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/personalAccessToken.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/refreshToken.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/paging.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,30 +1,26 @@
 # generated by datamodel-codegen:
-#   filename:  auth/refreshToken.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/paging.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..type import basic
-from . import emailVerificationToken
 
-
-class RefreshToken(BaseModel):
+class Paging(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    token: basic.Uuid = Field(..., description='Unique Refresh Token for user')
-    userId: basic.Uuid = Field(
-        ..., description='User Id of the User this refresh token is given to'
+    before: Optional[str] = Field(
+        None,
+        description='Before cursor used for getting the previous page (see API pagination for details).',
     )
-    tokenType: Optional[emailVerificationToken.TokenType] = Field(
-        emailVerificationToken.TokenType.REFRESH_TOKEN, description='Token Type'
+    after: Optional[str] = Field(
+        None,
+        description='After cursor used for getting the next page (see API pagination for details).',
     )
-    refreshCount: Optional[int] = Field(None, description='Refresh Count')
-    maxRefreshCount: Optional[int] = Field(None, description='Refresh Count')
-    expiryDate: basic.Timestamp = Field(
-        ..., description='Expiry Date-Time of the token'
+    total: int = Field(
+        ..., description='Total number of entries available to page through.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/registrationRequest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/registrationRequest.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/registrationRequest.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field, constr
 
 from ..type import basic
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/auth/ssoAuth.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/auth/ssoAuth.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  auth/ssoAuth.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/applicationConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/applicationConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/applicationConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/authConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/authConfig.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/authenticationConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/authenticationConfiguration.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authenticationConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/authorizerConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/authorizerConfiguration.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/authorizerConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/elasticSearchConfiguration.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/elasticSearchConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/jwtTokenConfiguration.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/jwtTokenConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/kafkaEventConfiguration.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/kafkaEventConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapConfiguration.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/customTrustManagerConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/customTrustManagerConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/hostNameConfig.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/hostNameConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/ldapTrustStoreConfig/truststoreConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/ldapTrustStoreConfig/truststoreConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/pipelineServiceClientConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -22,14 +22,18 @@
         ...,
         description='External API root to interact with the Pipeline Service Client',
     )
     hostIp: Optional[str] = Field(
         None,
         description='Pipeline Service Client host IP that will be used to connect to the sources.',
     )
+    ingestionIpInfoEnabled: Optional[bool] = Field(
+        False,
+        description='Enable or disable the API that fetches the public IP running the ingestion process.',
+    )
     metadataApiEndpoint: str = Field(
         ..., description='Metadata api endpoint, e.g., `http://localhost:8585/api`'
     )
     verifySSL: Optional[str] = Field(
         'no-ssl',
         description='Client SSL verification policy: no-ssl, ignore, validate.',
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/configuration/testResultNotificationConfiguration.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  configuration/testResultNotificationConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChart.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/dataInsightChart.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/dataInsightChart.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/dataInsightChartResult.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/dataInsightChartResult.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/basic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/kpi/basic.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/kpi/basic.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/kpi/kpi.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/kpi/kpi.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/kpi/kpi.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/dailyActiveUsers.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/dailyActiveUsers.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/mostActiveUsers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/mostActiveUsers.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/mostViewedEntities.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  dataInsight/type/mostViewedEntities.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  dataInsight/type/totalEntitiesByType.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field, confloat
 
+from ...type import basic
 
-class MostViewedEntities(BaseModel):
+
+class TotalEntitiesByType(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    entityFqn: Optional[str] = Field(None, description='Number of page views')
-    entityHref: Optional[str] = Field(None, description='Entity href link')
-    owner: Optional[str] = Field(None, description='Owner of the entity')
+    timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
     entityType: Optional[str] = Field(
-        None, description='Type of entity. Derived from the page URL.'
+        None, description='Type of entity. Derived from the entity class.'
+    )
+    entityCount: Optional[float] = Field(
+        None, description='Total count of entity for the given entity type'
     )
-    pageViews: Optional[float] = Field(
-        None, description='Type of entity. Derived from the page URL.'
+    entityCountFraction: Optional[confloat(ge=0.0, le=1.0)] = Field(
+        None, description='Total count of entity for the given entity type'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/pageViewsByEntities.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/pageViewsByEntities.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithDescriptionByType.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfEntitiesWithDescriptionByType.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/percentageOfEntitiesWithOwnerByType.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/percentageOfEntitiesWithOwnerByType.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/dataInsight/type/totalEntitiesByTier.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  dataInsight/type/totalEntitiesByTier.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/dataInsight/type/totalEntitiesByType.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvFile.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,27 +1,38 @@
 # generated by datamodel-codegen:
-#   filename:  dataInsight/type/totalEntitiesByType.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/csvFile.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field, confloat
+from pydantic import BaseModel, Extra, Field
 
-from ...type import basic
+from . import basic
 
 
-class TotalEntitiesByType(BaseModel):
+class CsvRecord(BaseModel):
+    __root__: List[str] = Field(
+        ...,
+        description='Represents a CSV record that contains one row values separated by a separator.',
+    )
+
+
+class CsvHeader(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    timestamp: Optional[basic.Timestamp] = Field(None, description='timestamp')
-    entityType: Optional[str] = Field(
-        None, description='Type of entity. Derived from the entity class.'
-    )
-    entityCount: Optional[float] = Field(
-        None, description='Total count of entity for the given entity type'
-    )
-    entityCountFraction: Optional[confloat(ge=0.0, le=1.0)] = Field(
-        None, description='Total count of entity for the given entity type'
+    name: str
+    required: Optional[bool] = False
+    description: basic.Markdown = Field(
+        ..., description='Description of the header field for documentation purposes.'
     )
+    examples: List[str] = Field(..., description='Example values for the field')
+
+
+class CsvFile(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    headers: Optional[List[CsvHeader]] = None
+    records: Optional[List[CsvRecord]] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/email/emailRequest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/email/emailRequest.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  email/emailRequest.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/email/smtpSettings.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/email/smtpSettings.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  email/smtpSettings.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/automations/testServiceConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/automations/testServiceConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/automations/testServiceConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
@@ -12,17 +12,17 @@
 from ...type import basic
 from ..services import (
     dashboardService,
     databaseService,
     messagingService,
     metadataService,
     mlmodelService,
-    objectstoreService,
     pipelineService,
     serviceType,
+    storageService,
 )
 
 
 class TestServiceConnectionRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
@@ -30,15 +30,15 @@
         Union[
             databaseService.DatabaseConnection,
             dashboardService.DashboardConnection,
             messagingService.MessagingConnection,
             pipelineService.PipelineConnection,
             mlmodelService.MlModelConnection,
             metadataService.MetadataConnection,
-            objectstoreService.ObjectStoreConnection,
+            storageService.StorageConnection,
         ]
     ] = Field(None, description='Connection object.')
     serviceType: Optional[serviceType.ServiceType] = Field(
         None, description='Type of service such as Database, Dashboard, Messaging, etc.'
     )
     connectionType: Optional[str] = Field(
         None,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/automations/workflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/automations/workflow.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/automations/workflow.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/bot.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/bot.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/bot.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/classification/classification.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/classification/classification.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/classification/classification.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/classification/tag.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/classification/tag.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/classification/tag.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/chart.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/chart.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/chart.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -59,17 +59,14 @@
     )
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
         None, description='Owner of this dashboard.'
     )
-    tables: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Link to table used in this chart.'
-    )
     followers: Optional[entityReference.EntityReferenceList] = Field(
         None, description='Followers of this chart.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags for this chart.'
     )
     service: entityReference.EntityReference = Field(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/container.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/topic.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,117 +1,118 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/container.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/data/topic.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Extra, Field, conint
 
-from ...type import basic, entityHistory, entityReference, tagLabel
-from ..services import objectstoreService
-from . import table
+from ...type import basic, entityHistory, entityReference, schema, tagLabel
+from ..services import messagingService
 
 
-class FileFormat(Enum):
-    zip = 'zip'
-    gz = 'gz'
-    zstd = 'zstd'
-    csv = 'csv'
-    tsv = 'tsv'
-    json = 'json'
-    parquet = 'parquet'
-    avro = 'avro'
+class CleanupPolicy(Enum):
+    delete = 'delete'
+    compact = 'compact'
 
 
-class ContainerDataModel(BaseModel):
+class TopicConfig(BaseModel):
+    pass
+
+
+class TopicSampleData(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    isPartitioned: Optional[bool] = Field(
-        False,
-        description='Whether the data under this container is partitioned by some property, eg. eventTime=yyyy-mm-dd',
-    )
-    columns: List[table.Column] = Field(
-        ..., description="Columns belonging to this container's schema"
+    messages: Optional[List[str]] = Field(
+        None, description='List of local sample messages for a topic.'
     )
 
 
-class Container(BaseModel):
+class Topic(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies this container instance.'
-    )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies the container.'
+        ..., description='Unique identifier that identifies this topic instance.'
     )
+    name: basic.EntityName = Field(..., description='Name that identifies the topic.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None,
-        description="Name that uniquely identifies a container in the format 'ServiceName.ContainerName'.",
+        description="Name that uniquely identifies a topic in the format 'messagingServiceName.topicName'.",
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this container.'
+        None,
+        description='Display Name that identifies this topic. It could be title or label from the source services.',
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of the container instance.'
+        None, description='Description of the topic instance.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this container.'
-    )
     service: entityReference.EntityReference = Field(
         ...,
-        description='Link to the object store service where this container is hosted in.',
+        description='Link to the messaging cluster/service where this topic is hosted in.',
     )
-    parent: Optional[entityReference.EntityReference] = Field(
-        None,
-        description='Link to the parent container under which this entity sits, if not top level.',
+    serviceType: Optional[messagingService.MessagingServiceType] = Field(
+        None, description='Service type where this topic is hosted in.'
     )
-    children: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='References to child containers residing under this entity.'
+    messageSchema: Optional[schema.Topic] = None
+    partitions: conint(ge=1) = Field(
+        ..., description='Number of partitions into which the topic is divided.'
     )
-    dataModel: Optional[ContainerDataModel] = Field(
+    cleanupPolicies: Optional[List[CleanupPolicy]] = Field(
         None,
-        description="References to the container's data model, if data is structured, or null otherwise",
+        description='Topic clean up policies. For Kafka - `cleanup.policy` configuration.',
     )
-    prefix: Optional[str] = Field(
-        None, description='Optional prefix path defined for this container'
+    retentionTime: Optional[float] = Field(
+        None,
+        description='Retention time in milliseconds. For Kafka - `retention.ms` configuration.',
     )
-    numberOfObjects: Optional[float] = Field(
-        None, description='The number of objects/files this container has.'
+    replicationFactor: Optional[int] = Field(
+        None, description='Replication Factor in integer (more than 1).'
     )
-    size: Optional[float] = Field(
-        None, description='The total size in KB this container has.'
+    maximumMessageSize: Optional[int] = Field(
+        None,
+        description='Maximum message size in bytes. For Kafka - `max.message.bytes` configuration.',
     )
-    fileFormats: Optional[List[FileFormat]] = Field(
+    minimumInSyncReplicas: Optional[int] = Field(
         None,
-        description='File & data formats identified for the container:  e.g. dataFormats=[csv, json]. These can be present both when the container has a dataModel or not',
+        description='Minimum number replicas in sync to control durability. For Kafka - `min.insync.replicas` configuration.',
+    )
+    retentionSize: Optional[float] = Field(
+        '-1',
+        description='Maximum size of a partition in bytes before old data is discarded. For Kafka - `retention.bytes` configuration.',
+    )
+    topicConfig: Optional[TopicConfig] = Field(
+        None, description='Contains key/value pair of topic configuration.'
     )
-    serviceType: Optional[objectstoreService.ObjectStoreServiceType] = Field(
-        None, description='Service type this table is hosted in.'
+    sampleData: Optional[TopicSampleData] = Field(
+        None, description='Sample data for a topic.'
+    )
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this topic.'
     )
     followers: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Followers of this container.'
+        None, description='Followers of this table.'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this container.'
+        None, description='Tags for this table.'
+    )
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/dashboard.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/dashboard.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/dashboard.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/dashboardDataModel.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/dashboardDataModel.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/dashboardDataModel.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -14,14 +14,16 @@
 from . import table
 
 
 class DataModelType(Enum):
     TableauSheet = 'TableauSheet'
     SupersetDataModel = 'SupersetDataModel'
     MetabaseDataModel = 'MetabaseDataModel'
+    LookMlView = 'LookMlView'
+    LookMlExplore = 'LookMlExplore'
 
 
 class DashboardDataModel(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
@@ -70,17 +72,15 @@
     )
     service: Optional[entityReference.EntityReference] = Field(
         None, description='Link to service where this data model is hosted in.'
     )
     serviceType: Optional[dashboardService.DashboardServiceType] = Field(
         None, description='Service type where this data model is hosted in.'
     )
-    dataModelType: Optional[DataModelType] = None
+    dataModelType: DataModelType
     sql: Optional[basic.SqlQuery] = Field(
         None, description='In case the Data Model is based on a SQL query.'
     )
-    columns: Optional[List[table.Column]] = Field(
-        None, description='Columns from the data model.'
-    )
+    columns: List[table.Column] = Field(..., description='Columns from the data model.')
     dataModels: Optional[entityReference.EntityReferenceList] = Field(
         None, description='List of data models used by this data model.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/database.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/database.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/database.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/databaseSchema.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/databaseSchema.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/databaseSchema.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/glossary.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/glossary.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/glossary.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/glossaryTerm.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/glossaryTerm.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/glossaryTerm.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/location.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,82 +1,73 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/location.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/testConnectionDefinition.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from enum import Enum
 from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field, constr
+from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, storage, tagLabel
+from ....type import basic, entityHistory, entityReference
 
 
-class EntityName(BaseModel):
-    __root__: constr(regex=r'^((?!::).)*$', min_length=1, max_length=128) = Field(
-        ..., description='Name of a location'
-    )
-
+class TestConnectionStep(BaseModel):
+    class Config:
+        extra = Extra.forbid
 
-class LocationType(Enum):
-    Bucket = 'Bucket'
-    Prefix = 'Prefix'
-    Database = 'Database'
-    Table = 'Table'
-    Iceberg = 'Iceberg'
+    name: str = Field(..., description='Name of the step being tested')
+    description: str = Field(..., description='What is the goal of the step')
+    errorMessage: Optional[str] = Field(
+        None,
+        description='In case of error this message should be displayed on UI, We define this message manually on test connection definition',
+    )
+    mandatory: bool = Field(..., description='Is this step mandatory to be passed?')
+    shortCircuit: Optional[bool] = Field(
+        False,
+        description='This field if set to true, indicates that the step is important enough to break the process in case of failure.',
+    )
 
 
-class Location(BaseModel):
+class TestConnectionDefinition(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this location instance.'
+        None, description='Unique identifier of this test case definition instance.'
+    )
+    name: basic.EntityName = Field(
+        ...,
+        description='Name of the Test Connection Definition. It should be the `type` of the service being tested, e.g., Mysql, or Snowflake.',
     )
-    name: EntityName
-    path: Optional[str] = Field(None, description='Location full path')
     displayName: Optional[str] = Field(
-        None,
-        description='Display Name that identifies this table. It could be title or label from the source services.',
+        None, description='Display Name that identifies this test definition.'
+    )
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the test connection def.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None,
-        description='Fully qualified name of a location in the form `serviceName.locationName`.',
+        None, description='FullyQualifiedName same as `name`.'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of a location.'
+    steps: List[TestConnectionStep] = Field(
+        ..., description='Steps to test the connection. Order matters.'
+    )
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this TestConnection definition.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
-        None, description='Link to this location resource.'
-    )
-    locationType: Optional[LocationType] = None
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this location.'
-    )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Followers of this location.'
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this location.'
-    )
-    service: entityReference.EntityReference = Field(
-        ...,
-        description='Link to the database service where this database is hosted in.',
-    )
-    serviceType: Optional[storage.StorageServiceType] = Field(
-        None, description='Service type where this storage location is hosted in.'
+        None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/metrics.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/metrics.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/metrics.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/mlmodel.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/mlmodel.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/mlmodel.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/pipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/pipeline.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/pipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/query.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/query.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/query.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/report.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/report.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/report.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/table.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/data/table.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/data/table.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field, constr
@@ -98,14 +98,17 @@
     INET = 'INET'
     CLOB = 'CLOB'
     ROWID = 'ROWID'
     LOWCARDINALITY = 'LOWCARDINALITY'
     YEAR = 'YEAR'
     POINT = 'POINT'
     POLYGON = 'POLYGON'
+    TUPLE = 'TUPLE'
+    SPATIAL = 'SPATIAL'
+    TABLE = 'TABLE'
 
 
 class Constraint(Enum):
     NULL = 'NULL'
     NOT_NULL = 'NOT_NULL'
     UNIQUE = 'UNIQUE'
     PRIMARY_KEY = 'PRIMARY_KEY'
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/data/topic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/user.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,122 +1,110 @@
 # generated by datamodel-codegen:
-#   filename:  entity/data/topic.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/teams/user.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional
+from typing import Optional, Union
 
-from pydantic import BaseModel, Extra, Field, conint
+from pydantic import BaseModel, Extra, Field, constr
 
-from ...type import basic, entityHistory, entityReference, schema, tagLabel
-from ..services import messagingService
+from ...auth import basicAuth, jwtAuth, ssoAuth
+from ...type import basic, entityHistory, entityReference, profile
 
 
-class CleanupPolicy(Enum):
-    delete = 'delete'
-    compact = 'compact'
+class EntityName(BaseModel):
+    __root__: constr(regex=r'^(?u)[\w\-.]+$', min_length=1, max_length=64) = Field(
+        ...,
+        description='Login name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
+    )
 
 
-class TopicConfig(BaseModel):
-    pass
+class AuthType(Enum):
+    JWT = 'JWT'
+    SSO = 'SSO'
+    BASIC = 'BASIC'
 
 
-class TopicSampleData(BaseModel):
+class AuthenticationMechanism(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    messages: Optional[List[str]] = Field(
-        None, description='List of local sample messages for a topic.'
-    )
+    config: Optional[
+        Union[
+            ssoAuth.SSOAuthMechanism,
+            jwtAuth.JWTAuthMechanism,
+            basicAuth.BasicAuthMechanism,
+        ]
+    ] = None
+    authType: Optional[AuthType] = None
 
 
-class Topic(BaseModel):
+class User(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies this topic instance.'
+        ..., description='Unique identifier that identifies a user entity instance.'
+    )
+    name: EntityName = Field(
+        ...,
+        description='A unique name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
     )
-    name: basic.EntityName = Field(..., description='Name that identifies the topic.')
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None,
-        description="Name that uniquely identifies a topic in the format 'messagingServiceName.topicName'.",
+        None, description='FullyQualifiedName same as `name`.'
+    )
+    description: Optional[basic.Markdown] = Field(
+        None, description='Used for user biography.'
     )
     displayName: Optional[str] = Field(
         None,
-        description='Display Name that identifies this topic. It could be title or label from the source services.',
-    )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the topic instance.'
+        description="Name used for display purposes. Example 'FirstName LastName'.",
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    service: entityReference.EntityReference = Field(
-        ...,
-        description='Link to the messaging cluster/service where this topic is hosted in.',
-    )
-    serviceType: Optional[messagingService.MessagingServiceType] = Field(
-        None, description='Service type where this topic is hosted in.'
+    email: basic.Email = Field(..., description='Email address of the user.')
+    href: basic.Href = Field(
+        ..., description='Link to the resource corresponding to this entity.'
     )
-    messageSchema: Optional[schema.Topic] = None
-    partitions: conint(ge=1) = Field(
-        ..., description='Number of partitions into which the topic is divided.'
+    timezone: Optional[str] = Field(None, description='Timezone of the user.')
+    isBot: Optional[bool] = Field(
+        None, description='When true indicates a special type of user called Bot.'
     )
-    cleanupPolicies: Optional[List[CleanupPolicy]] = Field(
+    isAdmin: Optional[bool] = Field(
         None,
-        description='Topic clean up policies. For Kafka - `cleanup.policy` configuration.',
+        description='When true indicates user is an administrator for the system with superuser privileges.',
     )
-    retentionTime: Optional[float] = Field(
-        None,
-        description='Retention time in milliseconds. For Kafka - `retention.ms` configuration.',
+    authenticationMechanism: Optional[AuthenticationMechanism] = None
+    profile: Optional[profile.Profile] = Field(None, description='Profile of the user.')
+    teams: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='Teams that the user belongs to.'
     )
-    replicationFactor: Optional[int] = Field(
-        None, description='Replication Factor in integer (more than 1).'
+    owns: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='List of entities owned by the user.'
     )
-    maximumMessageSize: Optional[int] = Field(
-        None,
-        description='Maximum message size in bytes. For Kafka - `max.message.bytes` configuration.',
-    )
-    minimumInSyncReplicas: Optional[int] = Field(
-        None,
-        description='Minimum number replicas in sync to control durability. For Kafka - `min.insync.replicas` configuration.',
-    )
-    retentionSize: Optional[float] = Field(
-        '-1',
-        description='Maximum size of a partition in bytes before old data is discarded. For Kafka - `retention.bytes` configuration.',
-    )
-    topicConfig: Optional[TopicConfig] = Field(
-        None, description='Contains key/value pair of topic configuration.'
-    )
-    sampleData: Optional[TopicSampleData] = Field(
-        None, description='Sample data for a topic.'
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this topic.'
-    )
-    followers: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Followers of this table.'
-    )
-    tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this table.'
-    )
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
+    follows: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='List of entities followed by the user.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    extension: Optional[basic.EntityExtension] = Field(
+    roles: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='Roles that the user has been assigned.'
+    )
+    inheritedRoles: Optional[entityReference.EntityReferenceList] = Field(
         None,
-        description='Entity extension data with custom attributes added to the entity.',
+        description='Roles that a user is inheriting through membership in teams that have set team default roles.',
+    )
+    isEmailVerified: Optional[bool] = Field(
+        None, description='If the User has verified the mail'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/events/webhook.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/events/webhook.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/events/webhook.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/feed/thread.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/feed/thread.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/feed/thread.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -27,14 +27,31 @@
 
 class ThreadType(Enum):
     Conversation = 'Conversation'
     Task = 'Task'
     Announcement = 'Announcement'
 
 
+class AnnouncementDetails(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    description: Optional[str] = Field(
+        None,
+        description='Announcement description in markdown format. See markdown support for more details.',
+    )
+    startTime: basic.Timestamp = Field(
+        ...,
+        description='Timestamp of the start time from when the announcement should be shown.',
+    )
+    endTime: basic.Timestamp = Field(
+        ..., description='Timestamp of when the announcement should end'
+    )
+
+
 class TaskDetails(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: int = Field(..., description='Unique identifier that identifies the task.')
     type: TaskType
     assignees: entityReference.EntityReferenceList = Field(
@@ -54,31 +71,14 @@
         description='The suggestion object to replace the old value for which the task is created.',
     )
     newValue: Optional[str] = Field(
         None, description='The new value object that was accepted to complete the task.'
     )
 
 
-class AnnouncementDetails(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    description: Optional[str] = Field(
-        None,
-        description='Announcement description in markdown format. See markdown support for more details.',
-    )
-    startTime: basic.Timestamp = Field(
-        ...,
-        description='Timestamp of the start time from when the announcement should be shown.',
-    )
-    endTime: basic.Timestamp = Field(
-        ..., description='Timestamp of when the announcement should end'
-    )
-
-
 class Post(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
         ..., description='Unique identifier that identifies the post.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/resourceDescriptor.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/resourcePermission.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/accessControl/rule.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/accessControl/rule.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/accessControl/rule.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/filters.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/filters.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/filters.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Any, List
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/deleteAction.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/filterPattern.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,23 +1,27 @@
 # generated by datamodel-codegen:
-#   filename:  entity/policies/lifecycle/deleteAction.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/filterPattern.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field, conint
+from pydantic import BaseModel, Extra, Field
 
 
-class LifecycleDeleteAction(BaseModel):
+class FilterPatternModel(BaseModel):
+    pass
+
+
+class FilterPattern(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    daysAfterCreation: Optional[conint(ge=1)] = Field(
+    includes: Optional[List[str]] = Field(
         None,
-        description='Number of days after creation of the entity that the deletion should be triggered.',
+        description='List of strings/regex patterns to match and include only database entities that match.',
     )
-    daysAfterModification: Optional[conint(ge=1)] = Field(
+    excludes: Optional[List[str]] = Field(
         None,
-        description='Number of days after last modification of the entity that the deletion should be triggered.',
+        description='List of strings/regex patterns to match and exclude only database entities that match.',
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/lifecycle/moveAction.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,45 +1,30 @@
 # generated by datamodel-codegen:
-#   filename:  entity/policies/lifecycle/moveAction.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  metadataIngestion/dbtconfig/dbtHttpConfig.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field, conint
+from pydantic import BaseModel, Extra, Field
 
-from ....type import storage
-from ...data import location
-from ...services import storageService
 
-
-class Destination(BaseModel):
+class DbtHttpConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    storageServiceType: Optional[storageService.StorageService] = Field(
-        None, description='The storage service to move this entity to.'
-    )
-    storageClassType: Optional[storage.StorageClassType] = Field(
-        None, description='The storage class to move this entity to.'
-    )
-    location: Optional[location.Location] = Field(
-        None, description='The location where to move this entity to.'
-    )
-
-
-class LifecycleMoveAction(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    daysAfterCreation: Optional[conint(ge=1)] = Field(
+    dbtCatalogHttpPath: Optional[str] = Field(
         None,
-        description='Number of days after creation of the entity that the move should be triggered.',
+        description='DBT catalog http file path to extract dbt models with their column schemas.',
+        title='DBT Catalog HTTP File Path',
     )
-    daysAfterModification: Optional[conint(ge=1)] = Field(
-        None,
-        description='Number of days after last modification of the entity that the move should be triggered.',
+    dbtManifestHttpPath: str = Field(
+        ...,
+        description='DBT manifest http file path to extract dbt models and associate with tables.',
+        title='DBT Manifest HTTP File Path',
     )
-    destination: Optional[Destination] = Field(
-        None, description='Location where this entity needs to be moved to.'
+    dbtRunResultsHttpPath: Optional[str] = Field(
+        None,
+        description='DBT run results http file path to extract the test results information.',
+        title='DBT Run Results HTTP File Path',
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/policies/policy.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/policies/policy.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/policies/policy.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/connectionBasicType.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/connectionBasicType.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/customDashboardConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/customDashboardConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/domoDashboardConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/domoDashboardConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/lookerConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,27 +1,35 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/lookerConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
+from .....security.credentials import githubCredentials
 from .. import connectionBasicType
 
 
 class LookerType(Enum):
     Looker = 'Looker'
 
 
+class NoGitHubCredentials(BaseModel):
+    pass
+
+    class Config:
+        extra = Extra.forbid
+
+
 class LookerConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[LookerType] = Field(
         LookerType.Looker, description='Service Type', title='Service Type'
     )
@@ -32,10 +40,17 @@
     )
     clientSecret: CustomSecretStr = Field(
         ..., description="User's Client Secret.", title='Client Secret'
     )
     hostPort: AnyUrl = Field(
         ..., description='URL to the Looker instance.', title='Host and Port'
     )
+    githubCredentials: Optional[
+        Union[NoGitHubCredentials, githubCredentials.GitHubCredentials]
+    ] = Field(
+        None,
+        description='Credentials to extract the .lkml files from a repository. This is required to get all the lineage and definitions.',
+        title='GitHub Credentials',
+    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/metabaseConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/metabaseConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/modeConnection.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/modeConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/powerBIConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/powerBIConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
@@ -46,10 +46,15 @@
         title='Scope',
     )
     pagination_entity_per_page: Optional[int] = Field(
         100,
         description='Entity Limit set here will be used to paginate the PowerBi APIs',
         title='Pagination Entity Per Page',
     )
+    useAdminApis: Optional[bool] = Field(
+        True,
+        description='Fetch the PowerBI metadata using admin APIs',
+        title='Use PowerBI Admin APIs',
+    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/quickSightConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/quickSightConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/redashConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/redashConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/supersetConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/supersetConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/dashboard/tableauConnection.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/dashboard/tableauConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import Optional
+from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
 
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
+from .....security.credentials import accessTokenAuth, basicAuth
 from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
 
 
 class TableauType(Enum):
     Tableau = 'Tableau'
 
@@ -22,40 +21,31 @@
 class TableauConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     type: Optional[TableauType] = Field(
         TableauType.Tableau, description='Service Type', title='Service Type'
     )
-    hostPort: Optional[AnyUrl] = Field(
-        None, description='Tableau Server.', title='Host and Port'
-    )
-    username: Optional[str] = Field(
-        None, description='Username for Tableau.', title='Username'
-    )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password for Tableau.', title='Password'
+    hostPort: AnyUrl = Field(..., description='Tableau Server.', title='Host and Port')
+    authType: Optional[
+        Union[basicAuth.BasicAuth, accessTokenAuth.AccessTokenAuth]
+    ] = Field(
+        None,
+        description='Types of methods used to authenticate to the tableau instance',
+        title='Authentication type for Tableau',
     )
     apiVersion: str = Field(
         ..., description='Tableau API version.', title='API Version'
     )
     siteName: Optional[str] = Field(
         None, description='Tableau Site Name.', title='Site Name'
     )
     siteUrl: Optional[str] = Field(
         None, description='Tableau Site Url.', title='Site Url'
     )
-    personalAccessTokenName: Optional[str] = Field(
-        None, description='Personal Access Token Name.', title='Personal Access Token'
-    )
-    personalAccessTokenSecret: Optional[CustomSecretStr] = Field(
-        None,
-        description='Personal Access Token Secret.',
-        title='Personal Access Token Secret',
-    )
     env: str = Field(
         ..., description='Tableau Environment Name.', title='Tableau Environment'
     )
     verifySSL: Optional[verifySSLConfig.VerifySSL] = verifySSLConfig.VerifySSL.no_ssl
     sslConfig: Optional[verifySSLConfig.SslConfig] = None
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/athenaConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/athenaConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
@@ -58,7 +58,10 @@
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
+    supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
+        None, title='Supports Query Comment'
+    )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/azureSQLConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/azureSQLConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/bigQueryConnection.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/bigQueryConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/clickhouseConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/clickhouseConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/customDatabaseConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/customDatabaseConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/databricksConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/databricksConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/azureConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/datalake/azureConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/database/datalake/gcsConfig.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ......security.credentials import azureCredentials
+from ......security.credentials import gcsCredentials
 
 
-class AzureConfig(BaseModel):
+class GCSConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    securityConfig: Optional[azureCredentials.AzureCredentials] = Field(
-        None, title='Azure Datalake Config Source'
+    securityConfig: Optional[gcsCredentials.GCSCredentials] = Field(
+        None, title='DataLake GCS Security Config'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/gcsConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/datalake/gcsConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  metadataIngestion/dbtconfig/dbtS3Config.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Field
 
-from ......security.credentials import gcsCredentials
+from ...security.credentials import awsCredentials
+from . import dbtBucketDetails
 
 
-class GCSConfig(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    securityConfig: Optional[gcsCredentials.GCSCredentials] = Field(
-        None, title='DataLake GCS Security Config'
+class DbtS3Config(BaseModel):
+    dbtSecurityConfig: Optional[awsCredentials.AWSCredentials] = Field(
+        None, title='DBT S3 Security Config'
+    )
+    dbtPrefixConfig: Optional[dbtBucketDetails.DbtBucketDetails] = Field(
+        None, title='DBT Prefix Config'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalake/s3Config.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalake/s3Config.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/datalakeConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/datalakeConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/db2Connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/db2Connection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/deltaLakeConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/deltaLakeConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/domoDatabaseConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/domoDatabaseConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/druidConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/druidConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/dynamoDBConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/dynamoDBConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/glueConnection.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/glueConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -23,17 +23,14 @@
 
     type: Optional[GlueType] = Field(
         GlueType.Glue, description='Service Type', title='Service Type'
     )
     awsConfig: awsCredentials.AWSCredentials = Field(
         ..., title='AWS Credentials Configuration'
     )
-    storageServiceName: str = Field(
-        ..., description='AWS storageServiceName Name.', title='Storage Service Name'
-    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/hiveConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,89 +1,75 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/hiveConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/database/singleStoreConnection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class HiveType(Enum):
-    Hive = 'Hive'
+class SingleStoreType(Enum):
+    SingleStore = 'SingleStore'
 
 
-class HiveScheme(Enum):
-    hive = 'hive'
-    hive_http = 'hive+http'
-    hive_https = 'hive+https'
-    impala = 'impala'
-    impala4 = 'impala4'
+class SingleStoreScheme(Enum):
+    mysql_pymysql = 'mysql+pymysql'
 
 
-class HiveConnection(BaseModel):
+class SingleStoreConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[HiveType] = Field(
-        HiveType.Hive, description='Service Type', title='Service Type'
+    type: Optional[SingleStoreType] = Field(
+        SingleStoreType.SingleStore, description='Service Type', title='Service Type'
     )
-    scheme: Optional[HiveScheme] = Field(
-        HiveScheme.hive,
+    scheme: Optional[SingleStoreScheme] = Field(
+        SingleStoreScheme.mysql_pymysql,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
-    username: Optional[str] = Field(
-        None,
-        description='Username to connect to Hive. This user should have privileges to read all the metadata in Hive.',
+    username: str = Field(
+        ...,
+        description='Username to connect to SingleStore. This user should have privileges to read all the metadata in MySQL.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Hive.', title='Password'
+        None, description='Password to connect to SingleStore.', title='Password'
     )
     hostPort: str = Field(
-        ..., description='Host and port of the Hive service.', title='Host and Port'
-    )
-    auth: Optional[str] = Field(
-        None,
-        description='Authentication mode to connect to hive, E.g, LDAP, CUSTOM etc',
-        title='Authentication Mode',
-    )
-    kerberosServiceName: Optional[str] = Field(
-        None,
-        description='If authenticating with Kerberos specify the Kerberos service name',
-        title='Kerberos Service Name',
-    )
-    databaseSchema: Optional[str] = Field(
-        None,
-        description='databaseSchema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single databaseSchema. When left blank, OpenMetadata Ingestion attempts to scan all the databaseSchema.',
-        title='databaseSchema',
+        ...,
+        description='Host and port of the SingleStore service.',
+        title='Host and Port',
     )
     databaseName: Optional[str] = Field(
         None,
         description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
         title='Database Name',
     )
-    authOptions: Optional[str] = Field(
+    databaseSchema: Optional[str] = Field(
         None,
-        description='Authentication options to pass to Hive connector. These options are based on SQLAlchemy.',
-        title='URL Authentication Options',
+        description='databaseSchema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single databaseSchema. When left blank, OpenMetadata Ingestion attempts to scan all the databaseSchema.',
+        title='databaseSchema',
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
+    supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
+        None, title='Supports Query Comment'
+    )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/mariaDBConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/mariaDBConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/mssqlConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/mssqlConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/mysqlConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/mysqlConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/oracleConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/oracleConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/pinotDBConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/pinotDBConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/postgresConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,33 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/postgresConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
+from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
 
 
+class SslMode(Enum):
+    disable = 'disable'
+    allow = 'allow'
+    prefer = 'prefer'
+    require = 'require'
+    verify_ca = 'verify-ca'
+    verify_full = 'verify-full'
+
+
 class PostgresType(Enum):
     Postgres = 'Postgres'
 
 
 class PostgresScheme(Enum):
     postgresql_psycopg2 = 'postgresql+psycopg2'
 
@@ -46,19 +56,20 @@
         ..., description='Host and port of the Postgres service.', title='Host and Port'
     )
     database: str = Field(
         ...,
         description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
         title='Database',
     )
-    sslMode: Optional[str] = Field(
-        None,
-        description='SSL Mode to connect to postgres database. E.g, prefer, verify-ca etc.',
+    sslMode: Optional[SslMode] = Field(
+        SslMode.disable,
+        description='SSL Mode to connect to postgres database.',
         title='SSL Mode',
     )
+    sslConfig: Optional[verifySSLConfig.SslConfig] = None
     classificationName: Optional[str] = Field(
         'PostgresPolicyTags',
         description='Custom OpenMetadata Classification name for Postgres policy tags.',
         title='Classification Name',
     )
     ingestAllDatabases: Optional[bool] = Field(
         False,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/prestoConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/prestoConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/redshiftConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,33 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/redshiftConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
+from .....security.ssl import verifySSLConfig
 from .. import connectionBasicType
 
 
+class SslMode(Enum):
+    disable = 'disable'
+    allow = 'allow'
+    prefer = 'prefer'
+    require = 'require'
+    verify_ca = 'verify-ca'
+    verify_full = 'verify-full'
+
+
 class RedshiftType(Enum):
     Redshift = 'Redshift'
 
 
 class RedshiftScheme(Enum):
     redshift_psycopg2 = 'redshift+psycopg2'
 
@@ -51,19 +61,20 @@
         title='Database',
     )
     ingestAllDatabases: Optional[bool] = Field(
         False,
         description='Ingest data from all databases in Redshift. You can use databaseFilterPattern on top of this.',
         title='Ingest All Databases',
     )
-    sslMode: Optional[str] = Field(
-        None,
-        description='SSL Mode to connect to postgres database. E.g, prefer, verify-ca etc.',
+    sslMode: Optional[SslMode] = Field(
+        SslMode.disable,
+        description='SSL Mode to connect to redshift database.',
         title='SSL Mode',
     )
+    sslConfig: Optional[verifySSLConfig.SslConfig] = None
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/salesforceConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/salesforceConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/singleStoreConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,65 +1,67 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/singleStoreConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/database/sqliteConnection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class SingleStoreType(Enum):
-    SingleStore = 'SingleStore'
+class SQLiteType(Enum):
+    SQLite = 'SQLite'
 
 
-class SingleStoreScheme(Enum):
-    mysql_pymysql = 'mysql+pymysql'
+class SQLiteScheme(Enum):
+    sqlite_pysqlite = 'sqlite+pysqlite'
 
 
-class SingleStoreConnection(BaseModel):
+class SQLiteConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[SingleStoreType] = Field(
-        SingleStoreType.SingleStore, description='Service Type', title='Service Type'
+    type: Optional[SQLiteType] = Field(
+        SQLiteType.SQLite, description='Service Type', title='Service Type'
     )
-    scheme: Optional[SingleStoreScheme] = Field(
-        SingleStoreScheme.mysql_pymysql,
+    scheme: Optional[SQLiteScheme] = Field(
+        SQLiteScheme.sqlite_pysqlite,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
-    username: str = Field(
-        ...,
-        description='Username to connect to SingleStore. This user should have privileges to read all the metadata in MySQL.',
+    username: Optional[str] = Field(
+        None,
+        description='Username to connect to SQLite. Blank for in-memory database.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to SingleStore.', title='Password'
+        None,
+        description='Password to connect to SQLite. Blank for in-memory database.',
+        title='Password',
     )
-    hostPort: str = Field(
-        ...,
-        description='Host and port of the SingleStore service.',
+    hostPort: Optional[str] = Field(
+        None,
+        description='Host and port of the SQLite service. Blank for in-memory database.',
         title='Host and Port',
     )
-    databaseName: Optional[str] = Field(
+    database: Optional[str] = Field(
         None,
-        description='Optional name to give to the database in OpenMetadata. If left blank, we will use default as the database name.',
-        title='Database Name',
+        description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
+        title='Database',
     )
-    databaseSchema: Optional[str] = Field(
-        None,
-        description='databaseSchema of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single databaseSchema. When left blank, OpenMetadata Ingestion attempts to scan all the databaseSchema.',
-        title='databaseSchema',
+    databaseMode: Optional[str] = Field(
+        ':memory:',
+        description='How to run the SQLite database. :memory: by default.',
+        title='Database Mode',
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/snowflakeConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/snowflakeConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/sqliteConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,77 +1,77 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/sqliteConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/database/verticaConnection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
 
 from .. import connectionBasicType
 
 
-class SQLiteType(Enum):
-    SQLite = 'SQLite'
+class VerticaType(Enum):
+    Vertica = 'Vertica'
 
 
-class SQLiteScheme(Enum):
-    sqlite_pysqlite = 'sqlite+pysqlite'
+class VerticaScheme(Enum):
+    vertica_vertica_python = 'vertica+vertica_python'
 
 
-class SQLiteConnection(BaseModel):
+class VerticaConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[SQLiteType] = Field(
-        SQLiteType.SQLite, description='Service Type', title='Service Type'
+    type: Optional[VerticaType] = Field(
+        VerticaType.Vertica, description='Service Type', title='Service Type'
     )
-    scheme: Optional[SQLiteScheme] = Field(
-        SQLiteScheme.sqlite_pysqlite,
+    scheme: Optional[VerticaScheme] = Field(
+        VerticaScheme.vertica_vertica_python,
         description='SQLAlchemy driver scheme options.',
         title='Connection Scheme',
     )
-    username: Optional[str] = Field(
-        None,
-        description='Username to connect to SQLite. Blank for in-memory database.',
+    username: str = Field(
+        ...,
+        description='Username to connect to Vertica. This user should have privileges to read all the metadata in Vertica.',
         title='Username',
     )
     password: Optional[CustomSecretStr] = Field(
-        None,
-        description='Password to connect to SQLite. Blank for in-memory database.',
-        title='Password',
+        None, description='Password to connect to Vertica.', title='Password'
     )
-    hostPort: Optional[str] = Field(
-        None,
-        description='Host and port of the SQLite service. Blank for in-memory database.',
-        title='Host and Port',
+    hostPort: str = Field(
+        ..., description='Host and port of the Vertica service.', title='Host and Port'
     )
     database: Optional[str] = Field(
         None,
         description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
         title='Database',
     )
-    databaseMode: Optional[str] = Field(
-        ':memory:',
-        description='How to run the SQLite database. :memory: by default.',
-        title='Database Mode',
-    )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
+    supportsUsageExtraction: Optional[
+        connectionBasicType.SupportsUsageExtraction
+    ] = None
+    supportsLineageExtraction: Optional[
+        connectionBasicType.SupportsLineageExtraction
+    ] = None
     supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
     supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
         None, title='Supports Profiler'
     )
+    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
+        None, title='Supports Database'
+    )
     supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
         None, title='Supports Query Comment'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/database/trinoConnection.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/database/trinoConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/database/verticaConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/druid/connection.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,77 +1,65 @@
-# generated by datamodel-codegen:
-#   filename:  entity/services/connections/database/verticaConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
-
-from __future__ import annotations
-
-from enum import Enum
+#  Copyright 2021 Collate
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#  http://www.apache.org/licenses/LICENSE-2.0
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+"""
+Source connection handler
+"""
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
-
-from metadata.ingestion.models.custom_pydantic import CustomSecretStr
-
-from .. import connectionBasicType
-
-
-class VerticaType(Enum):
-    Vertica = 'Vertica'
-
-
-class VerticaScheme(Enum):
-    vertica_vertica_python = 'vertica+vertica_python'
+from sqlalchemy.engine import Engine
 
+from metadata.generated.schema.entity.automations.workflow import (
+    Workflow as AutomationWorkflow,
+)
+from metadata.generated.schema.entity.services.connections.database.druidConnection import (
+    DruidConnection,
+)
+from metadata.ingestion.connections.builders import (
+    create_generic_db_connection,
+    get_connection_args_common,
+    get_connection_url_common,
+)
+from metadata.ingestion.connections.test_connections import test_connection_db_common
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
+
+
+def get_connection_url(connection: DruidConnection) -> str:
+    url = get_connection_url_common(connection)
+    return f"{url}/druid/v2/sql"
+
+
+def get_connection(connection: DruidConnection) -> Engine:
+    """
+    Create connection
+    """
+    return create_generic_db_connection(
+        connection=connection,
+        get_connection_url_fn=get_connection_url,
+        get_connection_args_fn=get_connection_args_common,
+    )
 
-class VerticaConnection(BaseModel):
-    class Config:
-        extra = Extra.forbid
 
-    type: Optional[VerticaType] = Field(
-        VerticaType.Vertica, description='Service Type', title='Service Type'
-    )
-    scheme: Optional[VerticaScheme] = Field(
-        VerticaScheme.vertica_vertica_python,
-        description='SQLAlchemy driver scheme options.',
-        title='Connection Scheme',
-    )
-    username: str = Field(
-        ...,
-        description='Username to connect to Vertica. This user should have privileges to read all the metadata in Vertica.',
-        title='Username',
-    )
-    password: Optional[CustomSecretStr] = Field(
-        None, description='Password to connect to Vertica.', title='Password'
-    )
-    hostPort: str = Field(
-        ..., description='Host and port of the Vertica service.', title='Host and Port'
-    )
-    database: Optional[str] = Field(
-        None,
-        description='Database of the data source. This is optional parameter, if you would like to restrict the metadata reading to a single database. When left blank, OpenMetadata Ingestion attempts to scan all the databases.',
-        title='Database',
-    )
-    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
-        None, title='Connection Options'
-    )
-    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
-        None, title='Connection Arguments'
-    )
-    supportsMetadataExtraction: Optional[
-        connectionBasicType.SupportsMetadataExtraction
-    ] = Field(None, title='Supports Metadata Extraction')
-    supportsUsageExtraction: Optional[
-        connectionBasicType.SupportsUsageExtraction
-    ] = None
-    supportsLineageExtraction: Optional[
-        connectionBasicType.SupportsLineageExtraction
-    ] = None
-    supportsDBTExtraction: Optional[connectionBasicType.SupportsDBTExtraction] = None
-    supportsProfiler: Optional[connectionBasicType.SupportsProfiler] = Field(
-        None, title='Supports Profiler'
-    )
-    supportsDatabase: Optional[connectionBasicType.SupportsDatabase] = Field(
-        None, title='Supports Database'
-    )
-    supportsQueryComment: Optional[connectionBasicType.SupportsQueryComment] = Field(
-        None, title='Supports Query Comment'
+def test_connection(
+    metadata: OpenMetadata,
+    engine: Engine,
+    service_connection: DruidConnection,
+    automation_workflow: Optional[AutomationWorkflow] = None,
+) -> None:
+    """
+    Test connection. This can be executed either as part
+    of a metadata workflow or during an Automation Workflow
+    """
+    test_connection_db_common(
+        metadata=metadata,
+        engine=engine,
+        service_connection=service_connection,
+        automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/customMessagingConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,33 +1,38 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/messaging/customMessagingConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/storage/gcsConnection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from .....security.credentials import gcsCredentials
 from .. import connectionBasicType
 
 
-class CustomMessagingType(Enum):
-    CustomMessaging = 'CustomMessaging'
+class GcsType(Enum):
+    Gcs = 'Gcs'
 
 
-class CustomMessagingConnection(BaseModel):
+class GcsConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: CustomMessagingType = Field(
-        ..., description='Custom messaging service type', title='Service Type'
+    type: Optional[GcsType] = Field(
+        GcsType.Gcs, description='Service Type', title='Service Type'
     )
-    sourcePythonClass: str = Field(
-        ...,
-        description='Source Python Class Name to instantiated by the ingestion workflow',
-        title='Source Python Class Name',
+    credentials: gcsCredentials.GCSCredentials = Field(
+        ..., description='GCS Credentials', title='GCS Credentials'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
+    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
+        None, title='Connection Arguments'
+    )
+    supportsMetadataExtraction: Optional[
+        connectionBasicType.SupportsMetadataExtraction
+    ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/kafkaConnection.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/kafkaConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/kinesisConnection.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/kinesisConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/pulsarConnection.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/pulsarConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/messaging/redpandaConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/messaging/redpandaConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Dict, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/amundsenConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/amundsenConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/atlasConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/atlasConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/metadataESConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -54,13 +54,22 @@
     includeDatabaseServices: Optional[bool] = Field(
         True, description='Include Database Services for Indexing'
     )
     includePipelineServices: Optional[bool] = Field(
         True, description='Include Pipeline Services for Indexing'
     )
     includeTags: Optional[bool] = Field(True, description='Include Tags for Indexing')
+    includeContainers: Optional[bool] = Field(
+        True, description='Include Containers for Indexing'
+    )
+    includeStorageServices: Optional[bool] = Field(
+        True, description='Include Storage Services for Indexing'
+    )
+    includeQueries: Optional[bool] = Field(
+        True, description='Include Queries for Indexing'
+    )
     limitRecords: Optional[int] = Field(
         '1000', description='Limit the number of records for Indexing.'
     )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/metadata/openMetadataConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/customMlModelConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/mlflowConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/sageMakerConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/mlmodel/sklearnConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/azureObjectStoreConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/objectstore/azureObjectStoreConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/storage/s3Connection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import azureCredentials
+from .....security.credentials import awsCredentials
 from .. import connectionBasicType
 
 
-class AzureType(Enum):
-    Azure = 'Azure'
+class S3Type(Enum):
+    S3 = 'S3'
 
 
-class AzureStoreConnection(BaseModel):
+class S3Connection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[AzureType] = Field(
-        AzureType.Azure, description='Service Type', title='Service Type'
+    type: Optional[S3Type] = Field(
+        S3Type.S3, description='Service Type', title='Service Type'
     )
-    credentials: azureCredentials.AzureCredentials = Field(
-        ..., description='Azure Credentials', title='Azure Credentials'
+    awsConfig: awsCredentials.AWSCredentials = Field(
+        ..., title='AWS Credentials Configuration'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/gcsObjectStoreConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,35 +1,35 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/objectstore/gcsObjectStoreConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/storage/adlsConection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from .....security.credentials import gcsCredentials
+from .....security.credentials import azureCredentials
 from .. import connectionBasicType
 
 
-class GcsType(Enum):
-    Gcs = 'Gcs'
+class AzureType(Enum):
+    Adls = 'Adls'
 
 
-class GcsStoreConnection(BaseModel):
+class AzureStoreConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[GcsType] = Field(
-        GcsType.Gcs, description='Service Type', title='Service Type'
+    type: Optional[AzureType] = Field(
+        AzureType.Adls, description='Service Type', title='Service Type'
     )
-    credentials: gcsCredentials.GCSCredentials = Field(
-        ..., description='GCS Credentials', title='GCS Credentials'
+    credentials: azureCredentials.AzureCredentials = Field(
+        ..., description='Azure Credentials', title='Azure Credentials'
     )
     connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
         None, title='Connection Options'
     )
     connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
         None, title='Connection Arguments'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/objectstore/s3ObjectStoreConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,38 +1,32 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/objectstore/s3ObjectStoreConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/connections/pipeline/gluePipelineConnection.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from .....security.credentials import awsCredentials
 from .. import connectionBasicType
 
 
-class S3Type(Enum):
-    S3 = 'S3'
+class GlueType(Enum):
+    GluePipeline = 'GluePipeline'
 
 
-class S3StoreConnection(BaseModel):
+class GluePipelineConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[S3Type] = Field(
-        S3Type.S3, description='Service Type', title='Service Type'
+    type: Optional[GlueType] = Field(
+        GlueType.GluePipeline, description='Service Type', title='Service Type'
     )
     awsConfig: awsCredentials.AWSCredentials = Field(
         ..., title='AWS Credentials Configuration'
     )
-    connectionOptions: Optional[connectionBasicType.ConnectionOptions] = Field(
-        None, title='Connection Options'
-    )
-    connectionArguments: Optional[connectionBasicType.ConnectionArguments] = Field(
-        None, title='Connection Arguments'
-    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/airbyteConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/airflowConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/backendConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/customPipelineConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/dagsterConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
@@ -25,10 +25,15 @@
     type: Optional[DagsterType] = Field(
         DagsterType.Dagster, description='Service Type', title='Service Type'
     )
     host: AnyUrl = Field(..., description='URL to the Dagster instance', title='Host')
     token: Optional[CustomSecretStr] = Field(
         None, description='To Connect to Dagster Cloud', title='Token'
     )
+    timeout: Optional[int] = Field(
+        '1000',
+        description='Connection Time Limit Between OM and Dagster Graphql API in second',
+        title='Time Out',
+    )
     supportsMetadataExtraction: Optional[
         connectionBasicType.SupportsMetadataExtraction
     ] = Field(None, title='Supports Metadata Extraction')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/databricksPipelineConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/domoPipelineConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/fivetranConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/pipeline/nifiConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/serviceConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/serviceConnection.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,40 +1,40 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/serviceConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
 
 from .. import (
     dashboardService,
     databaseService,
     messagingService,
     metadataService,
     mlmodelService,
-    objectstoreService,
     pipelineService,
+    storageService,
 )
 
 
 class ServiceConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
     __root__: Union[
         dashboardService.DashboardConnection,
         databaseService.DatabaseConnection,
         messagingService.MessagingConnection,
         metadataService.MetadataConnection,
         pipelineService.PipelineConnection,
         mlmodelService.MlModelConnection,
-        objectstoreService.ObjectStoreConnection,
+        storageService.StorageConnection,
     ] = Field(..., description='Supported services')
 
 
 class ServiceConnectionModel(BaseModel):
     class Config:
         extra = Extra.forbid
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/role.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,54 +1,38 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/connections/testConnectionDefinition.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/teams/role.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ....type import basic, entityHistory, entityReference
+from ...type import basic, entityHistory, entityReference
 
 
-class TestConnectionStep(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    name: str = Field(..., description='Name of the step being tested')
-    description: str = Field(..., description='What is the goal of the step')
-    mandatory: bool = Field(..., description='Is this step mandatory to be passed?')
+class RoleName(BaseModel):
+    __root__: basic.EntityName = Field(..., description='A unique name for the role.')
 
 
-class TestConnectionDefinition(BaseModel):
+class Role(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this test case definition instance.'
-    )
-    name: basic.EntityName = Field(
-        ...,
-        description='Name of the Test Connection Definition. It should be the `type` of the service being tested, e.g., Mysql, or Snowflake.',
-    )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this test definition.'
-    )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the test connection def.'
-    )
+    id: basic.Uuid
+    name: RoleName
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    steps: List[TestConnectionStep] = Field(
-        ..., description='Steps to test the connection. Order matters.'
+    displayName: Optional[str] = Field(
+        None, description="Name used for display purposes. Example 'Data Consumer'."
     )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this TestConnection definition.'
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the role.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
@@ -56,10 +40,30 @@
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
+    allowDelete: Optional[bool] = Field(
+        None, description="Some system roles can't be deleted"
+    )
+    allowEdit: Optional[bool] = Field(
+        None, description="Some system roles can't be edited"
+    )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
+    policies: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='Policies that is attached to this role.'
+    )
+    users: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='Users that have this role assigned to them.'
+    )
+    teams: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='Teams that have this role assigned to them.'
+    )
+    provider: Optional[basic.ProviderType] = basic.ProviderType.user
+    disabled: Optional[bool] = Field(
+        None,
+        description="System policy can't be deleted. Use this flag to disable them.",
+    )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/connections/testConnectionResult.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/connections/testConnectionResult.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -16,15 +16,20 @@
     class Config:
         extra = Extra.forbid
 
     name: str = Field(..., description='Name of the step being tested')
     mandatory: bool = Field(..., description='Is this step mandatory to be passed?')
     passed: bool = Field(..., description='Did the step pass successfully?')
     message: Optional[str] = Field(
-        None, description='Results or exceptions to be shared after running the test.'
+        None,
+        description='Results or exceptions to be shared after running the test. This message comes from the test connection definition',
+    )
+    errorLog: Optional[str] = Field(
+        None,
+        description='In case of failed step, this field would contain the actual error faced during the step.',
     )
 
 
 class StatusType(Enum):
     Successful = 'Successful'
     Failed = 'Failed'
     Running = 'Running'
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/dashboardService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/dashboardService.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/dashboardService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/databaseService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/databaseService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/databaseService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/ingestionPipelines/ingestionPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -157,7 +157,8 @@
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that led to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
+    provider: Optional[basic.ProviderType] = basic.ProviderType.user
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/messagingService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/messagingService.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/messagingService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/metadataService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/metadataService.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/metadataService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -90,10 +90,8 @@
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    allowServiceCreation: Optional[bool] = Field(
-        True, description='When `true` indicates the metadata service can be created'
-    )
+    provider: Optional[basic.ProviderType] = basic.ProviderType.user
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/mlmodelService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/mlmodelService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/mlmodelService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/objectstoreService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/storageService.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,81 +1,80 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/objectstoreService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  entity/services/storageService.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ...type import basic, entityHistory, entityReference, tagLabel
 from .connections import testConnectionResult
-from .connections.objectstore import s3ObjectStoreConnection
+from .connections.storage import s3Connection
 
 
-class ObjectStoreServiceType(Enum):
+class StorageServiceType(Enum):
     S3 = 'S3'
 
 
-class ObjectStoreConnection(BaseModel):
+class StorageConnection(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    config: Optional[s3ObjectStoreConnection.S3StoreConnection] = None
+    config: Optional[s3Connection.S3Connection] = None
 
 
-class ObjectStoreService(BaseModel):
+class StorageService(BaseModel):
     class Config:
         extra = Extra.forbid
 
     id: basic.Uuid = Field(
-        ..., description='Unique identifier of this object store service instance.'
+        ..., description='Unique identifier of this storage service instance.'
     )
     name: basic.EntityName = Field(
-        ..., description='Name that identifies this object store service.'
+        ..., description='Name that identifies this storage service.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
     displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this object store service.'
+        None, description='Display Name that identifies this storage service.'
     )
-    serviceType: ObjectStoreServiceType = Field(
-        ..., description='Type of object store service such as S3, GCS, AZURE...'
+    serviceType: StorageServiceType = Field(
+        ..., description='Type of storage service such as S3, GCS, AZURE...'
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Description of a object store service instance.'
+        None, description='Description of a storage service instance.'
     )
-    connection: Optional[ObjectStoreConnection] = None
+    connection: Optional[StorageConnection] = None
     pipelines: Optional[entityReference.EntityReferenceList] = Field(
         None,
-        description='References to pipelines deployed for this object store service to extract metadata, usage, lineage etc..',
+        description='References to pipelines deployed for this storage service to extract metadata, usage, lineage etc..',
     )
     testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = Field(
         None, description='Last test connection results for this service'
     )
     tags: Optional[List[tagLabel.TagLabel]] = Field(
-        None, description='Tags for this Object Store Service.'
+        None, description='Tags for this storage Service.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
-        None,
-        description='Link to the resource corresponding to this object store service.',
+        None, description='Link to the resource corresponding to this storage service.'
     )
     owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this object store service.'
+        None, description='Owner of this storage service.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/pipelineService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/services/pipelineService.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/services/pipelineService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/services/storageService.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/testSuite.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,55 +1,73 @@
 # generated by datamodel-codegen:
-#   filename:  entity/services/storageService.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  tests/testSuite.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from enum import Enum
+from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference, storage
+from ..entity.services.connections import testConnectionResult
+from ..type import basic, entityHistory, entityReference
 
 
-class StorageService(BaseModel):
+class ServiceType(Enum):
+    TestSuite = 'TestSuite'
+
+
+class TestSuiteConnection(BaseModel):
+    config: Optional[Any] = None
+
+
+class TestSuite(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier of this storage service instance.'
+    id: Optional[basic.Uuid] = Field(
+        None, description='Unique identifier of this test suite instance.'
     )
     name: basic.EntityName = Field(
-        ..., description='Name that identifies this storage service.'
+        ..., description='Name that identifies this test suite.'
+    )
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this test suite.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
         None, description='FullyQualifiedName same as `name`.'
     )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this storage service.'
+    description: basic.Markdown = Field(
+        ..., description='Description of the test suite.'
+    )
+    tests: Optional[List[entityReference.EntityReference]] = None
+    connection: Optional[TestSuiteConnection] = None
+    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = None
+    pipelines: Optional[entityReference.EntityReferenceList] = Field(
+        None,
+        description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
     )
-    serviceType: storage.StorageServiceType = Field(
-        ..., description='Type of storage service such as S3, GCS, HDFS...'
+    serviceType: Optional[ServiceType] = Field(
+        ServiceType.TestSuite,
+        description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of a storage service instance.'
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this TestCase definition.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
     )
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: basic.Href = Field(
-        ..., description='Link to the resource corresponding to this storage service.'
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this storage service.'
+    href: Optional[basic.Href] = Field(
+        None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/role.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/testDefinition.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,38 +1,90 @@
 # generated by datamodel-codegen:
-#   filename:  entity/teams/role.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  tests/testDefinition.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import Optional
+from enum import Enum
+from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ...type import basic, entityHistory, entityReference
+from ..entity.data import table
+from ..type import basic, entityHistory, entityReference
 
 
-class RoleName(BaseModel):
-    __root__: basic.EntityName = Field(..., description='A unique name for the role.')
+class TestPlatform(Enum):
+    OpenMetadata = 'OpenMetadata'
+    GreatExpectations = 'GreatExpectations'
+    DBT = 'DBT'
+    Deequ = 'Deequ'
+    Soda = 'Soda'
+    Other = 'Other'
+
+
+class TestDataType(Enum):
+    NUMBER = 'NUMBER'
+    INT = 'INT'
+    FLOAT = 'FLOAT'
+    DOUBLE = 'DOUBLE'
+    DECIMAL = 'DECIMAL'
+    TIMESTAMP = 'TIMESTAMP'
+    TIME = 'TIME'
+    DATE = 'DATE'
+    DATETIME = 'DATETIME'
+    ARRAY = 'ARRAY'
+    MAP = 'MAP'
+    SET = 'SET'
+    STRING = 'STRING'
+    BOOLEAN = 'BOOLEAN'
+
+
+class EntityType(Enum):
+    TABLE = 'TABLE'
+    COLUMN = 'COLUMN'
 
 
-class Role(BaseModel):
+class TestCaseParameterDefinition(BaseModel):
+    name: Optional[str] = Field(None, description='name of the parameter.')
+    displayName: Optional[str] = Field(
+        None, description='Display Name that identifies this parameter name.'
+    )
+    dataType: Optional[TestDataType] = Field(
+        None, description='Data type of the parameter (int, date etc.).'
+    )
+    description: Optional[basic.Markdown] = Field(
+        None, description='Description of the parameter.'
+    )
+    required: Optional[bool] = Field(False, description='Is this parameter required.')
+
+
+class TestDefinition(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid
-    name: RoleName
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
+    id: Optional[basic.Uuid] = Field(
+        None, description='Unique identifier of this test case definition instance.'
+    )
+    name: basic.EntityName = Field(
+        ..., description='Name that identifies this test case.'
     )
     displayName: Optional[str] = Field(
-        None, description="Name used for display purposes. Example 'Data Consumer'."
+        None, description='Display Name that identifies this test case.'
     )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the role.'
+    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
+        None, description='FullyQualifiedName same as `name`.'
+    )
+    description: basic.Markdown = Field(..., description='Description of the testcase.')
+    entityType: Optional[EntityType] = None
+    testPlatforms: List[TestPlatform]
+    supportedDataTypes: Optional[List[table.DataType]] = None
+    parameterDefinition: Optional[List[TestCaseParameterDefinition]] = None
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this TestCase definition.'
     )
     version: Optional[entityHistory.EntityVersion] = Field(
         None, description='Metadata version of the entity.'
     )
     updatedAt: Optional[basic.Timestamp] = Field(
         None,
         description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
@@ -40,30 +92,10 @@
     updatedBy: Optional[str] = Field(None, description='User who made the update.')
     href: Optional[basic.Href] = Field(
         None, description='Link to the resource corresponding to this entity.'
     )
     changeDescription: Optional[entityHistory.ChangeDescription] = Field(
         None, description='Change that lead to this version of the entity.'
     )
-    allowDelete: Optional[bool] = Field(
-        None, description="Some system roles can't be deleted"
-    )
-    allowEdit: Optional[bool] = Field(
-        None, description="Some system roles can't be edited"
-    )
     deleted: Optional[bool] = Field(
         False, description='When `true` indicates the entity has been soft deleted.'
     )
-    policies: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Policies that is attached to this role.'
-    )
-    users: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Users that have this role assigned to them.'
-    )
-    teams: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Teams that have this role assigned to them.'
-    )
-    provider: Optional[basic.ProviderType] = basic.ProviderType.user
-    disabled: Optional[bool] = Field(
-        None,
-        description="System policy can't be deleted. Use this flag to disable them.",
-    )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/team.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/team.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/team.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/teamHierarchy.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/teams/teamHierarchy.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/teams/teamHierarchy.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/teams/user.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityLineage.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,110 +1,85 @@
 # generated by datamodel-codegen:
-#   filename:  entity/teams/user.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/entityLineage.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from enum import Enum
-from typing import Optional, Union
+from typing import List, Optional
 
-from pydantic import BaseModel, Extra, Field, constr
+from pydantic import BaseModel, Extra, Field
 
-from ...auth import basicAuth, jwtAuth, ssoAuth
-from ...type import basic, entityHistory, entityReference, profile
+from . import basic, entityReference
 
 
-class EntityName(BaseModel):
-    __root__: constr(regex=r'^(?u)[\w\-.]+$', min_length=1, max_length=64) = Field(
-        ...,
-        description='Login name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
+class ColumnLineage(BaseModel):
+    fromColumns: Optional[List[basic.FullyQualifiedEntityName]] = Field(
+        None,
+        description='One or more source columns identified by fully qualified column name used by transformation function to create destination column.',
+    )
+    toColumn: Optional[basic.FullyQualifiedEntityName] = Field(
+        None,
+        description='Destination column identified by fully qualified column name created by the transformation of source columns.',
+    )
+    function: Optional[basic.SqlFunction] = Field(
+        None,
+        description='Transformation function applied to source columns to create destination column. That is `function(fromColumns) -> toColumn`.',
     )
 
 
-class AuthType(Enum):
-    JWT = 'JWT'
-    SSO = 'SSO'
-    BASIC = 'BASIC'
+class LineageDetails(BaseModel):
+    sqlQuery: Optional[basic.SqlQuery] = Field(
+        None, description='SQL used for transformation.'
+    )
+    columnsLineage: Optional[List[ColumnLineage]] = Field(
+        None,
+        description='Lineage information of how upstream columns were combined to get downstream column.',
+    )
+    pipeline: Optional[entityReference.EntityReference] = Field(
+        None, description='Pipeline where the sqlQuery is periodically run.'
+    )
 
 
-class AuthenticationMechanism(BaseModel):
+class Edge(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    config: Optional[
-        Union[
-            ssoAuth.SSOAuthMechanism,
-            jwtAuth.JWTAuthMechanism,
-            basicAuth.BasicAuthMechanism,
-        ]
-    ] = None
-    authType: Optional[AuthType] = None
+    fromEntity: basic.Uuid = Field(
+        ..., description='From entity that is upstream of lineage edge.'
+    )
+    toEntity: basic.Uuid = Field(
+        ..., description='To entity that is downstream of lineage edge.'
+    )
+    description: Optional[basic.Markdown] = None
+    lineageDetails: Optional[LineageDetails] = Field(
+        None,
+        description='Optional lineageDetails provided only for table to table lineage edge.',
+    )
 
 
-class User(BaseModel):
+class EntitiesEdge(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies a user entity instance.'
-    )
-    name: EntityName = Field(
-        ...,
-        description='A unique name of the user, typically the user ID from an identity provider. Example - uid from LDAP.',
-    )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
-    )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Used for user biography.'
-    )
-    displayName: Optional[str] = Field(
-        None,
-        description="Name used for display purposes. Example 'FirstName LastName'.",
+    fromEntity: entityReference.EntityReference = Field(
+        ..., description='From entity that is upstream of lineage edge.'
     )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
+    toEntity: entityReference.EntityReference = Field(
+        ..., description='To entity that is downstream of lineage edge.'
     )
-    updatedAt: Optional[basic.Timestamp] = Field(
+    description: Optional[basic.Markdown] = None
+    lineageDetails: Optional[LineageDetails] = Field(
         None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+        description='Optional lineageDetails provided only for table to table lineage edge.',
     )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    email: basic.Email = Field(..., description='Email address of the user.')
-    href: basic.Href = Field(
-        ..., description='Link to the resource corresponding to this entity.'
-    )
-    timezone: Optional[str] = Field(None, description='Timezone of the user.')
-    isBot: Optional[bool] = Field(
-        None, description='When true indicates a special type of user called Bot.'
-    )
-    isAdmin: Optional[bool] = Field(
-        None,
-        description='When true indicates user is an administrator for the system with superuser privileges.',
-    )
-    authenticationMechanism: Optional[AuthenticationMechanism] = None
-    profile: Optional[profile.Profile] = Field(None, description='Profile of the user.')
-    teams: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Teams that the user belongs to.'
-    )
-    owns: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='List of entities owned by the user.'
-    )
-    follows: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='List of entities followed by the user.'
-    )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
-    )
-    deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
-    )
-    roles: Optional[entityReference.EntityReferenceList] = Field(
-        None, description='Roles that the user has been assigned.'
-    )
-    inheritedRoles: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='Roles that a user is inheriting through membership in teams that have set team default roles.',
-    )
-    isEmailVerified: Optional[bool] = Field(
-        None, description='If the User has verified the mail'
+
+
+class EntityLineage(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    entity: entityReference.EntityReference = Field(
+        ..., description='Primary entity for which this lineage graph is created.'
     )
+    nodes: Optional[List[entityReference.EntityReference]] = None
+    upstreamEdges: Optional[List[Edge]] = None
+    downstreamEdges: Optional[List[Edge]] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/type.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/type.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/type.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/entitiesCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/entitiesCount.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/entitiesCount.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/servicesCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/servicesCount.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/servicesCount.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
@@ -24,10 +24,10 @@
     )
     pipelineServiceCount: Optional[int] = Field(
         None, description='Pipeline Service Count'
     )
     mlModelServiceCount: Optional[int] = Field(
         None, description='MlModel Service Count'
     )
-    objectStorageServiceCount: Optional[int] = Field(
-        None, description='Object Storage Service Count'
+    storageServiceCount: Optional[int] = Field(
+        None, description='Storage Service Count'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/entity/utils/supersetApiConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/entity/utils/supersetApiConnection.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  entity/utils/supersetApiConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/api/createEventSubscription.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/api/createEventSubscription.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/api/createEventSubscription.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/emailAlertConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/emailAlertConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/emailAlertConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/eventFilterRule.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/eventFilterRule.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/eventFilterRule.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/eventSubscription.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/eventSubscription.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/eventSubscription.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/events/subscriptionResourceDescriptor.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  events/subscriptionResourceDescriptor.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dashboardServiceMetadataPipeline.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dashboardServiceMetadataPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
@@ -21,18 +21,22 @@
         extra = Extra.forbid
 
     type: Optional[DashboardMetadataConfigType] = Field(
         DashboardMetadataConfigType.DashboardMetadata, description='Pipeline type'
     )
     dashboardFilterPattern: Optional[filterPattern.FilterPattern] = Field(
         None,
-        description='Regex to only fetch tables or databases that matches the pattern.',
+        description='Regex to exclude or include dashboards that matches the pattern.',
     )
     chartFilterPattern: Optional[filterPattern.FilterPattern] = Field(
-        None, description='Regex exclude tables or databases that matches the pattern.'
+        None, description='Regex exclude or include charts that matches the pattern.'
+    )
+    dataModelFilterPattern: Optional[filterPattern.FilterPattern] = Field(
+        None,
+        description='Regex exclude or include data models that matches the pattern.',
     )
     dbServiceNames: Optional[List] = Field(
         None,
         description='List of Database Service Name for creation of lineage',
         title='Database Service Name List',
     )
     overrideOwner: Optional[bool] = Field(
@@ -43,7 +47,11 @@
     markDeletedDashboards: Optional[bool] = Field(
         True,
         description='Optional configuration to soft delete dashboards in OpenMetadata if the source dashboards are deleted. Also, if the dashboard is deleted, all the associated entities like lineage, etc., with that dashboard will be deleted',
     )
     includeTags: Optional[bool] = Field(
         True, description='Optional configuration to toggle the tags ingestion.'
     )
+    includeDataModels: Optional[bool] = Field(
+        True,
+        description='Optional configuration to toggle the ingestion of data models.',
+    )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceMetadataPipeline.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceMetadataPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceProfilerPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceQueryLineagePipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/databaseServiceQueryUsagePipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtPipeline.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,21 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/dbtconfig/dbtBucketDetails.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  metadataIngestion/dbtconfig/dbtGCSConfig.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import BaseModel, Extra, Field
+from pydantic import BaseModel, Field
 
+from ...security.credentials import gcsCredentials
+from . import dbtBucketDetails
 
-class DbtBucketDetails(BaseModel):
-    class Config:
-        extra = Extra.forbid
 
-    dbtBucketName: Optional[str] = Field(
-        None,
-        description='Name of the bucket where the dbt files are stored',
-        title='DBT Bucket Name',
+class DbtGcsConfig(BaseModel):
+    dbtSecurityConfig: Optional[gcsCredentials.GCSCredentials] = Field(
+        None, title='DBT GCS Security Config'
     )
-    dbtObjectPrefix: Optional[str] = Field(
-        None,
-        description='Path of the folder where the dbt files are stored',
-        title='DBT Object Prefix',
+    dbtPrefixConfig: Optional[dbtBucketDetails.DbtBucketDetails] = Field(
+        None, title='DBT Prefix Config'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtCloudConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/dbtconfig/dbtLocalConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/messagingServiceMetadataPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/metadataToElasticSearchPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/mlmodelServiceMetadataPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/objectstore/containerMetadataConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/objectstore/containerMetadataConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  metadataIngestion/storage/containerMetadataConfig.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from ...entity.data import table
+
 
 class MetadataEntry(BaseModel):
     dataPath: str = Field(
         ...,
         description='The path where the data resides in the container, excluding the bucket name',
         title='Data path',
     )
@@ -21,22 +23,22 @@
         title='Schema format',
     )
     isPartitioned: Optional[bool] = Field(
         False,
         description="Flag indicating whether the container's data is partitioned",
         title='Is Partitioned',
     )
-    partitionColumn: Optional[str] = Field(
+    partitionColumns: Optional[List[table.Column]] = Field(
         None,
-        description="What is the partition column in case the container's data is partitioned",
-        title='Partition Column',
+        description="What are the partition columns in case the container's data is partitioned",
+        title='Partition Columns',
     )
 
 
-class ObjectStoreContainerConfig(BaseModel):
+class StorageContainerConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     entries: List[MetadataEntry] = Field(
         ...,
         description='List of metadata entries for the bucket containing information about where data resides and its structure',
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/objectstoreServiceMetadataPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 # generated by datamodel-codegen:
-#   filename:  metadataIngestion/objectstoreServiceMetadataPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  metadataIngestion/storageServiceMetadataPipeline.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 from ..type import filterPattern
 
 
-class ObjectstoreMetadataConfigType(Enum):
-    ObjectStoreMetadata = 'ObjectStoreMetadata'
+class StorageMetadataConfigType(Enum):
+    StorageMetadata = 'StorageMetadata'
 
 
-class ObjectStoreServiceMetadataPipeline(BaseModel):
+class StorageServiceMetadataPipeline(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    type: Optional[ObjectstoreMetadataConfigType] = Field(
-        ObjectstoreMetadataConfigType.ObjectStoreMetadata, description='Pipeline type'
+    type: Optional[StorageMetadataConfigType] = Field(
+        StorageMetadataConfigType.StorageMetadata, description='Pipeline type'
     )
     containerFilterPattern: Optional[filterPattern.FilterPattern] = Field(
         None, description='Regex to only fetch containers that matches the pattern.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/pipelineServiceMetadataPipeline.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/metadataIngestion/workflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/metadataIngestion/workflow.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  metadataIngestion/workflow.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -19,16 +19,16 @@
     databaseServiceQueryLineagePipeline,
     databaseServiceQueryUsagePipeline,
     dataInsightPipeline,
     dbtPipeline,
     messagingServiceMetadataPipeline,
     metadataToElasticSearchPipeline,
     mlmodelServiceMetadataPipeline,
-    objectstoreServiceMetadataPipeline,
     pipelineServiceMetadataPipeline,
+    storageServiceMetadataPipeline,
     testSuitePipeline,
 )
 
 
 class LogLevels(Enum):
     DEBUG = 'DEBUG'
     INFO = 'INFO'
@@ -87,15 +87,15 @@
             databaseServiceQueryUsagePipeline.DatabaseServiceQueryUsagePipeline,
             databaseServiceQueryLineagePipeline.DatabaseServiceQueryLineagePipeline,
             dashboardServiceMetadataPipeline.DashboardServiceMetadataPipeline,
             messagingServiceMetadataPipeline.MessagingServiceMetadataPipeline,
             databaseServiceProfilerPipeline.DatabaseServiceProfilerPipeline,
             pipelineServiceMetadataPipeline.PipelineServiceMetadataPipeline,
             mlmodelServiceMetadataPipeline.MlModelServiceMetadataPipeline,
-            objectstoreServiceMetadataPipeline.ObjectStoreServiceMetadataPipeline,
+            storageServiceMetadataPipeline.StorageServiceMetadataPipeline,
             testSuitePipeline.TestSuitePipeline,
             metadataToElasticSearchPipeline.MetadataToElasticSearchPipeline,
             dataInsightPipeline.DataInsightPipeline,
             dbtPipeline.DbtPipeline,
         ]
     ] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/auth0SSOClientConfig.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/auth0SSOClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/azureSSOClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/azureSSOClientConfig.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/azureSSOClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/customOidcSSOClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/googleSSOClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/googleSSOClientConfig.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/googleSSOClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/oktaSSOClientConfig.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/oktaSSOClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/client/samlSSOClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/client/samlSSOClientConfig.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/client/samlSSOClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/awsCredentials.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/awsCredentials.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/awsCredentials.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/azureCredentials.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/azureCredentials.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/azureCredentials.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 from metadata.ingestion.models.custom_pydantic import CustomSecretStr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/credentials/gcsValues.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/credentials/gcsValues.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/credentials/gcsValues.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional, Union
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/secrets/secretsManagerConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/securityConfiguration.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/securityConfiguration.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/securityConfiguration.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # generated by datamodel-codegen:
 #   filename:  security/ssl/validateSSLClientConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
 
 class ValidateSSLClientConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
     certificatePath: Optional[str] = Field(
         None,
-        description='CA certificate path. E.g., /path/to/public.cert. Will be used if Verify SSL is set to `validate`.',
+        description='CA certificate path. E.g., /path/to/public.cert. Will be used if Verify SSL is set to `validate` or `verify`.',
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/security/ssl/verifySSLConfig.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/security/ssl/verifySSLConfig.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  security/ssl/verifySSLConfig.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any
 
 from pydantic import BaseModel, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/settings/settings.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/settings/settings.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  settings/settings.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional, Union
 
 from pydantic import BaseModel, Extra, Field
@@ -15,14 +15,15 @@
     elasticSearchConfiguration,
     eventHandlerConfiguration,
     fernetConfiguration,
     jwtTokenConfiguration,
     pipelineServiceClientConfiguration,
     taskNotificationConfiguration,
 )
+from ..email import smtpSettings
 
 
 class SettingType(Enum):
     authorizerConfiguration = 'authorizerConfiguration'
     authenticationConfiguration = 'authenticationConfiguration'
     jwtTokenConfiguration = 'jwtTokenConfiguration'
     elasticsearch = 'elasticsearch'
@@ -31,14 +32,15 @@
     fernetConfiguration = 'fernetConfiguration'
     slackEventPublishers = 'slackEventPublishers'
     secretsManagerConfiguration = 'secretsManagerConfiguration'
     sandboxModeEnabled = 'sandboxModeEnabled'
     slackChat = 'slackChat'
     taskNotificationConfiguration = 'taskNotificationConfiguration'
     testResultNotificationConfiguration = 'testResultNotificationConfiguration'
+    emailConfiguration = 'emailConfiguration'
 
 
 class Settings(BaseModel):
     class Config:
         extra = Extra.forbid
 
     config_type: SettingType = Field(
@@ -50,9 +52,10 @@
             authenticationConfiguration.AuthenticationConfiguration,
             authorizerConfiguration.AuthorizerConfiguration,
             elasticSearchConfiguration.ElasticSearchConfiguration,
             eventHandlerConfiguration.EventHandlerConfiguration,
             fernetConfiguration.FernetConfiguration,
             jwtTokenConfiguration.JWTTokenConfiguration,
             taskNotificationConfiguration.TaskNotificationConfiguration,
+            smtpSettings.SmtpSettings,
         ]
     ] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/basic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/basic.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/basic.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/customMetric.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/customMetric.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/customMetric.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/testCase.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/tests/testCase.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  tests/testCase.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/testDefinition.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/databaseConnectionConfig.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,101 +1,53 @@
 # generated by datamodel-codegen:
-#   filename:  tests/testDefinition.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/databaseConnectionConfig.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from ..entity.data import table
-from ..type import basic, entityHistory, entityReference
 
-
-class TestPlatform(Enum):
-    OpenMetadata = 'OpenMetadata'
-    GreatExpectations = 'GreatExpectations'
-    DBT = 'DBT'
-    Deequ = 'Deequ'
-    Soda = 'Soda'
-    Other = 'Other'
-
-
-class TestDataType(Enum):
-    NUMBER = 'NUMBER'
-    INT = 'INT'
-    FLOAT = 'FLOAT'
-    DOUBLE = 'DOUBLE'
-    DECIMAL = 'DECIMAL'
-    TIMESTAMP = 'TIMESTAMP'
-    TIME = 'TIME'
-    DATE = 'DATE'
-    DATETIME = 'DATETIME'
-    ARRAY = 'ARRAY'
-    MAP = 'MAP'
-    SET = 'SET'
-    STRING = 'STRING'
-    BOOLEAN = 'BOOLEAN'
-
-
-class EntityType(Enum):
-    TABLE = 'TABLE'
-    COLUMN = 'COLUMN'
-
-
-class TestCaseParameterDefinition(BaseModel):
-    name: Optional[str] = Field(None, description='name of the parameter.')
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this parameter name.'
-    )
-    dataType: Optional[TestDataType] = Field(
-        None, description='Data type of the parameter (int, date etc.).'
-    )
-    description: Optional[basic.Markdown] = Field(
-        None, description='Description of the parameter.'
-    )
-    required: Optional[bool] = Field(False, description='Is this parameter required.')
-
-
-class TestDefinition(BaseModel):
+class DatabaseConnectionConfig(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this test case definition instance.'
+    username: Optional[str] = Field(
+        None, description='username to connect  to the data source.'
     )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this test case.'
+    password: Optional[str] = Field(
+        None, description='password to connect  to the data source.'
     )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this test case.'
+    hostPort: Optional[str] = Field(
+        None, description='Host and port of the data source.'
     )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
-    )
-    description: basic.Markdown = Field(..., description='Description of the testcase.')
-    entityType: Optional[EntityType] = None
-    testPlatforms: List[TestPlatform]
-    supportedDataTypes: Optional[List[table.DataType]] = None
-    parameterDefinition: Optional[List[TestCaseParameterDefinition]] = None
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this TestCase definition.'
+    database: Optional[str] = Field(None, description='Database of the data source.')
+    schema_: Optional[str] = Field(
+        None, alias='schema', description='schema of the data source.'
     )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
+    includeViews: Optional[bool] = Field(
+        True,
+        description='optional configuration to turn off fetching metadata for views.',
     )
-    updatedAt: Optional[basic.Timestamp] = Field(
-        None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
+    includeTables: Optional[bool] = Field(
+        True,
+        description='Optional configuration to turn off fetching metadata for tables.',
+    )
+    generateSampleData: Optional[bool] = Field(
+        True, description='Turn on/off collecting sample data.'
     )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
+    sampleDataQuery: Optional[str] = Field(
+        'select * from {}.{} limit 50', description='query to generate sample data.'
     )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
+    enableDataProfiler: Optional[bool] = Field(
+        False,
+        description='Run data profiler as part of ingestion to get table profile data.',
+    )
+    includeFilterPattern: Optional[List[str]] = Field(
+        None,
+        description='Regex to only fetch tables or databases that matches the pattern.',
     )
-    deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
+    excludeFilterPattern: Optional[List[str]] = Field(
+        None, description='Regex exclude tables or databases that matches the pattern.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/tests/testSuite.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/models.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,73 +1,50 @@
-# generated by datamodel-codegen:
-#   filename:  tests/testSuite.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#  Copyright 2021 Collate
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#  http://www.apache.org/licenses/LICENSE-2.0
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+"""
+Looker pydantic models
+"""
+
+from typing import List, NewType, Optional
+
+from pydantic import BaseModel, Field
+
+Includes = NewType("Includes", str)
+ViewName = NewType("ViewName", str)
+
+
+class LookMlField(BaseModel):
+    description: Optional[str] = Field(None, description="Field description")
+    label: Optional[str] = Field(None, description="Field display name")
+    type: Optional[str] = Field(None, description="Field type to be mapped to OM")
+    name: str = Field(..., description="Field name")
+
+
+class LookMlView(BaseModel):
+    name: ViewName = Field(..., description="View name")
+    description: Optional[str] = Field(None, description="View description")
+    sql_table_name: Optional[str] = Field(
+        None, description="To track lineage with the source"
+    )
+    measures: List[LookMlField] = Field([], description="Measures to ingest as cols")
+    dimensions: List[LookMlField] = Field(
+        [], description="Dimensions to ingest as cols"
+    )
+    source_file: Optional[Includes] = Field(None, description="lkml file path")
+
+
+class LkmlFile(BaseModel):
+    """
+    it might also have explores, but we don't care.
+    We'll pick explores from the API
+    """
 
-from __future__ import annotations
-
-from enum import Enum
-from typing import Any, List, Optional
-
-from pydantic import BaseModel, Extra, Field
-
-from ..entity.services.connections import testConnectionResult
-from ..type import basic, entityHistory, entityReference
-
-
-class ServiceType(Enum):
-    TestSuite = 'TestSuite'
-
-
-class TestSuiteConnection(BaseModel):
-    config: Optional[Any] = None
-
-
-class TestSuite(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    id: Optional[basic.Uuid] = Field(
-        None, description='Unique identifier of this test suite instance.'
-    )
-    name: basic.EntityName = Field(
-        ..., description='Name that identifies this test suite.'
-    )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this test suite.'
-    )
-    fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = Field(
-        None, description='FullyQualifiedName same as `name`.'
-    )
-    description: basic.Markdown = Field(
-        ..., description='Description of the test suite.'
-    )
-    tests: Optional[List[entityReference.EntityReference]] = None
-    connection: Optional[TestSuiteConnection] = None
-    testConnectionResult: Optional[testConnectionResult.TestConnectionResult] = None
-    pipelines: Optional[entityReference.EntityReferenceList] = Field(
-        None,
-        description='References to pipelines deployed for this database service to extract metadata, usage, lineage etc..',
-    )
-    serviceType: Optional[ServiceType] = Field(
-        ServiceType.TestSuite,
-        description='Type of database service such as MySQL, BigQuery, Snowflake, Redshift, Postgres...',
-    )
-    owner: Optional[entityReference.EntityReference] = Field(
-        None, description='Owner of this TestCase definition.'
-    )
-    version: Optional[entityHistory.EntityVersion] = Field(
-        None, description='Metadata version of the entity.'
-    )
-    updatedAt: Optional[basic.Timestamp] = Field(
-        None,
-        description='Last update time corresponding to the new version of the entity in Unix epoch time milliseconds.',
-    )
-    updatedBy: Optional[str] = Field(None, description='User who made the update.')
-    href: Optional[basic.Href] = Field(
-        None, description='Link to the resource corresponding to this entity.'
-    )
-    changeDescription: Optional[entityHistory.ChangeDescription] = Field(
-        None, description='Change that lead to this version of the entity.'
-    )
-    deleted: Optional[bool] = Field(
-        False, description='When `true` indicates the entity has been soft deleted.'
-    )
+    includes: List[Includes] = Field([], description="Full include list")
+    views: List[LookMlView] = Field([], description="Views we want to parse")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/auditLog.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/auditLog.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/auditLog.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/basic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/basic.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/basic.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from datetime import date, datetime, time
 from enum import Enum
 from typing import Any, Dict, Optional
 from uuid import UUID
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/changeEvent.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/changeEvent.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/changeEvent.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/collectionDescriptor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/collectionDescriptor.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/collectionDescriptor.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/csvDocumentation.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvDocumentation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvDocumentation.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/csvImportResult.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/csvImportResult.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/csvImportResult.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityHistory.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityHistory.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityHistory.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Any, List, Optional
 
 from pydantic import BaseModel, Extra, Field, confloat
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityReference.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/data/createChart.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,43 +1,40 @@
 # generated by datamodel-codegen:
-#   filename:  type/entityReference.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  api/data/createChart.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
 
-from . import basic
+from ...entity.data import chart
+from ...type import basic, entityReference, tagLabel
 
 
-class EntityReference(BaseModel):
+class CreateChartRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    id: basic.Uuid = Field(
-        ..., description='Unique identifier that identifies an entity instance.'
-    )
-    type: str = Field(
-        ...,
-        description='Entity type/class name - Examples: `database`, `table`, `metrics`, `databaseService`, `dashboardService`...',
-    )
-    name: Optional[str] = Field(None, description='Name of the entity instance.')
-    fullyQualifiedName: Optional[str] = Field(
+    name: basic.EntityName = Field(..., description='Name that identifies this Chart.')
+    displayName: Optional[str] = Field(
         None,
-        description="Fully qualified name of the entity instance. For entities such as tables, databases fullyQualifiedName is returned in this field. For entities that don't have name hierarchy such as `user` and `team` this will be same as the `name` field.",
+        description='Display Name that identifies this Chart. It could be title or label from the source services',
     )
     description: Optional[basic.Markdown] = Field(
-        None, description='Optional description of entity.'
+        None,
+        description='Description of the chart instance. What it has and how to use it.',
     )
-    displayName: Optional[str] = Field(
-        None, description='Display Name that identifies this entity.'
+    chartType: Optional[chart.ChartType] = None
+    chartUrl: Optional[str] = Field(
+        None, description='Chart URL suffix from its service.'
     )
-    deleted: Optional[bool] = Field(
-        None, description='If true the entity referred to has been soft-deleted.'
+    tags: Optional[List[tagLabel.TagLabel]] = Field(
+        None, description='Tags for this chart'
+    )
+    owner: Optional[entityReference.EntityReference] = Field(
+        None, description='Owner of this chart'
+    )
+    service: basic.FullyQualifiedEntityName = Field(
+        ..., description='Link to the chart service where this chart is hosted in'
     )
-    href: Optional[basic.Href] = Field(None, description='Link to the entity resource.')
-
-
-class EntityReferenceList(BaseModel):
-    __root__: List[EntityReference]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/entityRelationship.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/entityRelationship.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/entityRelationship.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/filterPattern.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/api/lineage/addLineage.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,27 +1,21 @@
 # generated by datamodel-codegen:
-#   filename:  type/filterPattern.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  api/lineage/addLineage.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
-from typing import List, Optional
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from ...type import basic, entityLineage
 
-class FilterPatternModel(BaseModel):
-    pass
 
-
-class FilterPattern(BaseModel):
+class AddLineageRequest(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    includes: Optional[List[str]] = Field(
-        None,
-        description='List of strings/regex patterns to match and include only database entities that match.',
-    )
-    excludes: Optional[List[str]] = Field(
-        None,
-        description='List of strings/regex patterns to match and exclude only database entities that match.',
+    description: Optional[basic.Markdown] = Field(
+        None, description='User provided description of the lineage details.'
     )
+    edge: entityLineage.EntitiesEdge = Field(..., description='Lineage edge details.')
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/function.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/votes.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,44 +1,32 @@
 # generated by datamodel-codegen:
-#   filename:  type/function.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   filename:  type/votes.json
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
-from typing import List, Optional
+from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
 
+from . import entityReference
 
-class ParameterType(Enum):
-    NotRequired = 'NotRequired'
-    AllIndexElasticSearch = 'AllIndexElasticSearch'
-    SpecificIndexElasticSearch = 'SpecificIndexElasticSearch'
-    ReadFromParamContext = 'ReadFromParamContext'
 
-
-class ParamAdditionalContext(BaseModel):
-    class Config:
-        extra = Extra.forbid
-
-    data: Optional[List[str]] = Field(
-        None, description='List of Entities', unique_items=True
-    )
+class VoteType(Enum):
+    votedUp = 'votedUp'
+    votedDown = 'votedDown'
+    unVoted = 'unVoted'
 
 
-class Function(BaseModel):
+class Votes(BaseModel):
     class Config:
         extra = Extra.forbid
 
-    name: Optional[str] = Field(None, description='Name of the function.')
-    input: Optional[str] = Field(
-        None, description='Description of input taken by the function.'
-    )
-    description: Optional[str] = Field(None, description='Description fo the function.')
-    examples: Optional[List] = Field(
-        None, description='Examples of the function to help users author conditions.'
+    upVotes: Optional[int] = Field(0, description='Total up-votes the entity has')
+    downVotes: Optional[int] = Field(0, description='Total down-votes the entity has')
+    upVoters: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='List of all the Users who upVoted'
     )
-    parameterInputType: Optional[ParameterType] = Field(
-        None, description='List of receivers to send mail to'
+    downVoters: Optional[entityReference.EntityReferenceList] = Field(
+        None, description='List of all the Users who downVoted'
     )
-    paramAdditionalContext: Optional[ParamAdditionalContext] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/jdbcConnection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/jdbcConnection.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/jdbcConnection.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from pydantic import BaseModel, Extra, Field
 
 
 class DriverClass(BaseModel):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/profile.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/profile.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/profile.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import AnyUrl, BaseModel, Extra
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/queryParserData.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/queryParserData.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/queryParserData.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/reaction.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/reaction.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/reaction.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/schedule.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/schedule.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/schedule.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/schema.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/schema.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/schema.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field, constr
@@ -36,14 +36,15 @@
     STRING = 'STRING'
     ARRAY = 'ARRAY'
     MAP = 'MAP'
     ENUM = 'ENUM'
     UNION = 'UNION'
     FIXED = 'FIXED'
     ERROR = 'ERROR'
+    UNKNOWN = 'UNKNOWN'
 
 
 class FieldName(BaseModel):
     __root__: constr(min_length=1, max_length=128) = Field(
         ..., description='Local name (not fully qualified name) of the field. '
     )
 
@@ -55,14 +56,18 @@
     name: FieldName
     displayName: Optional[str] = Field(
         None, description='Display Name that identifies this field name.'
     )
     dataType: DataTypeTopic = Field(
         ..., description='Data type of the field (int, date etc.).'
     )
+    dataTypeDisplay: Optional[str] = Field(
+        None,
+        description='Display name used for dataType. This is useful for complex types, such as `array<int>`, `map<int,string>`, `struct<>`, and union types.',
+    )
     description: Optional[basic.Markdown] = Field(
         None, description='Description of the column.'
     )
     fullyQualifiedName: Optional[basic.FullyQualifiedEntityName] = None
     tags: Optional[List[tagLabel.TagLabel]] = Field(
         None, description='Tags associated with the column.'
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/tableQuery.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/tableQuery.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/tableQuery.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/tableUsageCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/tableUsageCount.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/tableUsageCount.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import List, Optional
 
 from pydantic import BaseModel, Extra, Field
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/tagLabel.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/tagLabel.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/tagLabel.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, constr
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/generated/schema/type/usageDetails.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/generated/schema/type/usageDetails.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # generated by datamodel-codegen:
 #   filename:  type/usageDetails.json
-#   timestamp: 2023-03-30T09:54:39+00:00
+#   timestamp: 2023-04-14T12:26:07+00:00
 
 from __future__ import annotations
 
 from typing import Optional
 
 from pydantic import BaseModel, Extra, Field, confloat, conint
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/action.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/action.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/great_expectations/utils/ometa_config_handler.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/great_expectations/utils/ometa_config_handler.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/bulk_sink.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/bulk_sink.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/closeable.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/closeable.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/common.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/common.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/parser.py`

 * *Files 0% similar despite different names*

```diff
@@ -398,15 +398,17 @@
                 )
                 raise ParsingConfigurationError(
                     f"We encountered an error parsing the configuration of your {object_error}.\n"
                     "You might need to review your config based on the original cause of this failure:\n"
                     f"{_parse_validation_err(scoped_error)}"
                 )
             raise scoped_error
-        except Exception:  # Let's just raise the original error if any internal logic fails
+        except (
+            Exception
+        ):  # Let's just raise the original error if any internal logic fails
             raise ParsingConfigurationError(
                 f"We encountered an error parsing the configuration of your workflow.\n"
                 "You might need to review your config based on the original cause of this failure:\n"
                 f"{_parse_validation_err(original_error)}"
             )
 
     raise ParsingConfigurationError("Uncaught error when parsing the workflow!")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/processor.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/processor.py`

 * *Files 1% similar despite different names*

```diff
@@ -25,26 +25,24 @@
 from metadata.ingestion.api.status import Status
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class ProcessorStatus(Status):
-
     records: List[str] = Field(default_factory=list)
 
     def processed(self, record: Any):
         self.records.append(record)
 
     def warning(self, info: Any) -> None:
         self.warnings.append(info)
 
 
 class ProfilerProcessorStatus(Status):
-
     entity: Optional[str] = None
 
     def failed_profiler(self, error: str, stack_trace: Optional[str] = None) -> None:
         self.failed(self.entity if self.entity else "", error, stack_trace)
 
 
 @dataclass
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/sink.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/sink.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,14 @@
 )
 from metadata.ingestion.api.closeable import Closeable
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.status import Status
 
 
 class SinkStatus(Status):
-
     records: List[str] = Field(default_factory=list)
 
     def records_written(self, record: str) -> None:
         self.records.append(record)
 
     def warning(self, info: Any) -> None:
         self.warnings.append(info)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/stage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/stage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/status.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/topology_runner.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/topology_runner.py`

 * *Files 0% similar despite different names*

```diff
@@ -69,21 +69,19 @@
             child_nodes = (
                 [get_topology_node(child, self.topology) for child in node.children]
                 if node.children
                 else []
             )
 
             for element in node_producer() or []:
-
                 for stage in node.stages:
                     logger.debug(f"Processing stage: {stage}")
 
                     stage_fn = getattr(self, stage.processor)
                     for entity_request in stage_fn(element) or []:
-
                         try:
                             # yield and make sure the data is updated
                             yield from self.sink_request(
                                 stage=stage, entity_request=entity_request
                             )
                         except ValueError as err:
                             logger.debug(traceback.format_exc())
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/api/workflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/api/workflow.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/bulksink/metadata_usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/bulksink/metadata_usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/builders.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/builders.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/headers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/headers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/secrets.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/secrets.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/session.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/session.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/connections/test_connections.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/connections/test_connections.py`

 * *Files 10% similar despite different names*

```diff
@@ -70,16 +70,18 @@
     ```
 
     so that we can execute `step_1.function()`
     """
 
     function: Callable
     name: str
-    description: Optional[str] = None
+    error_message: Optional[str]
+    description: Optional[str]
     mandatory: bool = True
+    short_circuit: bool = False
 
 
 class TestConnectionIngestionResult(BaseModel):
     failed: List[str] = []
     success: List[str] = []
     warning: List[str] = []
 
@@ -101,15 +103,15 @@
     else:
         _test_connection_steps_during_ingestion(steps=steps)
 
 
 def _test_connection_steps_automation_workflow(
     metadata: OpenMetadata,
     steps: List[TestConnectionStep],
-    automation_workflow: Optional[AutomationWorkflow],
+    automation_workflow: AutomationWorkflow,
 ) -> None:
     """
     Run the test connection as part of the automation workflow
     We need to update the automation workflow in each step
     """
     test_connection_result = TestConnectionResult(
         status=StatusType.Running,
@@ -123,51 +125,44 @@
                     TestConnectionStepResult(
                         name=step.name,
                         mandatory=step.mandatory,
                         passed=True,
                     )
                 )
             except Exception as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(f"{step.name}-{err}")
                 test_connection_result.steps.append(
                     TestConnectionStepResult(
                         name=step.name,
                         mandatory=step.mandatory,
                         passed=False,
-                        message=str(err),
+                        message=step.error_message,
+                        errorLog=str(err),
                     )
                 )
+                if step.short_circuit:
+                    # break the workflow if the step is a short circuit step
+                    break
 
             test_connection_result.lastUpdatedAt = datetime.now().timestamp()
-            updated_workflow = CreateWorkflowRequest(
-                name=automation_workflow.name,
-                description=automation_workflow.description,
-                workflowType=automation_workflow.workflowType,
-                request=automation_workflow.request,
-                response=test_connection_result,
-                status=WorkflowStatus.Running,
+            metadata.patch_automation_workflow_response(
+                automation_workflow, test_connection_result, WorkflowStatus.Running
             )
-            metadata.create_or_update(updated_workflow)
 
         test_connection_result.lastUpdatedAt = datetime.now().timestamp()
 
         test_connection_result.status = (
             StatusType.Failed
             if any(step for step in test_connection_result.steps if not step.passed)
             else StatusType.Successful
         )
 
-        metadata.create_or_update(
-            CreateWorkflowRequest(
-                name=automation_workflow.name,
-                description=automation_workflow.description,
-                workflowType=automation_workflow.workflowType,
-                request=automation_workflow.request,
-                response=test_connection_result,
-                status=WorkflowStatus.Successful,
-            )
+        metadata.patch_automation_workflow_response(
+            automation_workflow, test_connection_result, WorkflowStatus.Successful
         )
 
     except Exception as err:
         logger.error(
             f"Wild error happened while testing the connection in the workflow - {err}"
         )
         logger.debug(traceback.format_exc())
@@ -206,14 +201,18 @@
 
             else:
                 test_connection_result.warning.append(
                     f"'{step.name}': This is a optional and the ingestion will continue to work as expected."
                     f"Failed due to: {exc}"
                 )
 
+            if step.short_circuit:
+                # break the workflow if the step is a short circuit step
+                break
+
     logger.info("Test connection results:")
     logger.info(test_connection_result)
 
     if test_connection_result.failed:
         raise SourceConnectionException(
             f"Some steps failed when testing the connection: [{test_connection_result}]"
         )
@@ -237,23 +236,26 @@
     test_connection_definition: TestConnectionDefinition = metadata.get_by_name(
         entity=TestConnectionDefinition,
         fqn=service_fqn,
     )
 
     if not test_connection_definition:
         raise SourceConnectionException(
-            f"Test connection definition for {service_fqn} not found please validate the token."
+            f"Test connection definition for {service_fqn} not found please review the Server Configuration of the "
+            f"Workflow configuration. Check that the Security Configuration has been set up correctly."
         )
 
     steps = [
         TestConnectionStep(
             name=step.name,
             description=step.description,
             mandatory=step.mandatory,
             function=test_fn[step.name],
+            error_message=step.errorMessage,
+            short_circuit=step.shortCircuit,
         )
         for step in test_connection_definition.steps
     ]
 
     return timeout(timeout_seconds)(_test_connection_steps)(
         metadata, steps, automation_workflow
     )
@@ -270,28 +272,27 @@
 def test_connection_db_common(
     metadata: OpenMetadata,
     engine: Engine,
     service_connection,
     automation_workflow: Optional[AutomationWorkflow] = None,
     queries: dict = None,
     timeout_seconds: int = 3 * 60,
-) -> TestConnectionResult:
-
+) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
 
     Args:
 
     metadata: Metadata Client to interact with the backend APIs
     engine: SqlAlchemy Engine
     service_connection: Service connection object of data source
     automation_workflow: Automation Workflow object expected when
                          test connection is hit via UI/Airflow
-    queries: expected when some queries has to be executed as part of
+    queries: expected when some queries have to be executed as part of
              test connection
     expected format for queries would be <TestConnectionStep>:<Query>
     queries = {
         "GetQueries": "select * from query_log",
     }
     """
 
@@ -330,15 +331,15 @@
     Args:
 
     metadata: Metadata Client to interact with the backend APIs
     engine: SqlAlchemy Engine
     service_connection: Service connection object of data source
     automation_workflow: Automation Workflow object expected when
                          test connection is hit via UI/Airflow
-    queries: expected when some queries has to be executed as part of
+    queries: expected when some queries have to be executed as part of
              test connection
     expected format for queries would be <TestConnectionStep>:<Query>
     queries = {
         "GetQueries": "select * from query_log",
     }
     """
     queries = queries or {}
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/lineage/sql_lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/lineage/sql_lineage.py`

 * *Files 1% similar despite different names*

```diff
@@ -55,16 +55,30 @@
     service_name: str,
     database: Optional[str],
     database_schema: Optional[str],
     table: str,
 ) -> Optional[List[Table]]:
     """
     Method to get table entity from database, database_schema & table name.
-    It uses ES to build the FQN if we miss some info and will run
+
+    It will try to search first in ES and doing an extra call to get Table entities
+    with the needed fields like columns for column lineage.
+
+    If the ES result is empty, it will try by running
     a request against the API to find the Entity.
+
+    Args:
+        metadata: OMeta client
+        service_name: service name
+        database: database name
+        database_schema: schema name
+        table: table name
+
+    Returns:
+        A list of Table entities, otherwise, None
     """
     search_tuple = (service_name, database, database_schema, table)
     if search_tuple in search_cache:
         return search_cache.get(search_tuple)
     try:
         table_entities: Optional[List[Table]] = []
         # search on ES first
@@ -74,29 +88,30 @@
         es_result_entities = metadata.es_search_from_fqn(
             entity_type=Table,
             fqn_search_string=fqn_search_string,
         )
         if es_result_entities:
             table_entities = es_result_entities
         else:
-            # build fqns without searching on ES
+            # build FQNs and search with the API in case ES response is empty
             table_fqns = fqn.build(
                 metadata,
                 entity_type=Table,
                 service_name=service_name,
                 database_name=database,
                 schema_name=database_schema,
                 table_name=table,
                 fetch_multiple_entities=True,
                 skip_es_search=True,
             )
             for table_fqn in table_fqns or []:
                 table_entity: Table = metadata.get_by_name(Table, fqn=table_fqn)
                 if table_entity:
                     table_entities.append(table_entity)
+        # added the search tuple to the cache
         search_cache.put(search_tuple, table_entities)
         return table_entities
     except Exception as exc:
         logger.debug(traceback.format_exc())
         logger.error(
             f"Error searching for table entities for service [{service_name}]: {exc}"
         )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/custom_pydantic.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/custom_pydantic.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/custom_types.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/custom_types.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/delete_entity.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/delete_entity.py`

 * *Files 17% similar despite different names*

```diff
@@ -7,20 +7,24 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Pydantic definition for deleting entites
 """
+import traceback
 from typing import Dict, Iterable, Optional
 
 from pydantic import BaseModel
 
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.utils.logger import ingestion_logger
+
+logger = ingestion_logger()
 
 
 class DeleteEntity(BaseModel):
     """
     Entity Reference of the entity to be deleted
     """
 
@@ -39,14 +43,18 @@
     Method to delete the entities
     :param metadata: OMeta client
     :param entity_type: Pydantic Entity model
     :param entity_source_state: Current state of the service
     :param mark_deleted_entity: Option to mark the entity as deleted or not
     :param params: param to fetch the entity state
     """
-    entity_state = metadata.list_all_entities(entity=entity_type, params=params)
-    for entity in entity_state:
-        if str(entity.fullyQualifiedName.__root__) not in entity_source_state:
-            yield DeleteEntity(
-                entity=entity,
-                mark_deleted_entities=mark_deleted_entity,
-            )
+    try:
+        entity_state = metadata.list_all_entities(entity=entity_type, params=params)
+        for entity in entity_state:
+            if str(entity.fullyQualifiedName.__root__) not in entity_source_state:
+                yield DeleteEntity(
+                    entity=entity,
+                    mark_deleted_entities=mark_deleted_entity,
+                )
+    except Exception as exc:
+        logger.debug(traceback.format_exc())
+        logger.warning(f"Error deleting {entity_type.__class__}: {exc}")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/encoders.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/encoders.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/es_documents.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/es_documents.py`

 * *Files 14% similar despite different names*

```diff
@@ -27,28 +27,17 @@
     EntityReference,
     EntityReferenceList,
 )
 from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.generated.schema.type.usageDetails import UsageDetails
 
 
-class ESEntityReference(BaseModel):
-    """JsonSchema generated pydantic contains many unnecessary fields its not one-to-one representation of JsonSchema
-    Example all the "__root__" fields. This will not index into ES elegantly hence we are creating special class
-    for EntityReference
-    """
-
-    id: str
-    name: str
-    displayName: str
-    description: str = ""
-    type: str
-    fullyQualifiedName: str
-    deleted: bool
-    href: str
+class ESSuggest(BaseModel):
+    input: str
+    weight: int
 
 
 class TableESDocument(BaseModel):
     """ElasticSearch Mapping doc"""
 
     entityType: str = "table"
     id: str
@@ -68,19 +57,19 @@
     location: Optional[EntityReference] = None
     usageSummary: UsageDetails = None
     deleted: bool
     serviceType: str
     tags: List[TagLabel]
     tier: Optional[TagLabel] = None
     followers: List[str]
-    suggest: List[dict]
-    column_suggest: List[dict]
-    database_suggest: List[dict]
-    schema_suggest: List[dict]
-    service_suggest: List[dict]
+    suggest: List[ESSuggest]
+    column_suggest: List[ESSuggest]
+    database_suggest: List[ESSuggest]
+    schema_suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest]
     doc_as_upsert: bool = True
 
 
 class TopicESDocument(BaseModel):
     """Topic ElasticSearch Mapping doc"""
 
     entityType: str = "topic"
@@ -91,22 +80,23 @@
     description: Optional[str] = None
     version: float
     updatedAt: Optional[int]
     updatedBy: Optional[str]
     href: Optional[str]
     deleted: bool
     service: EntityReference
+    serviceType: str
     schemaText: Optional[str] = None
     schemaType: Optional[schema.SchemaType] = None
     cleanupPolicies: List[str] = None
     replicationFactor: Optional[int] = None
     maximumMessageSize: Optional[int] = None
     retentionSize: Optional[int] = None
-    suggest: List[dict]
-    service_suggest: List[dict]
+    suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest]
     tags: List[TagLabel]
     tier: Optional[TagLabel] = None
     owner: EntityReference = None
     followers: List[str]
     doc_as_upsert: bool = True
 
 
@@ -129,17 +119,18 @@
     followers: List[str]
     service: EntityReference
     serviceType: str
     usageSummary: UsageDetails = None
     deleted: bool
     tags: List[TagLabel]
     tier: Optional[TagLabel] = None
-    suggest: List[dict]
-    chart_suggest: List[dict]
-    service_suggest: List[dict]
+    suggest: List[ESSuggest]
+    chart_suggest: List[ESSuggest]
+    data_model_suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest]
     doc_as_upsert: bool = True
 
 
 class PipelineESDocument(BaseModel):
     """ElasticSearch Mapping doc for Pipelines"""
 
     entityType: str = "pipeline"
@@ -157,17 +148,17 @@
     href: Optional[str]
     owner: EntityReference = None
     followers: List[str]
     tags: List[TagLabel]
     tier: Optional[TagLabel] = None
     service: EntityReference
     serviceType: str
-    suggest: List[dict]
-    task_suggest: List[dict]
-    service_suggest: List[dict]
+    suggest: List[ESSuggest]
+    task_suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest]
     doc_as_upsert: bool = True
 
 
 class MlModelESDocument(BaseModel):
     """ElasticSearch Mapping doc for MlModels"""
 
     entityType: str = "mlmodel"
@@ -185,21 +176,81 @@
     target: str
     dashboard: Optional[EntityReference] = None
     mlStore: Optional[MlStore] = None
     server: Optional[str] = None
     usageSummary: UsageDetails = None
     tags: List[TagLabel]
     tier: Optional[TagLabel] = None
-    owner: ESEntityReference = None
+    owner: EntityReference = None
+    followers: List[str]
+    href: Optional[str]
+    deleted: bool
+    suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest] = None
+    service: EntityReference
+    doc_as_upsert: bool = True
+
+
+class ContainerESDocument(BaseModel):
+    """ElasticSearch Mapping doc for Containers"""
+
+    entityType: str = "container"
+    id: str
+    name: str
+    displayName: str
+    fullyQualifiedName: str
+    description: Optional[str] = None
+    version: float
+    updatedAt: Optional[int]
+    updatedBy: Optional[str]
+    tags: List[TagLabel]
+    tier: Optional[TagLabel] = None
+    owner: EntityReference = None
+    followers: List[str]
+    href: Optional[str]
+    deleted: bool
+    suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest] = None
+    service: EntityReference
+    doc_as_upsert: bool = True
+    parent: Optional[dict] = None
+    dataModel: Optional[dict] = None
+    children: Optional[List[dict]] = None
+    prefix: Optional[str] = None
+    numberOfObjects: Optional[int] = None
+    size: Optional[float] = None
+    fileFormats: Optional[List[str]] = None
+
+
+class QueryESDocument(BaseModel):
+    """ElasticSearch Mapping doc for Containers"""
+
+    entityType: str = "query"
+    id: str
+    name: str
+    displayName: str
+    fullyQualifiedName: str
+    description: Optional[str] = None
+    version: float
+    updatedAt: Optional[int]
+    updatedBy: Optional[str]
+    tags: List[TagLabel]
+    tier: Optional[TagLabel] = None
+    owner: EntityReference = None
     followers: List[str]
     href: Optional[str]
     deleted: bool
-    suggest: List[dict]
-    service_suggest: List[dict] = None
+    suggest: List[ESSuggest]
+    service_suggest: List[ESSuggest] = None
     doc_as_upsert: bool = True
+    duration: Optional[float] = None
+    users: Optional[List[dict]] = None
+    votes: Optional[dict] = None
+    query: str
+    queryDate: float
 
 
 class UserESDocument(BaseModel):
     """ElasticSearch Mapping doc for Users"""
 
     entityType: str = "user"
     id: str
@@ -213,15 +264,15 @@
     email: str
     href: Optional[str]
     isAdmin: bool
     teams: EntityReferenceList
     roles: EntityReferenceList
     inheritedRoles: EntityReferenceList
     deleted: bool
-    suggest: List[dict]
+    suggest: List[ESSuggest]
     doc_as_upsert: bool = True
 
 
 class TeamESDocument(BaseModel):
     """ElasticSearch Mapping doc for Teams"""
 
     entityType: str = "team"
@@ -231,15 +282,15 @@
     displayName: str
     description: str
     teamType: str
     version: float
     updatedAt: Optional[int]
     updatedBy: Optional[str]
     href: Optional[str]
-    suggest: List[dict]
+    suggest: List[ESSuggest]
     users: EntityReferenceList
     defaultRoles: EntityReferenceList
     parents: EntityReferenceList
     isJoinable: bool
     deleted: bool
     doc_as_upsert: bool = True
 
@@ -261,15 +312,15 @@
     glossary: EntityReference
     children: Optional[List[EntityReference]]
     relatedTerms: Optional[List[EntityReference]]
     reviewers: Optional[List[EntityReference]]
     usageCount: Optional[int]
     tags: List[TagLabel]
     status: str
-    suggest: List[dict]
+    suggest: List[ESSuggest]
     deleted: bool
     doc_as_upsert: bool = True
 
 
 class TagESDocument(BaseModel):
     """ElasticSearch Mapping doc for Tag"""
 
@@ -278,11 +329,11 @@
     name: str
     fullyQualifiedName: str
     description: str
     version: float
     updatedAt: Optional[int]
     updatedBy: Optional[str]
     href: Optional[str]
-    suggest: List[dict]
+    suggest: List[ESSuggest]
     deleted: bool
     deprecated: bool
     doc_as_upsert: bool = True
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/ometa_classification.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/ometa_classification.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/ometa_topic_data.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/ometa_topic_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/pipeline_status.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/pipeline_status.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/profile_data.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/profile_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/table_metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/table_metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/tests_data.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/tests_data.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/topology.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/topology.py`

 * *Files 0% similar despite different names*

```diff
@@ -35,15 +35,15 @@
     context: Optional[str] = None  # context key storing stage state, if needed
     ack_sink: bool = True  # Validate that the request is present in OM and update the context with the results
     nullable: bool = False  # The yielded value can be null
     must_return: bool = False  # The sink MUST return a value back after ack. Useful to validate services are correct.
     cache_all: bool = (
         False  # If we need to cache all values being yielded in the context
     )
-    clear_cache: bool = False  # If we need to clean cache values  in the context for each produced element
+    clear_cache: bool = False  # If we need to clean cache values in the context for each produced element
     overwrite: bool = True  # If we want to overwrite existing data from OM
     consumer: Optional[
         List[str]
     ] = None  # keys in the source context to fetch state from the parent's context
 
 
 class TopologyNode(BaseModel):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/models/user.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/models/user.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/auth_provider.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/auth_provider.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/client.py`

 * *Files 3% similar despite different names*

```diff
@@ -225,26 +225,36 @@
                 raise RetryException() from http_error
             if "code" in resp.text:
                 error = resp.json()
                 if "code" in error:
                     raise APIError(error, http_error) from http_error
             else:
                 raise
+        except requests.ConnectionError as conn:
+            # Trying to solve https://github.com/psf/requests/issues/4664
+            try:
+                return self._session.request(method, url, **opts).json()
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Unexpected error while retrying after a connection error - {exc}"
+                )
+                raise conn
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Unexpected error calling [{url}] with method [{method}]: {exc}"
             )
         if resp.text != "":
             try:
                 return resp.json()
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(
-                    f"Unexpected error while returing response {resp} in json format - {exc}"
+                    f"Unexpected error while returning response {resp} in json format - {exc}"
                 )
         return None
 
     def get(self, path, data=None):
         """
         GET method
 
@@ -323,14 +333,14 @@
         """
         self._session.close()
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.close()
 
     def _mask_authorization_headers(self, opts: Dict[str, Any]) -> Dict[str, Any]:
-        if opts and opts["headers"]:
+        if opts and opts.get("headers"):
             if self.config.auth_header and opts["headers"][self.config.auth_header]:
                 masked_opts = deepcopy(opts)
                 if self.config.auth_header and opts["headers"][self.config.auth_header]:
                     masked_opts["headers"][self.config.auth_header] = "********"
                 return masked_opts
         return opts
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/client_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/client_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/credentials.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/credentials.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/dashboard_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/data_insight_mixin.py`

 * *Files 12% similar despite different names*

```diff
@@ -40,15 +40,15 @@
         """Given a ReportData object convert it to a json payload
         and send a POST request to the report data endpoint
 
         Args:
             record (ReportData): report data
         """
 
-        resp = self.client.post("/analytic/reportData", record.json())
+        resp = self.client.post("/analytics/dataInsights/data", record.json())
 
         return resp
 
     def add_kpi_result(self, fqn: str, record: KpiResult) -> KpiResult:
         """Given a ReportData object convert it to a json payload
         and send a POST request to the report data endpoint
 
@@ -62,15 +62,15 @@
 
     def add_web_analytic_events(
         self,
         event_data: WebAnalyticEventData,
     ) -> List[WebAnalyticEventData]:
         """Get web analytic event"""
 
-        resp = self.client.put("/analytics/webAnalyticEvent/collect", event_data.json())
+        resp = self.client.put("/analytics/web/events/collect", event_data.json())
 
         return resp
 
     def get_data_insight_report_data(
         self, start_ts: int, end_ts: int, report_data_type: str
     ) -> dict[str, list[ReportData]]:
         """Return dict with a list of report data given a start and end date
@@ -81,15 +81,15 @@
             report_data_type (ReportDataType): report data type
 
         Returns:
             List[ReportData]:
         """
 
         resp = self.client.get(
-            "/analytic/reportData",
+            "/analytics/dataInsights/data",
             {"startTs": start_ts, "endTs": end_ts, "reportDataType": report_data_type},
         )
 
         return resp
 
     def get_aggregated_data_insight_results(
         self,
@@ -119,15 +119,15 @@
             "dataReportIndex": data_report_index,
         }
 
         if params:
             request_params = {**request_params, **params}
 
         resp = self.client.get(
-            "/dataInsight/aggregate",
+            "/analytics/dataInsights/charts/aggregate",
             request_params,
         )
 
         return DataInsightChartResult.parse_obj(resp)
 
     def get_kpi_result(self, fqn: str, start_ts, end_ts) -> list[KpiResult]:
         """Given FQN return KPI results
@@ -155,24 +155,22 @@
     ) -> List[WebAnalyticEventData]:
         """Get web analytic event"""
 
         event_type_value = event_type.value
 
         params = {"eventType": event_type_value, "startTs": start_ts, "endTs": end_ts}
 
-        resp = self.client.get("/analytics/webAnalyticEvent/collect", params)
+        resp = self.client.get("/analytics/web/events/collect", params)
 
         return [WebAnalyticEventData(**data) for data in resp["data"]]
 
     def delete_web_analytic_event_before_ts_exclusive(
         self, event_type: WebAnalyticEventType, tmsp: int
     ):
         """Deletes web analytics events before a timestamp
 
         Args:
             event_type (WebAnalyticEventData): web analytic event type
             tmsp (int): timestamp
         """
         event_type_value = event_type.value
-        self.client.delete(
-            f"/analytics/webAnalyticEvent/{event_type_value}/{tmsp}/collect"
-        )
+        self.client.delete(f"/analytics/web/events/{event_type_value}/{tmsp}/collect")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/es_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/es_mixin.py`

 * *Files 3% similar despite different names*

```diff
@@ -43,15 +43,16 @@
     def _search_es_entity(
         self,
         entity_type: Type[T],
         query_string: str,
         fields: Optional[List[str]] = None,
     ) -> Optional[List[T]]:
         """
-        Run the ES query and return a list of entities that match
+        Run the ES query and return a list of entities that match. It does an extra query to the OM API with the
+        requested fields per each entity found in ES.
         :param entity_type: Entity to look for
         :param query_string: Query to run
         :return: List of Entities or None
         """
         response = self.client.get(query_string)
 
         if response:
@@ -77,14 +78,15 @@
         """
         Given a service_name and some filters, search for entities using ES
 
         :param entity_type: Entity to look for
         :param fqn_search_string: string used to search by FQN. E.g., service.*.schema.table
         :param from_count: Records to expect
         :param size: Number of records
+        :param fields: Fields to be returned
         :return: List of entities
         """
         query_string = self.fqdn_search.format(
             fqn=fqn_search_string,
             from_=from_count,
             size=size,
             index=ES_INDEX_MAP[entity_type.__name__],  # Fail if not exists
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/lineage_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/lineage_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/patch_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/patch_mixin.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,24 +11,32 @@
 """
 Mixin class containing PATCH specific methods
 
 To be used by OpenMetadata class
 """
 import json
 import traceback
-from typing import Dict, Generic, List, Optional, Type, TypeVar, Union
+from typing import Dict, List, Optional, Type, TypeVar, Union
 
 from pydantic import BaseModel
 
+from metadata.generated.schema.entity.automations.workflow import (
+    Workflow as AutomationWorkflow,
+)
+from metadata.generated.schema.entity.automations.workflow import WorkflowStatus
 from metadata.generated.schema.entity.data.table import Table, TableConstraint
+from metadata.generated.schema.entity.services.connections.testConnectionResult import (
+    TestConnectionResult,
+)
 from metadata.generated.schema.type import basic
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.generated.schema.type.tagLabel import LabelType, State, TagSource
 from metadata.ingestion.ometa.client import REST
-from metadata.ingestion.ometa.patch import (
+from metadata.ingestion.ometa.mixins.patch_mixin_utils import (
+    OMetaPatchMixinBase,
     PatchField,
     PatchOperation,
     PatchPath,
     PatchValue,
 )
 from metadata.ingestion.ometa.utils import model_str
 from metadata.utils.helpers import find_column_in_table_with_index
@@ -37,50 +45,23 @@
 logger = ometa_logger()
 
 T = TypeVar("T", bound=BaseModel)
 
 OWNER_TYPES: List[str] = ["user", "team"]
 
 
-class OMetaPatchMixin(Generic[T]):
+class OMetaPatchMixin(OMetaPatchMixinBase):
     """
     OpenMetadata API methods related to Tables.
 
     To be inherited by OpenMetadata
     """
 
     client: REST
 
-    def _fetch_entity_if_exists(
-        self, entity: Type[T], entity_id: Union[str, basic.Uuid]
-    ) -> Optional[T]:
-        """
-        Validates if we can update a description or not. Will return
-        the instance if it can be updated. None otherwise.
-
-        Args
-            entity (T): Entity Type
-            entity_id: ID
-            description: new description to add
-            force: if True, we will patch any existing description. Otherwise, we will maintain
-                the existing data.
-        Returns
-            instance to update
-        """
-
-        instance = self.get_by_id(entity=entity, entity_id=entity_id, fields=["*"])
-
-        if not instance:
-            logger.warning(
-                f"Cannot find an instance of '{entity.__class__.__name__}' with id [{str(entity_id)}]."
-            )
-            return None
-
-        return instance
-
     def patch_description(
         self,
         entity: Type[T],
         entity_id: Union[str, basic.Uuid],
         description: str,
         force: bool = False,
     ) -> Optional[T]:
@@ -484,7 +465,44 @@
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Error trying to PATCH description for {entity.__class__.__name__} [{entity_id}]: {exc}"
             )
 
         return None
+
+    def patch_automation_workflow_response(
+        self,
+        automation_workflow: AutomationWorkflow,
+        test_connection_result: TestConnectionResult,
+        workflow_status: WorkflowStatus,
+    ) -> None:
+        """
+        Given an AutomationWorkflow, JSON PATCH the status and response.
+        """
+        result_data: Dict = {
+            PatchField.PATH: PatchPath.RESPONSE,
+            PatchField.VALUE: test_connection_result.dict(),
+            PatchField.OPERATION: PatchOperation.ADD,
+        }
+
+        # for deserializing into json convert enum object to string
+        result_data[PatchField.VALUE]["status"] = result_data[PatchField.VALUE][
+            "status"
+        ].value
+
+        status_data: Dict = {
+            PatchField.PATH: PatchPath.STATUS,
+            PatchField.OPERATION: PatchOperation.ADD,
+            PatchField.VALUE: workflow_status.value,
+        }
+
+        try:
+            self.client.patch(
+                path=f"{self.get_suffix(AutomationWorkflow)}/{model_str(automation_workflow.id)}",
+                data=json.dumps([result_data, status_data]),
+            )
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(
+                f"Error trying to PATCH status for automation workflow [{model_str(automation_workflow)}]: {exc}"
+            )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/pipeline_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/query_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/query_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/role_policy_mixin.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,73 +11,43 @@
 """
 Mixin class containing Role and Policy specific methods
 
 To be used by OpenMetadata class
 """
 import json
 import traceback
-from typing import Dict, Generic, List, Optional, Type, TypeVar, Union
-
-from pydantic import BaseModel
+from typing import Dict, List, Optional, Union
 
 from metadata.generated.schema.entity.policies.accessControl.rule import Rule
 from metadata.generated.schema.entity.policies.policy import Policy
 from metadata.generated.schema.entity.teams.role import Role
 from metadata.generated.schema.type import basic
 from metadata.ingestion.ometa.client import REST
-from metadata.ingestion.ometa.patch import (
+from metadata.ingestion.ometa.mixins.patch_mixin_utils import (
+    OMetaPatchMixinBase,
     PatchField,
     PatchOperation,
     PatchPath,
     PatchValue,
 )
 from metadata.ingestion.ometa.utils import model_str
 from metadata.utils.logger import ometa_logger
 
 logger = ometa_logger()
 
-T = TypeVar("T", bound=BaseModel)
-
 
-class OMetaRolePolicyMixin(Generic[T]):
+class OMetaRolePolicyMixin(OMetaPatchMixinBase):
     """
     OpenMetadata API methods related to Roles and Policies.
 
     To be inherited by OpenMetadata
     """
 
     client: REST
 
-    def _fetch_entity_if_exists(
-        self, entity: Type[T], entity_id: Union[str, basic.Uuid]
-    ) -> Optional[T]:
-        """
-        Validates if we can update a description or not. Will return
-        the instance if it can be updated. None otherwise.
-
-        Args
-            entity (T): Entity Type
-            entity_id: ID
-            description: new description to add
-            force: if True, we will patch any existing description. Otherwise, we will maintain
-                the existing data.
-        Returns
-            instance to update
-        """
-
-        instance = self.get_by_id(entity=entity, entity_id=entity_id, fields=["*"])
-
-        if not instance:
-            logger.warning(
-                f"Cannot find an instance of '{entity.__class__.__name__}' with id [{str(entity_id)}]."
-            )
-            return None
-
-        return instance
-
     @staticmethod
     def _get_rule_merge_patches(
         previous: List,
         current: List,
         rule_index: int,
         path: str,
         is_enum: bool,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/server_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/server_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/service_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/service_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/table_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/table_mixin.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,25 +9,23 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Mixin class containing Table specific methods
 
 To be used by OpenMetadata class
 """
-import json
 import traceback
 from typing import List, Optional, Type, TypeVar
 
 from pydantic import BaseModel
 from requests.utils import quote
 
 from metadata.generated.schema.api.data.createTableProfile import (
     CreateTableProfileRequest,
 )
-from metadata.generated.schema.entity.data.location import Location
 from metadata.generated.schema.entity.data.table import (
     ColumnProfile,
     DataModel,
     Table,
     TableData,
     TableJoins,
     TableProfile,
@@ -35,15 +33,14 @@
 )
 from metadata.generated.schema.type.basic import FullyQualifiedEntityName, Uuid
 from metadata.generated.schema.type.usageRequest import UsageRequest
 from metadata.ingestion.ometa.client import REST
 from metadata.ingestion.ometa.models import EntityList
 from metadata.ingestion.ometa.utils import model_str
 from metadata.utils.logger import ometa_logger
-from metadata.utils.uuid_encoder import UUIDEncoder
 
 logger = ometa_logger()
 
 LRU_CACHE_SIZE = 4096
 T = TypeVar("T", bound=BaseModel)
 
 
@@ -52,26 +49,14 @@
     OpenMetadata API methods related to Tables.
 
     To be inherited by OpenMetadata
     """
 
     client: REST
 
-    def add_location(self, table: Table, location: Location) -> None:
-        """
-        PUT location for a table
-
-        :param table: Table Entity to update
-        :param location: Location Entity to add
-        """
-        self.client.put(
-            f"{self.get_suffix(Table)}/{table.id.__root__}/location",
-            data=json.dumps(location.id.__root__, cls=UUIDEncoder),
-        )
-
     def ingest_table_sample_data(
         self, table: Table, sample_data: TableData
     ) -> Optional[TableData]:
         """
         PUT sample data for a table
 
         :param table: Table Entity to update
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/tests_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/tests_mixin.py`

 * *Files 1% similar despite different names*

```diff
@@ -213,14 +213,14 @@
         # timestamp should be changed to milliseconds in https://github.com/open-metadata/OpenMetadata/issues/8930
         params = {
             "startTs": start_ts // 1000,
             "endTs": end_ts // 1000,
         }
 
         resp = self.client.get(
-            f"/testCase/{test_case_fqn}/testCaseResult",
+            f"/dataQuality/testCases/{test_case_fqn}/testCaseResult",
             params,
         )
 
         if resp:
             return [TestCaseResult.parse_obj(entity) for entity in resp["data"]]
         return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/topic_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/topic_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/user_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/user_mixin.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,14 @@
 
         Args:
             email: user email to search
             from_count: records to expect
             size: number of records
         """
         if email:
-
             query_string = self.email_search.format(
                 email=email, from_=from_count, size=size
             )
 
             try:
                 entity_list = self._search_es_entity(
                     entity_type=User, query_string=query_string, fields=fields
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/mixins/version_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/version_mixin.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/ometa_api.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/ometa_api.py`

 * *Files 1% similar despite different names*

```diff
@@ -40,15 +40,14 @@
 from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.data.dashboard import Dashboard
 from metadata.generated.schema.entity.data.dashboardDataModel import DashboardDataModel
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.glossary import Glossary
 from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
-from metadata.generated.schema.entity.data.location import Location
 from metadata.generated.schema.entity.data.metrics import Metrics
 from metadata.generated.schema.entity.data.mlmodel import MlModel
 from metadata.generated.schema.entity.data.pipeline import Pipeline
 from metadata.generated.schema.entity.data.query import Query
 from metadata.generated.schema.entity.data.report import Report
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.data.topic import Topic
@@ -63,17 +62,14 @@
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.ingestionPipelines.ingestionPipeline import (
     IngestionPipeline,
 )
 from metadata.generated.schema.entity.services.messagingService import MessagingService
 from metadata.generated.schema.entity.services.metadataService import MetadataService
 from metadata.generated.schema.entity.services.mlmodelService import MlModelService
-from metadata.generated.schema.entity.services.objectstoreService import (
-    ObjectStoreService,
-)
 from metadata.generated.schema.entity.services.pipelineService import PipelineService
 from metadata.generated.schema.entity.services.storageService import StorageService
 from metadata.generated.schema.entity.teams.role import Role
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import AuthenticationMechanism, User
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testDefinition import TestDefinition
@@ -280,19 +276,14 @@
 
         if issubclass(
             entity, get_args(Union[Pipeline, self.get_create_entity_type(Pipeline)])
         ):
             return "/pipelines"
 
         if issubclass(
-            entity, get_args(Union[Location, self.get_create_entity_type(Location)])
-        ):
-            return "/locations"
-
-        if issubclass(
             entity, get_args(Union[Policy, self.get_create_entity_type(Policy)])
         ):
             return "/policies"
 
         if issubclass(
             entity, get_args(Union[Table, self.get_create_entity_type(Table)])
         ):
@@ -366,15 +357,15 @@
             entity, get_args(Union[Container, self.get_create_entity_type(Container)])
         ):
             return "/containers"
 
         if issubclass(
             entity, get_args(Union[Workflow, self.get_create_entity_type(Workflow)])
         ):
-            return "/automations/workflow"
+            return "/automations/workflows"
 
         # Services Schemas
         if issubclass(
             entity,
             get_args(
                 Union[DatabaseService, self.get_create_entity_type(DatabaseService)]
             ),
@@ -428,58 +419,56 @@
             ),
         ):
             return "/services/metadataServices"
 
         if issubclass(
             entity,
             get_args(
-                Union[
-                    ObjectStoreService, self.get_create_entity_type(ObjectStoreService)
-                ]
+                Union[StorageService, self.get_create_entity_type(StorageService)]
             ),
         ):
-            return "/services/objectStoreServices"
+            return "/services/storageServices"
 
         if issubclass(
             entity,
             IngestionPipeline,
         ):
             return "/services/ingestionPipelines"
 
         if issubclass(
             entity,
             TestConnectionDefinition,
         ):
-            return "/services/testConnectionDefinition"
+            return "/services/testConnectionDefinitions"
 
         if issubclass(
             entity,
             get_args(
                 Union[TestDefinition, self.get_create_entity_type(TestDefinition)]
             ),
         ):
-            return "/testDefinition"
+            return "/dataQuality/testDefinitions"
 
         if issubclass(
             entity,
             get_args(Union[TestSuite, self.get_create_entity_type(TestSuite)]),
         ):
-            return "/testSuite"
+            return "/dataQuality/testSuites"
 
         if issubclass(
             entity,
             get_args(Union[TestCase, self.get_create_entity_type(TestCase)]),
         ):
-            return "/testCase"
+            return "/dataQuality/testCases"
 
         if issubclass(entity, WebAnalyticEventData):
-            return "/analytics/webAnalyticEvent/collect"
+            return "/analytics/web/events/collect"
 
         if issubclass(entity, DataInsightChart):
-            return "/dataInsight"
+            return "/analytics/dataInsights/charts"
 
         if issubclass(
             entity,
             Kpi,
         ):
             return "/kpi"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/provider_registry.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/provider_registry.py`

 * *Files 12% similar despite different names*

```diff
@@ -40,14 +40,26 @@
     GoogleAuthenticationProvider,
     NoOpAuthenticationProvider,
     OktaAuthenticationProvider,
     OpenMetadataAuthenticationProvider,
     OpenMetadataJWTClientConfig,
 )
 from metadata.utils.dispatch import enum_register
+from metadata.utils.logger import ometa_logger
+
+logger = ometa_logger()
+
+
+def warn_auth_deprecation(auth_provider: AuthProvider) -> None:
+    logger.warning(
+        "Please, configure the ingestion-bot with the 'OpenMetadata JWT' configuration.\n"
+        f"The '{auth_provider.value}' configuration is deprecated and will be removed in future releases.\n"
+        f"Visit https://docs.open-metadata.org/deployment/security/enable-jwt-tokens to learn how to "
+        f"configure the 'OpenMetadata JWT'."
+    )
 
 
 class InvalidAuthProviderException(Exception):
     """
     Raised when we cannot find a valid auth provider
     in the registry
     """
@@ -59,34 +71,39 @@
 @auth_provider_registry.add(AuthProvider.no_auth.value)
 def no_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
     return NoOpAuthenticationProvider.create(config)
 
 
 @auth_provider_registry.add(AuthProvider.google.value)
 def google_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
+    warn_auth_deprecation(config.authProvider)
     return GoogleAuthenticationProvider.create(config)
 
 
 @auth_provider_registry.add(AuthProvider.okta.value)
 def okta_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
+    warn_auth_deprecation(config.authProvider)
     return OktaAuthenticationProvider.create(config)
 
 
 @auth_provider_registry.add(AuthProvider.auth0.value)
 def auth0_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
+    warn_auth_deprecation(config.authProvider)
     return Auth0AuthenticationProvider.create(config)
 
 
 @auth_provider_registry.add(AuthProvider.azure.value)
 def azure_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
+    warn_auth_deprecation(config.authProvider)
     return AzureAuthenticationProvider.create(config)
 
 
 @auth_provider_registry.add(AuthProvider.custom_oidc.value)
 def custom_oidc_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
+    warn_auth_deprecation(config.authProvider)
     return CustomOIDCAuthenticationProvider.create(config)
 
 
 @auth_provider_registry.add(AuthProvider.openmetadata.value)
 def om_auth_init(config: OpenMetadataConnection) -> AuthenticationProvider:
     return OpenMetadataAuthenticationProvider.create(config)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/ometa/utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/processor/pii.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/processor/pii.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/processor/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/processor/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch.py`

 * *Files 25% similar despite different names*

```diff
@@ -14,57 +14,66 @@
 
 We disable unexpected-keyword-arg as we get a false positive for request_timeout in put_mappings
 """
 
 import json
 import ssl
 import traceback
-from typing import List, Optional
+from functools import singledispatch
+from typing import Any, List, Optional
 
 import boto3
 from elasticsearch import Elasticsearch, RequestsHttpConnection
 from elasticsearch.connection import create_ssl_context
 from requests_aws4auth import AWS4Auth
 
 from metadata.config.common import ConfigModel
 from metadata.data_insight.helper.data_insight_es_index import DataInsightEsIndex
 from metadata.generated.schema.analytics.reportData import ReportData
 from metadata.generated.schema.entity.classification.classification import (
     Classification,
 )
 from metadata.generated.schema.entity.classification.tag import Tag
+from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.data.dashboard import Dashboard
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
 from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
 from metadata.generated.schema.entity.data.mlmodel import MlModel
 from metadata.generated.schema.entity.data.pipeline import Pipeline
+from metadata.generated.schema.entity.data.query import Query
 from metadata.generated.schema.entity.data.table import Column, Table
 from metadata.generated.schema.entity.data.topic import Topic
+from metadata.generated.schema.entity.policies.policy import Policy
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
-from metadata.generated.schema.type.entityReference import EntityReference
+from metadata.generated.schema.type.entityReference import EntityReferenceList
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.sink import Sink
 from metadata.ingestion.models.es_documents import (
+    ContainerESDocument,
     DashboardESDocument,
-    ESEntityReference,
+    ESSuggest,
     GlossaryTermESDocument,
     MlModelESDocument,
     PipelineESDocument,
+    QueryESDocument,
     TableESDocument,
     TagESDocument,
     TeamESDocument,
     TopicESDocument,
     UserESDocument,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.sink.elasticsearch_mapping.container_search_index_mapping import (
+    CONTAINER_ELASTICSEARCH_INDEX_MAPPING,
+)
 from metadata.ingestion.sink.elasticsearch_mapping.dashboard_search_index_mapping import (
     DASHBOARD_ELASTICSEARCH_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.entity_report_data_index_mapping import (
     ENTITY_REPORT_DATA_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.glossary_term_search_index_mapping import (
@@ -72,14 +81,17 @@
 )
 from metadata.ingestion.sink.elasticsearch_mapping.mlmodel_search_index_mapping import (
     MLMODEL_ELASTICSEARCH_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.pipeline_search_index_mapping import (
     PIPELINE_ELASTICSEARCH_INDEX_MAPPING,
 )
+from metadata.ingestion.sink.elasticsearch_mapping.query_search_index_mapping import (
+    QUERY_ELASTICSEARCH_INDEX_MAPPING,
+)
 from metadata.ingestion.sink.elasticsearch_mapping.table_search_index_mapping import (
     TABLE_ELASTICSEARCH_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.tag_search_index_mapping import (
     TAG_ELASTICSEARCH_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.team_search_index_mapping import (
@@ -93,32 +105,20 @@
 )
 from metadata.ingestion.sink.elasticsearch_mapping.web_analytic_entity_view_report_data_index_mapping import (
     WEB_ANALYTIC_ENTITY_VIEW_REPORT_DATA_INDEX_MAPPING,
 )
 from metadata.ingestion.sink.elasticsearch_mapping.web_analytic_user_activity_report_data_index_mapping import (
     WEB_ANALYTIC_USER_ACTIVITY_REPORT_DATA_INDEX_MAPPING,
 )
+from metadata.utils.elasticsearch import ES_INDEX_MAP
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-def get_es_entity_ref(entity_ref: EntityReference) -> ESEntityReference:
-    return ESEntityReference(
-        id=str(entity_ref.id.__root__),
-        name=entity_ref.name,
-        displayName=entity_ref.displayName if entity_ref.displayName else "",
-        description=entity_ref.description.__root__ if entity_ref.description else "",
-        type=entity_ref.type,
-        fullyQualifiedName=entity_ref.fullyQualifiedName,
-        deleted=entity_ref.deleted,
-        href=entity_ref.href.__root__,
-    )
-
-
 class ElasticSearchConfig(ConfigModel):
     """
     Representation of the Elasticsearch connection
     to be used as a Sink.
     """
 
     es_host: str
@@ -130,33 +130,20 @@
     index_dashboards: Optional[bool] = True
     index_pipelines: Optional[bool] = True
     index_users: Optional[bool] = True
     index_teams: Optional[bool] = True
     index_mlmodels: Optional[bool] = True
     index_glossary_terms: Optional[bool] = True
     index_tags: Optional[bool] = True
+    index_containers: Optional[bool] = True
+    index_queries: Optional[bool] = True
     index_entity_report_data: Optional[bool] = True
     index_web_analytic_user_activity_report_data: Optional[bool] = True
     index_web_analytic_entity_view_report_data: Optional[bool] = True
-    table_index_name: str = "table_search_index"
-    topic_index_name: str = "topic_search_index"
-    dashboard_index_name: str = "dashboard_search_index"
-    pipeline_index_name: str = "pipeline_search_index"
-    user_index_name: str = "user_search_index"
-    team_index_name: str = "team_search_index"
-    glossary_term_index_name: str = "glossary_search_index"
-    mlmodel_index_name: str = "mlmodel_search_index"
-    tag_index_name: str = "tag_search_index"
-    entity_report_data_index_name: str = "entity_report_data_index"
-    web_analytic_user_activity_report_data_index_name: str = (
-        "web_analytic_user_activity_report_data_index"
-    )
-    web_analytic_entity_view_report_data_name: str = (
-        "web_analytic_entity_view_report_data_index"
-    )
+
     scheme: str = "http"
     use_ssl: bool = False
     verify_certs: bool = False
     timeout: int = 30
     ca_certs: Optional[str] = None
     recreate_indexes: Optional[bool] = False
     use_AWS_credentials: Optional[bool] = False
@@ -172,16 +159,15 @@
     DEFAULT_ELASTICSEARCH_INDEX_MAPPING = TABLE_ELASTICSEARCH_INDEX_MAPPING
 
     @classmethod
     def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
         config = ElasticSearchConfig.parse_obj(config_dict)
         return cls(config, metadata_config)
 
-    # to be fix in https://github.com/open-metadata/OpenMetadata/issues/8352
-    # pylint: disable=too-many-branches
+    # pylint: disable=too-many-branches, too-many-statements
     def __init__(
         self,
         config: ElasticSearchConfig,
         metadata_config: OpenMetadataConnection,
     ) -> None:
         super().__init__()
         self.config = config
@@ -219,78 +205,97 @@
                 else region_from_boto3,
                 service="es",
                 refreshable_credentials=credentials,
             )
             self.elasticsearch_client.http_auth = http_auth
             self.elasticsearch_client.connection_class = RequestsHttpConnection
 
+        # We'll be able to clean all this up after https://github.com/open-metadata/OpenMetadata/issues/9185
         if self.config.index_tables:
             self._check_or_create_index(
-                self.config.table_index_name, TABLE_ELASTICSEARCH_INDEX_MAPPING
+                ES_INDEX_MAP[Table.__name__], TABLE_ELASTICSEARCH_INDEX_MAPPING
             )
 
         if self.config.index_topics:
             self._check_or_create_index(
-                self.config.topic_index_name, TOPIC_ELASTICSEARCH_INDEX_MAPPING
+                ES_INDEX_MAP[Topic.__name__], TOPIC_ELASTICSEARCH_INDEX_MAPPING
             )
         if self.config.index_dashboards:
             self._check_or_create_index(
-                self.config.dashboard_index_name, DASHBOARD_ELASTICSEARCH_INDEX_MAPPING
+                ES_INDEX_MAP[Dashboard.__name__], DASHBOARD_ELASTICSEARCH_INDEX_MAPPING
             )
         if self.config.index_pipelines:
             self._check_or_create_index(
-                self.config.pipeline_index_name, PIPELINE_ELASTICSEARCH_INDEX_MAPPING
+                ES_INDEX_MAP[Pipeline.__name__], PIPELINE_ELASTICSEARCH_INDEX_MAPPING
             )
 
         if self.config.index_users:
             self._check_or_create_index(
-                self.config.user_index_name, USER_ELASTICSEARCH_INDEX_MAPPING
+                ES_INDEX_MAP[User.__name__], USER_ELASTICSEARCH_INDEX_MAPPING
             )
 
         if self.config.index_teams:
             self._check_or_create_index(
-                self.config.team_index_name, TEAM_ELASTICSEARCH_INDEX_MAPPING
+                ES_INDEX_MAP[Team.__name__], TEAM_ELASTICSEARCH_INDEX_MAPPING
             )
 
         if self.config.index_glossary_terms:
             self._check_or_create_index(
-                self.config.glossary_term_index_name,
+                ES_INDEX_MAP[GlossaryTerm.__name__],
                 GLOSSARY_TERM_ELASTICSEARCH_INDEX_MAPPING,
             )
 
         if self.config.index_mlmodels:
             self._check_or_create_index(
-                self.config.mlmodel_index_name,
+                ES_INDEX_MAP[MlModel.__name__],
                 MLMODEL_ELASTICSEARCH_INDEX_MAPPING,
             )
 
         if self.config.index_tags:
             self._check_or_create_index(
-                self.config.tag_index_name,
+                ES_INDEX_MAP[Tag.__name__],
                 TAG_ELASTICSEARCH_INDEX_MAPPING,
             )
 
         if self.config.index_entity_report_data:
             self._check_or_create_index(
-                self.config.entity_report_data_index_name,
+                ES_INDEX_MAP[ReportData.__name__],
                 ENTITY_REPORT_DATA_INDEX_MAPPING,
             )
 
         if self.config.index_web_analytic_user_activity_report_data:
             self._check_or_create_index(
-                self.config.web_analytic_user_activity_report_data_index_name,
+                ES_INDEX_MAP["web_analytic_user_activity_report"],
                 WEB_ANALYTIC_USER_ACTIVITY_REPORT_DATA_INDEX_MAPPING,
             )
 
         if self.config.index_web_analytic_entity_view_report_data:
             self._check_or_create_index(
-                self.config.web_analytic_entity_view_report_data_name,
+                ES_INDEX_MAP["web_analytic_entity_view_report"],
                 WEB_ANALYTIC_ENTITY_VIEW_REPORT_DATA_INDEX_MAPPING,
             )
 
+        if self.config.index_containers:
+            self._check_or_create_index(
+                ES_INDEX_MAP[Container.__name__],
+                CONTAINER_ELASTICSEARCH_INDEX_MAPPING,
+            )
+
+        if self.config.index_queries:
+            self._check_or_create_index(
+                ES_INDEX_MAP[Query.__name__],
+                QUERY_ELASTICSEARCH_INDEX_MAPPING,
+            )
+
+        # Prepare write record dispatching
+        self._write_record = singledispatch(self._write_record)
+        self._write_record.register(Classification, self._write_classification)
+        self._write_record.register(ReportData, self._write_report_data)
+        self._write_record.register(Policy, self._write_policy)
+
         super().__init__()
 
     def _check_or_create_index(self, index_name: str, es_mapping: str):
         """
         Retrieve all indices that currently have {elasticsearch_alias} alias
         :return: list of elasticsearch_mapping indices
         """
@@ -327,109 +332,67 @@
                     index=index_name, request_timeout=self.config.timeout
                 )
             self.elasticsearch_client.indices.create(
                 index=index_name, body=es_mapping, request_timeout=self.config.timeout
             )
 
     def write_record(self, record: Entity) -> None:
-        try:
-            if isinstance(record, Table):
-                table_doc = self._create_table_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.table_index_name,
-                    id=str(table_doc.id),
-                    body=table_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-            if isinstance(record, Topic):
-                topic_doc = self._create_topic_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.topic_index_name,
-                    id=str(topic_doc.id),
-                    body=topic_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-            if isinstance(record, Dashboard):
-                dashboard_doc = self._create_dashboard_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.dashboard_index_name,
-                    id=str(dashboard_doc.id),
-                    body=dashboard_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-            if isinstance(record, Pipeline):
-                pipeline_doc = self._create_pipeline_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.pipeline_index_name,
-                    id=str(pipeline_doc.id),
-                    body=pipeline_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-
-            if isinstance(record, User):
-                user_doc = self._create_user_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.user_index_name,
-                    id=str(user_doc.id),
-                    body=user_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-
-            if isinstance(record, Team):
-                team_doc = self._create_team_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.team_index_name,
-                    id=str(team_doc.id),
-                    body=team_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-
-            if isinstance(record, GlossaryTerm):
-                glossary_term_doc = self._create_glossary_term_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.glossary_term_index_name,
-                    id=str(glossary_term_doc.id),
-                    body=glossary_term_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
-
-            if isinstance(record, MlModel):
-                ml_model_doc = self._create_ml_model_es_doc(record)
-                self.elasticsearch_client.index(
-                    index=self.config.mlmodel_index_name,
-                    id=str(ml_model_doc.id),
-                    body=ml_model_doc.json(),
-                    request_timeout=self.config.timeout,
-                )
+        """
+        Default implementation for the single dispatch
+        """
 
-            if isinstance(record, Classification):
-                tag_docs = self._create_tag_es_doc(record)
-                for tag_doc in tag_docs:
-                    self.elasticsearch_client.index(
-                        index=self.config.tag_index_name,
-                        id=str(tag_doc.id),
-                        body=tag_doc.json(),
-                        request_timeout=self.config.timeout,
-                    )
-                    self.status.records_written(tag_doc.name)
-
-            if isinstance(record, ReportData):
-                self.elasticsearch_client.index(
-                    index=DataInsightEsIndex[record.data.__class__.__name__].value,
-                    id=record.id,
-                    body=record.json(),
-                    request_timeout=self.config.timeout,
-                )
-                self.status.records_written(
-                    f"Event written for record type {record.data.__class__.__name__}"
-                )
+        try:
+            self._write_record(record)
+            self.status.records_written(
+                record.name.__root__
+                if hasattr(record, "name")
+                else type(record).__name__
+            )
 
         except Exception as exc:
             logger.debug(traceback.format_exc())
-            logger.error(f"Failed to index entity {record}: {exc}")
+            logger.error(f"Failed to index due to {exc} - Entity: {record}")
+
+    def _write_record(self, record: Entity) -> None:  # pylint: disable=method-hidden
+        """
+        We disable method-hidden as we are just manually setting the singledispatch
+
+        Default implementation for the single dispatch
+        """
+        es_record = create_record_document(record, self.metadata)
+        self.elasticsearch_client.index(
+            index=ES_INDEX_MAP[type(record).__name__],
+            id=str(es_record.id),
+            body=es_record.json(),
+            request_timeout=self.config.timeout,
+        )
+        self.status.records_written(es_record.name)
+
+    def _write_report_data(self, record: ReportData) -> None:
+        self.elasticsearch_client.index(
+            index=DataInsightEsIndex[record.data.__class__.__name__].value,
+            id=record.id,
+            body=record.json(),
+            request_timeout=self.config.timeout,
+        )
+
+    def _write_classification(self, record: Classification) -> None:
+        es_record = create_record_document(record, self.metadata)
+        for es_record_elem in es_record:
+            self.elasticsearch_client.index(
+                index=ES_INDEX_MAP[Tag.__name__],
+                id=str(es_record_elem.id),
+                body=es_record_elem.json(),
+                request_timeout=self.config.timeout,
+            )
+            self.status.records_written(es_record_elem.name)
+
+    @staticmethod
+    def _write_policy(_: Policy) -> None:
+        logger.debug("Policies are not indexed")
 
     def read_records(self, index: str, query: dict):
         """Read records from es index
 
         Args:
             index: elasticsearch index
             query: query to be passed to the request body
@@ -446,449 +409,516 @@
         """Perform bulk operations.
 
         Args:
             body: https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html
         """
         return self.elasticsearch_client.bulk(body=body)
 
-    def _create_table_es_doc(self, table: Table):
-        suggest = [
-            {"input": [table.fullyQualifiedName.__root__], "weight": 5},
-            {"input": [table.name], "weight": 10},
-        ]
-        tags = []
-        tier = None
-        column_names = []
-        column_descriptions = []
-
-        for table_tag in table.tags:
-            if "Tier" in table_tag.tagFQN.__root__:
-                tier = table_tag
-            else:
-                tags.append(table_tag)
+    def close(self):
+        self.elasticsearch_client.close()
 
-        database_entity = self.metadata.get_by_id(
-            entity=Database, entity_id=str(table.database.id.__root__)
-        )
-        database_schema_entity = self.metadata.get_by_id(
-            entity=DatabaseSchema, entity_id=str(table.databaseSchema.id.__root__)
-        )
 
-        self._parse_columns(
-            columns=table.columns,
-            parent_column=None,
-            column_names=column_names,
-            column_descriptions=column_descriptions,
-            tags=tags,
-        )
+# Document creation functions by Entity
 
-        return TableESDocument(
-            id=str(table.id.__root__),
-            name=table.name.__root__,
-            displayName=table.displayName if table.displayName else table.name.__root__,
-            fullyQualifiedName=table.fullyQualifiedName.__root__,
-            version=table.version.__root__,
-            updatedAt=table.updatedAt.__root__,
-            updatedBy=table.updatedBy,
-            href=table.href.__root__,
-            columns=table.columns,
-            databaseSchema=table.databaseSchema,
-            database=table.database,
-            service=table.service,
-            owner=table.owner,
-            location=table.location,
-            usageSummary=table.usageSummary,
-            deleted=table.deleted,
-            serviceType=str(table.serviceType.name),
-            suggest=suggest,
-            service_suggest=[{"input": [table.service.name], "weight": 5}],
-            database_suggest=[{"input": [database_entity.name.__root__], "weight": 5}],
-            schema_suggest=[
-                {
-                    "input": [database_schema_entity.name.__root__],
-                    "weight": 5,
-                }
-            ],
-            column_suggest=[
-                {"input": [column], "weight": 5} for column in column_names
-            ],
-            description=table.description.__root__ if table.description else "",
-            tier=tier,
-            tags=list(tags),
-            followers=[
-                str(follower.id.__root__) for follower in table.followers.__root__
-            ]
-            if table.followers
-            else [],
-        )
 
-    def _create_topic_es_doc(self, topic: Topic):
-        service_suggest = []
-        suggest = [
-            {"input": [topic.name], "weight": 5},
-            {"input": [topic.fullyQualifiedName.__root__], "weight": 10},
-        ]
-        tags = []
-        topic_followers = []
-        if topic.followers:
-            for follower in topic.followers.__root__:
-                topic_followers.append(str(follower.id.__root__))
-        tier = None
-        for topic_tag in topic.tags:
-            if "Tier" in topic_tag.tagFQN.__root__:
-                tier = topic_tag
-            else:
-                tags.append(topic_tag)
-        service_suggest.append({"input": [topic.service.name], "weight": 5})
-        topic_doc = TopicESDocument(
-            id=str(topic.id.__root__),
-            name=topic.name.__root__,
-            displayName=topic.displayName if topic.displayName else topic.name.__root__,
-            description=topic.description.__root__ if topic.description else "",
-            fullyQualifiedName=topic.fullyQualifiedName.__root__,
-            version=topic.version.__root__,
-            updatedAt=topic.updatedAt.__root__,
-            updatedBy=topic.updatedBy,
-            href=topic.href.__root__,
-            deleted=topic.deleted,
-            service=topic.service,
-            serviceType=str(topic.serviceType.name),
-            schemaText=topic.messageSchema.schemaText if topic.messageSchema else None,
-            schemaType=topic.messageSchema.schemaType if topic.messageSchema else None,
-            cleanupPolicies=[str(policy.name) for policy in topic.cleanupPolicies],
-            replicationFactor=topic.replicationFactor,
-            maximumMessageSize=topic.maximumMessageSize,
-            retentionSize=topic.retentionSize,
-            suggest=suggest,
-            service_suggest=service_suggest,
-            tier=tier,
-            tags=list(tags),
-            owner=topic.owner,
-            followers=topic_followers,
-        )
-        return topic_doc
+def _parse_columns(
+    columns: List[Column],
+    parent_column: Optional[str],
+    column_names: List[str],
+    column_descriptions: List[str],
+    tags: List[str],
+):
+    """
+    Handle column names, descriptions and tags and recursively
+    add the information for its children.
+    """
+    for column in columns:
+        col_name = (
+            parent_column + "." + column.name.__root__
+            if parent_column
+            else column.name.__root__
+        )
+        column_names.append(col_name)
+        if column.description:
+            column_descriptions.append(column.description.__root__)
+        if len(column.tags) > 0:
+            for col_tag in column.tags:
+                tags.append(col_tag)
+        if column.children:
+            _parse_columns(
+                column.children,
+                column.name.__root__,
+                column_names,
+                column_descriptions,
+                tags,
+            )
 
-    def _create_dashboard_es_doc(self, dashboard: Dashboard):
-        display_name = (
-            dashboard.displayName if dashboard.displayName else dashboard.name.__root__
-        )
-        suggest = [
-            {"input": [dashboard.fullyQualifiedName.__root__], "weight": 10},
-            {"input": [display_name], "weight": 5},
-        ]
-        service_suggest = []
-        chart_suggest = []
-        tags = []
-        dashboard_followers = []
-        if dashboard.followers:
-            for follower in dashboard.followers.__root__:
-                dashboard_followers.append(str(follower.id.__root__))
-        tier = None
-        for dashboard_tag in dashboard.tags:
-            if "Tier" in dashboard_tag.tagFQN.__root__:
-                tier = dashboard_tag
-            else:
-                tags.append(dashboard_tag)
 
-        for chart in dashboard.charts:
-            chart_display_name = chart.displayName if chart.displayName else chart.name
-            chart_suggest.append({"input": [chart_display_name], "weight": 5})
-
-        service_suggest.append({"input": [dashboard.service.name], "weight": 5})
-
-        dashboard_doc = DashboardESDocument(
-            id=str(dashboard.id.__root__),
-            name=display_name,
-            displayName=display_name,
-            description=dashboard.description.__root__ if dashboard.description else "",
-            fullyQualifiedName=dashboard.fullyQualifiedName.__root__,
-            version=dashboard.version.__root__,
-            updatedAt=dashboard.updatedAt.__root__,
-            updatedBy=dashboard.updatedBy,
-            dashboardUrl=dashboard.dashboardUrl,
-            charts=dashboard.charts,
-            href=dashboard.href.__root__,
-            deleted=dashboard.deleted,
-            service=dashboard.service,
-            serviceType=str(dashboard.serviceType.name),
-            usageSummary=dashboard.usageSummary,
-            tier=tier,
-            tags=list(tags),
-            owner=dashboard.owner,
-            followers=dashboard_followers,
-            suggest=suggest,
-            chart_suggest=chart_suggest,
-            service_suggest=service_suggest,
-        )
+def get_es_tag_list_and_tier(record: Entity) -> (List[dict], Optional[str]):
+    """
+    Build ES tag list from any Entity
+    """
+    tags = []
+    tier = None
 
-        return dashboard_doc
+    for tag in record.tags or []:
+        if "Tier" in tag.tagFQN.__root__:
+            tier = tag
+        else:
+            tags.append(tag)
 
-    def _create_pipeline_es_doc(self, pipeline: Pipeline):
-        display_name = (
-            pipeline.displayName if pipeline.displayName else pipeline.name.__root__
-        )
-        suggest = [
-            {"input": [pipeline.fullyQualifiedName.__root__], "weight": 10},
-            {"input": [display_name], "weight": 5},
+    return tags, tier
+
+
+def get_es_followers(record: Entity) -> List[str]:
+    """
+    Build the ES follower list
+    """
+    if record.followers:
+        return [
+            follower.id.__root__
+            for follower in record.followers.__root__
+            if record.followers
         ]
-        service_suggest = []
-        task_suggest = []
-        tags = []
-        service_suggest.append({"input": [pipeline.service.name], "weight": 5})
-        pipeline_followers = []
-        if pipeline.followers:
-            for follower in pipeline.followers.__root__:
-                pipeline_followers.append(str(follower.id.__root__))
-        tier = None
-        for pipeline_tag in pipeline.tags:
-            if "Tier" in pipeline_tag.tagFQN.__root__:
-                tier = pipeline_tag
-            else:
-                tags.append(pipeline_tag)
 
-        for task in pipeline.tasks:
-            task_suggest.append({"input": [task.displayName], "weight": 5})
-            if tags in task and len(task.tags) > 0:
-                tags.extend(task.tags)
-
-        pipeline_doc = PipelineESDocument(
-            id=str(pipeline.id.__root__),
-            name=pipeline.name.__root__,
-            displayName=display_name,
-            description=pipeline.description.__root__ if pipeline.description else "",
-            fullyQualifiedName=pipeline.fullyQualifiedName.__root__,
-            version=pipeline.version.__root__,
-            updatedAt=pipeline.updatedAt.__root__,
-            updatedBy=pipeline.updatedBy,
-            pipelineUrl=pipeline.pipelineUrl,
-            tasks=pipeline.tasks,
-            href=pipeline.href.__root__,
-            deleted=pipeline.deleted,
-            service=pipeline.service,
-            serviceType=str(pipeline.serviceType.name),
-            suggest=suggest,
-            task_suggest=task_suggest,
-            service_suggest=service_suggest,
-            tier=tier,
-            tags=list(tags),
-            owner=pipeline.owner,
-            followers=pipeline_followers,
-        )
+    return []
 
-        return pipeline_doc
 
-    def _create_ml_model_es_doc(self, ml_model: MlModel):
-        display_name = (
-            ml_model.displayName if ml_model.displayName else ml_model.name.__root__
-        )
-        suggest = [{"input": [display_name], "weight": 10}]
-        tags = []
-        ml_model_followers = []
-        if ml_model.followers:
-            for follower in ml_model.followers.__root__:
-                ml_model_followers.append(str(follower.id.__root__))
-        tier = None
-        for ml_model_tag in ml_model.tags:
-            if "Tier" in ml_model_tag.tagFQN.__root__:
-                tier = ml_model_tag
-            else:
-                tags.append(ml_model_tag)
+def get_es_display_name(record: Entity) -> str:
+    """
+    Build the display name for ES
+    """
+    return record.displayName if record.displayName else record.name.__root__
 
-        service_entity = ESEntityReference(
-            id=str(ml_model.service.id.__root__),
-            name=ml_model.service.name,
-            displayName=ml_model.service.displayName
-            if ml_model.service.displayName
-            else "",
-            description=ml_model.service.description.__root__
-            if ml_model.service.description
-            else "",
-            type=ml_model.service.type,
-            fullyQualifiedName=ml_model.service.fullyQualifiedName,
-            deleted=ml_model.service.deleted,
-            href=ml_model.service.href.__root__,
-        )
 
-        ml_model_doc = MlModelESDocument(
-            id=str(ml_model.id.__root__),
-            name=ml_model.name.__root__,
-            displayName=display_name,
-            description=ml_model.description.__root__ if ml_model.description else "",
-            fullyQualifiedName=ml_model.fullyQualifiedName.__root__,
-            version=ml_model.version.__root__,
-            updatedAt=ml_model.updatedAt.__root__,
-            updatedBy=ml_model.updatedBy,
-            href=ml_model.href.__root__,
-            deleted=ml_model.deleted,
-            algorithm=ml_model.algorithm if ml_model.algorithm else "",
-            mlFeatures=ml_model.mlFeatures,
-            mlHyperParameters=ml_model.mlHyperParameters,
-            target=ml_model.target.__root__ if ml_model.target else "",
-            dashboard=ml_model.dashboard,
-            mlStore=ml_model.mlStore,
-            server=ml_model.server.__root__ if ml_model.server else "",
-            usageSummary=ml_model.usageSummary,
-            suggest=suggest,
-            tier=tier,
-            tags=list(tags),
-            owner=ml_model.owner,
-            followers=ml_model_followers,
-            service=service_entity,
-        )
+def _get_es_suggest(input_5: str, input_10: str) -> List[ESSuggest]:
+    """
+    Build the ES suggest field
+    """
 
-        return ml_model_doc
+    return [
+        ESSuggest(input=input_5, weight=5),
+        ESSuggest(input=input_10, weight=10),
+    ]
 
-    def _create_user_es_doc(self, user: User):
-        display_name = user.displayName if user.displayName else user.name.__root__
-        suggest = [
-            {"input": [display_name], "weight": 5},
-            {"input": [user.name], "weight": 10},
-        ]
-        user_doc = UserESDocument(
-            id=str(user.id.__root__),
-            name=user.name.__root__,
-            displayName=display_name,
-            description=user.description.__root__ if user.description else "",
-            fullyQualifiedName=user.fullyQualifiedName.__root__,
-            version=user.version.__root__,
-            updatedAt=user.updatedAt.__root__,
-            updatedBy=user.updatedBy,
-            href=user.href.__root__,
-            deleted=user.deleted,
-            email=user.email.__root__,
-            isAdmin=user.isAdmin if user.isAdmin else False,
-            teams=user.teams if user.teams else [],
-            roles=user.roles if user.roles else [],
-            inheritedRoles=user.inheritedRoles if user.inheritedRoles else [],
-            suggest=suggest,
-        )
 
-        return user_doc
+def _build_suggest_of(
+    entity_list: Optional[EntityReferenceList],
+) -> List[ESSuggest]:
+    """
+    Build the ES suggest field from a EntityReferenceList
+    Args:
+        entity_list: a EntityReferenceList
+    Returns:
+        The ES suggest field
+    """
+    suggest_list = []
+    if not entity_list:
+        return suggest_list
+    for entity in entity_list.__root__:
+        entity_display_name = entity.displayName if entity.displayName else entity.name
+        suggest_list.append(ESSuggest(input=entity_display_name, weight=5))
+    return suggest_list
 
-    def _create_team_es_doc(self, team: Team):
-        display_name = team.displayName if team.displayName else team.name.__root__
-        suggest = [
-            {"input": [display_name], "weight": 5},
-            {"input": [team.name], "weight": 10},
-        ]
-        team_doc = TeamESDocument(
-            id=str(team.id.__root__),
-            name=team.name.__root__,
-            displayName=display_name,
-            description=team.description.__root__ if team.description else "",
-            teamType=team.teamType.name,
-            fullyQualifiedName=team.fullyQualifiedName.__root__,
-            version=team.version.__root__,
-            updatedAt=team.updatedAt.__root__,
-            updatedBy=team.updatedBy,
-            href=team.href.__root__,
-            deleted=team.deleted,
-            suggest=suggest,
-            users=team.users if team.users else [],
-            defaultRoles=team.defaultRoles if team.defaultRoles else [],
-            parents=team.parents if team.parents else [],
-            isJoinable=team.isJoinable,
-        )
 
-        return team_doc
+@singledispatch
+def create_record_document(record: Entity, _: OpenMetadata) -> Any:
+    """
+    Entrypoint to create documents from records
+    and get them ready to send to ES
+    """
+    raise NotImplementedError(f"Record of type {type(record)} not implemented.")
 
-    def _create_glossary_term_es_doc(self, glossary_term: GlossaryTerm):
-        display_name = (
-            glossary_term.displayName
-            if glossary_term.displayName
-            else glossary_term.name.__root__
-        )
-        suggest = [
-            {"input": [display_name], "weight": 5},
-            {"input": [glossary_term.name], "weight": 10},
-        ]
-        glossary_term_doc = GlossaryTermESDocument(
-            id=str(glossary_term.id.__root__),
-            name=str(glossary_term.name.__root__),
-            displayName=display_name,
-            description=glossary_term.description.__root__
-            if glossary_term.description
-            else "",
-            fullyQualifiedName=glossary_term.fullyQualifiedName.__root__,
-            version=glossary_term.version.__root__,
-            updatedAt=glossary_term.updatedAt.__root__,
-            updatedBy=glossary_term.updatedBy,
-            href=glossary_term.href.__root__,
-            synonyms=[str(synonym.__root__) for synonym in glossary_term.synonyms],
-            glossary=glossary_term.glossary,
-            children=glossary_term.children if glossary_term.children else [],
-            relatedTerms=glossary_term.relatedTerms
-            if glossary_term.relatedTerms
-            else [],
-            reviewers=glossary_term.reviewers if glossary_term.reviewers else [],
-            usageCount=glossary_term.usageCount,
-            tags=glossary_term.tags if glossary_term.tags else [],
-            status=glossary_term.status.name,
-            suggest=suggest,
-            deleted=glossary_term.deleted,
-        )
 
-        return glossary_term_doc
+@create_record_document.register
+def _(record: Table, metadata: OpenMetadata) -> TableESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    suggest = _get_es_suggest(
+        input_5=record.fullyQualifiedName.__root__, input_10=record.name.__root__
+    )
+    display_name = get_es_display_name(record)
+    followers = get_es_followers(record)
 
-    def _create_tag_es_doc(self, classification: Classification):
-        tag_docs = []
+    column_names = []
+    column_descriptions = []
 
-        tag_list = self.metadata.list_entities(
-            entity=Tag, params={"parent": classification.name.__root__}
-        )
-        for tag in tag_list.entities or []:
-            suggest = [
-                {"input": [tag.name.__root__], "weight": 5},
-                {"input": [tag.fullyQualifiedName], "weight": 10},
-            ]
-            tag_doc = TagESDocument(
-                id=str(tag.id.__root__),
-                name=str(tag.name.__root__),
-                description=tag.description.__root__ if tag.description else "",
-                suggest=suggest,
-                fullyQualifiedName=tag.fullyQualifiedName,
-                version=tag.version.__root__,
-                updatedAt=tag.updatedAt.__root__,
-                updatedBy=tag.updatedBy,
-                href=tag.href.__root__,
-                deleted=tag.deleted,
-                deprecated=tag.deprecated,
-            )
-            tag_docs.append(tag_doc)
+    _parse_columns(
+        columns=record.columns,
+        parent_column=None,
+        column_names=column_names,
+        column_descriptions=column_descriptions,
+        tags=tags,
+    )
 
-        return tag_docs
+    database_entity = metadata.get_by_id(
+        entity=Database, entity_id=str(record.database.id.__root__)
+    )
+    database_schema_entity = metadata.get_by_id(
+        entity=DatabaseSchema, entity_id=str(record.databaseSchema.id.__root__)
+    )
 
-    def _parse_columns(
-        self,
-        columns: List[Column],
-        parent_column: Optional[str],
-        column_names: List[str],
-        column_descriptions: List[str],
-        tags: List[str],
-    ):
-        """
-        Handle column names, descriptions and tags and recursively
-        add the information for its children.
-        """
-        for column in columns:
-            col_name = (
-                parent_column + "." + column.name.__root__
-                if parent_column
-                else column.name.__root__
-            )
-            column_names.append(col_name)
-            if column.description:
-                column_descriptions.append(column.description.__root__)
-            if len(column.tags) > 0:
-                for col_tag in column.tags:
-                    tags.append(col_tag)
-            if column.children:
-                self._parse_columns(
-                    column.children,
-                    column.name.__root__,
-                    column_names,
-                    column_descriptions,
-                    tags,
-                )
+    return TableESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        columns=record.columns,
+        databaseSchema=record.databaseSchema,
+        database=record.database,
+        service=record.service,
+        owner=record.owner,
+        location=record.location,
+        usageSummary=record.usageSummary,
+        deleted=record.deleted,
+        serviceType=str(record.serviceType.name),
+        suggest=suggest,
+        service_suggest=[ESSuggest(input=record.service.name, weight=5)],
+        database_suggest=[ESSuggest(input=database_entity.name.__root__, weight=5)],
+        schema_suggest=[
+            ESSuggest(input=database_schema_entity.name.__root__, weight=5)
+        ],
+        column_suggest=[ESSuggest(input=column, weight=5) for column in column_names],
+        description=record.description.__root__ if record.description else "",
+        tier=tier,
+        tags=tags,
+        followers=followers,
+    )
 
-    def close(self):
-        self.elasticsearch_client.close()
+
+@create_record_document.register
+def _(record: Topic, _: OpenMetadata) -> TopicESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    suggest = _get_es_suggest(
+        input_5=record.fullyQualifiedName.__root__, input_10=record.name.__root__
+    )
+    display_name = get_es_display_name(record)
+    followers = get_es_followers(record)
+
+    return TopicESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        service=record.service,
+        serviceType=str(record.serviceType.name),
+        schemaText=record.messageSchema.schemaText if record.messageSchema else None,
+        schemaType=record.messageSchema.schemaType if record.messageSchema else None,
+        cleanupPolicies=[str(policy.name) for policy in record.cleanupPolicies],
+        replicationFactor=record.replicationFactor,
+        maximumMessageSize=record.maximumMessageSize,
+        retentionSize=record.retentionSize,
+        suggest=suggest,
+        service_suggest=[ESSuggest(input=record.service.name, weight=5)],
+        tier=tier,
+        tags=tags,
+        owner=record.owner,
+        followers=followers,
+    )
+
+
+@create_record_document.register
+def _(record: Dashboard, _: OpenMetadata) -> DashboardESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(
+        input_5=record.fullyQualifiedName.__root__, input_10=display_name
+    )
+    followers = get_es_followers(record)
+
+    chart_suggest = _build_suggest_of(record.charts)
+    data_model_suggest = _build_suggest_of(record.dataModels)
+
+    return DashboardESDocument(
+        id=str(record.id.__root__),
+        name=display_name,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        dashboardUrl=record.dashboardUrl,
+        charts=record.charts.__root__,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        service=record.service,
+        serviceType=str(record.serviceType.name),
+        usageSummary=record.usageSummary,
+        tier=tier,
+        tags=tags,
+        owner=record.owner,
+        followers=followers,
+        suggest=suggest,
+        chart_suggest=chart_suggest,
+        data_model_suggest=data_model_suggest,
+        service_suggest=[ESSuggest(input=record.service.name, weight=5)],
+    )
+
+
+@create_record_document.register
+def _create_pipeline_es_doc(record: Pipeline, _: OpenMetadata) -> PipelineESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(
+        input_5=record.fullyQualifiedName.__root__, input_10=display_name
+    )
+    followers = get_es_followers(record)
+
+    task_suggest = []
+    for task in record.tasks:
+        task_suggest.append(ESSuggest(input=task.displayName, weight=5))
+        if tags in task and len(task.tags) > 0:
+            tags.extend(task.tags)
+
+    return PipelineESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        pipelineUrl=record.pipelineUrl,
+        tasks=record.tasks,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        service=record.service,
+        serviceType=str(record.serviceType.name),
+        suggest=suggest,
+        task_suggest=task_suggest,
+        service_suggest=[ESSuggest(input=record.service.name, weight=5)],
+        tier=tier,
+        tags=list(tags),
+        owner=record.owner,
+        followers=followers,
+    )
+
+
+@create_record_document.register
+def _create_ml_model_es_doc(record: MlModel, _: OpenMetadata) -> MlModelESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(
+        input_5=record.fullyQualifiedName.__root__, input_10=record.name.__root__
+    )
+    followers = get_es_followers(record)
+
+    return MlModelESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        algorithm=record.algorithm if record.algorithm else "",
+        mlFeatures=record.mlFeatures,
+        mlHyperParameters=record.mlHyperParameters,
+        target=record.target.__root__ if record.target else "",
+        dashboard=record.dashboard,
+        mlStore=record.mlStore,
+        server=record.server.__root__ if record.server else "",
+        usageSummary=record.usageSummary,
+        suggest=suggest,
+        tier=tier,
+        tags=tags,
+        owner=record.owner,
+        followers=followers,
+        service=record.service,
+        service_suggest=[ESSuggest(input=record.service.name, weight=5)],
+    )
+
+
+@create_record_document.register
+def _create_container_es_doc(record: Container, _: OpenMetadata) -> ContainerESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(
+        input_5=record.fullyQualifiedName.__root__, input_10=record.name.__root__
+    )
+    followers = get_es_followers(record)
+
+    return ContainerESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        suggest=suggest,
+        tier=tier,
+        tags=tags,
+        owner=record.owner,
+        followers=followers,
+        service=record.service,
+        parent=record.parent,
+        dataModel=record.dataModel,
+        children=record.children,
+        prefix=record.prefix,
+        numberOfObjects=record.numberOfObjects,
+        size=record.size,
+        fileFormats=[file_format.value for file_format in record.fileFormats or []],
+        service_suggest=[ESSuggest(input=record.service.name, weight=5)],
+    )
+
+
+@create_record_document.register
+def _create_query_es_doc(record: Query, _: OpenMetadata) -> QueryESDocument:
+    tags, tier = get_es_tag_list_and_tier(record)
+    display_name = get_es_display_name(record)
+    followers = get_es_followers(record)
+
+    return QueryESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        suggest=[
+            ESSuggest(input=record.name.__root__, weight=10),
+        ],
+        tier=tier,
+        tags=list(tags),
+        owner=record.owner,
+        followers=followers,
+        duration=record.duration,
+        users=record.users,
+        votes=record.votes,
+        query=record.query.__root__,
+        queryDate=record.queryDate.__root__,
+    )
+
+
+@create_record_document.register
+def _create_user_es_doc(record: User, _: OpenMetadata) -> UserESDocument:
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(input_5=record.name.__root__, input_10=display_name)
+
+    return UserESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        email=record.email.__root__,
+        isAdmin=record.isAdmin if record.isAdmin else False,
+        teams=record.teams if record.teams else [],
+        roles=record.roles if record.roles else [],
+        inheritedRoles=record.inheritedRoles if record.inheritedRoles else [],
+        suggest=suggest,
+    )
+
+
+@create_record_document.register
+def _create_team_es_doc(record: Team, _: OpenMetadata) -> TeamESDocument:
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(input_5=record.name.__root__, input_10=display_name)
+
+    return TeamESDocument(
+        id=str(record.id.__root__),
+        name=record.name.__root__,
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        teamType=record.teamType.name,
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        deleted=record.deleted,
+        suggest=suggest,
+        users=record.users if record.users else [],
+        defaultRoles=record.defaultRoles if record.defaultRoles else [],
+        parents=record.parents if record.parents else [],
+        isJoinable=record.isJoinable,
+    )
+
+
+@create_record_document.register
+def _create_glossary_term_es_doc(
+    record: GlossaryTerm, _: OpenMetadata
+) -> GlossaryTermESDocument:
+    display_name = get_es_display_name(record)
+    suggest = _get_es_suggest(input_5=record.name.__root__, input_10=display_name)
+
+    return GlossaryTermESDocument(
+        id=str(record.id.__root__),
+        name=str(record.name.__root__),
+        displayName=display_name,
+        description=record.description.__root__ if record.description else "",
+        fullyQualifiedName=record.fullyQualifiedName.__root__,
+        version=record.version.__root__,
+        updatedAt=record.updatedAt.__root__,
+        updatedBy=record.updatedBy,
+        href=record.href.__root__,
+        synonyms=[str(synonym.__root__) for synonym in record.synonyms],
+        glossary=record.glossary,
+        children=record.children if record.children else [],
+        relatedTerms=record.relatedTerms if record.relatedTerms else [],
+        reviewers=record.reviewers if record.reviewers else [],
+        usageCount=record.usageCount,
+        tags=record.tags if record.tags else [],
+        status=record.status.name,
+        suggest=suggest,
+        deleted=record.deleted,
+    )
+
+
+@create_record_document.register
+def _create_tag_es_doc(
+    record: Classification, metadata: OpenMetadata
+) -> List[TagESDocument]:
+    tag_docs = []
+    tag_list = metadata.list_entities(
+        entity=Tag, params={"parent": record.name.__root__}
+    )
+
+    for tag in tag_list.entities or []:
+        suggest = [
+            ESSuggest(input=tag.fullyQualifiedName.__root__, weight=5),
+            ESSuggest(input=tag.name.__root__, weight=10),
+        ]
+
+        tag_doc = TagESDocument(
+            id=str(tag.id.__root__),
+            name=str(tag.name.__root__),
+            description=tag.description.__root__ if tag.description else "",
+            suggest=suggest,
+            fullyQualifiedName=tag.fullyQualifiedName,
+            version=tag.version.__root__,
+            updatedAt=tag.updatedAt.__root__,
+            updatedBy=tag.updatedBy,
+            href=tag.href.__root__,
+            deleted=tag.deleted,
+            deprecated=tag.deprecated,
+        )
+        tag_docs.append(tag_doc)
+
+    return tag_docs
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,22 +5,22 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Defines the Elasticsearch mapping for Dashboards
+Defines the Elasticsearch mapping for Pipelines
 """
 import textwrap
 
-DASHBOARD_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
+PIPELINE_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
-"settings": {
+  "settings": {
     "analysis": {
       "normalizer": {
         "lowercase_normalizer": {
           "type": "custom",
           "char_filter": [],
           "filter": [
             "lowercase"
@@ -55,15 +55,16 @@
           "keyword": {
             "type": "keyword",
             "ignore_above": 256
           }
         }
       },
       "fullyQualifiedName": {
-        "type": "text"
+        "type": "keyword",
+        "normalizer": "lowercase_normalizer"
       },
       "displayName": {
         "type": "text",
         "analyzer": "om_analyzer"
       },
       "description": {
         "type": "text",
@@ -78,51 +79,45 @@
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "dashboardUrl": {
+      "pipelineUrl": {
         "type": "text"
       },
-      "charts": {
+      "tasks": {
         "properties": {
-          "id": {
+          "name": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
-                "ignore_above": 36
+                "ignore_above": 256
               }
             }
           },
-          "type": {
-            "type": "text"
-          },
-          "name": {
-            "type": "keyword",
+          "displayName": {
+            "type": "text",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
-          "fullyQualifiedName": {
-            "type": "text"
-          },
           "description": {
             "type": "text",
             "analyzer": "om_analyzer"
           },
-          "deleted": {
+          "taskUrl": {
             "type": "text"
           },
-          "href": {
+          "taskType": {
             "type": "text"
           }
         }
       },
       "owner": {
         "properties": {
           "id": {
@@ -142,14 +137,23 @@
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
+          "displayName": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              }
+            }
+          },
           "fullyQualifiedName": {
             "type": "text"
           },
           "description": {
             "type": "text"
           },
           "deleted": {
@@ -193,48 +197,14 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "usageSummary": {
-        "properties": {
-          "dailyStats": {
-            "properties": {
-              "count": {
-                "type": "long"
-              },
-              "percentileRank": {
-                "type": "long"
-              }
-            }
-          },
-          "weeklyStats": {
-            "properties": {
-              "count": {
-                "type": "long"
-              },
-              "percentileRank": {
-                "type": "long"
-              }
-            }
-          },
-          "monthlyStats": {
-            "properties": {
-              "count": {
-                "type": "long"
-              },
-              "percentileRank": {
-                "type": "long"
-              }
-            }
-          }
-        }
-      },
       "deleted": {
         "type": "text"
       },
       "followers": {
         "type": "keyword"
       },
       "tier": {
@@ -287,15 +257,15 @@
           {
             "name": "deleted",
             "type": "category",
             "path": "deleted"
           }
         ]
       },
-      "chart_suggest": {
+      "task_suggest": {
         "type": "completion"
       },
       "service_suggest": {
         "type": "completion"
       }
     }
   }
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,55 +11,55 @@
 """
 Defines the Elasticsearch mapping for Dashboards
 """
 import textwrap
 
 ENTITY_REPORT_DATA_INDEX_MAPPING = textwrap.dedent(
     """
-    {
-        "mappings": {
-            "properties": {
-                "timestamp": {
-                    "type": "date"
-                },
-                "reportDataType": {
-                    "type": "keyword"
-                },
-                "data": {
-                    "properties": {
-                        "id": {
-                            "type": "text"
-                        },
-                        "team": {
-                            "type": "keyword"
-                        },
-                        "entityTier": {
-                            "type": "keyword"
-                        },
-                        "entityType": {
-                            "type": "keyword"
-                        },
-                        "organization": {
-                            "type": "text"
-                        },
-                        "hasOwner": {
-                            "type": "integer"
-                        },
-                        "missingOwner": {
-                            "type": "integer"
-                        },
-                        "missingDescriptions": {
-                            "type": "integer"
-                        },
-                        "completedDescriptions": {
-                            "type": "integer"
-                        },
-                        "entityCount": {
-                            "type": "integer"
-                        }
-                    }
-                }
-            }
+{
+  "mappings": {
+    "properties": {
+      "timestamp": {
+        "type": "date"
+      },
+      "reportDataType": {
+        "type": "keyword"
+      },
+      "data": {
+        "properties": {
+          "id": {
+            "type": "text"
+          },
+          "team": {
+            "type": "keyword"
+          },
+          "entityTier": {
+            "type": "keyword"
+          },
+          "entityType": {
+            "type": "keyword"
+          },
+          "organization": {
+            "type": "text"
+          },
+          "hasOwner": {
+            "type": "integer"
+          },
+          "missingOwner": {
+            "type": "integer"
+          },
+          "missingDescriptions": {
+            "type": "integer"
+          },
+          "completedDescriptions": {
+            "type": "integer"
+          },
+          "entityCount": {
+            "type": "integer"
+          }
         }
+      }
     }
+  }
+}
     """
 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,28 +5,28 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Defines the Elasticsearch mapping for Glossaries
+Defines the Elasticsearch mapping for Teams
 """
 import textwrap
 
-GLOSSARY_TERM_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
+TEAM_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
   "mappings": {
     "properties": {
       "id": {
         "type": "text"
       },
       "name": {
-        "type": "keyword",
+        "type": "text",
         "fields": {
           "keyword": {
             "type": "keyword",
             "ignore_above": 256
           }
         }
       },
@@ -41,31 +41,31 @@
             "ignore_above": 256
           }
         }
       },
       "description": {
         "type": "text"
       },
+      "teamType": { 
+        "type": "text"
+      },
       "version": {
         "type": "float"
       },
       "updatedAt": {
         "type": "date",
         "format": "epoch_second"
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "synonyms": {
-        "type": "text"
-      },
-      "glossary": {
+      "users": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -94,15 +94,15 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "children": {
+      "parents": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -131,15 +131,15 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "relatedTerms": {
+      "defaultRoles": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -168,78 +168,22 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "reviewers": {
-        "properties": {
-          "id": {
-            "type": "keyword",
-            "fields": {
-              "keyword": {
-                "type": "keyword",
-                "ignore_above": 36
-              }
-            }
-          },
-          "type": {
-            "type": "keyword"
-          },
-          "name": {
-            "type": "keyword",
-            "fields": {
-              "keyword": {
-                "type": "keyword",
-                "ignore_above": 256
-              }
-            }
-          },
-          "fullyQualifiedName": {
-            "type": "text"
-          },
-          "description": {
-            "type": "text"
-          },
-          "deleted": {
-            "type": "text"
-          },
-          "href": {
-            "type": "text"
-          }
-        }
-      },
-      "usageCount": {
-        "type": "integer"
-      },
-      "tags": {
-        "properties": {
-          "tagFQN": {
-            "type": "keyword"
-          },
-          "labelType": {
-            "type": "keyword"
-          },
-          "description": {
-            "type": "text"
-          },
-          "source": {
-            "type": "keyword"
-          },
-          "state": {
-            "type": "keyword"
-          }
-        }
+      "isJoinable": {
+        "type": "text"
       },
       "deleted": {
         "type": "text"
       },
-      "status": {
-        "type": "text"
+      "entityType": {
+        "type": "keyword"
       },
       "suggest": {
         "type": "completion",
         "contexts": [
           {
             "name": "deleted",
             "type": "category",
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 Defines the Elasticsearch mapping for ML Models
 """
 import textwrap
 
 MLMODEL_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
-"settings": {
+  "settings": {
     "analysis": {
       "normalizer": {
         "lowercase_normalizer": {
           "type": "custom",
           "char_filter": [],
           "filter": [
             "lowercase"
@@ -30,14 +30,19 @@
       "analyzer": {
         "om_analyzer": {
           "tokenizer": "letter",
           "filter": [
             "lowercase",
             "om_stemmer"
           ]
+        },
+        "om_ngram": {
+          "tokenizer": "ngram",
+          "min_gram": 1,
+          "max_gram": 2
         }
       },
       "filter": {
         "om_stemmer": {
           "type": "stemmer",
           "name": "english"
         }
@@ -60,19 +65,25 @@
       },
       "fullyQualifiedName": {
         "type": "keyword",
         "normalizer": "lowercase_normalizer"
       },
       "displayName": {
         "type": "text",
-        "analyzer": "om_analyzer"
+        "analyzer": "om_analyzer",
+        "fields": {
+          "ngram": {
+            "type": "text",
+            "analyzer": "om_ngram"
+          }
+        }
       },
       "description": {
         "type": "text",
-         "analyzer": "om_analyzer"
+        "analyzer": "om_analyzer"
       },
       "version": {
         "type": "float"
       },
       "updatedAt": {
         "type": "date",
         "format": "epoch_second"
@@ -98,15 +109,15 @@
             }
           },
           "dataType": {
             "type": "text"
           },
           "description": {
             "type": "text",
-             "analyzer": "om_analyzer"
+            "analyzer": "om_analyzer"
           },
           "fullyQualifiedName": {
             "type": "text"
           },
           "featureSources": {
             "properties": {
               "name": {
@@ -118,21 +129,16 @@
             }
           }
         }
       },
       "mlHyperParameters": {
         "properties": {
           "name": {
-            "type": "keyword",
-            "fields": {
-              "keyword": {
-                "type": "keyword",
-                "ignore_above": 256
-              }
-            }
+            "type": "text",
+            "analyzer": "om_analyzer"
           },
           "value": {
             "type": "text"
           }
         }
       },
       "target": {
@@ -206,14 +212,23 @@
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
+          },
+          "displayName": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              }
+            }
           },
           "fullyQualifiedName": {
             "type": "text"
           },
           "description": {
             "type": "text"
           },
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,22 +5,22 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Defines the Elasticsearch mapping for Pipelines
+Defines the Elasticsearch mapping for Glossaries
 """
 import textwrap
 
-PIPELINE_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
+GLOSSARY_TERM_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
-"settings": {
+  "settings": {
     "analysis": {
       "normalizer": {
         "lowercase_normalizer": {
           "type": "custom",
           "char_filter": [],
           "filter": [
             "lowercase"
@@ -30,14 +30,19 @@
       "analyzer": {
         "om_analyzer": {
           "tokenizer": "letter",
           "filter": [
             "lowercase",
             "om_stemmer"
           ]
+        },
+        "om_ngram": {
+          "tokenizer": "ngram",
+          "min_gram": 1,
+          "max_gram": 2
         }
       },
       "filter": {
         "om_stemmer": {
           "type": "stemmer",
           "name": "english"
         }
@@ -46,77 +51,125 @@
   },
   "mappings": {
     "properties": {
       "id": {
         "type": "text"
       },
       "name": {
-        "type": "text",
+        "type": "keyword",
         "fields": {
           "keyword": {
             "type": "keyword",
             "ignore_above": 256
           }
         }
       },
       "fullyQualifiedName": {
-        "type": "text"
+        "type": "keyword",
+        "normalizer": "lowercase_normalizer"
       },
       "displayName": {
         "type": "text",
-        "analyzer": "om_analyzer"
+        "analyzer": "om_analyzer",
+        "fields": {
+          "keyword": {
+            "type": "keyword",
+            "ignore_above": 256
+          },
+          "ngram": {
+            "type": "text",
+            "analyzer": "om_ngram"
+          }
+        }
       },
       "description": {
-        "type": "text",
-        "analyzer": "om_analyzer"
+        "type": "text"
       },
       "version": {
         "type": "float"
       },
       "updatedAt": {
         "type": "date",
         "format": "epoch_second"
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "pipelineUrl": {
-        "type": "text"
+      "synonyms": {
+        "type": "text",
+        "fields": {
+          "keyword": {
+            "type": "keyword",
+            "ignore_above": 256
+          },
+          "ngram": {
+            "type": "text",
+            "analyzer": "om_ngram"
+          }
+        }
       },
-      "tasks": {
+      "glossary": {
         "properties": {
-          "name": {
+          "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
+                "ignore_above": 36
+              }
+            }
+          },
+          "type": {
+            "type": "keyword"
+          },
+          "name": {
+            "type": "text",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
                 "ignore_above": 256
+              },
+              "ngram": {
+                "type": "text",
+                "analyzer": "om_ngram"
               }
             }
           },
           "displayName": {
             "type": "text",
-            "analyzer": "om_analyzer"
+            "analyzer": "om_analyzer",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              },
+              "ngram": {
+                "type": "text",
+                "analyzer": "om_ngram"
+              }
+            }
+          },
+          "fullyQualifiedName": {
+            "type": "text"
           },
           "description": {
-            "type": "text",
-            "analyzer": "om_analyzer"
+            "type": "text"
           },
-          "taskUrl": {
+          "deleted": {
             "type": "text"
           },
-          "taskType": {
+          "href": {
             "type": "text"
           }
         }
       },
-      "owner": {
+      "children": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -145,15 +198,15 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "service": {
+      "relatedTerms": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -182,39 +235,63 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "deleted": {
-        "type": "text"
-      },
-      "followers": {
-        "type": "keyword"
-      },
-      "tier": {
+      "reviewers": {
         "properties": {
-          "tagFQN": {
-            "type": "keyword"
+          "id": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 36
+              }
+            }
           },
-          "labelType": {
+          "type": {
             "type": "keyword"
           },
+          "name": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              }
+            }
+          },
+          "displayName": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              }
+            }
+          },
+          "fullyQualifiedName": {
+            "type": "text"
+          },
           "description": {
             "type": "text"
           },
-          "source": {
-            "type": "keyword"
+          "deleted": {
+            "type": "text"
           },
-          "state": {
-            "type": "keyword"
+          "href": {
+            "type": "text"
           }
         }
       },
+      "usageCount": {
+        "type": "integer"
+      },
       "tags": {
         "properties": {
           "tagFQN": {
             "type": "keyword"
           },
           "labelType": {
             "type": "keyword"
@@ -226,34 +303,28 @@
             "type": "keyword"
           },
           "state": {
             "type": "keyword"
           }
         }
       },
-      "serviceType": {
-        "type": "keyword"
+      "deleted": {
+        "type": "text"
       },
-      "entityType": {
-        "type": "keyword"
+      "status": {
+        "type": "text"
       },
       "suggest": {
         "type": "completion",
         "contexts": [
           {
             "name": "deleted",
             "type": "category",
             "path": "deleted"
           }
         ]
-      },
-      "task_suggest": {
-        "type": "completion"
-      },
-      "service_suggest": {
-        "type": "completion"
       }
     }
   }
 }
 """
 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py`

 * *Files 5% similar despite different names*

```diff
@@ -5,19 +5,19 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Defines the Elasticsearch mapping for Tables
+Defines the Elasticsearch mapping for Dashboards
 """
 import textwrap
 
-TABLE_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
+DASHBOARD_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
   "settings": {
     "analysis": {
       "normalizer": {
         "lowercase_normalizer": {
           "type": "custom",
@@ -30,14 +30,19 @@
       "analyzer": {
         "om_analyzer": {
           "tokenizer": "letter",
           "filter": [
             "lowercase",
             "om_stemmer"
           ]
+        },
+        "om_ngram": {
+          "tokenizer": "ngram",
+          "min_gram": 1,
+          "max_gram": 2
         }
       },
       "filter": {
         "om_stemmer": {
           "type": "stemmer",
           "name": "english"
         }
@@ -60,15 +65,21 @@
       },
       "fullyQualifiedName": {
         "type": "keyword",
         "normalizer": "lowercase_normalizer"
       },
       "displayName": {
         "type": "text",
-        "analyzer": "om_analyzer"
+        "analyzer": "om_analyzer",
+        "fields": {
+          "ngram": {
+            "type": "text",
+            "analyzer": "om_ngram"
+          }
+        }
       },
       "description": {
         "type": "text",
         "analyzer": "om_analyzer"
       },
       "version": {
         "type": "float"
@@ -79,167 +90,102 @@
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "columns": {
+      "dashboardUrl": {
+        "type": "text"
+      },
+      "charts": {
         "properties": {
-          "name": {
+          "id": {
             "type": "keyword",
-            "normalizer": "lowercase_normalizer",
             "fields": {
               "keyword": {
                 "type": "keyword",
-                "ignore_above": 256
+                "ignore_above": 36
               }
             }
           },
-          "dataType": {
-            "type": "text"
-          },
-          "dataTypeDisplay": {
-            "type": "text"
-          },
-          "description": {
-            "type": "text",
-            "analyzer": "om_analyzer"
-          },
-          "fullyQualifiedName": {
+          "type": {
             "type": "text"
           },
-          "tags": {
-            "properties": {
-              "tagFQN": {
-                "type": "keyword"
-              },
-              "labelType": {
-                "type": "keyword"
-              },
-              "description": {
-                "type": "text"
-              },
-              "source": {
-                "type": "keyword"
-              },
-              "state": {
-                "type": "keyword"
-              }
-            }
-          },
-          "ordinalPosition": {
-            "type": "integer"
-          }
-        }
-      },
-      "databaseSchema": {
-        "properties": {
-          "id": {
+          "name": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
-                "ignore_above": 36
+                "ignore_above": 256
               }
             }
           },
-          "type": {
-            "type": "text"
-          },
-          "name": {
-            "type": "keyword",
-            "normalizer": "lowercase_normalizer",
+          "displayName": {
+            "type": "text",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
           "fullyQualifiedName": {
             "type": "text"
           },
           "description": {
-            "type": "text"
+            "type": "text",
+            "analyzer": "om_analyzer"
           },
           "deleted": {
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "database": {
+      "dataModels": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
               }
             }
           },
           "type": {
-            "type": "keyword"
+            "type": "text"
           },
           "name": {
             "type": "keyword",
-            "normalizer": "lowercase_normalizer",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
-          "fullyQualifiedName": {
-            "type": "text"
-          },
-          "description": {
-            "type": "text"
-          },
-          "deleted": {
-            "type": "text"
-          },
-          "href": {
-            "type": "text"
-          }
-        }
-      },
-      "service": {
-        "properties": {
-          "id": {
-            "type": "keyword",
-            "fields": {
-              "keyword": {
-                "type": "keyword",
-                "ignore_above": 36
-              }
-            }
-          },
-          "type": {
-            "type": "keyword"
-          },
-          "name": {
-            "type": "keyword",
+          "displayName": {
+            "type": "text",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
           "fullyQualifiedName": {
             "type": "text"
           },
           "description": {
-            "type": "text"
+            "type": "text",
+            "analyzer": "om_analyzer"
           },
           "deleted": {
             "type": "text"
           },
           "href": {
             "type": "text"
           }
@@ -264,29 +210,38 @@
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
+          "displayName": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              }
+            }
+          },
           "fullyQualifiedName": {
             "type": "text"
           },
           "description": {
             "type": "text"
           },
           "deleted": {
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "location": {
+      "service": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -409,21 +364,18 @@
           {
             "name": "deleted",
             "type": "category",
             "path": "deleted"
           }
         ]
       },
-      "column_suggest": {
-        "type": "completion"
-      },
-      "schema_suggest": {
+      "chart_suggest": {
         "type": "completion"
       },
-      "database_suggest": {
+      "data_model_suggest": {
         "type": "completion"
       },
       "service_suggest": {
         "type": "completion"
       }
     }
   }
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 """
 import textwrap
 
 TAG_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
   "settings": {
-   "analysis": {
+    "analysis": {
       "normalizer": {
         "lowercase_normalizer": {
           "type": "custom",
           "char_filter": [],
           "filter": [
             "lowercase"
           ]
@@ -30,14 +30,19 @@
       "analyzer": {
         "om_analyzer": {
           "tokenizer": "letter",
           "filter": [
             "lowercase",
             "om_stemmer"
           ]
+        },
+        "om_ngram": {
+          "tokenizer": "ngram",
+          "min_gram": 1,
+          "max_gram": 2
         }
       },
       "filter": {
         "om_stemmer": {
           "type": "stemmer",
           "name": "english"
         }
@@ -46,29 +51,37 @@
   },
   "mappings": {
     "properties": {
       "id": {
         "type": "text"
       },
       "name": {
-        "type": "text",
+        "type": "keyword",
         "fields": {
           "keyword": {
             "type": "keyword",
             "ignore_above": 256
           }
         }
       },
       "fullyQualifiedName": {
         "type": "keyword",
         "normalizer": "lowercase_normalizer"
       },
-      "description": {
+      "displayName": {
         "type": "text",
-        "analyzer": "om_analyzer"
+        "fields": {
+          "keyword": {
+            "type": "keyword",
+            "ignore_above": 256
+          }
+        }
+      },
+      "description": {
+        "type": "text"
       },
       "version": {
         "type": "float"
       },
       "updatedAt": {
         "type": "date",
         "format": "epoch_second"
@@ -78,17 +91,14 @@
       },
       "href": {
         "type": "text"
       },
       "deleted": {
         "type": "text"
       },
-      "deprecated": {
-        "type": "boolean"
-      },
       "suggest": {
         "type": "completion",
         "contexts": [
           {
             "name": "deleted",
             "type": "category",
             "path": "deleted"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,19 +5,19 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Defines the Elasticsearch mapping for Teams
+Defines the Elasticsearch mapping for Users
 """
 import textwrap
 
-TEAM_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
+USER_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
   "mappings": {
     "properties": {
       "id": {
         "type": "text"
       },
@@ -38,34 +38,43 @@
         "fields": {
           "keyword": {
             "type": "keyword",
             "ignore_above": 256
           }
         }
       },
-      "teamType": {
-        "type": "text"
-      },
-      "description": {
-        "type": "text"
-      },
       "version": {
         "type": "float"
       },
       "updatedAt": {
         "type": "date",
         "format": "epoch_second"
       },
       "updatedBy": {
         "type": "text"
       },
       "href": {
         "type": "text"
       },
-      "users": {
+      "email": {
+        "type": "text",
+        "fields": {
+          "keyword": {
+            "type": "keyword",
+            "ignore_above": 256
+          }
+        }
+      },
+      "isAdmin": {
+        "type": "boolean"
+      },
+      "isBot": {
+        "type": "boolean"
+      },
+      "teams": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -94,15 +103,15 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "parents": {
+      "roles": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -131,15 +140,15 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "defaultRoles": {
+      "inheritedRoles": {
         "properties": {
           "id": {
             "type": "keyword",
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 36
@@ -168,17 +177,14 @@
             "type": "text"
           },
           "href": {
             "type": "text"
           }
         }
       },
-      "isJoinable": {
-        "type": "text"
-      },
       "deleted": {
         "type": "text"
       },
       "entityType": {
         "type": "keyword"
       },
       "suggest": {
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py`

 * *Files 11% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 Defines the Elasticsearch mapping for Topics
 """
 import textwrap
 
 TOPIC_ELASTICSEARCH_INDEX_MAPPING = textwrap.dedent(
     """
 {
-"settings": {
-   "analysis": {
+  "settings": {
+    "analysis": {
       "normalizer": {
         "lowercase_normalizer": {
           "type": "custom",
           "char_filter": [],
           "filter": [
             "lowercase"
           ]
@@ -30,14 +30,19 @@
       "analyzer": {
         "om_analyzer": {
           "tokenizer": "letter",
           "filter": [
             "lowercase",
             "om_stemmer"
           ]
+        },
+        "om_ngram": {
+          "tokenizer": "ngram",
+          "min_gram": 1,
+          "max_gram": 2
         }
       },
       "filter": {
         "om_stemmer": {
           "type": "stemmer",
           "name": "english"
         }
@@ -55,23 +60,30 @@
           "keyword": {
             "type": "keyword",
             "ignore_above": 256
           }
         }
       },
       "fullyQualifiedName": {
-        "type": "text"
+        "type": "keyword",
+        "normalizer": "lowercase_normalizer"
       },
       "displayName": {
         "type": "text",
-        "analyzer": "om_analyzer"
+        "analyzer": "om_analyzer",
+        "fields": {
+          "ngram": {
+            "type": "text",
+            "analyzer": "om_ngram"
+          }
+        }
       },
       "description": {
         "type": "text",
-         "analyzer": "om_analyzer"
+        "analyzer": "om_analyzer"
       },
       "version": {
         "type": "float"
       },
       "updatedAt": {
         "type": "date",
         "format": "epoch_second"
@@ -80,21 +92,20 @@
         "type": "text"
       },
       "href": {
         "type": "text"
       },
       "messageSchema": {
         "properties": {
-          "schemaType": {
-            "type": "keyword",
-            "normalizer": "lowercase_normalizer"
-          },
           "schemaText": {
             "type": "text"
           },
+          "schemaType": {
+            "type": "text"
+          },
           "schemaFields": {
             "properties": {
               "name": {
                 "type": "keyword",
                 "normalizer": "lowercase_normalizer",
                 "fields": {
                   "keyword": {
@@ -104,15 +115,17 @@
                 }
               },
               "dataType": {
                 "type": "text"
               },
               "description": {
                 "type": "text",
-                "analyzer": "om_analyzer"
+                "index_options": "docs",
+                "analyzer": "om_analyzer",
+                "norms": false
               },
               "fullyQualifiedName": {
                 "type": "text"
               },
               "tags": {
                 "properties": {
                   "tagFQN": {
@@ -127,14 +140,64 @@
                   "source": {
                     "type": "keyword"
                   },
                   "state": {
                     "type": "keyword"
                   }
                 }
+              },
+              "children": {
+                "properties": {
+                  "id": {
+                    "type": "keyword",
+                    "fields": {
+                      "keyword": {
+                        "type": "keyword",
+                        "ignore_above": 36
+                      }
+                    }
+                  },
+                  "type": {
+                    "type": "keyword"
+                  },
+                  "name": {
+                    "type": "keyword",
+                    "fields": {
+                      "keyword": {
+                        "type": "keyword",
+                        "ignore_above": 256
+                      }
+                    }
+                  },
+                  "fullyQualifiedName": {
+                    "type": "text"
+                  },
+                  "description": {
+                    "type": "text"
+                  },
+                  "tags": {
+                    "properties": {
+                      "tagFQN": {
+                        "type": "keyword"
+                      },
+                      "labelType": {
+                        "type": "keyword"
+                      },
+                      "description": {
+                        "type": "text"
+                      },
+                      "source": {
+                        "type": "keyword"
+                      },
+                      "state": {
+                        "type": "keyword"
+                      }
+                    }
+                  }
+                }
               }
             }
           }
         }
       },
       "cleanupPolicies": {
         "type": "keyword"
@@ -204,14 +267,23 @@
             "fields": {
               "keyword": {
                 "type": "keyword",
                 "ignore_above": 256
               }
             }
           },
+          "displayName": {
+            "type": "keyword",
+            "fields": {
+              "keyword": {
+                "type": "keyword",
+                "ignore_above": 256
+              }
+            }
+          },
           "fullyQualifiedName": {
             "type": "text"
           },
           "description": {
             "type": "text"
           },
           "deleted": {
@@ -278,14 +350,17 @@
           {
             "name": "deleted",
             "type": "category",
             "path": "deleted"
           }
         ]
       },
+      "field_suggest": {
+        "type": "completion"
+      },
       "service_suggest": {
         "type": "completion"
       }
     }
   }
 }
 """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,46 +11,46 @@
 """
 Defines the Elasticsearch mapping for web analytic entity views
 """
 import textwrap
 
 WEB_ANALYTIC_ENTITY_VIEW_REPORT_DATA_INDEX_MAPPING = textwrap.dedent(
     """
-    {
-        "mappings": {
-            "properties": {
-                "timestamp": {
-                    "type": "date"
-                },
-                "reportDataType": {
-                    "type": "keyword"
-                },
-                "data": {
-                    "properties": {
-                        "entityType": {
-                            "type": "keyword"
-                        },
-                        "entityHref": {
-                            "type": "keyword"
-                        },
-                        "entityTier": {
-                            "type": "keyword"
-                        },
-                        "entityFqn": {
-                            "type": "keyword"
-                        },
-                        "owner": {
-                            "type": "keyword"
-                        },
-                        "ownerId": {
-                            "type": "text"
-                        },
-                        "views": {
-                            "type": "integer"
-                        }
-                    }
-                }
-            }
+{
+  "mappings": {
+    "properties": {
+      "timestamp": {
+        "type": "date"
+      },
+      "reportDataType": {
+        "type": "keyword"
+      },
+      "data": {
+        "properties": {
+          "entityType": {
+            "type": "keyword"
+          },
+          "entityTier": {
+            "type": "keyword"
+          },
+          "entityFqn": {
+            "type": "keyword"
+          },
+          "entityHref": {
+            "type": "keyword"
+        },
+          "owner": {
+            "type": "keyword"
+          },
+          "ownerId": {
+            "type": "text"
+          },
+          "views": {
+            "type": "integer"
+          }
         }
+      }
     }
+  }
+}
     """
 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,46 +11,46 @@
 """
 Defines the Elasticsearch mapping for web analytic users
 """
 import textwrap
 
 WEB_ANALYTIC_USER_ACTIVITY_REPORT_DATA_INDEX_MAPPING = textwrap.dedent(
     """
-    {
-        "mappings": {
-            "properties": {
-                "timestamp": {
-                    "type": "date"
-                },
-                "reportDataType": {
-                    "type": "keyword"
-                },
-                "data": {
-                    "properties": {
-                        "userName": {
-                            "type": "keyword"
-                        },
-                        "userId": {
-                            "type": "text"
-                        },
-                        "team": {
-                            "type": "keyword"
-                        },
-                        "totalSessions": {
-                            "type": "integer"
-                        },
-                        "totalSessionDuration": {
-                            "type": "double"
-                        },
-                        "totalPageView": {
-                            "type": "integer"
-                        },
-                        "lastSession": {
-                            "type": "long"
-                        }
-                    }
-                }
-            }
+{
+  "mappings": {
+    "properties": {
+      "timestamp": {
+        "type": "date"
+      },
+      "reportDataType": {
+        "type": "keyword"
+      },
+      "data": {
+        "properties": {
+          "userName": {
+            "type": "keyword"
+          },
+          "userId": {
+            "type": "text"
+          },
+          "team": {
+            "type": "keyword"
+          },
+          "totalSessions": {
+            "type": "integer"
+          },
+          "totalSessionDuration": {
+            "type": "double"
+          },
+          "totalPageView": {
+            "type": "integer"
+          },
+          "lastSession": {
+            "type": "long"
+          }
         }
+      }
     }
+  }
+}
     """
 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/file.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/file.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,15 +53,14 @@
 
     @classmethod
     def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
         config = FileSinkConfig.parse_obj(config_dict)
         return cls(config, metadata_config)
 
     def write_record(self, record: Entity) -> None:
-
         if self.wrote_something:
             self.file.write(",\n")
 
         self.file.write(record.json())
         self.wrote_something = True
         self.status.records_written(record)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/sink/metadata_rest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/sink/metadata_rest.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,15 +21,14 @@
 from requests.exceptions import HTTPError
 
 from metadata.config.common import ConfigModel
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.api.teams.createRole import CreateRoleRequest
 from metadata.generated.schema.api.teams.createTeam import CreateTeamRequest
 from metadata.generated.schema.api.teams.createUser import CreateUserRequest
-from metadata.generated.schema.entity.data.location import Location
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.teams.role import Role
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.ingestion.api.common import Entity
@@ -45,18 +44,15 @@
     OMetaTestCaseSample,
     OMetaTestSuiteSample,
 )
 from metadata.ingestion.models.user import OMetaUserProfile
 from metadata.ingestion.ometa.client import APIError
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardUsage
-from metadata.ingestion.source.database.database_service import (
-    DataModelLink,
-    TableLocationLink,
-)
+from metadata.ingestion.source.database.database_service import DataModelLink
 from metadata.utils.helpers import calculate_execution_time
 from metadata.utils.logger import get_add_lineage_log_str, ingestion_logger
 
 logger = ingestion_logger()
 
 # Allow types from the generated pydantic models
 T = TypeVar("T", bound=BaseModel)
@@ -93,15 +89,14 @@
         self.write_record = singledispatch(self.write_record)
         self.write_record.register(AddLineageRequest, self.write_lineage)
         self.write_record.register(OMetaUserProfile, self.write_users)
         self.write_record.register(OMetaTagAndClassification, self.write_classification)
         self.write_record.register(DeleteEntity, self.delete_entity)
         self.write_record.register(OMetaPipelineStatus, self.write_pipeline_status)
         self.write_record.register(DataModelLink, self.write_datamodel)
-        self.write_record.register(TableLocationLink, self.write_table_location_link)
         self.write_record.register(DashboardUsage, self.write_dashboard_usage)
         self.write_record.register(OMetaTableConstraints, self.write_table_constraints)
         self.write_record.register(
             OMetaTableProfileSampleData, self.write_profile_sample_data
         )
         self.write_record.register(OMetaTestSuiteSample, self.write_test_suite_sample)
         self.write_record.register(OMetaTestCaseSample, self.write_test_case_sample)
@@ -163,46 +158,26 @@
         if table:
             self.metadata.ingest_table_data_model(
                 table=table, data_model=datamodel_link.datamodel
             )
             logger.debug(
                 f"Successfully ingested DataModel for {table.fullyQualifiedName.__root__}"
             )
+            self.status.records_written(
+                f"DataModel: {table.fullyQualifiedName.__root__}"
+            )
         else:
             logger.warning("Unable to ingest datamodel")
 
-    def write_table_location_link(self, table_location_link: TableLocationLink) -> None:
-        """
-        Send to OM the Table and Location Link based on FQNs
-        :param table_location_link: Table FQN + Location FQN
-        """
-        try:
-            table = self.metadata.get_by_name(
-                entity=Table, fqn=table_location_link.table_fqn
-            )
-            location = self.metadata.get_by_name(
-                entity=Location, fqn=table_location_link.location_fqn
-            )
-            self.metadata.add_location(table=table, location=location)
-        except Exception as exc:
-            name = f"{table_location_link.table_fqn} <-> {table_location_link.location_fqn}"
-            error = (
-                f"Failed to write table location link [{table_location_link}]: {exc}"
-            )
-            logger.debug(traceback.format_exc())
-            logger.warning(error)
-            self.status.failed(name, error, traceback.format_exc())
-
     def write_dashboard_usage(self, dashboard_usage: DashboardUsage) -> None:
         """
         Send a UsageRequest update to a dashboard entity
         :param dashboard_usage: dashboard entity and usage request
         """
         try:
-
             self.metadata.publish_dashboard_usage(
                 dashboard=dashboard_usage.dashboard,
                 dashboard_usage_request=dashboard_usage.usage,
             )
             logger.debug(
                 f"Successfully ingested usage for {dashboard_usage.dashboard.fullyQualifiedName.__root__}"
             )
@@ -338,15 +313,14 @@
             logger.debug(f"User: {user.displayName}")
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.error(f"Unexpected error writing user [{metadata_user}]: {exc}")
 
     def delete_entity(self, record: DeleteEntity):
         try:
-
             self.metadata.delete(
                 entity=type(record.entity),
                 entity_id=record.entity.id,
                 recursive=record.mark_deleted_entities,
             )
             logger.debug(
                 f"{record.entity.name} doesn't exist in source state, marking it as deleted"
@@ -389,15 +363,15 @@
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Unexpected error writing profile sample data [{record}]: {exc}"
             )
 
     def write_test_suite_sample(self, record: OMetaTestSuiteSample):
         """
-        Use the /testSuite endpoint to ingest sample test suite
+        Use the /testSuites endpoint to ingest sample test suite
         """
         try:
             self.metadata.create_or_update(record.test_suite)
             logger.debug(
                 f"Successfully created test Suite {record.test_suite.name.__root__}"
             )
             self.status.records_written(f"testSuite: {record.test_suite.name.__root__}")
@@ -405,29 +379,29 @@
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Unexpected error writing test suite sample [{record}]: {exc}"
             )
 
     def write_test_case_sample(self, record: OMetaTestCaseSample):
         """
-        Use the /testCase endpoint to ingest sample test suite
+        Use the /dataQuality/testCases endpoint to ingest sample test suite
         """
         try:
             self.metadata.create_or_update(record.test_case)
             logger.debug(
                 f"Successfully created test case {record.test_case.name.__root__}"
             )
             self.status.records_written(f"testCase: {record.test_case.name.__root__}")
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.error(f"Unexpected error writing test case sample [{record}]: {exc}")
 
     def write_test_case_results_sample(self, record: OMetaTestCaseResultsSample):
         """
-        Use the /testCase endpoint to ingest sample test suite
+        Use the /dataQuality/testCases endpoint to ingest sample test suite
         """
         try:
             self.metadata.add_test_case_results(
                 record.test_case_results,
                 record.test_case_name,
             )
             logger.debug(
@@ -440,15 +414,15 @@
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Unexpected error writing test case result sample [{record}]: {exc}"
             )
 
     def write_topic_sample_data(self, record: OMetaTopicSampleData):
         """
-        Use the /testCase endpoint to ingest sample test suite
+        Use the /dataQuality/testCases endpoint to ingest sample test suite
         """
         try:
             if record.sample_data.messages:
                 self.metadata.ingest_topic_sample_data(
                     record.topic,
                     record.sample_data,
                 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/connections.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/connections.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/dashboard_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,375 +5,289 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Base class for ingesting dashboard services
+Redash source module
 """
 import traceback
-from abc import ABC, abstractmethod
-from typing import Any, Iterable, List, Optional, Set
+from typing import Iterable, List, Optional
 
-from pydantic import BaseModel
+from packaging import version
 
+from metadata.generated.schema.api.classification.createClassification import (
+    CreateClassificationRequest,
+)
+from metadata.generated.schema.api.classification.createTag import CreateTagRequest
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.chart import Chart
-from metadata.generated.schema.entity.data.dashboard import Dashboard
+from metadata.generated.schema.entity.data.dashboard import (
+    Dashboard as LineageDashboard,
+)
 from metadata.generated.schema.entity.data.table import Table
+from metadata.generated.schema.entity.services.connections.dashboard.redashConnection import (
+    RedashConnection,
+)
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.dashboardService import (
-    DashboardConnection,
-    DashboardService,
-)
-from metadata.generated.schema.entity.teams.user import User
-from metadata.generated.schema.metadataIngestion.dashboardServiceMetadataPipeline import (
-    DashboardServiceMetadataPipeline,
-)
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.entityLineage import EntitiesEdge
 from metadata.generated.schema.type.entityReference import EntityReference
-from metadata.generated.schema.type.usageRequest import UsageRequest
-from metadata.ingestion.api.source import Source
-from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
-from metadata.ingestion.models.delete_entity import (
-    DeleteEntity,
-    delete_entity_from_source,
-)
+from metadata.ingestion.api.source import InvalidSourceException
+from metadata.ingestion.lineage.parser import LineageParser
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
-from metadata.ingestion.models.topology import (
-    NodeStage,
-    ServiceTopology,
-    TopologyNode,
-    create_source_context,
-)
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
-from metadata.utils import fqn
-from metadata.utils.filters import filter_by_dashboard
+from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
+from metadata.utils import fqn, tag_utils
+from metadata.utils.filters import filter_by_chart
+from metadata.utils.helpers import clean_uri, get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
+REDASH_TAG_CATEGORY = "RedashTags"
 
-class DashboardUsage(BaseModel):
-    """
-    Wrapper to handle type at the sink
-    """
-
-    dashboard: Dashboard
-    usage: UsageRequest
+INCOMPATIBLE_REDASH_VERSION = "8.0.0"
 
 
-class DashboardServiceTopology(ServiceTopology):
+class RedashSource(DashboardServiceSource):
     """
-    Defines the hierarchy in Dashboard Services.
-    service -> dashboard -> charts.
-
-    We could have a topology validator. We can only consume
-    data that has been produced by any parent node.
+    Redash Source Class
     """
 
-    root = TopologyNode(
-        producer="get_services",
-        stages=[
-            NodeStage(
-                type_=DashboardService,
-                context="dashboard_service",
-                processor="yield_create_request_dashboard_service",
-                overwrite=False,
-                must_return=True,
-            ),
-            NodeStage(
-                type_=OMetaTagAndClassification,
-                context="tags",
-                processor="yield_tag",
-                ack_sink=False,
-                nullable=True,
-            ),
-        ],
-        children=["dashboard"],
-        post_process=["mark_dashboards_as_deleted"],
-    )
-    dashboard = TopologyNode(
-        producer="get_dashboard",
-        stages=[
-            NodeStage(
-                type_=Chart,
-                context="charts",
-                processor="yield_dashboard_chart",
-                consumer=["dashboard_service"],
-                nullable=True,
-                cache_all=True,
-                clear_cache=True,
-            ),
-            NodeStage(
-                type_=Dashboard,
-                context="dashboard",
-                processor="yield_dashboard",
-                consumer=["dashboard_service"],
-            ),
-            NodeStage(
-                type_=User,
-                context="owner",
-                processor="process_owner",
-                consumer=["dashboard_service"],
-            ),
-            NodeStage(
-                type_=AddLineageRequest,
-                context="lineage",
-                processor="yield_dashboard_lineage",
-                consumer=["dashboard_service"],
-                ack_sink=False,
-                nullable=True,
-            ),
-            NodeStage(
-                type_=UsageRequest,
-                context="usage",
-                processor="yield_dashboard_usage",
-                consumer=["dashboard_service"],
-                ack_sink=False,
-                nullable=True,
-            ),
-        ],
-    )
-
-
-class DashboardServiceSource(TopologyRunnerMixin, Source, ABC):
-    """
-    Base class for Database Services.
-    It implements the topology and context.
-    """
-
-    source_config: DashboardServiceMetadataPipeline
-    config: WorkflowSource
-    metadata: OpenMetadata
-    # Big union of types we want to fetch dynamically
-    service_connection: DashboardConnection.__fields__["config"].type_
-
-    topology = DashboardServiceTopology()
-    context = create_source_context(topology)
-    dashboard_source_state: Set = set()
-
     def __init__(
         self,
         config: WorkflowSource,
         metadata_config: OpenMetadataConnection,
     ):
-        super().__init__()
-        self.config = config
-        self.metadata_config = metadata_config
-        self.metadata = OpenMetadata(metadata_config)
-        self.service_connection = self.config.serviceConnection.__root__.config
-        self.source_config: DashboardServiceMetadataPipeline = (
-            self.config.sourceConfig.config
-        )
-        self.client = get_connection(self.service_connection)
-
-        # Flag the connection for the test connection
-        self.connection_obj = self.client
-        self.test_connection()
-
-        self.metadata_client = OpenMetadata(self.metadata_config)
+        super().__init__(config, metadata_config)
+        self.dashboard_list = []  # We will populate this in `prepare`
+        self.tags = []  # To create the tags before yielding final entities
+
+    @classmethod
+    def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: RedashConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, RedashConnection):
+            raise InvalidSourceException(
+                f"Expected RedashConnection, but got {connection}"
+            )
+        return cls(config, metadata_config)
 
-    @abstractmethod
-    def yield_dashboard(
-        self, dashboard_details: Any
-    ) -> Iterable[CreateDashboardRequest]:
+    def prepare(self):
         """
-        Method to Get Dashboard Entity
+        Fetch the paginated list of dashboards and tags
         """
 
-    @abstractmethod
-    def yield_dashboard_lineage_details(
-        self, dashboard_details: Any, db_service_name: str
-    ) -> Optional[Iterable[AddLineageRequest]]:
-        """
-        Get lineage between dashboard and data sources
-        """
+        self.dashboard_list = self.client.paginate(self.client.dashboards)
 
-    @abstractmethod
-    def yield_dashboard_chart(
-        self, dashboard_details: Any
-    ) -> Optional[Iterable[CreateChartRequest]]:
-        """
-        Method to fetch charts linked to dashboard
-        """
+        # Collecting all the tags
+        if self.source_config.includeTags:
+            for dashboard in self.dashboard_list:
+                self.tags.extend(dashboard.get("tags") or [])
+
+    def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
+        """
+        Fetch Dashboard Tags
+        """
+        if self.source_config.includeTags:
+            for tag in self.tags:
+                try:
+                    classification = OMetaTagAndClassification(
+                        classification_request=CreateClassificationRequest(
+                            name=REDASH_TAG_CATEGORY,
+                            description="Tags associates with redash entities",
+                        ),
+                        tag_request=CreateTagRequest(
+                            classification=REDASH_TAG_CATEGORY,
+                            name=tag,
+                            description="Redash Tag",
+                        ),
+                    )
+                    yield classification
+                    logger.info(
+                        f"Classification {REDASH_TAG_CATEGORY}, Tag {tag} Ingested"
+                    )
+                except Exception as exc:
+                    logger.debug(traceback.format_exc())
+                    logger.warning(f"Error ingesting tag {tag}: {exc}")
 
-    @abstractmethod
-    def get_dashboards_list(self) -> Optional[List[Any]]:
+    def get_dashboards_list(self) -> Optional[List[dict]]:
         """
         Get List of all dashboards
         """
 
-    @abstractmethod
-    def get_dashboard_name(self, dashboard: Any) -> str:
-        """
-        Get Dashboard Name from each element coming from `get_dashboards_list`
-        """
+        return self.dashboard_list
 
-    @abstractmethod
-    def get_dashboard_details(self, dashboard: Any) -> Any:
+    def get_dashboard_name(self, dashboard: dict) -> str:
         """
-        Get Dashboard Details
+        Get Dashboard Name
         """
+        return dashboard["name"]
 
-    def yield_dashboard_lineage(
-        self, dashboard_details: Any
-    ) -> Optional[Iterable[AddLineageRequest]]:
+    def get_dashboard_details(self, dashboard: dict) -> dict:
         """
-        Yields lineage if config is enabled.
-
-        We will look for the data in all the services
-        we have informed.
+        Get Dashboard Details
         """
-        for db_service_name in self.source_config.dbServiceNames or []:
-            yield from self.yield_dashboard_lineage_details(
-                dashboard_details, db_service_name
-            ) or []
+        return self.client.get_dashboard(dashboard["slug"])
 
-    def yield_tag(
-        self, *args, **kwargs  # pylint: disable=W0613
-    ) -> Optional[Iterable[OMetaTagAndClassification]]:
-        """
-        Method to fetch dashboard tags
-        """
-        return  # Dashboard does not support fetching tags except Tableau and Redash
+    def get_owner_details(self, dashboard_details) -> Optional[EntityReference]:
+        """Get dashboard owner
 
-    def yield_dashboard_usage(
-        self, *args, **kwargs  # pylint: disable=W0613
-    ) -> Optional[Iterable[DashboardUsage]]:
-        """
-        Method to pick up dashboard usage data
+        Args:
+            dashboard_details:
+        Returns:
+            Optional[EntityReference]
         """
-        return  # Dashboard usage currently only available for Looker
-
-    def close(self):
-        self.metadata.close()
-
-    def get_services(self) -> Iterable[WorkflowSource]:
-        yield self.config
+        if dashboard_details.get("user") and dashboard_details["user"].get("email"):
+            user = self.metadata.get_user_by_email(
+                dashboard_details["user"].get("email")
+            )
+            if user:
+                return EntityReference(id=user.id.__root__, type="user")
+        return None
 
-    def yield_create_request_dashboard_service(self, config: WorkflowSource):
-        yield self.metadata.get_create_service_from_source(
-            entity=DashboardService, config=config
-        )
+    def get_dashboard_url(self, dashboard_details: dict) -> str:
+        if version.parse(self.service_connection.redashVersion) > version.parse(
+            INCOMPATIBLE_REDASH_VERSION
+        ):
+            dashboard_url = (
+                f"{clean_uri(self.service_connection.hostPort)}/dashboards"
+                f"/{dashboard_details.get('id', '')}"
+            )
+        else:
+            dashboard_url = (
+                f"{clean_uri(self.service_connection.hostPort)}/dashboards"
+                f"/{dashboard_details.get('slug', '')}"
+            )
+        return dashboard_url
 
-    def mark_dashboards_as_deleted(self) -> Iterable[DeleteEntity]:
+    def yield_dashboard(
+        self, dashboard_details: dict
+    ) -> Iterable[CreateDashboardRequest]:
         """
-        Method to mark the dashboards as deleted
+        Method to Get Dashboard Entity
         """
-        if self.source_config.markDeletedDashboards:
-            logger.info("Mark Deleted Dashboards set to True")
-            yield from delete_entity_from_source(
-                metadata=self.metadata,
-                entity_type=Dashboard,
-                entity_source_state=self.dashboard_source_state,
-                mark_deleted_entity=self.source_config.markDeletedDashboards,
-                params={
-                    "service": self.context.dashboard_service.fullyQualifiedName.__root__
-                },
-            )
-
-    def process_owner(self, dashboard_details):
         try:
-            owner = self.get_owner_details(  # pylint: disable=assignment-from-none
-                dashboard_details=dashboard_details
+            dashboard_description = ""
+            for widgets in dashboard_details.get("widgets") or []:
+                dashboard_description = widgets.get("text")
+
+            dashboard_request = CreateDashboardRequest(
+                name=dashboard_details["id"],
+                displayName=dashboard_details.get("name"),
+                description=dashboard_description,
+                charts=[
+                    fqn.build(
+                        self.metadata,
+                        entity_type=Chart,
+                        service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
+                        chart_name=chart.name.__root__,
+                    )
+                    for chart in self.context.charts
+                ],
+                service=self.context.dashboard_service.fullyQualifiedName.__root__,
+                dashboardUrl=self.get_dashboard_url(dashboard_details),
+                tags=tag_utils.get_tag_labels(
+                    metadata=self.metadata,
+                    tags=dashboard_details.get("tags"),
+                    classification_name=REDASH_TAG_CATEGORY,
+                    include_tags=self.source_config.includeTags,
+                ),
             )
-            if owner and self.source_config.overrideOwner:
-                self.metadata.patch_owner(
-                    entity=Dashboard,
-                    entity_id=self.context.dashboard.id,
-                    owner=owner,
-                    force=True,
-                )
+            yield dashboard_request
+            self.register_record(dashboard_request=dashboard_request)
+
         except Exception as exc:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Error processing owner for {dashboard_details}: {exc}")
+            logger.warning(f"Error to yield dashboard for {dashboard_details}: {exc}")
 
-    def register_record(self, dashboard_request: CreateDashboardRequest) -> None:
+    def yield_dashboard_lineage_details(
+        self, dashboard_details: dict, db_service_name: str
+    ) -> Optional[Iterable[AddLineageRequest]]:
         """
-        Mark the dashboard record as scanned and update the dashboard_source_state
+        Get lineage between dashboard and data sources
+        In redash we do not get table, database_schema or database name but we do get query
+        the lineage is being generated based on the query
         """
-        dashboard_fqn = fqn.build(
+
+        to_fqn = fqn.build(
             self.metadata,
-            entity_type=Dashboard,
-            service_name=dashboard_request.service.__root__,
-            dashboard_name=dashboard_request.name.__root__,
+            entity_type=LineageDashboard,
+            service_name=self.config.serviceName,
+            dashboard_name=str(dashboard_details.get("id")),
         )
-
-        self.dashboard_source_state.add(dashboard_fqn)
-        self.status.scanned(dashboard_fqn)
-
-    def get_owner_details(  # pylint: disable=useless-return
-        self, dashboard_details  # pylint: disable=unused-argument
-    ) -> Optional[EntityReference]:
-        """Get dashboard owner
-
-        Args:
-            dashboard_details:
-        Returns:
-            Optional[EntityReference]
-        """
-        logger.debug(
-            f"Processing ownership is not supported for {self.service_connection.type.name}"
+        to_entity = self.metadata.get_by_name(
+            entity=LineageDashboard,
+            fqn=to_fqn,
         )
-        return None
-
-    @staticmethod
-    def _get_add_lineage_request(
-        to_entity: Dashboard, from_entity: Table
-    ) -> Optional[AddLineageRequest]:
-        if from_entity and to_entity:
-            return AddLineageRequest(
-                edge=EntitiesEdge(
-                    fromEntity=EntityReference(
-                        id=from_entity.id.__root__, type="table"
-                    ),
-                    toEntity=EntityReference(
-                        id=to_entity.id.__root__, type="dashboard"
-                    ),
+        for widgets in dashboard_details.get("widgets") or []:
+            try:
+                visualization = widgets.get("visualization")
+                if not visualization:
+                    continue
+                if visualization.get("query", {}).get("query"):
+                    lineage_parser = LineageParser(visualization["query"]["query"])
+                    for table in lineage_parser.source_tables:
+                        table_name = str(table)
+                        database_schema_table = fqn.split_table_name(table_name)
+                        from_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Table,
+                            service_name=db_service_name,
+                            schema_name=database_schema_table.get("database_schema"),
+                            table_name=database_schema_table.get("table"),
+                            database_name=database_schema_table.get("database"),
+                        )
+                        from_entity = self.metadata.get_by_name(
+                            entity=Table,
+                            fqn=from_fqn,
+                        )
+                        if from_entity and to_entity:
+                            yield self._get_add_lineage_request(
+                                to_entity=to_entity, from_entity=from_entity
+                            )
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
                 )
-            )
-        return None
 
-    def get_dashboard(self) -> Any:
+    def yield_dashboard_chart(
+        self, dashboard_details: dict
+    ) -> Optional[Iterable[CreateChartRequest]]:
         """
-        Method to iterate through dashboard lists filter dashboards & yield dashboard details
+        Metod to fetch charts linked to dashboard
         """
-        for dashboard in self.get_dashboards_list():
-
-            dashboard_name = self.get_dashboard_name(dashboard)
-            if filter_by_dashboard(
-                self.source_config.dashboardFilterPattern,
-                dashboard_name,
-            ):
-                self.status.filter(
-                    dashboard_name,
-                    "Dashboard Filtered Out",
-                )
-                continue
-
+        for widgets in dashboard_details.get("widgets") or []:
             try:
-                dashboard_details = self.get_dashboard_details(dashboard)
+                visualization = widgets.get("visualization")
+                chart_display_name = str(
+                    visualization["query"]["name"] if visualization else widgets["id"]
+                )
+                if filter_by_chart(
+                    self.source_config.chartFilterPattern, chart_display_name
+                ):
+                    self.status.filter(chart_display_name, "Chart Pattern not allowed")
+                    continue
+                yield CreateChartRequest(
+                    name=widgets["id"],
+                    displayName=chart_display_name
+                    if visualization and visualization["query"]
+                    else "",
+                    chartType=get_standard_chart_type(
+                        visualization["type"] if visualization else ""
+                    ),
+                    service=self.context.dashboard_service.fullyQualifiedName.__root__,
+                    chartUrl=self.get_dashboard_url(dashboard_details),
+                    description=visualization["description"] if visualization else "",
+                )
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(
-                    f"Cannot extract dashboard details from {dashboard}: {exc}"
+                    f"Error to yield dashboard chart for widget_id: {widgets['id']} and {dashboard_details}: {exc}"
                 )
-                continue
-
-            yield dashboard_details
-
-    def test_connection(self) -> None:
-        test_connection_fn = get_test_connection_fn(self.service_connection)
-        test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
-
-    def prepare(self):
-        pass
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/domodashboard/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/domodashboard/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/looker/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/looker/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/dashboard_service.py`

 * *Files 18% similar despite different names*

```diff
@@ -5,483 +5,465 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Looker source module.
-Supports:
-- owner
-- lineage
-- usage
-
-Notes:
-- Filtering is applied on the Dashboard title or ID, if the title is missing
+Base class for ingesting dashboard services
 """
-
 import traceback
-from datetime import datetime
-from typing import Iterable, List, Optional, Set, cast
+from abc import ABC, abstractmethod
+from typing import Any, Iterable, List, Optional, Set, Union
 
-from looker_sdk.error import SDKError
-from looker_sdk.sdk.api31.models import Query
-from looker_sdk.sdk.api40.methods import Looker40SDK
-from looker_sdk.sdk.api40.models import Dashboard as LookerDashboard
-from looker_sdk.sdk.api40.models import (
-    DashboardBase,
-    DashboardElement,
-    LookmlModelExplore,
-)
+from pydantic import BaseModel
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
+from metadata.generated.schema.api.data.createDashboardDataModel import (
+    CreateDashboardDataModelRequest,
+)
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.chart import Chart
-from metadata.generated.schema.entity.data.dashboard import (
-    Dashboard as MetadataDashboard,
-)
+from metadata.generated.schema.entity.data.dashboard import Dashboard
+from metadata.generated.schema.entity.data.dashboardDataModel import DashboardDataModel
 from metadata.generated.schema.entity.data.table import Table
-from metadata.generated.schema.entity.services.connections.dashboard.lookerConnection import (
-    LookerConnection,
-)
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
+from metadata.generated.schema.entity.services.dashboardService import (
+    DashboardConnection,
+    DashboardService,
+)
+from metadata.generated.schema.entity.teams.user import User
+from metadata.generated.schema.metadataIngestion.dashboardServiceMetadataPipeline import (
+    DashboardServiceMetadataPipeline,
+)
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.entityLineage import EntitiesEdge
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.generated.schema.type.usageRequest import UsageRequest
-from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.dashboard.dashboard_service import (
-    DashboardServiceSource,
-    DashboardUsage,
+from metadata.ingestion.api.source import Source
+from metadata.ingestion.api.topology_runner import C, TopologyRunnerMixin
+from metadata.ingestion.models.delete_entity import (
+    DeleteEntity,
+    delete_entity_from_source,
+)
+from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
+from metadata.ingestion.models.topology import (
+    NodeStage,
+    ServiceTopology,
+    TopologyNode,
+    create_source_context,
 )
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.utils import fqn
-from metadata.utils.filters import filter_by_chart
-from metadata.utils.helpers import get_standard_chart_type
+from metadata.utils.filters import filter_by_dashboard
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-LIST_DASHBOARD_FIELDS = ["id", "title"]
+class DashboardUsage(BaseModel):
+    """
+    Wrapper to handle type at the sink
+    """
 
-# Here we can update the fields to get further information, such as:
-# created_at, updated_at, last_updater_id, deleted_at, deleter_id, favorite_count, last_viewed_at
-GET_DASHBOARD_FIELDS = [
-    "id",
-    "title",
-    "dashboard_elements",
-    "dashboard_filters",
-    "view_count",
-    "description",
-    "folder",
-    "user_id",  # Use as owner
-]
+    dashboard: Dashboard
+    usage: UsageRequest
 
 
-class LookerSource(DashboardServiceSource):
+class DashboardServiceTopology(ServiceTopology):
     """
-    Looker Source Class.
+    Defines the hierarchy in Dashboard Services.
+    service -> data models -> dashboard -> charts.
 
-    Its client uses Looker 40 from the SDK: client = looker_sdk.init40()
+    We could have a topology validator. We can only consume
+    data that has been produced by any parent node.
     """
 
+    root = TopologyNode(
+        producer="get_services",
+        stages=[
+            NodeStage(
+                type_=DashboardService,
+                context="dashboard_service",
+                processor="yield_create_request_dashboard_service",
+                overwrite=False,
+                must_return=True,
+            ),
+            NodeStage(
+                type_=OMetaTagAndClassification,
+                context="tags",
+                processor="yield_tag",
+                ack_sink=False,
+                nullable=True,
+            ),
+        ],
+        children=["bulk_data_model", "dashboard"],
+        post_process=["mark_dashboards_as_deleted"],
+    )
+    # Dashboard Services have very different approaches when
+    # when dealing with data models. Tableau has the models
+    # tightly coupled with dashboards, while Looker
+    # handles them as independent entities.
+    # When configuring a new source, we will either implement
+    # the yield_bulk_datamodel or yield_datamodel functions.
+    bulk_data_model = TopologyNode(
+        producer="list_datamodels",
+        stages=[
+            NodeStage(
+                type_=DashboardDataModel,
+                context="dataModel",
+                processor="yield_bulk_datamodel",
+                consumer=["dashboard_service"],
+            )
+        ],
+    )
+    dashboard = TopologyNode(
+        producer="get_dashboard",
+        stages=[
+            NodeStage(
+                type_=Chart,
+                context="charts",
+                processor="yield_dashboard_chart",
+                consumer=["dashboard_service"],
+                nullable=True,
+                cache_all=True,
+                clear_cache=True,
+            ),
+            NodeStage(
+                type_=DashboardDataModel,
+                context="dataModels",
+                processor="yield_datamodel",
+                consumer=["dashboard_service"],
+                nullable=True,
+                cache_all=True,
+                clear_cache=True,
+            ),
+            NodeStage(
+                type_=Dashboard,
+                context="dashboard",
+                processor="yield_dashboard",
+                consumer=["dashboard_service"],
+            ),
+            NodeStage(
+                type_=User,
+                context="owner",
+                processor="process_owner",
+                consumer=["dashboard_service"],
+            ),
+            NodeStage(
+                type_=AddLineageRequest,
+                context="lineage",
+                processor="yield_dashboard_lineage",
+                consumer=["dashboard_service"],
+                ack_sink=False,
+                nullable=True,
+            ),
+            NodeStage(
+                type_=UsageRequest,
+                context="usage",
+                processor="yield_dashboard_usage",
+                consumer=["dashboard_service"],
+                ack_sink=False,
+                nullable=True,
+            ),
+        ],
+    )
+
+
+# pylint: disable=too-many-public-methods
+class DashboardServiceSource(TopologyRunnerMixin, Source, ABC):
+    """
+    Base class for Database Services.
+    It implements the topology and context.
+    """
+
+    source_config: DashboardServiceMetadataPipeline
     config: WorkflowSource
-    metadata_config: OpenMetadataConnection
-    client: Looker40SDK
+    metadata: OpenMetadata
+    # Big union of types we want to fetch dynamically
+    service_connection: DashboardConnection.__fields__["config"].type_
+
+    topology = DashboardServiceTopology()
+    context = create_source_context(topology)
+    dashboard_source_state: Set = set()
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata_config: OpenMetadataConnection,
     ):
-        super().__init__(config, metadata_config)
-        self.today = datetime.now().strftime("%Y-%m-%d")
+        super().__init__()
+        self.config = config
+        self.metadata_config = metadata_config
+        self.metadata = OpenMetadata(metadata_config)
+        self.service_connection = self.config.serviceConnection.__root__.config
+        self.source_config: DashboardServiceMetadataPipeline = (
+            self.config.sourceConfig.config
+        )
+        self.client = get_connection(self.service_connection)
 
-        # Owners cache. The key will be the user_id and the value its OM user EntityRef
-        self._owners_ref = {}
+        # Flag the connection for the test connection
+        self.connection_obj = self.client
+        self.test_connection()
 
-    @classmethod
-    def create(
-        cls, config_dict: dict, metadata_config: OpenMetadataConnection
-    ) -> "LookerSource":
-        config = WorkflowSource.parse_obj(config_dict)
-        connection: LookerConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, LookerConnection):
-            raise InvalidSourceException(
-                f"Expected LookerConnection, but got {connection}"
-            )
-        return cls(config, metadata_config)
+        self.metadata_client = OpenMetadata(self.metadata_config)
 
-    def get_dashboards_list(self) -> List[DashboardBase]:
+    @abstractmethod
+    def yield_dashboard(
+        self, dashboard_details: Any
+    ) -> Iterable[CreateDashboardRequest]:
         """
-        Get List of all dashboards
+        Method to Get Dashboard Entity
         """
-        try:
-            return list(
-                self.client.all_dashboards(fields=",".join(LIST_DASHBOARD_FIELDS))
-            )
-        except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Wild error trying to obtain dashboard list {err}")
-            # If we cannot list the dashboards, let's blow up
-            raise err
 
-    def get_dashboard_name(self, dashboard: DashboardBase) -> str:
+    @abstractmethod
+    def yield_dashboard_lineage_details(
+        self, dashboard_details: Any, db_service_name: str
+    ) -> Optional[Iterable[AddLineageRequest]]:
         """
-        Get Dashboard Title. This will be used for filtering.
-        If the title is not present, we'll send the ID
+        Get lineage between dashboard and data sources
         """
-        return dashboard.title or dashboard.id
 
-    def get_dashboard_details(self, dashboard: DashboardBase) -> LookerDashboard:
+    @abstractmethod
+    def yield_dashboard_chart(
+        self, dashboard_details: Any
+    ) -> Optional[Iterable[CreateChartRequest]]:
         """
-        Get Dashboard Details
+        Method to fetch charts linked to dashboard
         """
-        return self.client.dashboard(
-            dashboard_id=dashboard.id, fields=",".join(GET_DASHBOARD_FIELDS)
-        )
-
-    def get_owner_details(
-        self, dashboard_details: LookerDashboard
-    ) -> Optional[EntityReference]:
-        """Get dashboard owner
 
-        Store the visited users in the _owners_ref cache, even if we found them
-        in OM or not.
-
-        If the user has not yet been visited, store it and return from cache.
+    @abstractmethod
+    def get_dashboards_list(self) -> Optional[List[Any]]:
+        """
+        Get List of all dashboards
+        """
 
-        Args:
-            dashboard_details: LookerDashboard
-        Returns:
-            Optional[EntityReference]
+    @abstractmethod
+    def get_dashboard_name(self, dashboard: Any) -> str:
+        """
+        Get Dashboard Name from each element coming from `get_dashboards_list`
         """
-        try:
-            if (
-                dashboard_details.user_id is not None
-                and dashboard_details.user_id not in self._owners_ref
-            ):
-                dashboard_owner = self.client.user(dashboard_details.user_id)
-                user = self.metadata.get_user_by_email(dashboard_owner.email)
-                if user:  # Save the EntityRef
-                    self._owners_ref[dashboard_details.user_id] = EntityReference(
-                        id=user.id, type="user"
-                    )
-                else:  # Otherwise, flag the user as missing in OM
-                    self._owners_ref[dashboard_details.user_id] = None
-                    logger.debug(
-                        f"User {dashboard_owner.email} not found in OpenMetadata."
-                    )
 
-        except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Could not fetch owner data due to {err}")
+    @abstractmethod
+    def get_dashboard_details(self, dashboard: Any) -> Any:
+        """
+        Get Dashboard Details
+        """
 
-        return self._owners_ref.get(dashboard_details.user_id)
+    def list_datamodels(self) -> Iterable[Any]:
+        """
+        Optional Node producer for processing datamodels in bulk
+        before the dashboards
+        """
+        return []
 
-    def yield_dashboard(
-        self, dashboard_details: LookerDashboard
-    ) -> CreateDashboardRequest:
+    def yield_datamodel(self, _) -> Optional[Iterable[CreateDashboardDataModelRequest]]:
         """
-        Method to Get Dashboard Entity
+        Method to fetch DataModel linked to Dashboard
         """
 
-        dashboard_request = CreateDashboardRequest(
-            name=dashboard_details.id.replace("::", "_"),
-            displayName=dashboard_details.title,
-            description=dashboard_details.description or None,
-            charts=[
-                fqn.build(
-                    self.metadata,
-                    entity_type=Chart,
-                    service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
-                    chart_name=chart.name.__root__,
-                )
-                for chart in self.context.charts
-            ],
-            dashboardUrl=f"/dashboards/{dashboard_details.id}",
-            service=self.context.dashboard_service.fullyQualifiedName.__root__,
+        logger.debug(
+            f"DataModel is not supported for {self.service_connection.type.name}"
         )
-        yield dashboard_request
-        self.register_record(dashboard_request=dashboard_request)
 
-    @staticmethod
-    def _clean_table_name(table_name: str) -> str:
+    def yield_bulk_datamodel(
+        self, _
+    ) -> Optional[Iterable[CreateDashboardDataModelRequest]]:
         """
-        sql_table_names might be renamed when defining
-        an explore. E.g., customers as cust
-        :param table_name: explore table name
-        :return: clean table name
+        Method to fetch DataModels in bulk
         """
 
-        return table_name.lower().split("as")[0].strip()
+        logger.debug(
+            f"DataModel is not supported for {self.service_connection.type.name}"
+        )
 
-    def _add_sql_table(self, query: Query, dashboard_sources: Set[str]):
+    def yield_dashboard_lineage(
+        self, dashboard_details: Any
+    ) -> Optional[Iterable[AddLineageRequest]]:
         """
-        Add the SQL table information to the dashboard_sources.
+        Yields lineage if config is enabled.
 
-        Updates the seen dashboards.
+        We will look for the data in all the services
+        we have informed.
 
-        :param query: Looker query, from a look or result_maker
-        :param dashboard_sources: seen tables so far
+        TODO: This we'll need to not make it dependant
+          on the dbServiceNames since our lineage will now be
+          model -> dashboard
         """
-        try:
-            explore: LookmlModelExplore = self.client.lookml_model_explore(
-                query.model, query.view
-            )
-            table_name = explore.sql_table_name
-
-            if table_name:
-                dashboard_sources.add(self._clean_table_name(table_name))
+        for db_service_name in self.source_config.dbServiceNames or []:
+            yield from self.yield_dashboard_lineage_details(
+                dashboard_details, db_service_name
+            ) or []
 
-        except SDKError as err:
-            logger.debug(traceback.format_exc())
-            logger.warning(
-                f"Cannot get explore from model={query.model}, view={query.view}: {err}"
-            )
-
-    def get_dashboard_sources(self, dashboard_details: LookerDashboard) -> Set[str]:
+    def yield_tag(
+        self, *args, **kwargs  # pylint: disable=W0613
+    ) -> Optional[Iterable[OMetaTagAndClassification]]:
         """
-        Set of source tables to build lineage for the processed dashboard
+        Method to fetch dashboard tags
         """
-        dashboard_sources: Set[str] = set()
-
-        for chart in cast(
-            Iterable[DashboardElement], dashboard_details.dashboard_elements
-        ):
-            if chart.query and chart.query.view:
-                self._add_sql_table(chart.query, dashboard_sources)
-            if chart.look and chart.look.query and chart.look.query.view:
-                self._add_sql_table(chart.look.query, dashboard_sources)
-            if (
-                chart.result_maker
-                and chart.result_maker.query
-                and chart.result_maker.query.view
-            ):
-                self._add_sql_table(chart.result_maker.query, dashboard_sources)
-
-        return dashboard_sources
+        return  # Dashboard does not support fetching tags except Tableau and Redash
 
-    def yield_dashboard_lineage_details(
-        self, dashboard_details: LookerDashboard, db_service_name: str
-    ) -> Optional[Iterable[AddLineageRequest]]:
+    def yield_dashboard_usage(
+        self, *args, **kwargs  # pylint: disable=W0613
+    ) -> Optional[Iterable[DashboardUsage]]:
         """
-        Get lineage between charts and data sources.
-
-        We look at:
-        - chart.query
-        - chart.look (chart.look.query)
-        - chart.result_maker
+        Method to pick up dashboard usage data
         """
-        datasource_list = self.get_dashboard_sources(dashboard_details)
+        return  # Dashboard usage currently only available for Looker
 
-        to_fqn = fqn.build(
-            self.metadata,
-            entity_type=MetadataDashboard,
-            service_name=self.config.serviceName,
-            dashboard_name=dashboard_details.id.replace("::", "_"),
-        )
-        to_entity = self.metadata.get_by_name(
-            entity=MetadataDashboard,
-            fqn=to_fqn,
-        )
+    def close(self):
+        self.metadata.close()
 
-        for source in datasource_list:
-            try:
-                yield self.build_lineage_request(
-                    source=source,
-                    db_service_name=db_service_name,
-                    to_entity=to_entity,
-                )
+    def get_services(self) -> Iterable[WorkflowSource]:
+        yield self.config
 
-            except (Exception, IndexError) as err:
-                logger.debug(traceback.format_exc())
-                logger.warning(
-                    f"Error building lineage for database service [{db_service_name}]: {err}"
-                )
+    def yield_create_request_dashboard_service(self, config: WorkflowSource):
+        yield self.metadata.get_create_service_from_source(
+            entity=DashboardService, config=config
+        )
 
-    def build_lineage_request(
-        self, source: str, db_service_name: str, to_entity: MetadataDashboard
-    ) -> Optional[AddLineageRequest]:
+    def mark_dashboards_as_deleted(self) -> Iterable[DeleteEntity]:
         """
-        Once we have a list of origin data sources, check their components
-        and build the lineage request.
-
-        We will try searching in ES with and without the `database`
-
-        Args:
-            source: table name from the source list
-            db_service_name: name of the service from the config
-            to_entity: Dashboard Entity being used
-        """
-
-        source_elements = fqn.split_table_name(table_name=source)
-
-        for database_name in [source_elements["database"], None]:
-
-            from_fqn = fqn.build(
-                self.metadata,
-                entity_type=Table,
-                service_name=db_service_name,
-                database_name=database_name,
-                schema_name=source_elements["database_schema"],
-                table_name=source_elements["table"],
+        Method to mark the dashboards as deleted
+        """
+        if self.source_config.markDeletedDashboards:
+            logger.info("Mark Deleted Dashboards set to True")
+            yield from delete_entity_from_source(
+                metadata=self.metadata,
+                entity_type=Dashboard,
+                entity_source_state=self.dashboard_source_state,
+                mark_deleted_entity=self.source_config.markDeletedDashboards,
+                params={
+                    "service": self.context.dashboard_service.fullyQualifiedName.__root__
+                },
             )
 
-            from_entity: Table = self.metadata.get_by_name(
-                entity=Table,
-                fqn=from_fqn,
+    def process_owner(self, dashboard_details):
+        try:
+            owner = self.get_owner_details(  # pylint: disable=assignment-from-none
+                dashboard_details=dashboard_details
             )
-
-            if from_entity:
-                return self._get_add_lineage_request(
-                    to_entity=to_entity, from_entity=from_entity
+            if owner and self.source_config.overrideOwner:
+                self.metadata.patch_owner(
+                    entity=Dashboard,
+                    entity_id=self.context.dashboard.id,
+                    owner=owner,
+                    force=True,
                 )
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Error processing owner for {dashboard_details}: {exc}")
 
-        return None
-
-    def yield_dashboard_chart(
-        self, dashboard_details: LookerDashboard
-    ) -> Optional[Iterable[CreateChartRequest]]:
+    def register_record(self, dashboard_request: CreateDashboardRequest) -> None:
         """
-        Method to fetch charts linked to dashboard
+        Mark the dashboard record as scanned and update the dashboard_source_state
         """
-        for chart in dashboard_details.dashboard_elements:
-            try:
-                if filter_by_chart(
-                    chart_filter_pattern=self.source_config.chartFilterPattern,
-                    chart_name=chart.id,
-                ):
-                    self.status.filter(chart.id, "Chart filtered out")
-                    continue
-
-                if not chart.id:
-                    logger.debug(f"Found chart {chart} without id. Skipping.")
-                    continue
-
-                yield CreateChartRequest(
-                    name=chart.id,
-                    displayName=chart.title or chart.id,
-                    description=self.build_chart_description(chart) or None,
-                    chartType=get_standard_chart_type(chart.type).value,
-                    chartUrl=f"/dashboard_elements/{chart.id}",
-                    service=self.context.dashboard_service.fullyQualifiedName.__root__,
-                )
-                self.status.scanned(chart.id)
+        dashboard_fqn = fqn.build(
+            self.metadata,
+            entity_type=Dashboard,
+            service_name=dashboard_request.service.__root__,
+            dashboard_name=dashboard_request.name.__root__,
+        )
 
-            except Exception as exc:
-                logger.debug(traceback.format_exc())
-                logger.warning(f"Error creating chart [{chart}]: {exc}")
+        self.dashboard_source_state.add(dashboard_fqn)
+        self.status.scanned(dashboard_fqn)
 
-    @staticmethod
-    def build_chart_description(chart: DashboardElement) -> Optional[str]:
-        """
-        Chart descriptions will be based on the subtitle + note_text, if exists.
-        If the chart is a text tile, we will add the text as the chart description as well.
-        This should keep the dashboard searchable without breaking the original metadata structure.
+    def get_owner_details(  # pylint: disable=useless-return
+        self, dashboard_details  # pylint: disable=unused-argument
+    ) -> Optional[EntityReference]:
+        """Get dashboard owner
+
+        Args:
+            dashboard_details:
+        Returns:
+            Optional[EntityReference]
         """
+        logger.debug(
+            f"Processing ownership is not supported for {self.service_connection.type.name}"
+        )
+        return None
 
-        # If the string is None or empty, filter it out.
-        try:
-            return "; ".join(
-                filter(
-                    lambda string: string,
-                    [chart.subtitle_text, chart.body_text, chart.note_text],
+    @staticmethod
+    def _get_add_lineage_request(
+        to_entity: Union[Dashboard, DashboardDataModel],
+        from_entity: Union[Table, DashboardDataModel],
+    ) -> Optional[AddLineageRequest]:
+        if from_entity and to_entity:
+            return AddLineageRequest(
+                edge=EntitiesEdge(
+                    fromEntity=EntityReference(
+                        id=from_entity.id.__root__,
+                        type="table"
+                        if isinstance(from_entity, Table)
+                        else "dashboardDataModel",
+                    ),
+                    toEntity=EntityReference(
+                        id=to_entity.id.__root__,
+                        type="dashboard"
+                        if isinstance(to_entity, Dashboard)
+                        else "dashboardDataModel",
+                    ),
                 )
-                or []
             )
-        except Exception as err:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Error getting chart description: {err}")
-            return None
+        return None
 
-    def yield_dashboard_usage(  # pylint: disable=W0221
-        self, dashboard_details: LookerDashboard
-    ) -> Optional[DashboardUsage]:
-        """
-        The dashboard.view_count gives us the total number of views. However, we need to
-        pass the views for each day (execution).
-
-        In this function we will first validate if the usageSummary
-        returns us some usage for today's date. If so, we will stop the
-        execution.
-
-        Otherwise, we will add the difference between the usage from the last time
-        the usage was reported and today's view_count from the dashboard.
-
-        Example usage summary from OM API:
-        "usageSummary": {
-            "dailyStats": {
-                "count": 51,
-                "percentileRank": 0.0
-            },
-            "date": "2022-06-23",
-            "monthlyStats": {
-                "count": 105,
-                "percentileRank": 0.0
-            },
-            "weeklyStats": {
-                "count": 105,
-                "percentileRank": 0.0
-            }
-        },
-        :param dashboard_details: Looker Dashboard
-        :return: UsageRequest, if not computed
+    def get_dashboard(self) -> Any:
         """
-
-        dashboard: MetadataDashboard = self.context.dashboard
-
-        try:
-            current_views = dashboard_details.view_count
-
-            if not current_views:
-                logger.debug(f"No usage to report for {dashboard_details.title}")
-
-            if not dashboard.usageSummary:
-                logger.info(
-                    f"Yielding fresh usage for {dashboard.fullyQualifiedName.__root__}"
+        Method to iterate through dashboard lists filter dashboards & yield dashboard details
+        """
+        for dashboard in self.get_dashboards_list():
+            dashboard_name = self.get_dashboard_name(dashboard)
+            if filter_by_dashboard(
+                self.source_config.dashboardFilterPattern,
+                dashboard_name,
+            ):
+                self.status.filter(
+                    dashboard_name,
+                    "Dashboard Filtered Out",
                 )
-                yield DashboardUsage(
-                    dashboard=dashboard,
-                    usage=UsageRequest(date=self.today, count=current_views),
+                continue
+
+            try:
+                dashboard_details = self.get_dashboard_details(dashboard)
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Cannot extract dashboard details from {dashboard}: {exc}"
                 )
+                continue
 
-            elif (
-                str(dashboard.usageSummary.date.__root__) != self.today
-                or not dashboard.usageSummary.dailyStats.count
-            ):
+            yield dashboard_details
 
-                latest_usage = dashboard.usageSummary.dailyStats.count
+    def test_connection(self) -> None:
+        test_connection_fn = get_test_connection_fn(self.service_connection)
+        test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
 
-                new_usage = current_views - latest_usage
-                if new_usage < 0:
-                    raise ValueError(
-                        f"Wrong computation of usage difference. Got new_usage={new_usage}."
-                    )
+    def prepare(self):
+        pass
 
-                logger.info(
-                    f"Yielding new usage for {dashboard.fullyQualifiedName.__root__}"
-                )
-                yield DashboardUsage(
-                    dashboard=dashboard,
-                    usage=UsageRequest(
-                        date=self.today, count=current_views - latest_usage
-                    ),
-                )
+    def fqn_from_context(self, stage: NodeStage, entity_request: C) -> str:
+        """
+        We are overriding this method since CreateDashboardDataModelRequest needs to add an extra value to the context
+        names.
 
-            else:
-                logger.debug(
-                    f"Latest usage {dashboard.usageSummary} vs. today {self.today}. Nothing to compute."
-                )
-                logger.info(
-                    f"Usage already informed for {dashboard.fullyQualifiedName.__root__}"
-                )
+        Read the context
+        :param stage: Topology node being processed
+        :param entity_request: Request sent to the sink
+        :return: Entity FQN derived from context
+        """
+        context_names = [
+            self.context.__dict__[dependency].name.__root__
+            for dependency in stage.consumer or []  # root nodes do not have consumers
+        ]
 
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.warning(
-                f"Exception computing dashboard usage for {dashboard.fullyQualifiedName.__root__}: {exc}"
-            )
+        if isinstance(entity_request, CreateDashboardDataModelRequest):
+            context_names.append("model")
+
+        return fqn._build(  # pylint: disable=protected-access
+            *context_names, entity_request.name.__root__
+        )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/connection.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,79 +8,54 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-import json
-from typing import Any, Dict, Optional
 
-import requests
+from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.dashboard.metabaseConnection import (
-    MetabaseConnection,
+from metadata.generated.schema.entity.services.connections.dashboard.redashConnection import (
+    RedashConnection,
 )
 from metadata.ingestion.connections.test_connections import (
     SourceConnectionException,
     test_connection_steps,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.dashboard.redash.client import RedashApiClient
 
 
-def get_connection(connection: MetabaseConnection) -> Dict[str, Any]:
+def get_connection(connection: RedashConnection) -> RedashApiClient:
     """
     Create connection
     """
     try:
-        params = {}
-        params["username"] = connection.username
-        params["password"] = connection.password.get_secret_value()
-
-        headers = {"Content-Type": "application/json", "Accept": "*/*"}
-
-        resp = requests.post(  # pylint: disable=missing-timeout
-            connection.hostPort + "/api/session/",
-            data=json.dumps(params),
-            headers=headers,
-        )
-
-        session_id = resp.json()["id"]
-        metabase_session = {"X-Metabase-Session": session_id}
-        conn = {"connection": connection, "metabase_session": metabase_session}
-        return conn
-
+        return RedashApiClient(connection)
     except Exception as exc:
         msg = f"Unknown error connecting with {connection}: {exc}."
         raise SourceConnectionException(msg) from exc
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client,
-    service_connection: MetabaseConnection,
+    client: RedashApiClient,
+    service_connection: RedashConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    def custom_executor():
-        result = requests.get(  # pylint: disable=missing-timeout
-            client["connection"].hostPort + "/api/dashboard",
-            headers=client["metabase_session"],
-        )
-
-        return list(result)
-
-    test_fn = {"GetDashboards": custom_executor}
+    test_fn = {"GetDashboards": client.dashboards}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_fqn=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/metabase/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/metadata.py`

 * *Files 11% similar despite different names*

```diff
@@ -9,16 +9,14 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """Metabase source module"""
 
 import traceback
 from typing import Iterable, List, Optional
 
-import requests
-
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.chart import Chart
 from metadata.generated.schema.entity.data.dashboard import (
     Dashboard as LineageDashboard,
 )
@@ -31,87 +29,82 @@
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.lineage.parser import LineageParser
 from metadata.ingestion.lineage.sql_lineage import search_table_entities
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
+from metadata.ingestion.source.dashboard.metabase.models import (
+    MetabaseChart,
+    MetabaseDashboard,
+    MetabaseDashboardDetails,
+)
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart
-from metadata.utils.helpers import get_standard_chart_type, replace_special_with
+from metadata.utils.helpers import (
+    clean_uri,
+    get_standard_chart_type,
+    replace_special_with,
+)
 from metadata.utils.logger import ingestion_logger
 
-HEADERS = {"Content-Type": "application/json", "Accept": "*/*"}
-
 logger = ingestion_logger()
 
 
 class MetabaseSource(DashboardServiceSource):
     """
     Metabase Source Class
     """
 
     config: WorkflowSource
     metadata_config: OpenMetadataConnection
 
-    def __init__(
-        self,
-        config: WorkflowSource,
-        metadata_config: OpenMetadataConnection,
-    ):
-        super().__init__(config, metadata_config)
-        self.metabase_session = self.client["metabase_session"]
-
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config = WorkflowSource.parse_obj(config_dict)
         connection: MetabaseConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, MetabaseConnection):
             raise InvalidSourceException(
                 f"Expected MetabaseConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def get_dashboards_list(self) -> Optional[List[dict]]:
+    def get_dashboards_list(self) -> Optional[List[MetabaseDashboard]]:
         """
         Get List of all dashboards
         """
-        resp_dashboards = self.req_get("/api/dashboard")
-        if resp_dashboards.status_code == 200:
-            return resp_dashboards.json()
-        return []
+        return self.client.get_dashboards_list()
 
-    def get_dashboard_name(self, dashboard: dict) -> str:
+    def get_dashboard_name(self, dashboard: MetabaseDashboard) -> str:
         """
         Get Dashboard Name
         """
-        return dashboard["name"]
+        return dashboard.name
 
-    def get_dashboard_details(self, dashboard: dict) -> dict:
+    def get_dashboard_details(self, dashboard: MetabaseDashboard) -> dict:
         """
         Get Dashboard Details
         """
-        resp_dashboard = self.req_get(f"/api/dashboard/{dashboard['id']}")
-        return resp_dashboard.json()
+        return self.client.get_dashboard_details(dashboard.id)
 
     def yield_dashboard(
-        self, dashboard_details: dict
+        self, dashboard_details: MetabaseDashboardDetails
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity
         """
         dashboard_url = (
-            f"/dashboard/{dashboard_details['id']}-"
-            f"{replace_special_with(raw=dashboard_details['name'].lower(), replacement='-')}"
+            f"{clean_uri(self.service_connection.hostPort)}/dashboard/{dashboard_details.id}-"
+            f"{replace_special_with(raw=dashboard_details.name.lower(), replacement='-')}"
         )
         dashboard_request = CreateDashboardRequest(
-            name=dashboard_details["id"],
+            name=dashboard_details.id,
             dashboardUrl=dashboard_url,
-            displayName=dashboard_details.get("name"),
-            description=dashboard_details.get("description", ""),
+            displayName=dashboard_details.name,
+            description=dashboard_details.description,
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
@@ -119,193 +112,132 @@
             ],
             service=self.context.dashboard_service.fullyQualifiedName.__root__,
         )
         yield dashboard_request
         self.register_record(dashboard_request=dashboard_request)
 
     def yield_dashboard_chart(
-        self, dashboard_details: dict
+        self, dashboard_details: MetabaseDashboardDetails
     ) -> Optional[Iterable[CreateChartRequest]]:
         """Get chart method
 
         Args:
             dashboard_details:
         Returns:
             Iterable[CreateChartRequest]
         """
-        charts = dashboard_details["ordered_cards"]
+        charts = dashboard_details.ordered_cards
         for chart in charts:
             try:
-                chart_details = chart["card"]
-                if "id" not in chart_details:
+                chart_details = chart.card
+                if not chart_details.id or not chart_details.name:
                     continue
                 chart_url = (
-                    f"/question/{chart_details['id']}-"
-                    f"{replace_special_with(raw=chart_details['name'].lower(), replacement='-')}"
+                    f"{clean_uri(self.service_connection.hostPort)}/question/{chart_details.id}-"
+                    f"{replace_special_with(raw=chart_details.name.lower(), replacement='-')}"
                 )
-
-                if "name" not in chart_details:
-                    continue
                 if filter_by_chart(
-                    self.source_config.chartFilterPattern, chart_details["name"]
+                    self.source_config.chartFilterPattern, chart_details.name
                 ):
-                    self.status.filter(
-                        chart_details["name"], "Chart Pattern not allowed"
-                    )
+                    self.status.filter(chart_details.name, "Chart Pattern not allowed")
                     continue
                 yield CreateChartRequest(
-                    name=chart_details["id"],
-                    displayName=chart_details.get("name"),
-                    description=chart_details.get("description", ""),
-                    chartType=get_standard_chart_type(
-                        str(chart_details["display"])
-                    ).value,
+                    name=chart_details.id,
+                    displayName=chart_details.name,
+                    description=chart_details.description,
+                    chartType=get_standard_chart_type(chart_details.display).value,
                     chartUrl=chart_url,
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                 )
-                self.status.scanned(chart_details["name"])
+                self.status.scanned(chart_details.name)
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Error creating chart [{chart}]: {exc}")
                 continue
 
     def yield_dashboard_lineage_details(
-        self, dashboard_details: dict, db_service_name
+        self,
+        dashboard_details: MetabaseDashboardDetails,
+        db_service_name: Optional[str],
     ) -> Optional[Iterable[AddLineageRequest]]:
         """Get lineage method
 
         Args:
             dashboard_details
         """
         if not db_service_name:
             return
         chart_list, dashboard_name = (
-            dashboard_details["ordered_cards"],
-            str(dashboard_details["id"]),
+            dashboard_details.ordered_cards,
+            str(dashboard_details.id),
         )
         for chart in chart_list:
             try:
-                chart_details = chart["card"]
+                chart_details = chart.card
                 if (
-                    "dataset_query" not in chart_details
-                    or "type" not in chart_details["dataset_query"]
+                    chart_details.dataset_query is None
+                    or chart_details.dataset_query.type is None
                 ):
                     continue
-                if chart_details["dataset_query"]["type"] == "native":
-                    if not chart_details.get("database_id"):
+                if chart_details.dataset_query.type == "native":
+                    if not chart_details.database_id:
                         continue
                     yield from self._yield_lineage_from_query(
                         chart_details=chart_details,
                         db_service_name=db_service_name,
                         dashboard_name=dashboard_name,
                     ) or []
 
                 # TODO: this method below only gets a single table, but if the chart of type query has a join the other
                 # table_ids will be ignored within a nested object
-                elif chart_details["dataset_query"]["type"] == "query":
-                    if not chart_details.get("table_id"):
+                elif chart_details.dataset_query.type == "query":
+                    if not chart_details.table_id:
                         continue
                     yield from self._yield_lineage_from_api(
                         chart_details=chart_details,
                         db_service_name=db_service_name,
                         dashboard_name=dashboard_name,
                     ) or []
 
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.error(f"Error creating chart [{chart}]: {exc}")
 
-    def req_get(self, path):
-        """Send get request method
-
-        Args:
-            path:
-        """
-        return requests.get(
-            self.service_connection.hostPort + path,
-            headers=self.metabase_session,
-            timeout=30,
-        )
-
     def _yield_lineage_from_query(
-        self, chart_details: dict, db_service_name: str, dashboard_name: str
+        self, chart_details: MetabaseChart, db_service_name: str, dashboard_name: str
     ) -> Optional[AddLineageRequest]:
-        resp_database = self.req_get(f"/api/database/{chart_details['database_id']}")
-        if resp_database.status_code == 200:
-            database = resp_database.json()
-            query = (
-                chart_details.get("dataset_query", {})
-                .get("native", {})
-                .get("query", "")
-            )
-            lineage_parser = LineageParser(query)
-            for table in lineage_parser.source_tables:
-                database_schema_name, table = fqn.split(str(table))[-2:]
-                database_schema_name = (
-                    None
-                    if database_schema_name == "<default>"
-                    else database_schema_name
-                )
-                database = database.get("details", {}).get("db", None)
-                if database:
-                    from_entities = search_table_entities(
-                        metadata=self.metadata,
-                        database=database,
-                        service_name=db_service_name,
-                        database_schema=database_schema_name,
-                        table=table,
-                    )
-                else:
-                    from_entities = search_table_entities(
-                        metadata=self.metadata,
-                        service_name=db_service_name,
-                        database=None,
-                        database_schema=database_schema_name,
-                        table=table,
-                    )
+        database = self.client.get_database(chart_details.database_id)
 
-                to_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=LineageDashboard,
-                    service_name=self.config.serviceName,
-                    dashboard_name=dashboard_name,
-                )
-                to_entity = self.metadata.get_by_name(
-                    entity=LineageDashboard,
-                    fqn=to_fqn,
-                )
+        query = None
+        if (
+            chart_details.dataset_query
+            and chart_details.dataset_query.native
+            and chart_details.dataset_query.native.query
+        ):
+            query = chart_details.dataset_query.native.query
 
-                for from_entity in from_entities:
-                    yield self._get_add_lineage_request(
-                        to_entity=to_entity, from_entity=from_entity
-                    )
+        if database is None or query is None:
+            return
 
-    def _yield_lineage_from_api(
-        self, chart_details: dict, db_service_name: str, dashboard_name: str
-    ) -> Optional[AddLineageRequest]:
-        resp_tables = self.req_get(f"/api/table/{chart_details['table_id']}")
-        if resp_tables.status_code == 200:
-            table = resp_tables.json()
-            database_name = table.get("db", {}).get("details", {}).get("db", None)
-            if database_name:
-                from_entities = search_table_entities(
-                    metadata=self.metadata,
-                    database=database_name,
-                    service_name=db_service_name,
-                    database_schema=table.get("schema"),
-                    table=table.get("display_name"),
-                )
-            else:
-                from_entities = search_table_entities(
-                    metadata=self.metadata,
-                    service_name=db_service_name,
-                    database=None,
-                    database_schema=table.get("schema"),
-                    table=table.get("display_name"),
-                )
+        database_name = database.details.db if database.details else None
+
+        lineage_parser = LineageParser(query)
+        for table in lineage_parser.source_tables:
+            database_schema_name, table = fqn.split(str(table))[-2:]
+            database_schema_name = (
+                None if database_schema_name == "<default>" else database_schema_name
+            )
+
+            from_entities = search_table_entities(
+                metadata=self.metadata,
+                database=database_name,
+                service_name=db_service_name,
+                database_schema=database_schema_name,
+                table=table,
+            )
 
             to_fqn = fqn.build(
                 self.metadata,
                 entity_type=LineageDashboard,
                 service_name=self.config.serviceName,
                 dashboard_name=dashboard_name,
             )
@@ -314,7 +246,41 @@
                 fqn=to_fqn,
             )
 
             for from_entity in from_entities:
                 yield self._get_add_lineage_request(
                     to_entity=to_entity, from_entity=from_entity
                 )
+
+    def _yield_lineage_from_api(
+        self, chart_details: MetabaseChart, db_service_name: str, dashboard_name: str
+    ) -> Optional[AddLineageRequest]:
+        table = self.client.get_table(chart_details.table_id)
+
+        if table is None or table.display_name is None:
+            return
+
+        database_name = table.db.details.db if table.db and table.db.details else None
+        from_entities = search_table_entities(
+            metadata=self.metadata,
+            database=database_name,
+            service_name=db_service_name,
+            database_schema=table.table_schema,
+            table=table.display_name,
+        )
+
+        to_fqn = fqn.build(
+            self.metadata,
+            entity_type=LineageDashboard,
+            service_name=self.config.serviceName,
+            dashboard_name=dashboard_name,
+        )
+
+        to_entity = self.metadata.get_by_name(
+            entity=LineageDashboard,
+            fqn=to_fqn,
+        )
+
+        for from_entity in from_entities:
+            yield self._get_add_lineage_request(
+                to_entity=to_entity, from_entity=from_entity
+            )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/mode/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/mode/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,194 +4,211 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""
-REST Auth & Client for PowerBi
-"""
+"""ml flow source module"""
+
+import ast
 import json
-import math
 import traceback
-from time import sleep
-from typing import List, Optional, Tuple
-
-import msal
+from typing import Iterable, List, Optional, Tuple, cast
 
+from mlflow.entities import RunData
+from mlflow.entities.model_registry import ModelVersion, RegisteredModel
+from pydantic import ValidationError
+
+from metadata.generated.schema.api.data.createMlModel import CreateMlModelRequest
+from metadata.generated.schema.entity.data.mlmodel import (
+    FeatureType,
+    MlFeature,
+    MlHyperParameter,
+    MlStore,
+)
+from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
+    OpenMetadataConnection,
+)
+from metadata.generated.schema.entity.services.connections.mlmodel.mlflowConnection import (
+    MlflowConnection,
+)
+from metadata.generated.schema.metadataIngestion.workflow import (
+    Source as WorkflowSource,
+)
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.ometa.client import REST, ClientConfig
-from metadata.utils.logger import utils_logger
+from metadata.ingestion.source.mlmodel.mlmodel_service import MlModelServiceSource
+from metadata.utils.filters import filter_by_mlmodel
+from metadata.utils.logger import ingestion_logger
 
-logger = utils_logger()
+logger = ingestion_logger()
 
 
-# Similar inner methods with mode client. That's fine.
-# pylint: disable=duplicate-code
-class PowerBiApiClient:
-    """
-    REST Auth & Client for PowerBi
+class MlflowSource(MlModelServiceSource):
     """
+    Source implementation to ingest MLFlow data.
 
-    client: REST
-
-    def __init__(self, config):
-        self.config = config
-        self.msal_client = msal.ConfidentialClientApplication(
-            client_id=self.config.clientId,
-            client_credential=self.config.clientSecret.get_secret_value(),
-            authority=self.config.authorityURI + self.config.tenantId,
-        )
-        self.auth_token = self.get_auth_token()
-        client_config = ClientConfig(
-            base_url="https://api.powerbi.com",
-            api_version="v1.0",
-            auth_token=lambda: self.auth_token,
-            auth_header="Authorization",
-            allow_redirects=True,
-        )
-        self.client = REST(client_config)
-
-    def get_auth_token(self) -> Tuple[str, str]:
-        """
-        Method to generate PowerBi access token
-        """
-        logger.info("Generating PowerBi access token")
-
-        auth_response = self.msal_client.acquire_token_silent(
-            scopes=self.config.scope, account=None
-        )
-
-        if not auth_response:
-            logger.info("Token does not exist in the cache. Getting a new token.")
-            auth_response = self.msal_client.acquire_token_for_client(
-                scopes=self.config.scope
-            )
+    We will iterate on the registered ML Models
+    and prepare an iterator of CreateMlModelRequest
+    """
 
-        if not auth_response.get("access_token"):
+    @classmethod
+    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: MlflowConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, MlflowConnection):
             raise InvalidSourceException(
-                "Failed to generate the PowerBi access token. Please check provided config"
+                f"Expected MlFlowConnection, but got {connection}"
             )
+        return cls(config, metadata_config)
 
-        logger.info("PowerBi Access Token generated successfully")
-        access_token = auth_response.get("access_token")
-        expiry = auth_response.get("expires_in")
-
-        return access_token, expiry
+    def get_mlmodels(  # pylint: disable=arguments-differ
+        self,
+    ) -> Iterable[Tuple[RegisteredModel, ModelVersion]]:
+        """
+        List and filters models from the registry
+        """
+        for model in cast(RegisteredModel, self.client.list_registered_models()):
+            if filter_by_mlmodel(
+                self.source_config.mlModelFilterPattern, mlmodel_name=model.name
+            ):
+                self.status.filter(
+                    model.name,
+                    "MlModel name pattern not allowed",
+                )
+                continue
+
+            # Get the latest version
+            latest_version: Optional[ModelVersion] = next(
+                (
+                    ver
+                    for ver in model.latest_versions
+                    if ver.last_updated_timestamp == model.last_updated_timestamp
+                ),
+                None,
+            )
+            if not latest_version:
+                self.status.failed(model.name, "Invalid version")
+                continue
+
+            yield model, latest_version
+
+    def _get_algorithm(self) -> str:  # pylint: disable=arguments-differ
+        logger.info("Setting algorithm with default value `mlmodel` for Mlflow")
+        return "mlmodel"
+
+    def yield_mlmodel(  # pylint: disable=arguments-differ
+        self, model_and_version: Tuple[RegisteredModel, ModelVersion]
+    ) -> Iterable[CreateMlModelRequest]:
+        """
+        Prepare the Request model
+        """
+        model, latest_version = model_and_version
+        self.status.scanned(model.name)
+
+        run = self.client.get_run(latest_version.run_id)
+
+        mlmodel_request = CreateMlModelRequest(
+            name=model.name,
+            description=model.description,
+            algorithm=self._get_algorithm(),  # Setting this to a constant
+            mlHyperParameters=self._get_hyper_params(run.data),
+            mlFeatures=self._get_ml_features(
+                run.data, latest_version.run_id, model.name
+            ),
+            mlStore=self._get_ml_store(latest_version),
+            service=self.context.mlmodel_service.fullyQualifiedName,
+        )
+        yield mlmodel_request
+        self.register_record(mlmodel_request=mlmodel_request)
 
-    def fetch_dashboards(self) -> Optional[dict]:
-        """Get dashboards method
-        Returns:
-            dict
+    def _get_hyper_params(  # pylint: disable=arguments-differ
+        self,
+        data: RunData,
+    ) -> Optional[List[MlHyperParameter]]:
         """
-        try:
-            return self.client.get("/myorg/admin/dashboards")
-        except Exception as exc:  # pylint: disable=broad-except
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Error fetching dashboards: {exc}")
-
-        return None
-
-    def fetch_all_workspaces(self) -> Optional[List[dict]]:
-        """Method to fetch all powerbi workspace details
-        Returns:
-            dict
+        Get the hyper parameters from the parameters
+        logged in the run data object.
         """
         try:
-            entities_per_page = min(100, self.config.pagination_entity_per_page)
-            params_data = {"$top": "1"}
-            response = self.client.get("/myorg/admin/groups", data=params_data)
-            count = response.get("@odata.count")
-            indexes = math.ceil(count / entities_per_page)
-
-            workspaces = []
-            for index in range(indexes):
-                params_data = {
-                    "$top": str(entities_per_page),
-                    "$skip": str(index * entities_per_page),
-                }
-                response = self.client.get("/myorg/admin/groups", data=params_data)
-                workspaces.extend(response.get("value"))
-            return workspaces
-        except Exception as exc:  # pylint: disable=broad-except
+            if data.params:
+                return [
+                    MlHyperParameter(name=param[0], value=param[1])
+                    for param in data.params.items()
+                ]
+        except ValidationError as err:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Error fetching workspaces: {exc}")
-        return None
-
-    def initiate_workspace_scan(self, workspace_ids: List[str]) -> Optional[dict]:
-        """Method to initiate workspace scan
-        Args:
-            workspace_ids:
-        Returns:
-            dict
-        """
-        try:
-            data = json.dumps({"workspaces": workspace_ids})
-            path = (
-                "/myorg/admin/workspaces/getInfo?"
-                "datasetExpressions=True&datasetSchema=True"
-                "&datasourceDetails=True&getArtifactUsers=True&lineage=True"
+            logger.warning(
+                f"Validation error adding hyper parameters from RunData: {data} - {err}"
             )
-            return self.client.post(path=path, data=data)
-        except Exception as exc:  # pylint: disable=broad-except
+        except Exception as err:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Error initiating workspace scan: {exc}")
+            logger.warning(
+                f"Wild error adding hyper parameters from RunData: {data} - {err}"
+            )
 
         return None
 
-    def fetch_workspace_scan_status(self, scan_id: str) -> Optional[dict]:
-        """Get Workspace scan status by id method
-        Args:
-            scan_id:
-        Returns:
-            dict
+    def _get_ml_store(  # pylint: disable=arguments-differ
+        self,
+        version: ModelVersion,
+    ) -> Optional[MlStore]:
         """
-        try:
-            return self.client.get(f"/myorg/admin/workspaces/scanStatus/{scan_id}")
-        except Exception as exc:  # pylint: disable=broad-except
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Error fetching workspace scan status: {exc}")
-
-        return None
-
-    def fetch_workspace_scan_result(self, scan_id: str) -> Optional[dict]:
-        """Get Workspace scan result by id method
-        Args:
-            scan_id:
-        Returns:
-            dict
+        Get the Ml Store from the model version object
         """
         try:
-            return self.client.get(f"/myorg/admin/workspaces/scanResult/{scan_id}")
-        except Exception as exc:  # pylint: disable=broad-except
+            if version.source:
+                return MlStore(storage=version.source)
+        except ValidationError as err:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Error fetching workspace scan result: {exc}")
-
+            logger.warning(
+                f"Validation error adding the MlModel store from ModelVersion: {version} - {err}"
+            )
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(
+                f"Wild error adding the MlModel store from ModelVersion: {version} - {err}"
+            )
         return None
 
-    def wait_for_scan_complete(self, scan_id, timeout=180) -> bool:
-        """
-        Method to poll the scan status endpoint until the timeout
-        """
-        min_sleep_time = 3
-        if min_sleep_time > timeout:
-            logger.info(f"Timeout is set to minimum sleep time: {timeout}")
-            timeout = min_sleep_time
-
-        max_poll = timeout // min_sleep_time
-        poll = 1
-        while True:
-            logger.info(f"Starting poll - {poll}/{max_poll}")
-            response = self.fetch_workspace_scan_status(scan_id=scan_id)
-            status = response.get("status")
-            if status:
-                if status.lower() == "succeeded":
-                    return True
-
-            if poll == max_poll:
-                break
-            logger.info(f"Sleeping for {min_sleep_time} seconds")
-            sleep(min_sleep_time)
-            poll += 1
+    def _get_ml_features(  # pylint: disable=arguments-differ
+        self, data: RunData, run_id: str, model_name: str
+    ) -> Optional[List[MlFeature]]:
+        """
+        The RunData object comes with stringified `tags`.
+        Let's transform those and try to extract the `signature`
+        information
+        """
+        if data.tags:
+            try:
+                props = json.loads(data.tags["mlflow.log-model.history"])
+                latest_props = next(
+                    (prop for prop in props if prop["run_id"] == run_id), None
+                )
+                if not latest_props:
+                    reason = f"Cannot find the run ID properties for {run_id}"
+                    logger.warning(reason)
+                    self.status.warning(model_name, reason)
+                    return None
+
+                if latest_props.get("signature") and latest_props["signature"].get(
+                    "inputs"
+                ):
+                    features = ast.literal_eval(latest_props["signature"]["inputs"])
+
+                    return [
+                        MlFeature(
+                            name=feature["name"],
+                            dataType=FeatureType.categorical
+                            if feature["type"] == "string"
+                            else FeatureType.numerical,
+                        )
+                        for feature in features
+                    ]
+
+            except Exception as exc:  # pylint: disable=broad-except
+                logger.debug(traceback.format_exc())
+                reason = f"Cannot extract properties from RunData: {exc}"
+                logger.warning(reason)
+                self.status.warning(model_name, reason)
 
-        return False
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/powerbi/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/powerbi/metadata.py`

 * *Files 15% similar despite different names*

```diff
@@ -26,22 +26,22 @@
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
+from metadata.ingestion.source.dashboard.powerbi.models import Dataset, PowerBIDashboard
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_chart, filter_by_dashboard
+from metadata.utils.helpers import clean_uri
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
-POWERBI_APP_URL = "https://app.powerbi.com"
-
 
 class PowerbiSource(DashboardServiceSource):
     """
     PowerBi Source Class
     """
 
     config: WorkflowSource
@@ -55,18 +55,60 @@
         super().__init__(config, metadata_config)
         self.pagination_entity_per_page = min(
             100, self.service_connection.pagination_entity_per_page
         )
         self.workspace_data = []
 
     def prepare(self):
-        # fetch all the workspace ids
+        if self.service_connection.useAdminApis:
+            self.get_admin_workspace_data()
+        else:
+            self.get_org_workspace_data()
+        return super().prepare()
+
+    def get_org_workspace_data(self):
+        """
+        fetch all the group workspace ids
+        """
+        groups = self.client.fetch_all_workspaces()
+        for group in groups:
+            # add the dashboards to the groups
+            group.dashboards.extend(
+                self.client.fetch_all_org_dashboards(group_id=group.id) or []
+            )
+            for dashboard in group.dashboards:
+                # add the tiles to the dashboards
+                dashboard.tiles.extend(
+                    self.client.fetch_all_org_tiles(
+                        group_id=group.id, dashboard_id=dashboard.id
+                    )
+                    or []
+                )
+
+            # add the datasets to the groups
+            group.datasets.extend(
+                self.client.fetch_all_org_datasets(group_id=group.id) or []
+            )
+            for dataset in group.datasets:
+                # add the tables to the datasets
+                dataset.tables.extend(
+                    self.client.fetch_dataset_tables(
+                        group_id=group.id, dataset_id=dataset.id
+                    )
+                    or []
+                )
+        self.workspace_data = groups
+
+    def get_admin_workspace_data(self):
+        """
+        fetch all the workspace ids
+        """
         workspaces = self.client.fetch_all_workspaces()
         if workspaces:
-            workspace_id_list = [workspace.get("id") for workspace in workspaces]
+            workspace_id_list = [workspace.id for workspace in workspaces]
 
             # Start the scan of the available workspaces for dashboard metadata
             workspace_paginated_list = [
                 workspace_id_list[i : i + self.pagination_entity_per_page]
                 for i in range(
                     0, len(workspace_id_list), self.pagination_entity_per_page
                 )
@@ -75,37 +117,35 @@
             for workspace_ids_chunk in workspace_paginated_list:
                 logger.info(
                     f"Scanning {count}/{len(workspace_paginated_list)} set of workspaces"
                 )
                 workspace_scan = self.client.initiate_workspace_scan(
                     workspace_ids_chunk
                 )
-                workspace_scan_id = workspace_scan.get("id")
 
                 # Keep polling the scan status endpoint to check if scan is succeeded
                 workspace_scan_status = self.client.wait_for_scan_complete(
-                    scan_id=workspace_scan_id
+                    scan_id=workspace_scan.id
                 )
                 if workspace_scan_status:
                     response = self.client.fetch_workspace_scan_result(
-                        scan_id=workspace_scan_id
+                        scan_id=workspace_scan.id
                     )
                     self.workspace_data.extend(
                         [
                             active_workspace
-                            for active_workspace in response.get("workspaces")
-                            if active_workspace.get("state") == "Active"
+                            for active_workspace in response.workspaces
+                            if active_workspace.state == "Active"
                         ]
                     )
                 else:
                     logger.error("Error in fetching dashboards and charts")
                 count += 1
         else:
             logger.error("Unable to fetch any Powerbi workspaces")
-        return super().prepare()
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config = WorkflowSource.parse_obj(config_dict)
         connection: PowerBIConnection = config.serviceConnection.__root__.config
         if not isinstance(connection, PowerBIConnection):
             raise InvalidSourceException(
@@ -137,47 +177,68 @@
                     self.status.filter(
                         dashboard_name,
                         "Dashboard Fltered Out",
                     )
                     continue
                 yield dashboard_details
 
-    def get_dashboards_list(self) -> Optional[List[dict]]:
+    def get_dashboards_list(self) -> Optional[List[PowerBIDashboard]]:
         """
         Get List of all dashboards
         """
-        return self.context.workspace.get("dashboards", [])
+        return self.context.workspace.dashboards
 
-    def get_dashboard_name(self, dashboard: dict) -> str:
+    def get_dashboard_name(self, dashboard: PowerBIDashboard) -> str:
         """
         Get Dashboard Name
         """
-        return dashboard["displayName"]
+        return dashboard.displayName
 
-    def get_dashboard_details(self, dashboard: dict) -> dict:
+    def get_dashboard_details(self, dashboard: PowerBIDashboard) -> PowerBIDashboard:
         """
         Get Dashboard Details
         """
         return dashboard
 
+    def get_dashboard_url(self, workspace_id: str, dashboard_id: str) -> str:
+        """
+        Method to build the dashboard url
+        """
+        return (
+            f"{clean_uri(self.service_connection.hostPort)}/groups/"
+            f"{workspace_id}/dashboards/{dashboard_id}"
+        )
+
+    def get_chart_url(
+        self, report_id: Optional[str], workspace_id: str, dashboard_id: str
+    ) -> str:
+        """
+        Method to build the chart url
+        """
+        chart_url_postfix = (
+            f"reports/{report_id}" if report_id else f"dashboards/{dashboard_id}"
+        )
+        return (
+            f"{clean_uri(self.service_connection.hostPort)}/groups/"
+            f"{workspace_id}/{chart_url_postfix}"
+        )
+
     def yield_dashboard(
-        self, dashboard_details: dict
+        self, dashboard_details: PowerBIDashboard
     ) -> Iterable[CreateDashboardRequest]:
         """
         Method to Get Dashboard Entity, Dashboard Charts & Lineage
         """
-        dashboard_url = (
-            f"/groups/{self.context.workspace.get('id')}"
-            f"/dashboards/{dashboard_details.get('id')}"
-        )
         dashboard_request = CreateDashboardRequest(
-            name=dashboard_details["id"],
-            # PBI has no hostPort property. Urls are built manually.
-            dashboardUrl=dashboard_url,
-            displayName=dashboard_details["displayName"],
+            name=dashboard_details.id,
+            dashboardUrl=self.get_dashboard_url(
+                workspace_id=self.context.workspace.id,
+                dashboard_id=dashboard_details.id,
+            ),
+            displayName=dashboard_details.displayName,
             description="",
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
@@ -186,112 +247,108 @@
             ],
             service=self.context.dashboard_service.fullyQualifiedName.__root__,
         )
         yield dashboard_request
         self.register_record(dashboard_request=dashboard_request)
 
     def yield_dashboard_lineage_details(
-        self, dashboard_details: dict, db_service_name: str
+        self, dashboard_details: PowerBIDashboard, db_service_name: str
     ) -> Optional[Iterable[AddLineageRequest]]:
         """
         Get lineage between dashboard and data sources
         """
         try:
-            charts = dashboard_details.get("tiles")
-            for chart in charts:
-                dataset_id = chart.get("datasetId")
-                if dataset_id:
-                    dataset = self.fetch_dataset_from_workspace(dataset_id)
-                    if dataset:
-                        for table in dataset.get("tables"):
-                            table_name = table.get("name")
-
-                            from_fqn = fqn.build(
-                                self.metadata,
-                                entity_type=Table,
-                                service_name=db_service_name,
-                                database_name=None,
-                                schema_name=None,
-                                table_name=table_name,
-                            )
-                            from_entity = self.metadata.get_by_name(
-                                entity=Table,
-                                fqn=from_fqn,
-                            )
-                            to_fqn = fqn.build(
-                                self.metadata,
-                                entity_type=Dashboard,
-                                service_name=self.config.serviceName,
-                                dashboard_name=dashboard_details["id"],
-                            )
-                            to_entity = self.metadata.get_by_name(
-                                entity=Dashboard,
-                                fqn=to_fqn,
-                            )
+            charts = dashboard_details.tiles
+            for chart in charts or []:
+                dataset = self.fetch_dataset_from_workspace(chart.datasetId)
+                if dataset:
+                    for table in dataset.tables:
+                        table_name = table.name
+
+                        from_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Table,
+                            service_name=db_service_name,
+                            database_name=None,
+                            schema_name=None,
+                            table_name=table_name,
+                        )
+                        from_entity = self.metadata.get_by_name(
+                            entity=Table,
+                            fqn=from_fqn,
+                        )
+                        to_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Dashboard,
+                            service_name=self.config.serviceName,
+                            dashboard_name=dashboard_details.id,
+                        )
+                        to_entity = self.metadata.get_by_name(
+                            entity=Dashboard,
+                            fqn=to_fqn,
+                        )
+                        if from_entity and to_entity:
                             yield self._get_add_lineage_request(
                                 to_entity=to_entity, from_entity=from_entity
                             )
         except Exception as exc:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
             )
 
     def yield_dashboard_chart(
-        self, dashboard_details: dict
+        self, dashboard_details: PowerBIDashboard
     ) -> Optional[Iterable[CreateChartRequest]]:
         """Get chart method
         Args:
             dashboard_details:
         Returns:
             Iterable[Chart]
         """
-        charts = dashboard_details.get("tiles")
-        for chart in charts:
+        charts = dashboard_details.tiles
+        for chart in charts or []:
             try:
-                chart_title = chart.get("title")
-                chart_display_name = chart_title if chart_title else chart.get("id")
+                chart_title = chart.title
+                chart_display_name = chart_title if chart_title else chart.id
                 if filter_by_chart(
                     self.source_config.chartFilterPattern, chart_display_name
                 ):
                     self.status.filter(chart_display_name, "Chart Pattern not Allowed")
                     continue
-                report_id = chart.get("reportId")
-                chart_url_postfix = (
-                    f"reports/{report_id}"
-                    if report_id
-                    else f"dashboards/{dashboard_details.get('id')}"
-                )
-                chart_url = (
-                    f"/groups/{self.context.workspace.get('id')}/{chart_url_postfix}"
-                )
                 yield CreateChartRequest(
-                    name=chart["id"],
+                    name=chart.id,
                     displayName=chart_display_name,
                     description="",
                     chartType=ChartType.Other.value,
-                    # PBI has no hostPort property. All URL details are present in the webUrl property.
-                    chartUrl=chart_url,
+                    chartUrl=self.get_chart_url(
+                        report_id=chart.reportId,
+                        workspace_id=self.context.workspace.id,
+                        dashboard_id=dashboard_details.id,
+                    ),
                     service=self.context.dashboard_service.fullyQualifiedName.__root__,
                 )
                 self.status.scanned(chart_display_name)
             except Exception as exc:
-                name = chart.get("title")
+                name = chart.title
                 error = f"Error creating chart [{name}]: {exc}"
                 logger.debug(traceback.format_exc())
                 logger.warning(error)
                 self.status.failed(name, error, traceback.format_exc())
 
-    def fetch_dataset_from_workspace(self, dataset_id: str) -> Optional[dict]:
+    def fetch_dataset_from_workspace(
+        self, dataset_id: Optional[str]
+    ) -> Optional[Dataset]:
         """
         Method to search the dataset using id in the workspace dict
         """
-
-        dataset_data = next(
-            (
-                dataset
-                for dataset in self.context.workspace.get("datasets") or []
-                if dataset["id"] == dataset_id
-            ),
-            None,
-        )
-        return dataset_data
+        if dataset_id:
+            dataset_data = next(
+                (
+                    dataset
+                    for dataset in self.context.workspace.datasets or []
+                    if dataset.id == dataset_id
+                ),
+                None,
+            )
+            return dataset_data
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/quicksight/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/quicksight/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/redash/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/connection.py`

 * *Files 13% similar despite different names*

```diff
@@ -8,54 +8,46 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.dashboard.redashConnection import (
-    RedashConnection,
-)
-from metadata.ingestion.connections.test_connections import (
-    SourceConnectionException,
-    test_connection_steps,
+from metadata.generated.schema.entity.services.connections.pipeline.fivetranConnection import (
+    FivetranConnection,
 )
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.dashboard.redash.client import RedashApiClient
+from metadata.ingestion.source.pipeline.fivetran.client import FivetranClient
 
 
-def get_connection(connection: RedashConnection) -> RedashApiClient:
+def get_connection(connection: FivetranConnection) -> FivetranClient:
     """
     Create connection
     """
-    try:
-        return RedashApiClient(connection)
-    except Exception as exc:
-        msg = f"Unknown error connecting with {connection}: {exc}."
-        raise SourceConnectionException(msg) from exc
+    return FivetranClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: RedashApiClient,
-    service_connection: RedashConnection,
+    client: FivetranClient,
+    service_connection: FivetranConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetDashboards": client.dashboards}
+    test_fn = {"GetPipelines": client.list_groups}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_fqn=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/redash/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/salesforce/metadata.py`

 * *Files 24% similar despite different names*

```diff
@@ -5,283 +5,265 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Redash source module
+Salesforce source ingestion
 """
 import traceback
-from typing import Iterable, List, Optional
+from typing import Iterable, Optional, Tuple
 
-from packaging import version
-
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
+from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
+from metadata.generated.schema.api.data.createDatabaseSchema import (
+    CreateDatabaseSchemaRequest,
 )
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
-from metadata.generated.schema.api.data.createChart import CreateChartRequest
-from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
+from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.entity.data.chart import Chart
-from metadata.generated.schema.entity.data.dashboard import (
-    Dashboard as LineageDashboard,
+from metadata.generated.schema.entity.data.table import (
+    Column,
+    Constraint,
+    DataType,
+    Table,
+    TableType,
 )
-from metadata.generated.schema.entity.data.table import Table
-from metadata.generated.schema.entity.services.connections.dashboard.redashConnection import (
-    RedashConnection,
+from metadata.generated.schema.entity.services.connections.database.salesforceConnection import (
+    SalesforceConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
+from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
+    DatabaseServiceMetadataPipeline,
+)
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.lineage.parser import LineageParser
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
-from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
-from metadata.utils import fqn, tag_utils
-from metadata.utils.filters import filter_by_chart
-from metadata.utils.helpers import get_standard_chart_type
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
+from metadata.ingestion.source.database.database_service import DatabaseServiceSource
+from metadata.utils import fqn
+from metadata.utils.constants import DEFAULT_DATABASE
+from metadata.utils.filters import filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
-REDASH_TAG_CATEGORY = "RedashTags"
-
-INCOMPATIBLE_REDASH_VERSION = "8.0.0"
 
-
-class RedashSource(DashboardServiceSource):
+class SalesforceSource(DatabaseServiceSource):
     """
-    Redash Source Class
+    Implements the necessary methods to extract
+    Database metadata from Salesforce Source
     """
 
-    def __init__(
-        self,
-        config: WorkflowSource,
-        metadata_config: OpenMetadataConnection,
-    ):
-        super().__init__(config, metadata_config)
-        self.dashboard_list = []  # We will populate this in `prepare`
-        self.tags = []  # To create the tags before yielding final entities
+    def __init__(self, config, metadata_config: OpenMetadataConnection):
+        super().__init__()
+        self.config = config
+        self.source_config: DatabaseServiceMetadataPipeline = (
+            self.config.sourceConfig.config
+        )
+        self.metadata_config = metadata_config
+        self.metadata = OpenMetadata(metadata_config)
+        self.service_connection = self.config.serviceConnection.__root__.config
+        self.client = get_connection(self.service_connection)
+        self.table_constraints = None
+        self.data_models = {}
+        self.dbt_tests = {}
+        self.database_source_state = set()
 
     @classmethod
-    def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
+    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: RedashConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, RedashConnection):
+        connection: SalesforceConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, SalesforceConnection):
             raise InvalidSourceException(
-                f"Expected RedashConnection, but got {connection}"
+                f"Expected SalesforceConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def prepare(self):
-        """
-        Fetch the paginated list of dashboards and tags
+    def get_database_names(self) -> Iterable[str]:
         """
+        Default case with a single database.
 
-        self.dashboard_list = self.client.paginate(self.client.dashboards)
+        It might come informed - or not - from the source.
 
-        # Collecting all the tags
-        if self.source_config.includeTags:
-            for dashboard in self.dashboard_list:
-                self.tags.extend(dashboard.get("tags") or [])
-
-    def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
-        """
-        Fetch Dashboard Tags
-        """
-        if self.source_config.includeTags:
-            for tag in self.tags:
-                try:
-                    classification = OMetaTagAndClassification(
-                        classification_request=CreateClassificationRequest(
-                            name=REDASH_TAG_CATEGORY,
-                            description="Tags associates with redash entities",
-                        ),
-                        tag_request=CreateTagRequest(
-                            classification=REDASH_TAG_CATEGORY,
-                            name=tag,
-                            description="Redash Tag",
-                        ),
-                    )
-                    yield classification
-                    logger.info(
-                        f"Classification {REDASH_TAG_CATEGORY}, Tag {tag} Ingested"
-                    )
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(f"Error ingesting tag {tag}: {exc}")
+        Sources with multiple databases should overwrite this and
+        apply the necessary filters.
+        """
+        database_name = self.service_connection.databaseName or DEFAULT_DATABASE
+        yield database_name
 
-    def get_dashboards_list(self) -> Optional[List[dict]]:
+    def yield_database(self, database_name: str) -> Iterable[CreateDatabaseRequest]:
         """
-        Get List of all dashboards
+        From topology.
+        Prepare a database request and pass it to the sink
         """
+        yield CreateDatabaseRequest(
+            name=database_name,
+            service=self.context.database_service.fullyQualifiedName,
+        )
 
-        return self.dashboard_list
-
-    def get_dashboard_name(self, dashboard: dict) -> str:
+    def get_database_schema_names(self) -> Iterable[str]:
         """
-        Get Dashboard Name
+        return schema names
         """
-        return dashboard["name"]
+        schema_name = self.service_connection.scheme.name
+        yield schema_name
 
-    def get_dashboard_details(self, dashboard: dict) -> dict:
+    def yield_database_schema(
+        self, schema_name: str
+    ) -> Iterable[CreateDatabaseSchemaRequest]:
         """
-        Get Dashboard Details
+        From topology.
+        Prepare a database schema request and pass it to the sink
         """
-        return self.client.get_dashboard(dashboard["slug"])
-
-    def get_owner_details(self, dashboard_details) -> Optional[EntityReference]:
-        """Get dashboard owner
+        yield CreateDatabaseSchemaRequest(
+            name=schema_name,
+            database=self.context.database.fullyQualifiedName,
+        )
 
-        Args:
-            dashboard_details:
-        Returns:
-            Optional[EntityReference]
-        """
-        if dashboard_details.get("user") and dashboard_details["user"].get("email"):
-            user = self.metadata.get_user_by_email(
-                dashboard_details["user"].get("email")
-            )
-            if user:
-                return EntityReference(id=user.id.__root__, type="user")
-        return None
-
-    def get_dashboard_url(self, dashboard_details: dict) -> str:
-        if version.parse(self.service_connection.redashVersion) > version.parse(
-            INCOMPATIBLE_REDASH_VERSION
-        ):
-            dashboard_url = f"/dashboards/{dashboard_details.get('id', '')}"
-        else:
-            dashboard_url = f"/dashboards/{dashboard_details.get('slug', '')}"
-        return dashboard_url
-
-    def yield_dashboard(
-        self, dashboard_details: dict
-    ) -> Iterable[CreateDashboardRequest]:
+    def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
         """
-        Method to Get Dashboard Entity
+        Handle table and views.
+
+        Fetches them up using the context information and
+        the inspector set when preparing the db.
+
+        :return: tables or views, depending on config
         """
+        schema_name = self.context.database_schema.name.__root__
         try:
-            dashboard_description = ""
-            for widgets in dashboard_details.get("widgets") or []:
-                dashboard_description = widgets.get("text")
-
-            dashboard_request = CreateDashboardRequest(
-                name=dashboard_details["id"],
-                displayName=dashboard_details.get("name"),
-                description=dashboard_description,
-                charts=[
-                    fqn.build(
+            if self.service_connection.sobjectName:
+                table_name = self.standardize_table_name(
+                    schema_name, self.service_connection.sobjectName
+                )
+                yield table_name, TableType.Regular
+            else:
+                for salesforce_object in self.client.describe()["sobjects"]:
+                    table_name = salesforce_object["name"]
+                    table_name = self.standardize_table_name(schema_name, table_name)
+                    table_fqn = fqn.build(
                         self.metadata,
-                        entity_type=Chart,
-                        service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
-                        chart_name=chart.name.__root__,
+                        entity_type=Table,
+                        service_name=self.context.database_service.name.__root__,
+                        database_name=self.context.database.name.__root__,
+                        schema_name=self.context.database_schema.name.__root__,
+                        table_name=table_name,
                     )
-                    for chart in self.context.charts
-                ],
-                service=self.context.dashboard_service.fullyQualifiedName.__root__,
-                dashboardUrl=self.get_dashboard_url(dashboard_details),
-                tags=tag_utils.get_tag_labels(
-                    metadata=self.metadata,
-                    tags=dashboard_details.get("tags"),
-                    classification_name=REDASH_TAG_CATEGORY,
-                    include_tags=self.source_config.includeTags,
-                ),
+                    if filter_by_table(
+                        self.config.sourceConfig.config.tableFilterPattern,
+                        table_fqn
+                        if self.config.sourceConfig.config.useFqnForFiltering
+                        else table_name,
+                    ):
+                        self.status.filter(
+                            table_fqn,
+                            "Table Filtered Out",
+                        )
+                        continue
+
+                    yield table_name, TableType.Regular
+        except Exception as exc:
+            error = f"Unexpected exception for schema name [{schema_name}]: {exc}"
+            logger.debug(traceback.format_exc())
+            logger.warning(error)
+            self.status.failed(schema_name, error, traceback.format_exc())
+
+    def yield_table(
+        self, table_name_and_type: Tuple[str, str]
+    ) -> Iterable[Optional[CreateTableRequest]]:
+        """
+        From topology.
+        Prepare a table request and pass it to the sink
+        """
+        table_name, table_type = table_name_and_type
+        try:
+            table_constraints = None
+            salesforce_objects = self.client.restful(
+                f"sobjects/{table_name}/describe/",
+                params=None,
             )
-            yield dashboard_request
-            self.register_record(dashboard_request=dashboard_request)
+            columns = self.get_columns(salesforce_objects["fields"])
+            table_request = CreateTableRequest(
+                name=table_name,
+                tableType=table_type,
+                description="",
+                columns=columns,
+                tableConstraints=table_constraints,
+                databaseSchema=self.context.database_schema.fullyQualifiedName,
+            )
+            yield table_request
+            self.register_record(table_request=table_request)
 
         except Exception as exc:
+            error = f"Unexpected exception for table [{table_name}]: {exc}"
             logger.debug(traceback.format_exc())
-            logger.warning(f"Error to yield dashboard for {dashboard_details}: {exc}")
+            logger.warning(error)
+            self.status.failed(table_name, error, traceback.format_exc())
 
-    def yield_dashboard_lineage_details(
-        self, dashboard_details: dict, db_service_name: str
-    ) -> Optional[Iterable[AddLineageRequest]]:
-        """
-        Get lineage between dashboard and data sources
-        In redash we do not get table, database_schema or database name but we do get query
-        the lineage is being generated based on the query
-        """
-
-        to_fqn = fqn.build(
-            self.metadata,
-            entity_type=LineageDashboard,
-            service_name=self.config.serviceName,
-            dashboard_name=str(dashboard_details.get("id")),
-        )
-        to_entity = self.metadata.get_by_name(
-            entity=LineageDashboard,
-            fqn=to_fqn,
-        )
-        for widgets in dashboard_details.get("widgets") or []:
-            try:
-                visualization = widgets.get("visualization")
-                if not visualization:
-                    continue
-                if visualization.get("query", {}).get("query"):
-                    lineage_parser = LineageParser(visualization["query"]["query"])
-                    for table in lineage_parser.source_tables:
-                        table_name = str(table)
-                        database_schema_table = fqn.split_table_name(table_name)
-                        from_fqn = fqn.build(
-                            self.metadata,
-                            entity_type=Table,
-                            service_name=db_service_name,
-                            schema_name=database_schema_table.get("database_schema"),
-                            table_name=database_schema_table.get("table"),
-                            database_name=database_schema_table.get("database"),
-                        )
-                        from_entity = self.metadata.get_by_name(
-                            entity=Table,
-                            fqn=from_fqn,
-                        )
-                        if from_entity and to_entity:
-                            yield self._get_add_lineage_request(
-                                to_entity=to_entity, from_entity=from_entity
-                            )
-            except Exception as exc:
-                logger.debug(traceback.format_exc())
-                logger.warning(
-                    f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {exc}"
+    def get_columns(self, salesforce_fields):
+        """
+        Method to handle column details
+        """
+        row_order = 1
+        columns = []
+        for column in salesforce_fields:
+            col_constraint = None
+            if column["nillable"]:
+                col_constraint = Constraint.NULL
+            elif not column["nillable"]:
+                col_constraint = Constraint.NOT_NULL
+            if column["unique"]:
+                col_constraint = Constraint.UNIQUE
+
+            columns.append(
+                Column(
+                    name=column["name"],
+                    description=column["label"],
+                    dataType=self.column_type(column["type"].upper()),
+                    dataTypeDisplay=column["type"],
+                    constraint=col_constraint,
+                    ordinalPosition=row_order,
+                    dataLength=column["length"],
                 )
+            )
+            row_order += 1
+        return columns
 
-    def yield_dashboard_chart(
-        self, dashboard_details: dict
-    ) -> Optional[Iterable[CreateChartRequest]]:
-        """
-        Metod to fetch charts linked to dashboard
-        """
-        for widgets in dashboard_details.get("widgets") or []:
-            try:
-                visualization = widgets.get("visualization")
-                chart_display_name = str(
-                    visualization["query"]["name"] if visualization else widgets["id"]
-                )
-                if filter_by_chart(
-                    self.source_config.chartFilterPattern, chart_display_name
-                ):
-                    self.status.filter(chart_display_name, "Chart Pattern not allowed")
-                    continue
-                yield CreateChartRequest(
-                    name=widgets["id"],
-                    displayName=chart_display_name
-                    if visualization and visualization["query"]
-                    else "",
-                    chartType=get_standard_chart_type(
-                        visualization["type"] if visualization else ""
-                    ),
-                    service=self.context.dashboard_service.fullyQualifiedName.__root__,
-                    chartUrl=self.get_dashboard_url(dashboard_details),
-                    description=visualization["description"] if visualization else "",
-                )
-            except Exception as exc:
-                logger.debug(traceback.format_exc())
-                logger.warning(
-                    f"Error to yield dashboard chart for widget_id: {widgets['id']} and {dashboard_details}: {exc}"
-                )
+    def column_type(self, column_type: str):
+        if column_type in {
+            "ID",
+            "PHONE",
+            "EMAIL",
+            "ENCRYPTEDSTRING",
+            "COMBOBOX",
+            "URL",
+            "TEXTAREA",
+            "ADDRESS",
+            "REFERENCE",
+        }:
+            return DataType.VARCHAR.value
+        return DataType.UNKNOWN.value
+
+    def yield_view_lineage(self) -> Optional[Iterable[AddLineageRequest]]:
+        yield from []
+
+    def yield_tag(self, schema_name: str) -> Iterable[OMetaTagAndClassification]:
+        pass
+
+    def standardize_table_name(  # pylint: disable=unused-argument
+        self, schema: str, table: str
+    ) -> str:
+        return table
+
+    def prepare(self):
+        pass
+
+    def close(self):
+        pass
+
+    def test_connection(self) -> None:
+        test_connection_fn = get_test_connection_fn(self.service_connection)
+        test_connection_fn(self.client, self.service_connection)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/api_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/api_source.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 from metadata.generated.schema.api.data.createChart import CreateChartRequest
 from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
 from metadata.generated.schema.entity.data.chart import Chart, ChartType
 from metadata.generated.schema.entity.data.table import Table
 from metadata.ingestion.source.dashboard.superset.mixin import SupersetSourceMixin
 from metadata.utils import fqn
-from metadata.utils.helpers import get_standard_chart_type
+from metadata.utils.helpers import clean_uri, get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class SupersetAPISource(SupersetSourceMixin):
     """
@@ -66,15 +66,15 @@
         """
         Method to Get Dashboard Entity
         """
         dashboard_request = CreateDashboardRequest(
             name=dashboard_details["id"],
             displayName=dashboard_details["dashboard_title"],
             description="",
-            dashboardUrl=dashboard_details["url"],
+            dashboardUrl=f"{clean_uri(self.service_connection.hostPort)}{dashboard_details['url']}",
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
@@ -106,15 +106,15 @@
             chart = CreateChartRequest(
                 name=chart_json["id"],
                 displayName=chart_json.get("slice_name"),
                 description=chart_json.get("description"),
                 chartType=get_standard_chart_type(
                     chart_json.get("viz_type", ChartType.Other.value)
                 ),
-                chartUrl=chart_json.get("url"),
+                chartUrl=f"{clean_uri(self.service_connection.hostPort)}{chart_json.get('url')}",
                 service=self.context.dashboard_service.fullyQualifiedName.__root__,
             )
             yield chart
 
     def _get_datasource_fqn(
         self, datasource_id: str, db_service_name: str
     ) -> Optional[str]:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/db_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/db_source.py`

 * *Files 5% similar despite different names*

```diff
@@ -29,15 +29,15 @@
 )
 from metadata.ingestion.source.dashboard.superset.mixin import SupersetSourceMixin
 from metadata.ingestion.source.dashboard.superset.queries import (
     FETCH_ALL_CHARTS,
     FETCH_DASHBOARDS,
 )
 from metadata.utils import fqn
-from metadata.utils.helpers import get_standard_chart_type
+from metadata.utils.helpers import clean_uri, get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class SupersetDBSource(SupersetSourceMixin):
     """
@@ -72,15 +72,15 @@
         """
         Method to Get Dashboard Entity
         """
         dashboard_request = CreateDashboardRequest(
             name=dashboard_details["id"],
             displayName=dashboard_details["dashboard_title"],
             description="",
-            dashboardUrl=f"/superset/dashboard/{dashboard_details['id']}/",
+            dashboardUrl=f"{clean_uri(self.service_connection.hostPort)}/superset/dashboard/{dashboard_details['id']}/",
             charts=[
                 fqn.build(
                     self.metadata,
                     entity_type=Chart,
                     service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
                     chart_name=chart.name.__root__,
                 )
@@ -112,15 +112,15 @@
             chart = CreateChartRequest(
                 name=chart_json["id"],
                 displayName=chart_json.get("slice_name"),
                 description=chart_json.get("description"),
                 chartType=get_standard_chart_type(
                     chart_json.get("viz_type", ChartType.Other.value)
                 ),
-                chartUrl=f"/explore/?slice_id={chart_json['id']}",
+                chartUrl=f"{clean_uri(self.service_connection.hostPort)}/explore/?slice_id={chart_json['id']}",
                 service=self.context.dashboard_service.fullyQualifiedName.__root__,
             )
             yield chart
 
     def _get_database_name(self, sqa_str: str) -> str:
         if sqa_str:
             return sqa_str.split("/")[-1]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/mixin.py`

 * *Files 0% similar despite different names*

```diff
@@ -75,25 +75,24 @@
     def get_dashboard_details(self, dashboard: dict) -> dict:
         """
         Get Dashboard Details
         """
         return dashboard
 
     def _get_user_by_email(self, email: str) -> EntityReference:
-
         if email:
             user = self.metadata.get_user_by_email(email)
             if user:
                 return EntityReference(id=user.id.__root__, type="user")
 
         return None
 
     def get_owner_details(self, dashboard_details: dict) -> EntityReference:
         for owner in dashboard_details.get("owners", []):
-            user = self._get_user_by_email(owner["email"])
+            user = self._get_user_by_email(owner.get("email"))
             if user:
                 return user
         if dashboard_details.get("email"):
             user = self._get_user_by_email(dashboard_details["email"])
             if user:
                 return user
         return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/superset/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/superset/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/__init__.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/queries.py`

 * *Files 17% similar despite different names*

```diff
@@ -4,16 +4,13 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Module constants
+SQL Queries used during ingestion
 """
 
-TABLEAU_GET_WORKBOOKS_PARAM_DICT = {
-    "fields": "fields=_default_,owner.email,description"
-}
-TABLEAU_GET_VIEWS_PARAM_DICT = {"fields": "fields=_default_,sheetType"}
+
+PRESTO_SHOW_CATALOGS = "SHOW CATALOGS"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/tableau/connection.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,91 +10,66 @@
 #  limitations under the License.
 
 """
 Source connection handler
 """
 import traceback
 from functools import partial
-from typing import Optional
+from typing import Any, Dict, Optional
 
-from tableau_api_lib import TableauServerConnection
 from tableau_api_lib.utils import extract_pages
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.services.connections.dashboard.tableauConnection import (
     TableauConnection,
 )
+from metadata.generated.schema.security.credentials.accessTokenAuth import (
+    AccessTokenAuth,
+)
+from metadata.generated.schema.security.credentials.basicAuth import BasicAuth
 from metadata.ingestion.connections.test_connections import (
     SourceConnectionException,
     test_connection_steps,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.dashboard.tableau import (
     TABLEAU_GET_VIEWS_PARAM_DICT,
     TABLEAU_GET_WORKBOOKS_PARAM_DICT,
 )
+from metadata.ingestion.source.dashboard.tableau.client import TableauClient
 from metadata.utils.logger import ingestion_logger
 from metadata.utils.ssl_registry import get_verify_ssl_fn
 
 logger = ingestion_logger()
 
 
-def get_connection(connection: TableauConnection) -> TableauServerConnection:
+def get_connection(connection: TableauConnection) -> TableauClient:
     """
     Create connection
     """
-    tableau_server_config = {
-        f"{connection.env}": {
-            "server": connection.hostPort,
-            "api_version": connection.apiVersion,
-            "site_name": connection.siteName if connection.siteName else "",
-            "site_url": connection.siteUrl if connection.siteUrl else "",
-        }
-    }
-    if connection.username and connection.password:
-        tableau_server_config[connection.env]["username"] = connection.username
-        tableau_server_config[connection.env][
-            "password"
-        ] = connection.password.get_secret_value()
-    elif (
-        connection.personalAccessTokenName
-        and connection.personalAccessTokenSecret.get_secret_value()
-    ):
-        tableau_server_config[connection.env][
-            "personal_access_token_name"
-        ] = connection.personalAccessTokenName
-        tableau_server_config[connection.env][
-            "personal_access_token_secret"
-        ] = connection.personalAccessTokenSecret.get_secret_value()
+    tableau_server_config = build_server_config(connection)
+    get_verify_ssl = get_verify_ssl_fn(connection.verifySSL)
     try:
-
-        get_verify_ssl = get_verify_ssl_fn(connection.verifySSL)
-        # ssl_verify is typed as a `bool` in TableauServerConnection
-        # However, it is passed as `verify=self.ssl_verify` in each `requests` call.
-        # In requests (https://requests.readthedocs.io/en/latest/user/advanced/#ssl-cert-verification)
-        # the param can be None, False to ignore HTTPS certs or a string with the path to the cert.
-        conn = TableauServerConnection(
-            config_json=tableau_server_config,
+        return TableauClient(
+            config=tableau_server_config,
             env=connection.env,
             ssl_verify=get_verify_ssl(connection.sslConfig),
         )
-        conn.sign_in().json()
-        return conn
     except Exception as exc:
         logger.debug(traceback.format_exc())
         raise SourceConnectionException(
             f"Unknown error connecting with {connection}: {exc}."
         )
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: TableauServerConnection,
+    client: TableauClient,
     service_connection: TableauConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
@@ -116,7 +91,38 @@
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_fqn=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
+
+
+def build_server_config(connection: TableauConnection) -> Dict[str, Dict[str, Any]]:
+    """
+    Build client configuration
+    Args:
+        connection: configuration of Tableau Connection
+    Returns:
+        Client configuration
+    """
+    tableau_server_config = {
+        f"{connection.env}": {
+            "server": connection.hostPort,
+            "api_version": connection.apiVersion,
+            "site_name": connection.siteName if connection.siteName else "",
+            "site_url": connection.siteUrl if connection.siteUrl else "",
+        }
+    }
+    if isinstance(connection.authType, BasicAuth):
+        tableau_server_config[connection.env]["username"] = connection.authType.username
+        tableau_server_config[connection.env][
+            "password"
+        ] = connection.authType.password.get_secret_value()
+    elif isinstance(connection.authType, AccessTokenAuth):
+        tableau_server_config[connection.env][
+            "personal_access_token_name"
+        ] = connection.authType.personalAccessTokenName
+        tableau_server_config[connection.env][
+            "personal_access_token_secret"
+        ] = connection.authType.personalAccessTokenSecret.get_secret_value()
+    return tableau_server_config
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/metadata.py`

 * *Files 21% similar despite different names*

```diff
@@ -4,425 +4,460 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Tableau source module
+Atlas source to extract metadata
 """
-import json
-import traceback
-from typing import Iterable, List, Optional
 
-from pydantic import BaseModel, Extra
-from requests.utils import urlparse
-from tableau_api_lib.utils import extract_pages
+import traceback
+from dataclasses import dataclass
+from typing import Any, Dict, Iterable, List
 
 from metadata.generated.schema.api.classification.createClassification import (
     CreateClassificationRequest,
 )
 from metadata.generated.schema.api.classification.createTag import CreateTagRequest
-from metadata.generated.schema.api.data.createChart import CreateChartRequest
-from metadata.generated.schema.api.data.createDashboard import CreateDashboardRequest
+from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.entity.data.chart import Chart
-from metadata.generated.schema.entity.data.dashboard import (
-    Dashboard as LineageDashboard,
+from metadata.generated.schema.api.services.createDatabaseService import (
+    CreateDatabaseServiceRequest,
 )
-from metadata.generated.schema.entity.data.table import Table
-from metadata.generated.schema.entity.services.connections.dashboard.tableauConnection import (
-    TableauConnection,
+from metadata.generated.schema.api.services.createMessagingService import (
+    CreateMessagingServiceRequest,
+)
+from metadata.generated.schema.entity.classification.tag import Tag
+from metadata.generated.schema.entity.data.database import Database
+from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
+from metadata.generated.schema.entity.data.pipeline import Pipeline
+from metadata.generated.schema.entity.data.table import Column, Table
+from metadata.generated.schema.entity.data.topic import Topic
+from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
+    AtlasConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
+from metadata.generated.schema.entity.services.databaseService import DatabaseService
+from metadata.generated.schema.entity.services.messagingService import MessagingService
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.entityLineage import EntitiesEdge
 from metadata.generated.schema.type.entityReference import EntityReference
-from metadata.ingestion.api.source import InvalidSourceException
+from metadata.generated.schema.type.tagLabel import TagLabel
+from metadata.ingestion.api.source import InvalidSourceException, Source
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
-from metadata.ingestion.source.dashboard.dashboard_service import DashboardServiceSource
-from metadata.ingestion.source.dashboard.tableau import (
-    TABLEAU_GET_VIEWS_PARAM_DICT,
-    TABLEAU_GET_WORKBOOKS_PARAM_DICT,
-)
-from metadata.ingestion.source.dashboard.tableau.queries import (
-    TABLEAU_LINEAGE_GRAPHQL_QUERY,
-)
-from metadata.utils import fqn, tag_utils
-from metadata.utils.filters import filter_by_chart
-from metadata.utils.helpers import get_standard_chart_type
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
+from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
+from metadata.ingestion.source.metadata.atlas.client import AtlasClient
+from metadata.utils import fqn
 from metadata.utils.logger import ingestion_logger
+from metadata.utils.metadata_service_helper import SERVICE_TYPE_MAPPER
 
 logger = ingestion_logger()
 
-TABLEAU_TAG_CATEGORY = "TableauTags"
-
-
-class TableauBaseModel(BaseModel):
-    """
-    Tableau basic configurations
-    """
-
-    class Config:
-        extra = Extra.allow
-
-    id: str
-    name: str
-
-
-class TableauOwner(TableauBaseModel):
-    """
-    Tableau Owner Details
-    """
-
-    email: str
+ATLAS_TAG_CATEGORY = "AtlasMetadata"
+ATLAS_TABLE_TAG = "atlas_table"
 
 
-class TableauChart(TableauBaseModel):
+@dataclass
+class AtlasSource(Source):
     """
-    Chart (View) representation from API
-    """
-
-    workbook_id: str
-    sheet_type: str
-    view_url_name: str
-    content_url: str
-    tags: List[str]
-
-
-class ChartUrl:
-    workbook_name: str
-    sheets: str
-    chart_url_name: str
-
-    def __init__(self, context_url: str) -> None:
-        self.workbook_name, self.sheets, self.chart_url_name = (
-            context_url.split("/") if "/" in context_url else ["", "", ""]
-        )
-
-
-class TableauDashboard(TableauBaseModel):
-    """
-    Response from Tableau API
-    """
-
-    description: Optional[str]
-    tags: List[str]
-    owner: Optional[TableauOwner]
-    charts: Optional[List[TableauChart]]
-    webpage_url: Optional[str]
-
-
-class TableauSource(DashboardServiceSource):
-    """
-    Tableau Source Class
+    Atlas source class
     """
 
     config: WorkflowSource
-    metadata_config: OpenMetadataConnection
+    atlas_client: AtlasClient
+    tables: Dict[str, Any]
+    topics: Dict[str, Any]
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata_config: OpenMetadataConnection,
     ):
+        super().__init__()
+        self.config = config
+        self.metadata_config = metadata_config
+        self.metadata = OpenMetadata(metadata_config)
+        self.service_connection = self.config.serviceConnection.__root__.config
+
+        self.atlas_client = get_connection(self.service_connection)
+        self.connection_obj = self.atlas_client
+        self.tables: Dict[str, Any] = {}
+        self.topics: Dict[str, Any] = {}
+
+        self.service = None
+        self.message_service = None
+        self.entity_types = {
+            "Table": {
+                self.service_connection.entity_type: {"db": "db", "column": "columns"}
+            },
+            "Topic": {"Topic": {"schema": "schema"}},
+        }
+        self.test_connection()
 
-        super().__init__(config, metadata_config)
-        self.workbooks = None  # We will populate this in `prepare`
-        self.tags = set()  # To create the tags before yielding final entities
-        self.workbook_datasources = {}
+    @classmethod
+    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: AtlasConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, AtlasConnection):
+            raise InvalidSourceException(
+                f"Expected AtlasConnection, but got {connection}"
+            )
+        return cls(config, metadata_config)
 
     def prepare(self):
         """
-        Restructure the API response to
+        Not required to implement
         """
-        # Available fields information:
-        # https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_concepts_fields.htm#query_workbooks_site
-        # We can also get project.description as folder
-        self.workbooks = [
-            TableauDashboard(
-                id=workbook["id"],
-                name=workbook["name"],
-                description=workbook.get("description"),
-                tags=[tag["label"] for tag in workbook.get("tags", {}).get("tag") or []]
-                if self.source_config.includeTags
-                else [],
-                owner=TableauOwner(
-                    id=workbook.get("owner", {}).get("id"),
-                    name=workbook.get("owner", {}).get("name"),
-                    email=workbook.get("owner", {}).get("email"),
-                )
-                if workbook.get("owner", {}).get("email")
-                else None,
-                webpage_url=workbook.get("webpageUrl"),
-            )
-            for workbook in extract_pages(
-                self.client.query_workbooks_for_site,
-                parameter_dict=TABLEAU_GET_WORKBOOKS_PARAM_DICT,
-            )
-        ]
 
-        # For charts, we can also pick up usage as a field
-        charts = [
-            TableauChart(
-                id=chart["id"],
-                name=chart["name"],
-                # workbook.id is always included in the response
-                workbook_id=chart["workbook"]["id"],
-                sheet_type=chart["sheetType"],
-                view_url_name=chart["viewUrlName"],
-                tags=[tag["label"] for tag in chart.get("tags", {}).get("tag") or []]
-                if self.source_config.includeTags
-                else [],
-                content_url=chart.get("contentUrl", ""),
+    def next_record(self):
+        for service in self.service_connection.databaseServiceName or []:
+            check_service = self.metadata.get_by_name(
+                entity=DatabaseService, fqn=service
             )
-            for chart in extract_pages(
-                self.client.query_views_for_site,
-                content_id=self.client.site_id,
-                parameter_dict=TABLEAU_GET_VIEWS_PARAM_DICT,
-            )
-        ]
-
-        # Add all the charts (views) from the API to each workbook
-        for workbook in self.workbooks:
-            workbook.charts = [
-                chart for chart in charts if chart.workbook_id == workbook.id
-            ]
-
-        # Collecting all view & workbook tags
-        if self.source_config.includeTags:
-            for container in [self.workbooks, charts]:
-                for elem in container:
-                    self.tags.update(elem.tags)
-
-        if self.source_config.dbServiceNames:
-            try:
-                # Fetch Datasource information for lineage
-                graphql_query_result = self.client.metadata_graphql_query(
-                    query=TABLEAU_LINEAGE_GRAPHQL_QUERY
-                )
-                self.workbook_datasources = json.loads(graphql_query_result.text)[
-                    "data"
-                ].get("workbooks")
-            except Exception:
-                logger.debug(traceback.format_exc())
+            if check_service:
+                for key in self.entity_types["Table"]:
+                    self.service = check_service
+                    self.tables[key] = self.atlas_client.list_entities()
+                    if self.tables.get(key, None):
+                        for key in self.tables:
+                            yield from self._parse_table_entity(key, self.tables[key])
+            else:
                 logger.warning(
-                    "\nSomething went wrong while connecting to Tableau Metadata APIs\n"
-                    "Please check if the Tableau Metadata APIs are enabled for you Tableau instance\n"
-                    "For more information on enabling the Tableau Metadata APIs follow the link below\n"
-                    "https://help.tableau.com/current/api/metadata_api/en-us/docs/meta_api_start.html"
-                    "#enable-the-tableau-metadata-api-for-tableau-server\n"
+                    f"Cannot find service for {service} - type DatabaseService"
                 )
 
-        return super().prepare()
-
-    @classmethod
-    def create(cls, config_dict: dict, metadata_config: OpenMetadataConnection):
-        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: TableauConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, TableauConnection):
-            raise InvalidSourceException(
-                f"Expected TableauConnection, but got {connection}"
+        for service in self.service_connection.messagingServiceName or []:
+            check_service = self.metadata.get_by_name(
+                entity=MessagingService, fqn=service
             )
-        return cls(config, metadata_config)
+            if check_service:
+                for key in self.entity_types["Topic"]:
+                    self.message_service = check_service
+                    self.topics[key] = self.atlas_client.list_entities()
+                    if self.topics.get(key, None):
+                        for topic in self.topics:
+                            yield from self._parse_topic_entity(topic)
+            else:
+                logger.warning(
+                    f"Cannot find service for {service} - type MessagingService"
+                )
 
-    def get_dashboards_list(self) -> Optional[List[TableauDashboard]]:
+    def close(self):
         """
-        Get List of all dashboards
+        Not required to implement
         """
-        return self.workbooks
 
-    def get_dashboard_name(self, dashboard: TableauDashboard) -> str:
-        """
-        Get Dashboard Name
-        """
-        return dashboard.name
+    def _parse_topic_entity(self, name):
+        for key in self.topics:
+            topic_entity = self.atlas_client.get_entity(self.topics[key])
+            tpc_entities = topic_entity["entities"]
+            for tpc_entity in tpc_entities:
+                try:
+                    tpc_attrs = tpc_entity["attributes"]
+                    topic_name = tpc_attrs["name"]
 
-    def get_dashboard_details(self, dashboard: TableauDashboard) -> TableauDashboard:
-        """
-        Get Dashboard Details. Returning the identity here as we prepare everything
-        during the `prepare` stage
-        """
-        return dashboard
+                    topic_fqn = fqn.build(
+                        self.metadata,
+                        entity_type=Topic,
+                        service_name=self.message_service.id,
+                        topic_name=topic_name,
+                    )
 
-    def get_owner_details(
-        self, dashboard_details: TableauDashboard
-    ) -> Optional[EntityReference]:
-        """Get dashboard owner
-
-        Args:
-            dashboard_details:
-        Returns:
-            Optional[EntityReference]
-        """
-        if dashboard_details.owner and dashboard_details.owner.email:
-            user = self.metadata.get_user_by_email(dashboard_details.owner.email)
-            if user:
-                return EntityReference(id=user.id.__root__, type="user")
-        return None
+                    topic_object = self.metadata.get_by_name(
+                        entity=Topic, fqn=topic_fqn
+                    )
 
-    def yield_tag(self, *_, **__) -> OMetaTagAndClassification:
-        """
-        Fetch Dashboard Tags
-        """
-        if self.source_config.includeTags:
-            for tag in self.tags:
+                    if tpc_attrs.get("description") and topic_object:
+                        self.metadata.patch_description(
+                            entity=Topic,
+                            entity_id=topic_object.id,
+                            description=tpc_attrs["description"],
+                            force=True,
+                        )
+
+                    yield from self.ingest_lineage(tpc_entity["guid"], name)
+
+                except Exception as exc:
+                    logger.debug(traceback.format_exc())
+                    logger.warning(
+                        f"Failed to parse topi entry [{topic_entity}]: {exc}"
+                    )
+
+    def _parse_table_entity(self, name, entity):  # pylint: disable=too-many-locals
+        for table in entity:
+            table_entity = self.atlas_client.get_entity(table)
+            tbl_entities = table_entity["entities"]
+            db_entity = None
+            for tbl_entity in tbl_entities:
                 try:
-                    classification = OMetaTagAndClassification(
-                        classification_request=CreateClassificationRequest(
-                            name=TABLEAU_TAG_CATEGORY,
-                            description="Tags associates with tableau entities",
-                        ),
-                        tag_request=CreateTagRequest(
-                            classification=TABLEAU_TAG_CATEGORY,
-                            name=tag,
-                            description="Tableau Tag",
-                        ),
-                    )
-                    yield classification
-                    logger.info(
-                        f"Classification {TABLEAU_TAG_CATEGORY}, Tag {tag} Ingested"
+                    tbl_attrs = tbl_entity["attributes"]
+                    db_entity = tbl_entity["relationshipAttributes"][
+                        self.entity_types["Table"][name]["db"]
+                    ]
+
+                    database_fqn = fqn.build(
+                        self.metadata,
+                        entity_type=Database,
+                        service_name=self.service.name.__root__,
+                        database_name=db_entity["displayText"],
                     )
-                except Exception as err:
-                    logger.debug(traceback.format_exc())
-                    logger.error(f"Error ingesting tag [{tag}]: {err}")
+                    database_object = self.metadata.get_by_name(
+                        entity=Database, fqn=database_fqn
+                    )
+                    if db_entity.get("description", None) and database_object:
+                        self.metadata.patch_description(
+                            entity=Database,
+                            entity_id=database_object.id,
+                            description=db_entity["description"],
+                            force=True,
+                        )
 
-    def yield_dashboard(
-        self, dashboard_details: TableauDashboard
-    ) -> Iterable[CreateDashboardRequest]:
-        """
-        Method to Get Dashboard Entity
-        """
-        try:
-            workbook_url = urlparse(dashboard_details.webpage_url).fragment
-            dashboard_request = CreateDashboardRequest(
-                name=dashboard_details.id,
-                displayName=dashboard_details.name,
-                description=dashboard_details.description,
-                charts=[
-                    fqn.build(
+                    database_schema_fqn = fqn.build(
                         self.metadata,
-                        entity_type=Chart,
-                        service_name=self.context.dashboard_service.fullyQualifiedName.__root__,
-                        chart_name=chart.name.__root__,
-                    )
-                    for chart in self.context.charts
-                ],
-                tags=tag_utils.get_tag_labels(
-                    metadata=self.metadata,
-                    tags=dashboard_details.tags,
-                    classification_name=TABLEAU_TAG_CATEGORY,
-                    include_tags=self.source_config.includeTags,
+                        entity_type=DatabaseSchema,
+                        service_name=self.service.name.__root__,
+                        database_name=db_entity["displayText"],
+                        schema_name=db_entity["displayText"],
+                    )
+                    database_schema_object = self.metadata.get_by_name(
+                        entity=DatabaseSchema, fqn=database_schema_fqn
+                    )
+
+                    if db_entity.get("description", None) and database_schema_object:
+                        self.metadata.patch_description(
+                            entity=DatabaseSchema,
+                            entity_id=database_schema_object.id,
+                            description=db_entity["description"],
+                            force=True,
+                        )
+
+                    yield self.create_tag()
+
+                    table_fqn = fqn.build(
+                        metadata=self.metadata,
+                        entity_type=Table,
+                        service_name=self.service.name.__root__,
+                        database_name=db_entity["displayText"],
+                        schema_name=db_entity["displayText"],
+                        table_name=tbl_attrs["name"],
+                    )
+
+                    table_object = self.metadata.get_by_name(
+                        entity=Table, fqn=table_fqn
+                    )
+
+                    if table_object:
+                        if tbl_attrs.get("description", None):
+                            self.metadata.patch_description(
+                                entity_id=table_object.id,
+                                entity=Table,
+                                description=tbl_attrs["description"],
+                                force=True,
+                            )
+
+                        tag_fqn = fqn.build(
+                            self.metadata,
+                            entity_type=Tag,
+                            classification_name=ATLAS_TAG_CATEGORY,
+                            tag_name=ATLAS_TABLE_TAG,
+                        )
+
+                        self.metadata.patch_tag(
+                            entity=Table, entity_id=table_object.id, tag_fqn=tag_fqn
+                        )
+
+                    yield from self.ingest_lineage(tbl_entity["guid"], name)
+
+                except Exception as exc:
+                    logger.debug(traceback.format_exc())
+                    logger.warning(
+                        f"Failed to parse for database : {db_entity} - table {table}: {exc}"
+                    )
+
+    def get_tags(self):
+        tags = [
+            TagLabel(
+                tagFQN=fqn.build(
+                    self.metadata,
+                    Tag,
+                    tag_category_name=ATLAS_TAG_CATEGORY,
+                    tag_name=ATLAS_TABLE_TAG,
                 ),
-                dashboardUrl=f"#{workbook_url}",
-                service=self.context.dashboard_service.fullyQualifiedName.__root__,
+                labelType="Automated",
+                state="Suggested",
+                source="Classification",
             )
-            yield dashboard_request
-            self.register_record(dashboard_request=dashboard_request)
-        except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Error to yield dashboard for {dashboard_details}: {exc}")
-
-    def yield_dashboard_lineage_details(
-        self, dashboard_details: TableauDashboard, db_service_name: str
-    ) -> Optional[Iterable[AddLineageRequest]]:
-        """
-        Get lineage between dashboard and data sources
-        """
+        ]
+        return tags
 
-        data_source = next(
-            (
-                data_source
-                for data_source in self.workbook_datasources or []
-                if data_source.get("luid") == dashboard_details.id
+    def create_tag(self) -> OMetaTagAndClassification:
+        atlas_table_tag = OMetaTagAndClassification(
+            classification_request=CreateClassificationRequest(
+                name=ATLAS_TAG_CATEGORY,
+                description="Tags associates with atlas entities",
+            ),
+            tag_request=CreateTagRequest(
+                classification=ATLAS_TAG_CATEGORY,
+                name=ATLAS_TABLE_TAG,
+                description="Atlas Cluster Tag",
             ),
-            None,
-        )
-        to_fqn = fqn.build(
-            self.metadata,
-            entity_type=LineageDashboard,
-            service_name=self.config.serviceName,
-            dashboard_name=dashboard_details.id,
-        )
-        to_entity = self.metadata.get_by_name(
-            entity=LineageDashboard,
-            fqn=to_fqn,
         )
+        return atlas_table_tag
 
-        try:
-            upstream_tables = data_source.get("upstreamTables")
-            for upstream_table in upstream_tables:
-                database_schema_table = fqn.split_table_name(upstream_table.get("name"))
-                from_fqn = fqn.build(
-                    self.metadata,
-                    entity_type=Table,
-                    service_name=db_service_name,
-                    schema_name=database_schema_table.get(
-                        "database_schema", upstream_table.get("schema")
+    def _parse_table_columns(self, table_response, tbl_entity, name) -> List[Column]:
+        om_cols = []
+        col_entities = tbl_entity["relationshipAttributes"][
+            self.entity_types["Table"][name]["column"]
+        ]
+        referred_entities = table_response["referredEntities"]
+        ordinal_pos = 1
+        for col in col_entities:
+            try:
+                col_guid = col["guid"]
+                col_ref_entity = referred_entities[col_guid]
+                column = col_ref_entity["attributes"]
+                col_data_length = "1"
+                om_column = Column(
+                    name=column["name"],
+                    description=column.get("comment", None),
+                    dataType=ColumnTypeParser.get_column_type(
+                        column["dataType"].upper()
                     ),
-                    table_name=database_schema_table.get("table"),
-                    database_name=database_schema_table.get("database"),
-                )
-                from_entity = self.metadata.get_by_name(
-                    entity=Table,
-                    fqn=from_fqn,
+                    dataTypeDisplay=column["dataType"],
+                    dataLength=col_data_length,
+                    ordinalPosition=ordinal_pos,
                 )
-                yield self._get_add_lineage_request(
-                    to_entity=to_entity, from_entity=from_entity
-                )
-        except (Exception, IndexError) as err:
-            logger.debug(traceback.format_exc())
-            logger.error(
-                f"Error to yield dashboard lineage details for DB service name [{db_service_name}]: {err}"
-            )
+                om_cols.append(om_column)
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.warning(f"Error parsing column [{col}]: {exc}")
+                continue
+        return om_cols
+
+    def get_database_entity(self, database_name: str) -> Database:
+        return CreateDatabaseRequest(
+            name=database_name,
+            service=self.service.fullyQualifiedName,
+        )
 
-    def yield_dashboard_chart(
-        self, dashboard_details: TableauDashboard
-    ) -> Optional[Iterable[CreateChartRequest]]:
+    def ingest_lineage(self, source_guid, name) -> Iterable[AddLineageRequest]:
         """
-        Method to fetch charts linked to dashboard
+        Fetch and ingest lineage
         """
-        for chart in dashboard_details.charts or []:
-            try:
-                if filter_by_chart(self.source_config.chartFilterPattern, chart.name):
-                    self.status.filter(chart.name, "Chart Pattern not allowed")
+        lineage_response = self.atlas_client.get_lineage(source_guid)
+        lineage_relations = lineage_response["relations"]
+        tbl_entity = self.atlas_client.get_entity(lineage_response["baseEntityGuid"])
+        for key in tbl_entity["referredEntities"].keys():
+            if not tbl_entity["entities"][0]["relationshipAttributes"].get(
+                self.entity_types["Table"][name]["db"]
+            ):
+                continue
+            db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
+                self.entity_types["Table"][name]["db"]
+            ]
+            if not tbl_entity["referredEntities"].get(key):
+                continue
+            table_name = tbl_entity["referredEntities"][key]["relationshipAttributes"][
+                "table"
+            ]["displayText"]
+            from_fqn = fqn.build(
+                self.metadata,
+                entity_type=Table,
+                service_name=self.config.serviceName,
+                database_name=db_entity["displayText"],
+                schema_name=db_entity["displayText"],
+                table_name=table_name,
+            )
+            from_entity_ref = self.get_lineage_entity_ref(
+                from_fqn, self.metadata_config, "table"
+            )
+            for edge in lineage_relations:
+                if (
+                    lineage_response["guidEntityMap"][edge["toEntityId"]]["typeName"]
+                    == "processor"
+                ):
                     continue
-                site_url = (
-                    f"site/{self.service_connection.siteUrl}/"
-                    if self.service_connection.siteUrl
-                    else ""
-                )
-                workbook_chart_name = ChartUrl(chart.content_url)
 
-                chart_url = (
-                    f"#{site_url}"
-                    f"views/{workbook_chart_name.workbook_name}/"
-                    f"{workbook_chart_name.chart_url_name}"
-                )
+                tbl_entity = self.atlas_client.get_entity(edge["toEntityId"])
+                for key in tbl_entity["referredEntities"]:
+                    db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
+                        self.entity_types["Table"][name]["db"]
+                    ]
+
+                    db = self.get_database_entity(db_entity["displayText"])
+                    table_name = tbl_entity["referredEntities"][key][
+                        "relationshipAttributes"
+                    ]["table"]["displayText"]
+                    to_fqn = fqn.build(
+                        self.metadata,
+                        entity_type=Table,
+                        service_name=self.config.serviceName,
+                        database_name=db.name.__root__,
+                        schema_name=db_entity["displayText"],
+                        table_name=table_name,
+                    )
+                    to_entity_ref = self.get_lineage_entity_ref(
+                        to_fqn, self.metadata_config, "table"
+                    )
+                    yield from self.yield_lineage(from_entity_ref, to_entity_ref)
 
-                yield CreateChartRequest(
-                    name=chart.id,
-                    displayName=chart.name,
-                    chartType=get_standard_chart_type(chart.sheet_type),
-                    chartUrl=chart_url,
-                    tags=tag_utils.get_tag_labels(
-                        metadata=self.metadata,
-                        tags=chart.tags,
-                        classification_name=TABLEAU_TAG_CATEGORY,
-                        include_tags=self.source_config.includeTags,
-                    ),
-                    service=self.context.dashboard_service.fullyQualifiedName.__root__,
-                )
-                self.status.scanned(chart.id)
-            except Exception as exc:
-                logger.debug(traceback.format_exc())
-                logger.warning(f"Error to yield dashboard chart [{chart}]: {exc}")
+    def get_database_service(self):
+        service = self.metadata.create_or_update(
+            CreateDatabaseServiceRequest(
+                name=SERVICE_TYPE_MAPPER.get("hive")["service_name"],
+                displayName=f"{self.config.serviceName}_database",
+                serviceType=SERVICE_TYPE_MAPPER.get("hive")["service_name"],
+                connection=SERVICE_TYPE_MAPPER["hive"]["connection"],
+            )
+        )
+        if service is not None:
+            return service
+        logger.error("Failed to create a service with name detlaLake")
+        return None
 
-    def close(self):
-        try:
-            self.client.sign_out()
-        except ConnectionError as err:
-            logger.debug(f"Error closing connection - {err}")
+    def get_message_service(self):
+        service = self.metadata.create_or_update(
+            CreateMessagingServiceRequest(
+                name=SERVICE_TYPE_MAPPER.get("kafka")["service_name"],
+                displayName=f"{self.config.serviceName}_messaging",
+                serviceType=SERVICE_TYPE_MAPPER.get("kafka")["service_name"],
+                connection=SERVICE_TYPE_MAPPER.get("kafka")["connection"],
+            )
+        )
+        if service is not None:
+            return service
+        logger.error("Failed to create a service with name kafka")
+        return None
+
+    def yield_lineage(self, from_entity_ref, to_entity_ref):
+        if from_entity_ref and to_entity_ref and from_entity_ref != to_entity_ref:
+            lineage = AddLineageRequest(
+                edge=EntitiesEdge(fromEntity=from_entity_ref, toEntity=to_entity_ref)
+            )
+            yield lineage
+
+    def get_lineage_entity_ref(
+        self, to_fqn, metadata_config, entity_type
+    ) -> EntityReference:
+        metadata = OpenMetadata(metadata_config)
+        if entity_type == "table":
+            table = metadata.get_by_name(entity=Table, fqn=to_fqn)
+            if table:
+                return EntityReference(id=table.id.__root__, type="table")
+        if entity_type == "pipeline":
+            pipeline = metadata.get_by_name(entity=Pipeline, fqn=to_fqn)
+            if pipeline:
+                return EntityReference(id=pipeline.id.__root__, type="pipeline")
+        return None
+
+    def test_connection(self) -> None:
+        test_connection_fn = get_test_connection_fn(self.service_connection)
+        test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/dashboard/tableau/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/base.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,32 +4,26 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-GraphQL queries used during ingestion
+Base local reader
 """
+from abc import ABC, abstractmethod
 
-TABLEAU_LINEAGE_GRAPHQL_QUERY = """
-{
-  workbooks {
-    id
-    luid
-    name
-    upstreamTables{
-      name
-      schema
-      upstreamDatabases{
-        name
-      }
-      referencedByQueries{
-        name
-        query
-      }
-    }
-  }
-}
-"""
+
+class ReadException(Exception):
+    """
+    To be raised by any errors with the read calls
+    """
+
+
+class Reader(ABC):
+    @abstractmethod
+    def read(self, path: str) -> str:
+        """
+        Given a string, return a string
+        """
+        raise NotImplementedError("Missing read implementation")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/athena/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/athena/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/athena/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/azuresql/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/azuresql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/azuresql/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/azuresql/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -376,15 +376,14 @@
     ) -> Tuple[bool, TablePartition]:
         """
         check if the table is partitioned table and return the partition details
         """
         database = self.context.database.name.__root__
         table = self.client.get_table(f"{database}.{schema_name}.{table_name}")
         if table.time_partitioning is not None:
-
             if table.time_partitioning.field:
                 table_partition = TablePartition(
                     interval=str(table.time_partitioning.type_),
                     intervalType=IntervalType.TIME_UNIT.value,
                 )
                 table_partition.columns = [table.time_partitioning.field]
                 return True, table_partition
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/bigquery/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/bigquery/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -74,15 +74,14 @@
 )
 
 
 @reflection.cache
 def _get_column_type(
     self, name, spec
 ):  # pylint: disable=protected-access,too-many-branches,too-many-return-statements
-
     if spec.startswith("Array"):
         return self.ischema_names["Array"]
 
     if spec.startswith("FixedString"):
         return self.ischema_names["FixedString"]
 
     if spec.startswith("Nullable"):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/clickhouse/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/clickhouse/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/column_helpers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/column_helpers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/column_type_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/column_type_parser.py`

 * *Files 1% similar despite different names*

```diff
@@ -117,14 +117,16 @@
         "HYPERLOGLOG": "BINARY",
         "IMAGE": "BINARY",
         "INT": "INT",
         "INT2": "SMALLINT",
         "INT4": "INT",
         "INT8": "BIGINT",
         "INT16": "BIGINT",
+        "TUPLE": "TUPLE",
+        "SPATIAL": "SPATIAL",
         "INT32": "BIGINT",
         "INT64": "BIGINT",
         "INT128": "BIGINT",
         "INT256": "BIGINT",
         "INTEGER": "INT",
         "UINT": "INT",
         "UINT2": "SMALLINT",
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/common_db_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/common_db_source.py`

 * *Files 0% similar despite different names*

```diff
@@ -299,15 +299,14 @@
                 f"Fetching tables names failed for schema {schema_name} due to - {err}"
             )
             logger.debug(traceback.format_exc())
 
     def get_view_definition(
         self, table_type: str, table_name: str, schema_name: str, inspector: Inspector
     ) -> Optional[str]:
-
         if table_type == TableType.View:
             try:
                 view_definition = inspector.get_view_definition(table_name, schema_name)
                 view_definition = (
                     "" if view_definition is None else str(view_definition)
                 )
                 return view_definition
@@ -352,15 +351,14 @@
         """
         From topology.
         Prepare a table request and pass it to the sink
         """
         table_name, table_type = table_name_and_type
         schema_name = self.context.database_schema.name.__root__
         try:
-
             (
                 columns,
                 table_constraints,
                 foreign_columns,
             ) = self.get_columns_and_constraints(
                 schema_name=schema_name,
                 table_name=table_name,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/database_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/database_service.py`

 * *Files 7% similar despite different names*

```diff
@@ -17,55 +17,44 @@
 from pydantic import BaseModel
 from sqlalchemy.engine import Inspector
 
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
-from metadata.generated.schema.api.data.createLocation import CreateLocationRequest
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.api.services.createStorageService import (
-    CreateStorageServiceRequest,
-)
 from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
-from metadata.generated.schema.entity.data.location import Location
 from metadata.generated.schema.entity.data.table import (
     Column,
     DataModel,
     Table,
     TableType,
 )
 from metadata.generated.schema.entity.services.databaseService import (
     DatabaseConnection,
     DatabaseService,
 )
-from metadata.generated.schema.entity.services.storageService import StorageService
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.basic import FullyQualifiedEntityName
-from metadata.generated.schema.type.storage import StorageServiceType
 from metadata.generated.schema.type.tagLabel import (
     LabelType,
     State,
     TagLabel,
     TagSource,
 )
 from metadata.ingestion.api.source import Source
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
-from metadata.ingestion.models.delete_entity import (
-    DeleteEntity,
-    delete_entity_from_source,
-)
+from metadata.ingestion.models.delete_entity import delete_entity_from_source
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.models.table_metadata import OMetaTableConstraints
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
     TopologyNode,
     create_source_context,
@@ -83,23 +72,14 @@
     Tmp model to handle data model ingestion
     """
 
     table_entity: Table
     datamodel: DataModel
 
 
-class TableLocationLink(BaseModel):
-    """
-    Model to handle table and location link
-    """
-
-    table_fqn: FullyQualifiedEntityName
-    location_fqn: FullyQualifiedEntityName
-
-
 class DatabaseServiceTopology(ServiceTopology):
     """
     Defines the hierarchy in Database Services.
     service -> db -> schema -> table.
 
     We could have a topology validator. We can only consume
     data that has been produced by any parent node.
@@ -111,20 +91,14 @@
             NodeStage(
                 type_=DatabaseService,
                 context="database_service",
                 processor="yield_create_request_database_service",
                 overwrite=False,
                 must_return=True,
             ),
-            NodeStage(
-                type_=StorageService,
-                context="storage_service",
-                processor="yield_storage_service",
-                nullable=True,
-            ),
         ],
         children=["database"],
         post_process=["yield_view_lineage", "yield_table_constraints"],
     )
     database = TopologyNode(
         producer="get_database_names",
         stages=[
@@ -163,27 +137,14 @@
         stages=[
             NodeStage(
                 type_=Table,
                 context="table",
                 processor="yield_table",
                 consumer=["database_service", "database", "database_schema"],
             ),
-            NodeStage(
-                type_=Location,
-                context="location",
-                processor="yield_location",
-                consumer=["storage_service"],
-                nullable=True,
-            ),
-            NodeStage(
-                type_=TableLocationLink,
-                processor="yield_table_location_link",
-                ack_sink=False,
-                nullable=True,
-            ),
         ],
     )
 
 
 class DatabaseServiceSource(
     TopologyRunnerMixin, Source, ABC
 ):  # pylint: disable=too-many-public-methods
@@ -211,23 +172,14 @@
         yield self.config
 
     def yield_create_request_database_service(self, config: WorkflowSource):
         yield self.metadata.get_create_service_from_source(
             entity=DatabaseService, config=config
         )
 
-    def yield_storage_service(self, config: WorkflowSource):
-        if hasattr(self.service_connection, "storageServiceName"):
-            service_json = {
-                "name": self.service_connection.storageServiceName,
-                "serviceType": StorageServiceType.S3,
-            }
-            storage_service = CreateStorageServiceRequest(**service_json)
-            yield storage_service
-
     @abstractmethod
     def get_database_names(self) -> Iterable[str]:
         """
         Prepares the database name to be sent to stage.
         Filtering happens here.
         """
 
@@ -303,40 +255,20 @@
         """
         From topology.
         Prepare a table request and pass it to the sink.
 
         Also, update the self.inspector value to the current db.
         """
 
-    def yield_location(
-        self,
-        table_name_and_type: Tuple[str, TableType],  # pylint: disable=unused-argument
-    ) -> Iterable[CreateLocationRequest]:
-        """
-        From topology.
-        Prepare a location request and pass it to the sink.
-        """
-        return
-
     def get_raw_database_schema_names(self) -> Iterable[str]:
         """
         fetch all schema names without any filtering.
         """
         yield from self.get_database_schema_names()
 
-    def yield_table_location_link(
-        self,
-        table_name_and_type: Tuple[str, TableType],  # pylint: disable=unused-argument
-    ) -> Iterable[TableLocationLink]:
-        """
-        Gets the current location being processed, fetches its data model
-        and sends it ot the sink
-        """
-        return
-
     def get_tag_by_fqn(self, entity_fqn: str) -> Optional[List[TagLabel]]:
         """
         Pick up the tags registered in the context
         searching by entity FQN
         """
         return [
             TagLabel(
@@ -401,43 +333,50 @@
             table_name=table_request.name.__root__,
             skip_es_search=True,
         )
 
         self.database_source_state.add(table_fqn)
         self.status.scanned(table_fqn)
 
-    def delete_schema_tables(self, schema_fqn: str) -> Iterable[DeleteEntity]:
-        """
-        Returns Deleted tables
-        """
-        yield from delete_entity_from_source(
-            metadata=self.metadata,
-            entity_type=Table,
-            entity_source_state=self.database_source_state,
-            mark_deleted_entity=self.source_config.markDeletedTables,
-            params={"database": schema_fqn},
-        )
-
     def fetch_all_schema_and_delete_tables(self):
         """
         Fetch all schemas and delete tables
         """
         database_fqn = fqn.build(
             self.metadata,
             entity_type=Database,
             service_name=self.config.serviceName,
             database_name=self.context.database.name.__root__,
         )
         schema_list = self.metadata.list_all_entities(
             entity=DatabaseSchema, params={"database": database_fqn}
         )
         for schema in schema_list:
-            yield from self.delete_schema_tables(schema.fullyQualifiedName.__root__)
+            yield from delete_entity_from_source(
+                metadata=self.metadata,
+                entity_type=Table,
+                entity_source_state=self.database_source_state,
+                mark_deleted_entity=self.source_config.markDeletedTables,
+                params={"database": schema.fullyQualifiedName.__root__},
+            )
+
+        # Delete the schema
+        yield from delete_entity_from_source(
+            metadata=self.metadata,
+            entity_type=DatabaseSchema,
+            entity_source_state=list(
+                self._get_filtered_schema_names(return_fqn=True, add_to_status=False)
+            ),
+            mark_deleted_entity=self.source_config.markDeletedTables,
+            params={"database": database_fqn},
+        )
 
-    def _get_filtered_schema_names(self, add_to_status: bool = True) -> Iterable[str]:
+    def _get_filtered_schema_names(
+        self, return_fqn: bool = False, add_to_status: bool = True
+    ) -> Iterable[str]:
         for schema_name in self.get_raw_database_schema_names():
             schema_fqn = fqn.build(
                 self.metadata,
                 entity_type=DatabaseSchema,
                 service_name=self.context.database_service.name.__root__,
                 database_name=self.context.database.name.__root__,
                 schema_name=schema_name,
@@ -445,15 +384,15 @@
             if filter_by_schema(
                 self.source_config.schemaFilterPattern,
                 schema_fqn if self.source_config.useFqnForFiltering else schema_name,
             ):
                 if add_to_status:
                     self.status.filter(schema_fqn, "Schema Filtered Out")
                 continue
-            yield schema_name
+            yield schema_fqn if return_fqn else schema_name
 
     def mark_tables_as_deleted(self):
         """
         Use the current inspector to mark tables as deleted
         """
         if self.source_config.markDeletedTables:
             logger.info(
@@ -461,22 +400,23 @@
             )
             # If markAllDeletedTables is True, all tables Which are not in FilterPattern will be deleted
             if self.source_config.markAllDeletedTables:
                 yield from self.fetch_all_schema_and_delete_tables()
 
             # If markAllDeletedTables is False (Default), Only delete tables which are deleted from the datasource
             else:
-                schema_names_list = self._get_filtered_schema_names(add_to_status=False)
-                for schema_name in schema_names_list:
-                    schema_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=DatabaseSchema,
-                        service_name=self.config.serviceName,
-                        database_name=self.context.database.name.__root__,
-                        schema_name=schema_name,
+                schema_fqn_list = self._get_filtered_schema_names(
+                    return_fqn=True, add_to_status=False
+                )
+
+                for schema_fqn in schema_fqn_list:
+                    yield from delete_entity_from_source(
+                        metadata=self.metadata,
+                        entity_type=Table,
+                        entity_source_state=self.database_source_state,
+                        mark_deleted_entity=self.source_config.markDeletedTables,
+                        params={"database": schema_fqn},
                     )
 
-                    yield from self.delete_schema_tables(schema_fqn)
-
     def test_connection(self) -> None:
         test_connection_fn = get_test_connection_fn(self.service_connection)
         test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/client.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,20 +18,19 @@
 
 import requests
 
 from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
     DatabricksConnection,
 )
 from metadata.ingestion.ometa.client import APIError
+from metadata.utils.constants import QUERY_WITH_DBT, QUERY_WITH_OM_VERSION
 from metadata.utils.helpers import datetime_to_ts
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
-QUERY_WITH_OM_VERSION = '/* {"app": "OpenMetadata"'
-QUERY_WITH_DBT = '/* {"app": "dbt"'
 API_TIMEOUT = 10
 
 
 class DatabricksClient:
     """
     DatabricksClient creates a Databricks connection based on DatabricksCredentials.
     """
@@ -100,25 +99,23 @@
                 while True:
                     if result:
                         query_details.extend(result)
 
                         next_page_token = response.get("next_page_token", None)
                         has_next_page = response.get("has_next_page", None)
                         if next_page_token:
-
                             data["page_token"] = next_page_token
 
                         if not has_next_page:
                             data = {}
                             break
                     else:
                         break
 
                     if result[-1]["execution_end_time_ms"] <= end_time:
-
                         response = self.client.get(
                             self.base_query_url,
                             data=json.dumps(data),
                             headers=self.headers,
                             timeout=API_TIMEOUT,
                         ).json()
                         result = response.get("res")
@@ -150,15 +147,14 @@
                 headers=self.headers,
                 timeout=API_TIMEOUT,
             ).json()
 
             job_list.extend(response.get("jobs") or [])
 
             while response["has_more"]:
-
                 data["offset"] = len(response.get("jobs") or [])
 
                 response = self.client.get(
                     self.jobs_list_url,
                     data=json.dumps(data),
                     headers=self.headers,
                     timeout=API_TIMEOUT,
@@ -192,15 +188,14 @@
                 headers=self.headers,
                 timeout=API_TIMEOUT,
             ).json()
 
             job_runs.extend(response.get("runs") or [])
 
             while response["has_more"]:
-
                 params.update({"start_time_to": response["runs"][-1]["start_time"]})
 
                 response = self.client.get(
                     self.jobs_run_list_url,
                     params=params,
                     headers=self.headers,
                     timeout=API_TIMEOUT,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/lineage.py`

 * *Files 20% similar despite different names*

```diff
@@ -7,15 +7,14 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Databricks lineage module
 """
-import csv
 import traceback
 from datetime import datetime
 from typing import Iterator, Optional
 
 from metadata.generated.schema.type.tableQuery import TableQuery
 from metadata.ingestion.source.database.databricks.query_parser import (
     DatabricksQueryParserSource,
@@ -27,54 +26,26 @@
 
 
 class DatabricksLineageSource(DatabricksQueryParserSource, LineageSource):
     """
     Databricks Lineage Source
     """
 
-    def get_table_query(self) -> Optional[Iterator[TableQuery]]:
-        """
-        If queryLogFilePath available in config iterate through log file
-        otherwise execute the sql query to fetch TableQuery data.
-
-        This is a simplified version of the UsageSource query parsing.
-        """
-        if self.config.sourceConfig.config.queryLogFilePath:
-
-            with open(
-                self.config.sourceConfig.config.queryLogFilePath, "r", encoding="utf-8"
-            ) as file:
-                for row in csv.DictReader(file):
-                    query_dict = dict(row)
+    def yield_table_query(self) -> Optional[Iterator[TableQuery]]:
+        data = self.client.list_query_history(
+            start_date=self.start,
+            end_date=self.end,
+        )
+        for row in data:
+            try:
+                if self.client.is_query_valid(row):
                     yield TableQuery(
-                        query=query_dict["query_text"],
-                        databaseName=self.get_database_name(query_dict),
+                        query=row.get("query_text"),
+                        userName=row.get("user_name"),
+                        startTime=row.get("query_start_time_ms"),
+                        endTime=row.get("execution_end_time_ms"),
+                        analysisDate=datetime.now(),
                         serviceName=self.config.serviceName,
-                        databaseSchema=self.get_schema_name(query_dict),
                     )
-
-        else:
-            logger.info(
-                f"Scanning query logs for {self.start.date()} - {self.end.date()}"
-            )
-            try:
-                data = self.client.list_query_history(
-                    start_date=self.start,
-                    end_date=self.end,
-                )
-                for row in data:
-                    try:
-                        if self.client.is_query_valid(row):
-                            yield TableQuery(
-                                query=row.get("query_text"),
-                                userName=row.get("user_name"),
-                                startTime=row.get("query_start_time_ms"),
-                                endTime=row.get("execution_end_time_ms"),
-                                analysisDate=datetime.now(),
-                                serviceName=self.config.serviceName,
-                            )
-                    except Exception as exc:
-                        logger.debug(traceback.format_exc())
-                        logger.warning(f"Error processing query_dict {row}: {exc}")
             except Exception as exc:
                 logger.debug(traceback.format_exc())
-                logger.error(f"Source usage processing error: {exc}")
+                logger.warning(f"Error processing query_dict {row}: {exc}")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -108,15 +108,15 @@
     value should match what is provided in the 'source.config.database' field in the
     Databricks ingest config file.
     """
     db_name = kw["db_name"] if "db_name" in kw else None
 
     rows = _get_column_rows(self, connection, table_name, schema)
     result = []
-    for (col_name, col_type, _comment) in rows:
+    for col_name, col_type, _comment in rows:
         # Handle both oss hive and Databricks' hive partition header, respectively
         if col_name in ("# Partition Information", "# Partitioning"):
             break
         # Take out the more detailed type information
         # e.g. 'map<ixnt,int>' -> 'map'
         #      'decimal(10,1)' -> decimal
         raw_col_type = col_type
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/databricks/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/databricks/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/lineage_source.py`

 * *Files 15% similar despite different names*

```diff
@@ -5,113 +5,115 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Databricks usage module
+Lineage Source Module
 """
 import csv
 import traceback
-from datetime import datetime
-from typing import Iterable, Optional
+from abc import ABC
+from typing import Iterable, Iterator, Optional
 
-from metadata.generated.schema.type.tableQuery import TableQueries, TableQuery
-from metadata.ingestion.source.database.databricks.query_parser import (
-    DatabricksQueryParserSource,
-)
-from metadata.ingestion.source.database.usage_source import UsageSource
+from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
+from metadata.generated.schema.type.tableQuery import TableQuery
+from metadata.ingestion.lineage.models import ConnectionTypeDialectMapper
+from metadata.ingestion.lineage.sql_lineage import get_lineage_by_query
+from metadata.ingestion.source.database.query_parser_source import QueryParserSource
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class DatabricksUsageSource(DatabricksQueryParserSource, UsageSource):
-    """
-    Databricks Usage Source
+class LineageSource(QueryParserSource, ABC):
     """
+    This is the base source to handle Lineage-only ingestion.
 
-    def get_table_query(self) -> Iterable[TableQuery]:
+    We will still use TableQuery as the data, but only fill up those elements
+    that are truly required for the lineage use case, such as:
+    - query
+    - service
+    - database
+    - schema
+    """
 
+    def yield_table_queries_from_logs(self) -> Optional[Iterator[TableQuery]]:
+        """
+        Method to handle the usage from query logs
+        """
         try:
-            if self.config.sourceConfig.config.queryLogFilePath:
-                table_query_list = []
-                with open(
-                    self.config.sourceConfig.config.queryLogFilePath,
-                    "r",
-                    encoding="utf-8",
-                ) as query_log_file:
-
-                    for raw in csv.DictReader(query_log_file):
-                        query_dict = dict(raw)
-
-                        analysis_date = (
-                            datetime.utcnow()
-                            if not query_dict.get("session_start_time")
-                            else datetime.strptime(
-                                query_dict.get("session_start_time"),
-                                "%Y-%m-%d %H:%M:%S+%f",
-                            )
-                        )
-
-                        query_dict["aborted"] = query_dict["sql_state_code"] == "00000"
-                        if "statement" in query_dict["message"]:
-                            query_dict["message"] = query_dict["message"].split(":")[1]
-
-                        table_query_list.append(
-                            TableQuery(
-                                query=query_dict["message"],
-                                userName=query_dict.get("user_name", ""),
-                                startTime=query_dict.get("session_start_time", ""),
-                                endTime=query_dict.get("log_time", ""),
-                                analysisDate=analysis_date,
-                                aborted=self.get_aborted_status(query_dict),
-                                databaseName=self.get_database_name(query_dict),
-                                serviceName=self.config.serviceName,
-                                databaseSchema=self.get_schema_name(query_dict),
-                            )
-                        )
-                yield TableQueries(queries=table_query_list)
-
-            else:
-
-                yield from self.process_table_query()
-
+            with open(
+                self.config.sourceConfig.config.queryLogFilePath, "r", encoding="utf-8"
+            ) as file:
+                for row in csv.DictReader(file):
+                    query_dict = dict(row)
+                    yield TableQuery(
+                        query=query_dict["query_text"],
+                        databaseName=self.get_database_name(query_dict),
+                        serviceName=self.config.serviceName,
+                        databaseSchema=self.get_schema_name(query_dict),
+                    )
         except Exception as err:
-            logger.error(f"Source usage processing error - {err}")
             logger.debug(traceback.format_exc())
+            logger.warning(f"Failed to read queries form log file due to: {err}")
 
-    def process_table_query(self) -> Optional[Iterable[TableQuery]]:
+    def get_table_query(self) -> Optional[Iterator[TableQuery]]:
         """
-        Method to yield TableQueries
+        If queryLogFilePath available in config iterate through log file
+        otherwise execute the sql query to fetch TableQuery data.
+
+        This is a simplified version of the UsageSource query parsing.
         """
-        try:
-            queries = []
-            data = self.client.list_query_history(
-                start_date=self.start,
-                end_date=self.end,
+        if self.config.sourceConfig.config.queryLogFilePath:
+            yield from self.yield_table_queries_from_logs()
+        else:
+            logger.info(
+                f"Scanning query logs for {self.start.date()} - {self.end.date()}"
             )
-            for row in data:
+            yield from self.yield_table_query()
+
+    def yield_table_query(self) -> Iterator[TableQuery]:
+        """
+        Given an engine, iterate over the query results to
+        yield a TableQuery with query parsing info
+        """
+        with self.engine.connect() as conn:
+            rows = conn.execute(
+                self.get_sql_statement(
+                    start_time=self.start,
+                    end_time=self.end,
+                )
+            )
+            for row in rows:
+                query_dict = dict(row)
                 try:
-                    if self.client.is_query_valid(row):
-                        queries.append(
-                            TableQuery(
-                                query=row.get("query_text"),
-                                userName=row.get("user_name"),
-                                startTime=row.get("query_start_time_ms"),
-                                endTime=row.get("execution_end_time_ms"),
-                                analysisDate=datetime.now(),
-                                serviceName=self.config.serviceName,
-                                duration=row.get("duration") / 1000
-                                if row.get("duration")
-                                else None,
-                            )
-                        )
-                except Exception as err:
+                    yield TableQuery(
+                        query=query_dict["query_text"],
+                        databaseName=self.get_database_name(query_dict),
+                        serviceName=self.config.serviceName,
+                        databaseSchema=self.get_schema_name(query_dict),
+                    )
+                except Exception as exc:
                     logger.debug(traceback.format_exc())
-                    logger.error(str(err))
+                    logger.warning(f"Error processing query_dict {query_dict}: {exc}")
 
-            yield TableQueries(queries=queries)
-        except Exception as err:
-            logger.error(f"Source usage processing error - {err}")
-            logger.debug(traceback.format_exc())
+    def next_record(self) -> Iterable[AddLineageRequest]:
+        """
+        Based on the query logs, prepare the lineage
+        and send it to the sink
+        """
+        connection_type = str(self.service_connection.type.value)
+        dialect = ConnectionTypeDialectMapper.dialect_of(connection_type)
+        for table_query in self.get_table_query():
+            lineages = get_lineage_by_query(
+                self.metadata,
+                query=table_query.query,
+                service_name=table_query.serviceName,
+                database_name=table_query.databaseName,
+                schema_name=table_query.databaseSchema,
+                dialect=dialect,
+            )
+
+            for lineage_request in lineages or []:
+                yield lineage_request
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/metadata.py`

 * *Files 9% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 DataLake connector to fetch metadata from a files stored s3, gcs and Hdfs
 """
 import traceback
-from typing import Iterable, Optional, Tuple
+from typing import Iterable, List, Optional, Tuple
 
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
@@ -51,14 +51,15 @@
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.database_service import DatabaseServiceSource
 from metadata.ingestion.source.database.datalake.models import DatalakeColumnWrapper
+from metadata.ingestion.source.database.datalake.utils import COMPLEX_COLUMN_SEPARATOR
 from metadata.utils import fqn
 from metadata.utils.constants import DEFAULT_DATABASE
 from metadata.utils.filters import filter_by_schema, filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
@@ -82,32 +83,48 @@
 ) + JSON_SUPPORTED_TYPES
 
 
 def ometa_to_dataframe(config_source, client, table):
     """
     Method to get dataframe for profiling
     """
+
     data = None
     if isinstance(config_source, GCSConfig):
         data = DatalakeSource.get_gcs_files(
             client=client,
             key=table.name.__root__,
             bucket_name=table.databaseSchema.name,
         )
     if isinstance(config_source, S3Config):
         data = DatalakeSource.get_s3_files(
             client=client,
             key=table.name.__root__,
             bucket_name=table.databaseSchema.name,
         )
+    if isinstance(config_source, AzureConfig):
+        connection_args = config_source.securityConfig
+        data = DatalakeSource.get_azure_files(
+            client=client,
+            key=table.name.__root__,
+            container_name=table.databaseSchema.name,
+            storage_options={
+                "tenant_id": connection_args.tenantId,
+                "client_id": connection_args.clientId,
+                "client_secret": connection_args.clientSecret.get_secret_value(),
+                "account_name": connection_args.accountName,
+            },
+        )
     if isinstance(data, DatalakeColumnWrapper):
         data = data.dataframes
+
     return data
 
 
+# pylint: disable=too-many-public-methods
 class DatalakeSource(DatabaseServiceSource):
     """
     Implements the necessary methods to extract
     Database metadata from Datalake Source
     """
 
     def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
@@ -337,15 +354,14 @@
                     )
                     if filter_by_table(
                         self.config.sourceConfig.config.tableFilterPattern,
                         table_fqn
                         if self.config.sourceConfig.config.useFqnForFiltering
                         else table_name,
                     ):
-
                         self.status.filter(
                             table_fqn,
                             "Object Filtered Out",
                         )
                         continue
                     if not self.check_valid_file_type(key["Key"]):
                         logger.debug(
@@ -562,43 +578,157 @@
             logger.debug(traceback.format_exc())
             logger.error(
                 f"Unexpected exception to get S3 file [{key}] from bucket [{bucket_name}]: {exc}"
             )
         return None
 
     @staticmethod
+    def _parse_complex_column(
+        data_frame,
+        column,
+        final_column_list: List[Column],
+        complex_col_dict: dict,
+        processed_complex_columns: set,
+    ) -> None:
+        """
+        This class parses the complex columns
+
+        for example consider this data:
+            {
+                "level1": {
+                    "level2":{
+                        "level3": 1
+                    }
+                }
+            }
+
+        pandas would name this column as: _##level1_##level2_##level3
+        (_## being the custom separator)
+
+        this function would parse this column name and prepare a Column object like
+        Column(
+            name="level1",
+            dataType="RECORD",
+            children=[
+                Column(
+                    name="level2",
+                    dataType="RECORD",
+                    children=[
+                        Column(
+                            name="level3",
+                            dataType="INT",
+                        )
+                    ]
+                )
+            ]
+        )
+        """
+        try:
+            # pylint: disable=bad-str-strip-call
+            column_name = str(column).strip(COMPLEX_COLUMN_SEPARATOR)
+            col_hierarchy = tuple(column_name.split(COMPLEX_COLUMN_SEPARATOR))
+            parent_col: Optional[Column] = None
+            root_col: Optional[Column] = None
+            for index, col_name in enumerate(col_hierarchy[:-1]):
+                if complex_col_dict.get(col_hierarchy[: index + 1]):
+                    parent_col = complex_col_dict.get(col_hierarchy[: index + 1])
+                else:
+                    intermediate_column = Column(
+                        name=col_name[:64],
+                        dataType=DataType.RECORD.value,
+                        children=[],
+                        dataTypeDisplay=DataType.RECORD.value,
+                    )
+                    if parent_col:
+                        parent_col.children.append(intermediate_column)
+                        root_col = parent_col
+                    parent_col = intermediate_column
+                    complex_col_dict[col_hierarchy[: index + 1]] = parent_col
+
+            # use String by default
+            data_type = DataType.STRING.value
+            if hasattr(data_frame[column], "dtypes"):
+                data_type = DATALAKE_DATA_TYPES.get(
+                    data_frame[column].dtypes.name, DataType.STRING.value
+                )
+            leaf_column = Column(
+                name=col_hierarchy[-1],
+                dataType=data_type,
+                dataTypeDisplay=data_type,
+            )
+            parent_col.children.append(leaf_column)
+
+            if col_hierarchy[0] not in processed_complex_columns and root_col:
+                processed_complex_columns.add(col_hierarchy[0])
+                final_column_list.append(root_col)
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Unexpected exception parsing column [{column}]: {exc}")
+
+    @staticmethod
+    def fetch_col_types(data_frame, column_name):
+        data_type = DATALAKE_DATA_TYPES.get(
+            data_frame[column_name].dtypes.name, DataType.STRING.value
+        )
+        if data_type == DataType.FLOAT.value:
+            try:
+                if data_frame[column_name].dropna().any():
+                    if isinstance(data_frame[column_name].iloc[0], dict):
+                        return DataType.JSON.value
+                    if isinstance(data_frame[column_name].iloc[0], str):
+                        return DataType.STRING.value
+            except Exception as err:
+                logger.warning(
+                    f"Failed to disinguish data type for column {column_name}, Falling back to {data_type}, exc: {err}"
+                )
+                logger.debug(traceback.format_exc())
+        return data_type
+
+    @staticmethod
     def get_columns(data_frame):
         """
         method to process column details
         """
         cols = []
+        complex_col_dict = {}
+        processed_complex_columns = set()
         if hasattr(data_frame, "columns"):
             df_columns = list(data_frame.columns)
             for column in df_columns:
-                # use String by default
-                data_type = DataType.STRING.value
-                try:
-                    if hasattr(data_frame[column], "dtypes"):
-                        data_type = DATALAKE_DATA_TYPES.get(
-                            data_frame[column].dtypes.name, DataType.STRING.value
-                        )
 
-                    parsed_string = {
-                        "dataTypeDisplay": data_type,
-                        "dataType": data_type,
-                        "name": column[:64],
-                    }
-                    parsed_string["dataLength"] = parsed_string.get("dataLength", 1)
-                    cols.append(Column(**parsed_string))
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(
-                        f"Unexpected exception parsing column [{column}]: {exc}"
+                if COMPLEX_COLUMN_SEPARATOR in column:
+                    DatalakeSource._parse_complex_column(
+                        data_frame,
+                        column,
+                        cols,
+                        complex_col_dict,
+                        processed_complex_columns,
                     )
+                else:
+                    # use String by default
+                    data_type = DataType.STRING.value
+                    try:
+                        if hasattr(data_frame[column], "dtypes"):
+                            data_type = DatalakeSource.fetch_col_types(
+                                data_frame, column_name=column
+                            )
 
+                        parsed_string = {
+                            "dataTypeDisplay": data_type,
+                            "dataType": data_type,
+                            "name": column[:64],
+                        }
+                        parsed_string["dataLength"] = parsed_string.get("dataLength", 1)
+                        cols.append(Column(**parsed_string))
+                    except Exception as exc:
+                        logger.debug(traceback.format_exc())
+                        logger.warning(
+                            f"Unexpected exception parsing column [{column}]: {exc}"
+                        )
+        complex_col_dict.clear()
         return cols
 
     def yield_view_lineage(self) -> Optional[Iterable[AddLineageRequest]]:
         yield from []
 
     def yield_tag(self, schema_name: str) -> Iterable[OMetaTagAndClassification]:
         pass
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/datalake/utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/datalake/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,14 +40,15 @@
     DataTypeTopic.FLOAT.value: "float",
     DataTypeTopic.DOUBLE.value: "float",
     DataTypeTopic.TIMESTAMP.value: "float",
     DataTypeTopic.TIMESTAMPZ.value: "float",
 }
 
 AVRO_SCHEMA = "avro.schema"
+COMPLEX_COLUMN_SEPARATOR = "_##"
 
 
 def read_from_avro(
     avro_text: bytes,
 ) -> Union[DatalakeColumnWrapper, List[pd.DataFrame]]:
     """
     Method to parse the avro data from storage sources
@@ -99,11 +100,9 @@
         logger.debug("Failed to read as JSON object trying to read as JSON Lines")
         data = [
             json.loads(json_obj)
             for json_obj in json_text.strip().split("\n")[:sample_size]
         ]
 
     if isinstance(data, list):
-        return [pd.DataFrame.from_dict(data[:sample_size])]
-    return [
-        pd.DataFrame.from_dict({key: pd.Series(value) for key, value in data.items()})
-    ]
+        return [pd.json_normalize(data[:sample_size], sep=COMPLEX_COLUMN_SEPARATOR)]
+    return [pd.json_normalize(data, sep=COMPLEX_COLUMN_SEPARATOR)]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/db2/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/db2/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/db2/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/db2/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dbt/dbt_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dbt/dbt_service.py`

 * *Files 10% similar despite different names*

```diff
@@ -38,26 +38,23 @@
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class DbtServiceTopology(ServiceTopology):
     """
-    Defines the hierarchy in Database Services.
-    service -> db -> schema -> table.
-
-    We could have a topology validator. We can only consume
-    data that has been produced by any parent node.
+    Defines the hierarchy in dbt Services.
+    dbt files -> dbt tags -> data models -> descriptions -> lineage -> tests.
     """
 
     root = TopologyNode(
         producer="get_dbt_files",
         stages=[
             NodeStage(
-                type_=OMetaTagAndClassification,
+                type_=DbtFiles,
                 processor="validate_dbt_files",
                 ack_sink=False,
                 nullable=True,
             )
         ],
         children=[
             "process_dbt_data_model",
@@ -137,23 +134,44 @@
     """
     Class for defining the topology of the DBT source
     """
 
     topology = DbtServiceTopology()
     context = create_source_context(topology)
 
+    def remove_manifest_non_required_keys(self, manifest_dict: dict):
+        """
+        Method to remove the non required keys from manifest file
+        """
+        # To ensure smooth ingestion of data,
+        # we are selectively processing the metadata, nodes, and sources from the manifest file
+        # while trimming out any other irrelevant data that might be present.
+        # This step is necessary as the manifest file may not always adhere to the schema definition
+        # and the presence of other nodes can hinder the ingestion process from progressing any further.
+        # Therefore, we are only retaining the essential data for further processing.
+        required_manifest_keys = ["nodes", "sources", "metadata"]
+        manifest_dict.update(
+            {
+                key: {}
+                for key in manifest_dict
+                if key.lower() not in required_manifest_keys
+            }
+        )
+
     def get_dbt_files(self) -> DbtFiles:
         dbt_files = get_dbt_details(
             self.source_config.dbtConfigSource  # pylint: disable=no-member
         )
-
         self.context.dbt_files = dbt_files
         yield dbt_files
 
     def get_dbt_objects(self) -> DbtObjects:
+        self.remove_manifest_non_required_keys(
+            manifest_dict=self.context.dbt_files.dbt_manifest
+        )
         dbt_objects = DbtObjects(
             dbt_catalog=parse_catalog(self.context.dbt_files.dbt_catalog)
             if self.context.dbt_files.dbt_catalog
             else None,
             dbt_manifest=parse_manifest(self.context.dbt_files.dbt_manifest),
             dbt_run_results=parse_run_results(self.context.dbt_files.dbt_run_results)
             if self.context.dbt_files.dbt_run_results
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dbt/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dbt/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -225,27 +225,27 @@
 
     def check_columns(self, catalog_node):
         for catalog_key, catalog_column in catalog_node.get("columns").items():
             if all(
                 required_catalog_key in catalog_column
                 for required_catalog_key in REQUIRED_CATALOG_KEYS
             ):
-                logger.info(f"Successfully Validated DBT Column: {catalog_key}")
+                logger.debug(f"Successfully Validated DBT Column: {catalog_key}")
             else:
                 logger.warning(
                     f"Error validating DBT Column: {catalog_key}\n"
                     f"Please check if following keys exist for the column node: {REQUIRED_CATALOG_KEYS}"
                 )
 
     def validate_dbt_files(self, dbt_files: DbtFiles):
         """
         Method to validate DBT files
         """
         # Validate the Manifest File
-        logger.info("Validating Manifest File")
+        logger.debug("Validating Manifest File")
 
         if self.source_config.dbtConfigSource and dbt_files.dbt_manifest:
             manifest_entities = {
                 **dbt_files.dbt_manifest[DbtCommonEnum.NODES.value],
                 **dbt_files.dbt_manifest[DbtCommonEnum.SOURCES.value],
             }
             if dbt_files.dbt_catalog:
@@ -260,15 +260,15 @@
                     continue
 
                 # Validate if all the required keys are present in the manifest nodes
                 if all(
                     required_key in manifest_node
                     for required_key in REQUIRED_MANIFEST_KEYS
                 ):
-                    logger.info(f"Successfully Validated DBT Node: {key}")
+                    logger.debug(f"Successfully Validated DBT Node: {key}")
                 else:
                     logger.warning(
                         f"Error validating DBT Node: {key}\n"
                         f"Please check if following keys exist for the node: {REQUIRED_MANIFEST_KEYS}"
                     )
 
                 # Validate the catalog file if it is passed
@@ -292,15 +292,15 @@
             and dbt_objects.dbt_manifest
             and self.source_config.includeTags
         ):
             manifest_entities = {
                 **dbt_objects.dbt_manifest.nodes,
                 **dbt_objects.dbt_manifest.sources,
             }
-            logger.info("Processing DBT Tags")
+            logger.debug("Processing DBT Tags")
             dbt_tags_list = []
             for key, manifest_node in manifest_entities.items():
                 try:
                     if manifest_node.resource_type in [
                         item.value for item in SkipResourceTypeEnum
                     ]:
                         continue
@@ -364,29 +364,28 @@
         )
 
     def yield_data_models(self, dbt_objects: DbtObjects) -> Iterable[DataModelLink]:
         """
         Yield the data models
         """
         if self.source_config.dbtConfigSource and dbt_objects.dbt_manifest:
-            logger.info("Parsing DBT Data Models")
+            logger.debug("Parsing DBT Data Models")
             manifest_entities = {
                 **dbt_objects.dbt_manifest.nodes,
                 **dbt_objects.dbt_manifest.sources,
             }
             if dbt_objects.dbt_catalog:
                 catalog_entities = {
                     **dbt_objects.dbt_catalog.nodes,
                     **dbt_objects.dbt_catalog.sources,
                 }
             self.context.data_model_links = []
             self.context.dbt_tests = {}
             for key, manifest_node in manifest_entities.items():
                 try:
-
                     # If the run_results file is passed then only DBT tests will be processed
                     if (
                         dbt_objects.dbt_run_results
                         and manifest_node.resource_type.value
                         == SkipResourceTypeEnum.TEST.value
                     ):
                         # Test nodes will be processed further in the topology
@@ -398,23 +397,23 @@
                         )
                         continue
 
                     # Skip the analysis and test nodes
                     if manifest_node.resource_type.value in [
                         item.value for item in SkipResourceTypeEnum
                     ]:
-                        logger.info(f"Skipping DBT node: {key}.")
+                        logger.debug(f"Skipping DBT node: {key}.")
                         continue
 
                     model_name = (
                         manifest_node.alias
                         if hasattr(manifest_node, "alias") and manifest_node.alias
                         else manifest_node.name
                     )
-                    logger.info(f"Processing DBT node: {model_name}")
+                    logger.debug(f"Processing DBT node: {model_name}")
 
                     catalog_node = None
                     if dbt_objects.dbt_catalog:
                         catalog_node = catalog_entities.get(key)
 
                     dbt_table_tags_list = None
                     if manifest_node.tags:
@@ -474,16 +473,16 @@
                             ),
                         )
                         yield data_model_link
                         self.context.data_model_links.append(data_model_link)
                     else:
                         logger.warning(
                             f"Unable to find the table '{table_fqn}' in OpenMetadata"
-                            f"Please check if the table exists is ingested in OpenMetadata"
-                            f"And name, database, schema of the manifest node matches with the table present in OpenMetadata"  # pylint: disable=line-too-long
+                            f"Please check if the table exists and is ingested in OpenMetadata"
+                            f"Also name, database, schema of the manifest node matches with the table present in OpenMetadata"  # pylint: disable=line-too-long
                         )
                 except Exception as exc:
                     logger.debug(traceback.format_exc())
                     logger.warning(
                         f"Unexpected exception parsing DBT node:{model_name} - {exc}"
                     )
 
@@ -546,15 +545,15 @@
         """
         Method to parse the DBT columns
         """
         columns = []
         manifest_columns = manifest_node.columns
         for key, manifest_column in manifest_columns.items():
             try:
-                logger.info(f"Processing DBT column: {key}")
+                logger.debug(f"Processing DBT column: {key}")
                 # If catalog file is passed pass the column information from catalog file
                 catalog_column = None
                 if catalog_node and catalog_node.columns:
                     catalog_column = catalog_node.columns.get(key)
                 column_name = (
                     catalog_column.name if catalog_column else manifest_column.name
                 )
@@ -581,29 +580,29 @@
                             metadata=self.metadata,
                             tags=manifest_column.tags,
                             classification_name=self.tag_classification_name,
                             include_tags=self.source_config.includeTags,
                         ),
                     )
                 )
-                logger.info(f"Successfully processed DBT column: {key}")
+                logger.debug(f"Successfully processed DBT column: {key}")
             except Exception as exc:  # pylint: disable=broad-except
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Failed to parse DBT column {column_name}: {exc}")
 
         return columns
 
     def create_dbt_lineage(
         self, data_model_link: DataModelLink
     ) -> Iterable[AddLineageRequest]:
         """
         Method to process DBT lineage from upstream nodes
         """
         to_entity: Table = data_model_link.table_entity
-        logger.info(
+        logger.debug(
             f"Processing DBT lineage for: {to_entity.fullyQualifiedName.__root__}"
         )
 
         for upstream_node in data_model_link.datamodel.upstream:
             try:
                 from_es_result = self.metadata.es_search_from_fqn(
                     entity_type=Table,
@@ -637,15 +636,15 @@
     def create_dbt_query_lineage(
         self, data_model_link: DataModelLink
     ) -> Iterable[AddLineageRequest]:
         """
         Method to process DBT lineage from queries
         """
         to_entity: Table = data_model_link.table_entity
-        logger.info(
+        logger.debug(
             f"Processing DBT Query lineage for: {to_entity.fullyQualifiedName.__root__}"
         )
 
         try:
             source_elements = fqn.split(to_entity.fullyQualifiedName.__root__)
             # remove service name from fqn to make it parseable in format db.schema.table
             query_fqn = fqn._build(  # pylint: disable=protected-access
@@ -676,15 +675,15 @@
             )
 
     def process_dbt_descriptions(self, data_model_link: DataModelLink):
         """
         Method to process DBT descriptions using patch APIs
         """
         table_entity: Table = data_model_link.table_entity
-        logger.info(
+        logger.debug(
             f"Processing DBT Descriptions for: {table_entity.fullyQualifiedName.__root__}"
         )
         if table_entity:
             try:
                 data_model = data_model_link.datamodel
                 # Patch table descriptions from DBT
                 if data_model.description:
@@ -716,15 +715,15 @@
         """
         Method to add the DBT tests suites
         """
         try:
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
                 test_name = manifest_node.name
-                logger.info(f"Processing DBT Tests Suite for node: {test_name}")
+                logger.debug(f"Processing DBT Tests Suite for node: {test_name}")
                 test_suite_name = manifest_node.meta.get(
                     DbtCommonEnum.TEST_SUITE_NAME.value,
                     DbtCommonEnum.DBT_TEST_SUITE.value,
                 )
                 test_suite_desciption = manifest_node.meta.get(
                     "test_suite_desciption", ""
                 )
@@ -745,15 +744,15 @@
     ) -> Iterable[CreateTestDefinitionRequest]:
         """
         A Method to add DBT test definitions
         """
         try:
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
-                logger.info(
+                logger.debug(
                     f"Processing DBT Tests Suite Definition for node: {manifest_node.name}"
                 )
                 check_test_definition_exists = self.metadata.get_by_name(
                     fqn=manifest_node.name,
                     entity=TestDefinition,
                 )
                 if not check_test_definition_exists:
@@ -778,15 +777,15 @@
     def create_dbt_test_case(self, dbt_test: dict) -> Iterable[CreateTestCaseRequest]:
         """
         After test suite and test definitions have been processed, add the tests cases info
         """
         try:
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
-                logger.info(
+                logger.debug(
                     f"Processing DBT Test Case Definition for node: {manifest_node.name}"
                 )
                 entity_link_list = self.generate_entity_link(dbt_test)
                 for entity_link_str in entity_link_list:
                     test_suite_name = manifest_node.meta.get(
                         DbtCommonEnum.TEST_SUITE_NAME.value,
                         DbtCommonEnum.DBT_TEST_SUITE.value,
@@ -813,15 +812,15 @@
         """
         After test cases has been processed, add the tests results info
         """
         try:
             # Process the Test Status
             manifest_node = dbt_test.get(DbtCommonEnum.MANIFEST_NODE.value)
             if manifest_node:
-                logger.info(
+                logger.debug(
                     f"Processing DBT Test Case Results for node: {manifest_node.name}"
                 )
                 dbt_test_result = dbt_test.get(DbtCommonEnum.RESULTS.value)
                 test_case_status = TestCaseStatus.Aborted
                 test_result_value = 0
                 if dbt_test_result.status.value in [
                     item.value for item in DbtTestSuccessEnum
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/deltalake/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/deltalake/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/deltalake/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/deltalake/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/domodatabase/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/domodatabase/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/domodatabase/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/druid/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlite/connection.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,46 +15,46 @@
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.druidConnection import (
-    DruidConnection,
+from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
+    SQLiteConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
-    get_connection_url_common,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
-def get_connection_url(connection: DruidConnection) -> str:
-    url = get_connection_url_common(connection)
-    return f"{url}/druid/v2/sql"
+def get_connection_url(connection: SQLiteConnection) -> str:
+    database_mode = connection.databaseMode if connection.databaseMode else ":memory:"
 
+    return f"{connection.scheme.value}:///{database_mode}"
 
-def get_connection(connection: DruidConnection) -> Engine:
+
+def get_connection(connection: SQLiteConnection) -> Engine:
     """
     Create connection
     """
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: DruidConnection,
+    service_connection: SQLiteConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
     test_connection_db_common(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/druid/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/druid/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dynamodb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/dynamodb/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/dynamodb/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/glue/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/glue/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/glue/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/glue/metadata.py`

 * *Files 12% similar despite different names*

```diff
@@ -14,43 +14,37 @@
 import traceback
 from typing import Iterable, List, Optional, Tuple
 
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
-from metadata.generated.schema.api.data.createLocation import CreateLocationRequest
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
-from metadata.generated.schema.entity.data.location import Location, LocationType
 from metadata.generated.schema.entity.data.table import Column, Table, TableType
 from metadata.generated.schema.entity.services.connections.database.glueConnection import (
     GlueConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
     DatabaseServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
-from metadata.ingestion.source.database.database_service import (
-    DatabaseServiceSource,
-    TableLocationLink,
-)
+from metadata.ingestion.source.database.database_service import DatabaseServiceSource
 from metadata.utils import fqn
 from metadata.utils.filters import filter_by_database, filter_by_schema, filter_by_table
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
@@ -233,25 +227,19 @@
                     self.status.filter(
                         table_fqn,
                         "Table Filtered Out",
                     )
                     continue
 
                 parameters = table.get("Parameters")
-                location_type = LocationType.Table
-                if parameters:
+
+                table_type: TableType = TableType.Regular
+                if parameters.get("table_type") == "ICEBERG":
                     # iceberg tables need to pass a key/value pair in the DDL `'table_type'='ICEBERG'`
                     # https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg-creating-tables.html
-                    location_type = (
-                        location_type
-                        if parameters.get("table_type") != "ICEBERG"
-                        else LocationType.Iceberg
-                    )
-                table_type: TableType = TableType.Regular
-                if location_type == LocationType.Iceberg:
                     table_type = TableType.Iceberg
                 elif table["TableType"] == "EXTERNAL_TABLE":
                     table_type = TableType.External
                 elif table["TableType"] == "VIRTUAL_VIEW":
                     table_type = TableType.View
 
                 self.context.table_data = table
@@ -287,43 +275,14 @@
             self.register_record(table_request=table_request)
         except Exception as exc:
             error = f"Unexpected exception to yield table [{table_name}]: {exc}"
             logger.debug(traceback.format_exc())
             logger.warning(error)
             self.status.failed(table_name, error, traceback.format_exc())
 
-    def yield_location(
-        self, table_name_and_type: Tuple[str, str]
-    ) -> Iterable[Optional[CreateLocationRequest]]:
-        """
-        From topology.
-        Prepare a table request and pass it to the sink
-        """
-        table_name, table_type = table_name_and_type
-        table = self.context.table_data
-        try:
-            location_type: LocationType = LocationType.Table
-            if table_type == TableType.Iceberg:
-                location_type = LocationType.Iceberg
-            location_request = CreateLocationRequest(
-                name=table["Name"][:128],
-                path=table["StorageDescriptor"]["Location"],
-                description=table.get("Description", ""),
-                locationType=location_type,
-                service=EntityReference(
-                    id=self.context.storage_service.id, type="storageService"
-                ),
-            )
-            yield location_request
-        except Exception as exc:
-            error = f"Unexpected exception to yield location for table [{table_name}]: {exc}"
-            logger.debug(traceback.format_exc())
-            logger.warning(error)
-            self.status.failed(table_name, error, traceback.format_exc())
-
     def prepare(self):
         pass
 
     def get_columns(self, column_data):
         for column in column_data["Columns"]:
             if column["Type"].lower().startswith("union"):
                 column["Type"] = column["Type"].replace(" ", "")
@@ -335,41 +294,14 @@
                 parsed_string["dataTypeDisplay"] = str(column["Type"])
                 parsed_string["dataType"] = "UNION"
             parsed_string["name"] = column["Name"][:64]
             parsed_string["dataLength"] = parsed_string.get("dataLength", 1)
             parsed_string["description"] = column.get("Comment")
             yield Column(**parsed_string)
 
-    def yield_table_location_link(
-        self, table_name_and_type: Tuple[str, TableType]
-    ) -> Iterable[TableLocationLink]:
-        """
-        Gets the current location being processed, fetches its data model
-        and sends it ot the sink
-        """
-
-        table_name, _ = table_name_and_type
-        table_fqn = fqn.build(
-            self.metadata,
-            entity_type=Table,
-            service_name=self.context.database_service.name.__root__,
-            database_name=self.context.database.name.__root__,
-            schema_name=self.context.database_schema.name.__root__,
-            table_name=table_name,
-        )
-
-        location_fqn = fqn.build(
-            self.metadata,
-            entity_type=Location,
-            service_name=self.context.storage_service.name.__root__,
-            location_name=self.context.location.name.__root__,
-        )
-        if table_fqn and location_fqn:
-            yield TableLocationLink(table_fqn=table_fqn, location_fqn=location_fqn)
-
     def standardize_table_name(self, _: str, table: str) -> str:
         return table[:128]
 
     def yield_view_lineage(self) -> Optional[Iterable[AddLineageRequest]]:
         yield from []
 
     def yield_tag(self, schema_name: str) -> Iterable[OMetaTagAndClassification]:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,15 +40,15 @@
     """
     Build the URL handling auth requirements
     """
     url = f"{connection.scheme.value}://"
     if (
         connection.username
         and connection.auth
-        and connection.auth in ("LDAP", "CUSTOM")
+        and connection.auth.value in ("LDAP", "CUSTOM")
     ):
         url += quote_plus(connection.username)
         if not connection.password:
             connection.password = SecretStr("")
         url += f":{quote_plus(connection.password.get_secret_value())}"
         url += "@"
 
@@ -76,15 +76,15 @@
     """
     Create connection
     """
 
     if connection.auth:
         if not connection.connectionArguments:
             connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["auth"] = connection.auth
+        connection.connectionArguments.__root__["auth"] = connection.auth.value
 
     if connection.kerberosServiceName:
         if not connection.connectionArguments:
             connection.connectionArguments = init_empty_connection_arguments()
         connection.connectionArguments.__root__[
             "kerberos_service_name"
         ] = connection.kerberosServiceName
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/metadata.py`

 * *Files 16% similar despite different names*

```diff
@@ -26,19 +26,17 @@
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.database.column_helpers import (
-    remove_table_from_column_name,
-)
 from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
 from metadata.ingestion.source.database.hive.queries import HIVE_GET_COMMENTS
+from metadata.profiler.orm.registry import Dialects
 
 complex_data_types = ["struct", "map", "array", "union"]
 
 _type_map.update(
     {
         "binary": types.BINARY,
         "char": types.CHAR,
@@ -55,15 +53,15 @@
     """
     rows = self._get_table_columns(  # pylint: disable=protected-access
         connection, table_name, schema
     )
     rows = [[col.strip() if col else None for col in row] for row in rows]
     rows = [row for row in rows if row[0] and row[0] != "# col_name"]
     result = []
-    for (col_name, col_type, comment) in rows:
+    for col_name, col_type, comment in rows:
         if col_name == "# Partition Information":
             break
 
         col_raw_type = col_type
         attype = re.sub(r"\(.*\)", "", col_type)
         col_type = re.search(r"^\w+", col_type).group(0)
         try:
@@ -180,49 +178,137 @@
             if data[1] and data[1].strip() == "comment":
                 return {"text": data[2] if data and data[2] else None}
     except Exception:
         return {"text": None}
     return {"text": None}
 
 
-def get_impala_columns(self, connection, table_name, schema=None, **kwargs):
+def get_impala_table_or_view_names(connection, schema=None, target_type="table"):
+    """
+    Depending on the targetType returns either the Views or Tables
+    since they share the same method for getting their names.
+    """
+    query = "show tables"
+    if schema:
+        query += " IN " + schema
+
+    cursor = connection.execute(query)
+    results = cursor.fetchall()
+    tables_and_views = [result[0] for result in results]
+
+    retvalue = []
+
+    for table_view in tables_and_views:
+        query = f"describe formatted `{schema}`.`{table_view}`"
+        cursor = connection.execute(query)
+        results = cursor.fetchall()
+
+        for result in list(results):
+            data = result
+            if data[0].strip() == "Table Type:":
+                if target_type.lower() in data[1].lower():
+                    retvalue.append(table_view)
+    return retvalue
+
+
+def get_impala_view_names(
+    self, connection, schema=None, **kw
+):  # pylint: disable=unused-argument
+    results = get_impala_table_or_view_names(connection, schema, "view")
+    return results
+
+
+def get_impala_table_names(
+    self, connection, schema=None, **kw
+):  # pylint: disable=unused-argument
+    results = get_impala_table_or_view_names(connection, schema, "table")
+    return results
+
+
+def get_impala_table_comment(
+    self, connection, table_name, schema_name, **kw
+):  # pylint: disable=unused-argument
+    """
+    Gets the table comment from the describe formatted query result under the Table Parameters section.
+    """
+    full_table_name = (
+        f"{schema_name}.{table_name}" if schema_name is not None else table_name
+    )
+    split_name = full_table_name.split(".")
+    query = f"describe formatted `{split_name[0]}`.`{split_name[1]}`"
+    cursor = connection.execute(query)
+    results = cursor.fetchall()
+
+    found_table_parameters = False
+    try:
+        for result in list(results):
+            data = result
+            if not found_table_parameters and data[0].strip() == "Table Parameters:":
+                found_table_parameters = True
+            if found_table_parameters:
+                coltext = data[1].strip() if data[1] is not None else ""
+                if coltext == "comment":
+                    return {"text": data[2]}
+    except Exception:
+        return {"text": None}
+    return {"text": None}
+
+
+def get_impala_columns(
+    self, connection, table_name, schema=None, **kwargs
+):  # pylint: disable=unused-argument
+    # pylint: disable=too-many-locals
     """
     Extracted from the Impala Dialect. We'll tune the implementation.
 
     By default, this gives us the column name as `table.column`. We just
     want to get `column`.
     """
-    # pylint: disable=unused-argument
     full_table_name = f"{schema}.{table_name}" if schema is not None else table_name
-    query = f"SELECT * FROM {full_table_name} LIMIT 0"
-    cursor = connection.execute(query)
-    schema = cursor.cursor.description
-    # We need to fetch the empty results otherwise these queries remain in
-    # flight
-    cursor.fetchall()
+    split_name = full_table_name.split(".")
+    query = f"DESCRIBE `{split_name[0]}`.`{split_name[1]}`"
+    describe_table_rows = connection.execute(query)
     column_info = []
-    for col in schema:
-        column_info.append(
-            {
-                "name": remove_table_from_column_name(table_name, col[0]),
-                # Using Hive's map instead of Impala's, as we are pointing to a Hive Server
-                # Passing the lower as Hive's map is based on lower strings.
-                "type": _type_map[col[1].lower()],
-                "nullable": True,
-                "autoincrement": False,
-            }
-        )
+    ordinal_pos = 0
+    for col in describe_table_rows:
+        ordinal_pos = ordinal_pos + 1
+        col_raw = col[1]
+        attype = re.sub(r"\(.*\)", "", col[1])
+        col_type = re.search(r"^\w+", col[1]).group(0)
+        try:
+            coltype = _type_map[col_type]
+        except KeyError:
+            util.warn(f"Did not recognize type '{col_raw}' of column '{col[0]}'")
+            coltype = types.NullType
+        charlen = re.search(r"\(([\d,]+)\)", col_raw.lower())
+        if charlen:
+            charlen = charlen.group(1)
+            if attype == "decimal":
+                prec, scale = charlen.split(",")
+                args = (int(prec), int(scale))
+            else:
+                args = (int(charlen),)
+            coltype = coltype(*args)
+        add_column = {
+            "name": col[0],
+            "type": coltype,
+            "comment": col[2],
+            "nullable": True,
+            "autoincrement": False,
+            "ordinalPosition": ordinal_pos,
+        }
+        column_info.append(add_column)
     return column_info
 
 
 HiveDialect.get_columns = get_columns
 HiveDialect.get_table_comment = get_table_comment
 
 ImpalaDialect.get_columns = get_impala_columns
-ImpalaDialect.get_table_comment = get_table_comment
+ImpalaDialect.get_table_comment = get_impala_table_comment
 
 
 HIVE_VERSION_WITH_VIEW_SUPPORT = "2.2.0"
 
 
 class HiveSource(CommonDbSourceService):
     """
@@ -247,17 +333,24 @@
 
     def prepare(self):
         """
         Based on the version of hive update the get_table_names method
         Fetching views in hive server with query "SHOW VIEWS" was possible
         only after hive 2.2.0 version
         """
-        result = dict(self.engine.execute("SELECT VERSION()").fetchone())
-        version = result.get("_c0", "").split()
-        if version and self._parse_version(version[0]) >= self._parse_version(
-            HIVE_VERSION_WITH_VIEW_SUPPORT
-        ):
-            HiveDialect.get_table_names = get_table_names
-            HiveDialect.get_view_names = get_view_names
+        if self.engine.driver == Dialects.Impala:
+            ImpalaDialect.get_table_names = get_impala_table_names
+            ImpalaDialect.get_view_names = get_impala_view_names
+            ImpalaDialect.get_table_comment = get_impala_table_comment
+            ImpalaDialect.get_columns = get_impala_columns
         else:
-            HiveDialect.get_table_names = get_table_names_older_versions
-            HiveDialect.get_view_names = get_view_names_older_versions
+            result = dict(self.engine.execute("SELECT VERSION()").fetchone())
+
+            version = result.get("_c0", "").split()
+            if version and self._parse_version(version[0]) >= self._parse_version(
+                HIVE_VERSION_WITH_VIEW_SUPPORT
+            ):
+                HiveDialect.get_table_names = get_table_names
+                HiveDialect.get_view_names = get_view_names
+            else:
+                HiveDialect.get_table_names = get_table_names_older_versions
+                HiveDialect.get_view_names = get_view_names_older_versions
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/hive/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/hive/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/lineage_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/query_parser.py`

 * *Files 24% similar despite different names*

```diff
@@ -5,116 +5,99 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Lineage Source Module
+Snowflake Query parser module
 """
-import csv
-import traceback
 from abc import ABC
-from typing import Iterable, Iterator, Optional
+from datetime import datetime
+from typing import Iterable
 
-from sqlalchemy.engine import Engine
-
-from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
+from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
+    SnowflakeConnection,
+)
+from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
+    OpenMetadataConnection,
+)
+from metadata.generated.schema.metadataIngestion.workflow import (
+    Source as WorkflowSource,
+)
 from metadata.generated.schema.type.tableQuery import TableQuery
-from metadata.ingestion.lineage.models import ConnectionTypeDialectMapper
-from metadata.ingestion.lineage.sql_lineage import get_lineage_by_query
+from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.query_parser_source import QueryParserSource
+from metadata.ingestion.source.database.snowflake.queries import (
+    SNOWFLAKE_SESSION_TAG_QUERY,
+)
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
+SNOWFLAKE_ABORTED_CODE = "1969"
 
 
-class LineageSource(QueryParserSource, ABC):
+class SnowflakeQueryParserSource(QueryParserSource, ABC):
     """
-    This is the base source to handle Lineage-only ingestion.
-
-    We will still use TableQuery as the data, but only fill up those elements
-    that are truly required for the lineage use case, such as:
-    - query
-    - service
-    - database
-    - schema
+    Snowflake base for Usage and Lineage
     """
 
-    def get_table_query(self) -> Optional[Iterator[TableQuery]]:
-        """
-        If queryLogFilePath available in config iterate through log file
-        otherwise execute the sql query to fetch TableQuery data.
+    @classmethod
+    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
+        connection: SnowflakeConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, SnowflakeConnection):
+            raise InvalidSourceException(
+                f"Expected SnowflakeConnection, but got {connection}"
+            )
+        return cls(config, metadata_config)
 
-        This is a simplified version of the UsageSource query parsing.
+    def get_sql_statement(self, start_time: datetime, end_time: datetime) -> str:
+        """
+        returns sql statement to fetch query logs
         """
-        if self.config.sourceConfig.config.queryLogFilePath:
-            with open(
-                self.config.sourceConfig.config.queryLogFilePath, "r", encoding="utf-8"
-            ) as file:
-                for row in csv.DictReader(file):
-                    query_dict = dict(row)
-                    yield TableQuery(
-                        query=query_dict["query_text"],
-                        databaseName=self.get_database_name(query_dict),
-                        serviceName=self.config.serviceName,
-                        databaseSchema=self.get_schema_name(query_dict),
-                    )
+        return self.sql_stmt.format(
+            start_time=start_time,
+            end_time=end_time,
+            result_limit=self.config.sourceConfig.config.resultLimit,
+            filters=self.filters,
+        )
 
-        else:
-            logger.info(
-                f"Scanning query logs for {self.start.date()} - {self.end.date()}"
-            )
-            try:
-                engine = get_connection(self.service_connection)
-                yield from self.yield_table_query(engine)
-
-            except Exception as exc:
-                logger.debug(traceback.format_exc())
-                logger.error(f"Source usage processing error: {exc}")
-
-    def yield_table_query(self, engine: Engine) -> Iterator[TableQuery]:
-        """
-        Given an engine, iterate over the query results to
-        yield a TableQuery with query parsing info
-        """
-        with engine.connect() as conn:
-            rows = conn.execute(
-                self.get_sql_statement(
-                    start_time=self.start,
-                    end_time=self.end,
+    def set_session_query_tag(self) -> None:
+        """
+        Method to set query tag for current session
+        """
+        if self.service_connection.queryTag:
+            self.engine.execute(
+                SNOWFLAKE_SESSION_TAG_QUERY.format(
+                    query_tag=self.service_connection.queryTag
                 )
             )
-            for row in rows:
-                query_dict = dict(row)
-                try:
-                    yield TableQuery(
-                        query=query_dict["query_text"],
-                        databaseName=self.get_database_name(query_dict),
-                        serviceName=self.config.serviceName,
-                        databaseSchema=self.get_schema_name(query_dict),
-                    )
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(f"Error processing query_dict {query_dict}: {exc}")
-
-    def next_record(self) -> Iterable[AddLineageRequest]:
-        """
-        Based on the query logs, prepare the lineage
-        and send it to the sink
-        """
-        connection_type = str(self.service_connection.type.value)
-        dialect = ConnectionTypeDialectMapper.dialect_of(connection_type)
-        for table_query in self.get_table_query():
-
-            lineages = get_lineage_by_query(
-                self.metadata,
-                query=table_query.query,
-                service_name=table_query.serviceName,
-                database_name=table_query.databaseName,
-                schema_name=table_query.databaseSchema,
-                dialect=dialect,
-            )
 
-            for lineage_request in lineages or []:
-                yield lineage_request
+    def get_table_query(self) -> Iterable[TableQuery]:
+        database = self.config.serviceConnection.__root__.config.database
+        if database:
+            use_db_query = f"USE DATABASE {database}"
+            self.engine.execute(use_db_query)
+            self.set_session_query_tag()
+            yield from super().get_table_query()
+        else:
+            query = "SHOW DATABASES"
+            results = self.engine.execute(query)
+            for res in results:
+                row = list(res)
+                use_db_query = f"USE DATABASE {row[1]}"
+                self.engine.execute(use_db_query)
+                logger.info(f"Ingesting from database: {row[1]}")
+                self.config.serviceConnection.__root__.config.database = row[1]
+                self.engine = get_connection(self.service_connection)
+                self.set_session_query_tag()
+                yield from super().get_table_query()
+
+    def get_database_name(self, data: dict) -> str:  # pylint: disable=arguments-differ
+        """
+        Method to get database name
+        """
+        if not data["database_name"] and self.service_connection.database:
+            return self.service_connection.database
+        return data["database_name"]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mariadb/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mariadb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mariadb/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mariadb/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/lineage.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,11 +13,10 @@
 """
 from metadata.ingestion.source.database.lineage_source import LineageSource
 from metadata.ingestion.source.database.mssql.queries import MSSQL_SQL_STATEMENT
 from metadata.ingestion.source.database.mssql.query_parser import MssqlQueryParserSource
 
 
 class MssqlLineageSource(MssqlQueryParserSource, LineageSource):
-
     sql_stmt = MSSQL_SQL_STATEMENT
 
     filters = ""  # No filtering in the queries
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/usage.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,11 +13,10 @@
 """
 from metadata.ingestion.source.database.mssql.queries import MSSQL_SQL_STATEMENT
 from metadata.ingestion.source.database.mssql.query_parser import MssqlQueryParserSource
 from metadata.ingestion.source.database.usage_source import UsageSource
 
 
 class MssqlUsageSource(MssqlQueryParserSource, UsageSource):
-
     sql_stmt = MSSQL_SQL_STATEMENT
 
     filters = ""  # No filtering in the queries
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mssql/utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mssql/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/mysql/utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/mysql/utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/oracle/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -5,247 +5,220 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
-# pylint: disable=protected-access
-"""Oracle source module"""
-import re
-
-from sqlalchemy import sql, util
-from sqlalchemy.dialects.oracle.base import (
-    FLOAT,
-    INTEGER,
-    INTERVAL,
-    NUMBER,
-    TIMESTAMP,
-    OracleDialect,
-    ischema_names,
-)
+"""
+Module to define overriden dialect methods
+"""
+
+import sqlalchemy.types as sqltypes
+from sqlalchemy import exc as sa_exc
+from sqlalchemy import util as sa_util
 from sqlalchemy.engine import reflection
-from sqlalchemy.sql import sqltypes
+from sqlalchemy.sql import text
+from sqlalchemy.types import FLOAT
 
-from metadata.generated.schema.entity.services.connections.database.oracleConnection import (
-    OracleConnection,
-)
-from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
-    OpenMetadataConnection,
-)
-from metadata.generated.schema.metadataIngestion.workflow import (
-    Source as WorkflowSource,
-)
-from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.database.column_type_parser import create_sqlalchemy_type
-from metadata.ingestion.source.database.common_db_source import CommonDbSourceService
-from metadata.ingestion.source.database.oracle.queries import (
-    ORACLE_ALL_TABLE_COMMENTS,
-    ORACLE_ALL_VIEW_DEFINITIONS,
-    ORACLE_GET_COLUMNS,
-    ORACLE_IDENTITY_TYPE,
+from metadata.ingestion.source.database.snowflake.queries import (
+    SNOWFLAKE_GET_COMMENTS,
+    SNOWFLAKE_GET_SCHEMA_COLUMNS,
+    SNOWFLAKE_GET_TABLE_NAMES,
+    SNOWFLAKE_GET_VIEW_NAMES,
+    SNOWFLAKE_GET_WITHOUT_TRANSIENT_TABLE_NAMES,
 )
 from metadata.utils.sqlalchemy_utils import (
-    get_all_table_comments,
-    get_all_view_definitions,
+    get_display_datatype,
     get_table_comment_wrapper,
-    get_view_definition_wrapper,
 )
 
-ischema_names.update(
-    {
-        "ROWID": create_sqlalchemy_type("ROWID"),
-        "XMLTYPE": create_sqlalchemy_type("XMLTYPE"),
-        "INTERVAL YEAR TO MONTH": INTERVAL,
-    }
-)
 
+def get_table_names_reflection(self, schema=None, **kw):
+    """Return all table names in referred to within a particular schema.
 
-@reflection.cache
-def get_table_comment(
-    self,
-    connection,
-    table_name: str,
-    schema: str = None,
-    resolve_synonyms=False,
-    dblink="",
-    **kw,
-):  # pylint: disable=unused-argument
-    return get_table_comment_wrapper(
-        self,
-        connection,
-        table_name=table_name.lower(),
-        schema=schema.lower() if schema else None,
-        query=ORACLE_ALL_TABLE_COMMENTS,
-    )
+    The names are expected to be real tables only, not views.
+    Views are instead returned using the
+    :meth:`_reflection.Inspector.get_view_names`
+    method.
 
 
-@reflection.cache
-def get_view_definition(
-    self,
-    connection,
-    view_name: str,
-    schema: str = None,
-    resolve_synonyms=False,
-    dblink="",
-    **kw,
-):  # pylint: disable=unused-argument
-
-    return get_view_definition_wrapper(
-        self,
-        connection,
-        table_name=view_name.lower(),
-        schema=schema.lower() if schema else None,
-        query=ORACLE_ALL_VIEW_DEFINITIONS,
-    )
+    :param schema: Schema name. If ``schema`` is left at ``None``, the
+        database's default schema is
+        used, else the named schema is searched.  If the database does not
+        support named schemas, behavior is undefined if ``schema`` is not
+        passed as ``None``.  For special quoting, use :class:`.quoted_name`.
 
+    .. seealso::
 
-def _get_col_type(
-    self, coltype, precision, scale, length, colname
-):  # pylint: disable=too-many-branches
-    raw_type = coltype
-    if coltype == "NUMBER":
-        if precision is None and scale == 0:
-            coltype = INTEGER()
-        else:
-            coltype = NUMBER(precision, scale)
-            if precision is not None:
-                if scale is not None:
-                    raw_type += f"({precision},{scale})"
-                else:
-                    raw_type += f"({precision})"
-
-    elif coltype == "FLOAT":
-        # TODO: support "precision" here as "binary_precision"
-        coltype = FLOAT()
-    elif coltype in ("VARCHAR2", "NVARCHAR2", "CHAR", "NCHAR"):
-        coltype = self.ischema_names.get(coltype)(length)
-        if length:
-            raw_type += f"({length})"
-    elif "WITH TIME ZONE" in coltype or "TIMESTAMP" in coltype:
-        coltype = TIMESTAMP(timezone=True)
-    elif "INTERVAL" in coltype:
-        coltype = INTERVAL()
-    else:
-        coltype = re.sub(r"\(\d+\)", "", coltype)
-        try:
-            coltype = self.ischema_names[coltype]
-        except KeyError:
-            util.warn(f"Did not recognize type '{coltype}' of column '{colname}'")
-            coltype = sqltypes.NULLTYPE
-    return coltype, raw_type
+        :meth:`_reflection.Inspector.get_sorted_table_and_fkc_names`
 
+        :attr:`_schema.MetaData.sorted_tables`
 
-# pylint: disable=too-many-locals
-@reflection.cache
-def get_columns(self, connection, table_name, schema=None, **kw):
     """
 
-    Dialect method overridden to add raw data type
+    with self._operation_context() as conn:  # pylint: disable=protected-access
+        return self.dialect.get_table_names(
+            conn, schema, info_cache=self.info_cache, **kw
+        )
+
+
+def get_table_names(self, connection, schema, **kw):
+    if kw.get("include_temp_tables"):
+        cursor = connection.execute(SNOWFLAKE_GET_TABLE_NAMES.format(schema))
+        result = [self.normalize_name(row[0]) for row in cursor]
+        return result
 
-    kw arguments can be:
+    cursor = connection.execute(
+        SNOWFLAKE_GET_WITHOUT_TRANSIENT_TABLE_NAMES.format(schema)
+    )
+    result = [self.normalize_name(row[0]) for row in cursor]
+    return result
 
-        oracle_resolve_synonyms
 
-        dblink
+def get_view_names(self, connection, schema, **kw):  # pylint: disable=unused-argument
+    cursor = connection.execute(SNOWFLAKE_GET_VIEW_NAMES.format(schema))
+    result = [self.normalize_name(row[0]) for row in cursor]
+    return result
 
+
+@reflection.cache
+def get_view_definition(  # pylint: disable=unused-argument
+    self, connection, view_name, schema=None, **kw
+):
+    """
+    Gets the view definition
     """
-    resolve_synonyms = kw.get("oracle_resolve_synonyms", False)
-    dblink = kw.get("dblink", "")
-    info_cache = kw.get("info_cache")
+    schema = schema or self.default_schema_name
+    if schema:
+        cursor = connection.execute(
+            "SHOW /* sqlalchemy:get_view_definition */ VIEWS "
+            f"LIKE '{view_name}' IN {schema}"
+        )
+    else:
+        cursor = connection.execute(
+            "SHOW /* sqlalchemy:get_view_definition */ VIEWS " f"LIKE '{view_name}'"
+        )
+    n2i = self.__class__._map_name_to_idx(cursor)  # pylint: disable=protected-access
+    try:
+        ret = cursor.fetchone()
+        if ret:
+            return ret[n2i["text"]]
+    except Exception:
+        pass
+    return None
 
-    (table_name, schema, dblink, _) = self._prepare_reflection_args(
+
+@reflection.cache
+def get_table_comment(
+    self, connection, table_name, schema=None, **kw
+):  # pylint: disable=unused-argument
+    return get_table_comment_wrapper(
+        self,
         connection,
-        table_name,
-        schema,
-        resolve_synonyms,
-        dblink,
-        info_cache=info_cache,
+        table_name=table_name,
+        schema=schema,
+        query=SNOWFLAKE_GET_COMMENTS,
     )
-    columns = []
-
-    char_length_col = "data_length"
-    if self._supports_char_length:
-        char_length_col = "char_length"
 
-    identity_cols = "NULL as default_on_null, NULL as identity_options"
-    if self.server_version_info >= (12,):
-        identity_cols = ORACLE_IDENTITY_TYPE.format(dblink=dblink)
 
-    params = {"table_name": table_name}
+@reflection.cache
+def get_unique_constraints(  # pylint: disable=unused-argument
+    self, connection, table_name, schema=None, **kw
+):
+    return []
 
-    text = ORACLE_GET_COLUMNS.format(
-        dblink=dblink, char_length_col=char_length_col, identity_cols=identity_cols
-    )
-    if schema is not None:
-        params["owner"] = schema
-        text += " AND col.owner = :owner "
-    text += " ORDER BY col.column_id"
-
-    cols = connection.execute(sql.text(text), params)
-
-    for row in cols:
-        colname = self.normalize_name(row[0])
-        length = row[2]
-        nullable = row[5] == "Y"
-        default = row[6]
-        generated = row[8]
-        default_on_nul = row[9]
-        identity_options = row[10]
-
-        coltype, raw_coltype = self._get_col_type(
-            row.data_type, row.data_precision, row.data_scale, length, colname
-        )
-
-        computed = None
-        if generated == "YES":
-            computed = {"sqltext": default}
-            default = None
-
-        identity = None
-        if identity_options is not None:
-            identity = self._parse_identity_options(identity_options, default_on_nul)
-            default = None
-
-        cdict = {
-            "name": colname,
-            "type": coltype,
-            "nullable": nullable,
-            "default": default,
-            "autoincrement": "auto",
-            "comment": row.comments,
-            "system_data_type": raw_coltype,
-        }
-        if row.column_name.lower() == row.column_name:
-            cdict["quote"] = True
-        if computed is not None:
-            cdict["computed"] = computed
-        if identity is not None:
-            cdict["identity"] = identity
-
-        columns.append(cdict)
-    return columns
-
-
-OracleDialect.get_table_comment = get_table_comment
-OracleDialect.get_columns = get_columns
-OracleDialect._get_col_type = _get_col_type
-OracleDialect.get_view_definition = get_view_definition
-OracleDialect.get_all_view_definitions = get_all_view_definitions
-OracleDialect.get_all_table_comments = get_all_table_comments
 
+def normalize_names(self, name):  # pylint: disable=unused-argument
+    return name
 
-class OracleSource(CommonDbSourceService):
-    """
-    Implements the necessary methods to extract
-    Database metadata from Oracle Source
-    """
 
-    @classmethod
-    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        config = WorkflowSource.parse_obj(config_dict)
-        connection: OracleConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, OracleConnection):
-            raise InvalidSourceException(
-                f"Expected OracleConnection, but got {connection}"
+# pylint: disable=too-many-locals,protected-access
+@reflection.cache
+def get_schema_columns(self, connection, schema, **kw):
+    """Get all columns in the schema, if we hit 'Information schema query returned too much data' problem return
+    None, as it is cacheable and is an unexpected return type for this function"""
+    ans = {}
+    current_database, _ = self._current_database_schema(connection, **kw)
+    full_schema_name = self._denormalize_quote_join(current_database, schema)
+    try:
+        schema_primary_keys = self._get_schema_primary_keys(
+            connection, full_schema_name, **kw
+        )
+        result = connection.execute(
+            text(SNOWFLAKE_GET_SCHEMA_COLUMNS),
+            {"table_schema": self.denormalize_name(schema)},
+        )
+    except sa_exc.ProgrammingError as p_err:
+        if p_err.orig.errno == 90030:
+            # This means that there are too many tables in the schema, we need to go more granular
+            return None  # None triggers _get_table_columns while staying cacheable
+        raise
+    for (
+        table_name,
+        column_name,
+        coltype,
+        character_maximum_length,
+        numeric_precision,
+        numeric_scale,
+        is_nullable,
+        column_default,
+        is_identity,
+        comment,
+        identity_start,
+        identity_increment,
+    ) in result:
+        table_name = self.normalize_name(table_name)
+        column_name = self.normalize_name(column_name)
+        if table_name not in ans:
+            ans[table_name] = []
+        if column_name.startswith("sys_clustering_column"):
+            continue  # ignoring clustering column
+        col_type = self.ischema_names.get(coltype, None)
+        col_type_kw = {}
+        if col_type is None:
+            sa_util.warn(
+                f"Did not recognize type '{coltype}' of column '{column_name}'"
             )
-        return cls(config, metadata_config)
+            col_type = sqltypes.NULLTYPE
+        else:
+            if issubclass(col_type, FLOAT):
+                col_type_kw["precision"] = numeric_precision
+                col_type_kw["decimal_return_scale"] = numeric_scale
+            elif issubclass(col_type, sqltypes.Numeric):
+                col_type_kw["precision"] = numeric_precision
+                col_type_kw["scale"] = numeric_scale
+            elif issubclass(col_type, (sqltypes.String, sqltypes.BINARY)):
+                col_type_kw["length"] = character_maximum_length
+
+        type_instance = col_type(**col_type_kw)
+
+        current_table_pks = schema_primary_keys.get(table_name)
+
+        ans[table_name].append(
+            {
+                "name": column_name,
+                "type": type_instance,
+                "nullable": is_nullable == "YES",
+                "default": column_default,
+                "autoincrement": is_identity == "YES",
+                "system_data_type": get_display_datatype(
+                    coltype,
+                    char_len=character_maximum_length,
+                    precision=numeric_precision,
+                    scale=numeric_scale,
+                ),
+                "comment": comment,
+                "primary_key": (
+                    column_name
+                    in schema_primary_keys[table_name]["constrained_columns"]
+                )
+                if current_table_pks
+                else False,
+            }
+        )
+        if is_identity == "YES":
+            ans[table_name][-1]["identity"] = {
+                "start": identity_start,
+                "increment": identity_increment,
+            }
+    return ans
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/pinotdb/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/pinotdb/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/pinotdb/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/connection.py`

 * *Files 5% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.services.connections.database.postgresConnection import (
     PostgresConnection,
+    SslMode,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
     init_empty_connection_arguments,
 )
@@ -41,15 +42,19 @@
 def get_connection(connection: PostgresConnection) -> Engine:
     """
     Create connection
     """
     if connection.sslMode:
         if not connection.connectionArguments:
             connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["sslmode"] = connection.sslMode
+        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
+        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
+            connection.connectionArguments.__root__[
+                "sslrootcert"
+            ] = connection.sslConfig.__root__.certificatePath
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/query_parser.py`

 * *Files 0% similar despite different names*

```diff
@@ -69,24 +69,22 @@
         """
         return self.sql_stmt.format(
             result_limit=self.config.sourceConfig.config.resultLimit,
             filters=self.filters,
         )
 
     def get_table_query(self) -> Iterable[TableQuery]:
-
         try:
             if self.config.sourceConfig.config.queryLogFilePath:
                 table_query_list = []
                 with open(
                     self.config.sourceConfig.config.queryLogFilePath,
                     "r",
                     encoding="utf-8",
                 ) as query_log_file:
-
                     for record in csv.DictReader(query_log_file):
                         query_dict = dict(record)
 
                         analysis_date = (
                             datetime.utcnow()
                             if not query_dict.get("session_start_time")
                             else datetime.strptime(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/postgres/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/postgres/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/presto/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/presto/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/runner/models.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,13 +4,21 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-SQL Queries used during ingestion
+test case result response object
 """
 
+from pydantic import BaseModel
+
+from metadata.generated.schema.tests.basic import TestCaseResult
+from metadata.generated.schema.tests.testCase import TestCase
+
 
-PRESTO_SHOW_CATALOGS = "SHOW CATALOGS"
+class TestCaseResultResponse(BaseModel):
+    testCaseResult: TestCaseResult
+    testCase: TestCase
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/query_parser_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/query_parser_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/connection.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
 from metadata.generated.schema.entity.services.connections.database.redshiftConnection import (
     RedshiftConnection,
+    SslMode,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
     get_connection_url_common,
     init_empty_connection_arguments,
 )
@@ -40,15 +41,19 @@
 def get_connection(connection: RedshiftConnection) -> Engine:
     """
     Create connection
     """
     if connection.sslMode:
         if not connection.connectionArguments:
             connection.connectionArguments = init_empty_connection_arguments()
-        connection.connectionArguments.__root__["sslmode"] = connection.sslMode
+        connection.connectionArguments.__root__["sslmode"] = connection.sslMode.value
+        if connection.sslMode in (SslMode.verify_ca, SslMode.verify_full):
+            connection.connectionArguments.__root__[
+                "sslrootcert"
+            ] = connection.sslConfig.__root__.certificatePath
     return create_generic_db_connection(
         connection=connection,
         get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/lineage.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,15 +34,14 @@
 from metadata.ingestion.source.database.redshift.queries import REDSHIFT_SQL_STATEMENT
 from metadata.ingestion.source.database.redshift.query_parser import (
     RedshiftQueryParserSource,
 )
 
 
 class RedshiftLineageSource(RedshiftQueryParserSource, LineageSource):
-
     filters = """
         AND (
           querytxt ILIKE '%%create table%%as%%select%%'
           OR querytxt ILIKE '%%insert%%'
         )
     """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -76,14 +76,15 @@
 
 ischema_names = pg_ischema_names
 GEOGRAPHY = create_sqlalchemy_type("GEOGRAPHY")
 ischema_names["geography"] = GEOGRAPHY
 ischema_names.update({"binary varying": sqltypes.VARBINARY})
 ischema_names.update(REDSHIFT_ISCHEMA_NAMES)
 
+
 # pylint: disable=protected-access
 @reflection.cache
 def get_columns(self, connection, table_name, schema=None, **kw):
     """
     Return information about columns in `table_name`.
 
     Overrides interface
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/query_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/redshift/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/redshift/usage.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,15 +15,14 @@
 from metadata.ingestion.source.database.redshift.query_parser import (
     RedshiftQueryParserSource,
 )
 from metadata.ingestion.source.database.usage_source import UsageSource
 
 
 class RedshiftUsageSource(RedshiftQueryParserSource, UsageSource):
-
     filters = """
         AND querytxt NOT ILIKE 'fetch %%'
         AND querytxt NOT ILIKE 'padb_fetch_sample: %%'
         AND querytxt NOT ILIKE 'Undoing %% transactions on table %% with current xid%%'
         AND querytxt NOT ILIKE '%%create table%%as%%select%%'
         AND querytxt NOT ILIKE '%%insert%%'
     """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/salesforce/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/salesforce/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/salesforce/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/pandas/pandas_profiler_interface.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,267 +4,359 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Salesforce source ingestion
+Interfaces with database for all database engine
+supporting sqlalchemy abstraction layer
 """
 import traceback
-from typing import Iterable, Optional, Tuple
+from collections import defaultdict
+from datetime import datetime, timezone
+from typing import Dict, List
+
+from sqlalchemy import Column
 
-from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
-from metadata.generated.schema.api.data.createDatabaseSchema import (
-    CreateDatabaseSchemaRequest,
-)
-from metadata.generated.schema.api.data.createTable import CreateTableRequest
-from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
 from metadata.generated.schema.entity.data.table import (
-    Column,
-    Constraint,
-    DataType,
-    Table,
-    TableType,
-)
-from metadata.generated.schema.entity.services.connections.database.salesforceConnection import (
-    SalesforceConnection,
+    PartitionProfilerConfig,
+    TableData,
 )
-from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
-    OpenMetadataConnection,
+from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
+    DatalakeConnection,
 )
-from metadata.generated.schema.metadataIngestion.databaseServiceMetadataPipeline import (
-    DatabaseServiceMetadataPipeline,
+from metadata.ingestion.api.processor import ProfilerProcessorStatus
+from metadata.ingestion.source.connections import get_connection
+from metadata.ingestion.source.database.datalake.metadata import (
+    DATALAKE_DATA_TYPES,
+    DatalakeSource,
 )
-from metadata.generated.schema.metadataIngestion.workflow import (
-    Source as WorkflowSource,
-)
-from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
-from metadata.ingestion.source.database.database_service import DatabaseServiceSource
-from metadata.utils import fqn
-from metadata.utils.constants import DEFAULT_DATABASE
-from metadata.utils.filters import filter_by_table
-from metadata.utils.logger import ingestion_logger
+from metadata.mixins.pandas.pandas_mixin import PandasInterfaceMixin
+from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
+from metadata.profiler.metrics.core import MetricTypes
+from metadata.profiler.metrics.registry import Metrics
+from metadata.profiler.processor.datalake_sampler import DatalakeSampler
+from metadata.utils.dispatch import valuedispatch
+from metadata.utils.logger import profiler_interface_registry_logger
+from metadata.utils.sqa_like_column import SQALikeColumn, Type
 
-logger = ingestion_logger()
+logger = profiler_interface_registry_logger()
 
 
-class SalesforceSource(DatabaseServiceSource):
+class PandasProfilerInterface(ProfilerProtocol, PandasInterfaceMixin):
     """
-    Implements the necessary methods to extract
-    Database metadata from Salesforce Source
+    Interface to interact with registry supporting
+    sqlalchemy.
     """
 
-    def __init__(self, config, metadata_config: OpenMetadataConnection):
-        super().__init__()
-        self.config = config
-        self.source_config: DatabaseServiceMetadataPipeline = (
-            self.config.sourceConfig.config
+    _profiler_type: str = DatalakeConnection.__name__
+
+    def __init__(
+        self,
+        service_connection_config,
+        ometa_client,
+        thread_count,
+        entity,
+        profile_sample_config,
+        source_config,
+        sample_query,
+        table_partition_config=None,
+        **kwargs,
+    ):
+        """Instantiate SQA Interface object"""
+        self._thread_count = thread_count
+        self.table_entity = entity
+        self.ometa_client = ometa_client
+        self.source_config = source_config
+        self.service_connection_config = service_connection_config
+        self.client = self.get_connection_client()
+        self.processor_status = ProfilerProcessorStatus()
+        self.processor_status.entity = (
+            self.table_entity.fullyQualifiedName.__root__
+            if self.table_entity.fullyQualifiedName
+            else None
         )
-        self.metadata_config = metadata_config
-        self.metadata = OpenMetadata(metadata_config)
-        self.service_connection = self.config.serviceConnection.__root__.config
-        self.client = get_connection(self.service_connection)
-        self.table_constraints = None
-        self.data_models = {}
-        self.dbt_tests = {}
-        self.database_source_state = set()
-
-    @classmethod
-    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: SalesforceConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, SalesforceConnection):
-            raise InvalidSourceException(
-                f"Expected SalesforceConnection, but got {connection}"
-            )
-        return cls(config, metadata_config)
+        self.profile_sample_config = profile_sample_config
+        self.profile_query = sample_query
+        self.table_partition_config: PartitionProfilerConfig = table_partition_config
+        self._table = entity
+        self.dfs = self.return_ometa_dataframes_sampled(
+            service_connection_config=self.service_connection_config,
+            client=self.client,
+            table=self.table,
+            profile_sample_config=self.profile_sample_config,
+        )
+        if self.dfs and self.table_partition_config:
+            self.dfs = self.get_partitioned_df(self.dfs)
 
-    def get_database_names(self) -> Iterable[str]:
+    @valuedispatch
+    def _get_metrics(self, *args, **kwargs):
+        """Generic getter method for metrics. To be used with
+        specific dispatch methods
+        """
+        logger.warning("Could not get metric. No function registered.")
+
+    # pylint: disable=unused-argument
+    @_get_metrics.register(MetricTypes.Table.value)
+    def _(
+        self,
+        metric_type: str,
+        metrics: List[Metrics],
+        *args,
+        **kwargs,
+    ):
+        """Given a list of metrics, compute the given results
+        and returns the values
+
+        Args:
+            metrics: list of metrics to compute
+        Returns:
+            dictionnary of results
         """
-        Default case with a single database.
+        import pandas as pd  # pylint: disable=import-outside-toplevel
 
-        It might come informed - or not - from the source.
+        try:
+            row_dict = {}
+            df_list = [df.where(pd.notnull(df), None) for df in self.dfs]
+            for metric in metrics:
+                row_dict[metric.name()] = metric().df_fn(df_list)
+            return row_dict
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Error trying to compute profile for {exc}")
+            raise RuntimeError(exc)
 
-        Sources with multiple databases should overwrite this and
-        apply the necessary filters.
+    # pylint: disable=unused-argument
+    @_get_metrics.register(MetricTypes.Static.value)
+    def _(
+        self,
+        metric_type: str,
+        metrics: List[Metrics],
+        column,
+        *args,
+        **kwargs,
+    ):
+        """Given a list of metrics, compute the given results
+        and returns the values
+
+        Args:
+            column: the column to compute the metrics against
+            metrics: list of metrics to compute
+        Returns:
+            dictionnary of results
         """
-        database_name = self.service_connection.databaseName or DEFAULT_DATABASE
-        yield database_name
+        import pandas as pd  # pylint: disable=import-outside-toplevel
 
-    def yield_database(self, database_name: str) -> Iterable[CreateDatabaseRequest]:
-        """
-        From topology.
-        Prepare a database request and pass it to the sink
-        """
-        yield CreateDatabaseRequest(
-            name=database_name,
-            service=self.context.database_service.fullyQualifiedName,
-        )
+        try:
+            row_dict = {}
+            for metric in metrics:
+                metric_resp = metric(column).df_fn(self.dfs)
+                row_dict[metric.name()] = (
+                    None if pd.isnull(metric_resp) else metric_resp
+                )
+            return row_dict
+        except Exception as exc:
+            logger.debug(
+                f"{traceback.format_exc()}\nError trying to compute profile for {exc}"
+            )
+            raise RuntimeError(exc)
 
-    def get_database_schema_names(self) -> Iterable[str]:
+    # pylint: disable=unused-argument
+    @_get_metrics.register(MetricTypes.Query.value)
+    def _(
+        self,
+        metric_type: str,
+        metrics: Metrics,
+        column,
+        *args,
+        **kwargs,
+    ):
+        """Given a list of metrics, compute the given results
+        and returns the values
+
+        Args:
+            column: the column to compute the metrics against
+            metrics: list of metrics to compute
+        Returns:
+            dictionnary of results
+        """
+        col_metric = None
+        col_metric = metrics(column).df_fn(self.dfs)
+        if not col_metric:
+            return None
+        return {metrics.name(): col_metric}
+
+    # pylint: disable=unused-argument
+    @_get_metrics.register(MetricTypes.Window.value)
+    def _(
+        self,
+        metric_type: str,
+        metrics: Metrics,
+        column,
+        *args,
+        **kwargs,
+    ):
         """
-        return schema names
+        Given a list of metrics, compute the given results
+        and returns the values
         """
-        schema_name = self.service_connection.scheme.name
-        yield schema_name
+        try:
+            metric_values = {}
+            for metric in metrics:
+                metric_values[metric.name()] = metric(column).df_fn(self.dfs)
+            return metric_values if metric_values else None
+        except Exception as exc:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Unexpected exception computing metrics: {exc}")
+            return None
 
-    def yield_database_schema(
-        self, schema_name: str
-    ) -> Iterable[CreateDatabaseSchemaRequest]:
-        """
-        From topology.
-        Prepare a database schema request and pass it to the sink
-        """
-        yield CreateDatabaseSchemaRequest(
-            name=schema_name,
-            database=self.context.database.fullyQualifiedName,
+    @_get_metrics.register(MetricTypes.System.value)
+    def _(
+        self,
+        *args,
+        **kwargs,
+    ):
+        """
+        Given a list of metrics, compute the given results
+        and returns the values
+        """
+        return None  # to be implemented
+
+    def compute_metrics(
+        self,
+        metrics,
+        metric_type,
+        column,
+        table,
+    ):
+        """Run metrics in processor worker"""
+        logger.debug(f"Running profiler for {table}")
+        try:
+            row = None
+            if self.dfs:
+                row = self._get_metrics(
+                    metric_type.value,
+                    metrics,
+                    session=self.client,
+                    column=column,
+                )
+        except Exception as exc:
+            name = f"{column if column is not None else table}"
+            error = f"{name} metric_type.value: {exc}"
+            logger.error(error)
+            self.processor_status.failed_profiler(error, traceback.format_exc())
+            row = None
+        if column:
+            column = column.name
+        return row, column, metric_type.value
+
+    def fetch_sample_data(self, table) -> TableData:
+        """Fetch sample data from database
+
+        Args:
+            table: ORM declarative table
+
+        Returns:
+            TableData: sample table data
+        """
+        sampler = DatalakeSampler(
+            session=self.client,
+            table=self.dfs,
+            profile_sample_config=self.profile_sample_config,
+            profile_sample_query=self.profile_query,
         )
+        return sampler.fetch_dl_sample_data()
 
-    def get_tables_name_and_type(self) -> Optional[Iterable[Tuple[str, str]]]:
-        """
-        Handle table and views.
-
-        Fetches them up using the context information and
-        the inspector set when preparing the db.
-
-        :return: tables or views, depending on config
+    def get_composed_metrics(
+        self, column: Column, metric: Metrics, column_results: Dict
+    ):
+        """Given a list of metrics, compute the given results
+        and returns the values
+
+        Args:
+            column: the column to compute the metrics against
+            metric: list of metrics to compute
+            column_results: computed values for the column
+        Returns:
+            dictionary of results
         """
-        schema_name = self.context.database_schema.name.__root__
         try:
-            if self.service_connection.sobjectName:
-                table_name = self.standardize_table_name(
-                    schema_name, self.service_connection.sobjectName
-                )
-                yield table_name, TableType.Regular
-            else:
-                for salesforce_object in self.client.describe()["sobjects"]:
-                    table_name = salesforce_object["name"]
-                    table_name = self.standardize_table_name(schema_name, table_name)
-                    table_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Table,
-                        service_name=self.context.database_service.name.__root__,
-                        database_name=self.context.database.name.__root__,
-                        schema_name=self.context.database_schema.name.__root__,
-                        table_name=table_name,
-                    )
-                    if filter_by_table(
-                        self.config.sourceConfig.config.tableFilterPattern,
-                        table_fqn
-                        if self.config.sourceConfig.config.useFqnForFiltering
-                        else table_name,
-                    ):
-                        self.status.filter(
-                            table_fqn,
-                            "Table Filtered Out",
-                        )
-                        continue
-
-                    yield table_name, TableType.Regular
+            return metric(column).fn(column_results)
         except Exception as exc:
-            error = f"Unexpected exception for schema name [{schema_name}]: {exc}"
             logger.debug(traceback.format_exc())
-            logger.warning(error)
-            self.status.failed(schema_name, error, traceback.format_exc())
+            logger.warning(f"Unexpected exception computing metrics: {exc}")
+            return None
 
-    def yield_table(
-        self, table_name_and_type: Tuple[str, str]
-    ) -> Iterable[Optional[CreateTableRequest]]:
-        """
-        From topology.
-        Prepare a table request and pass it to the sink
+    def get_hybrid_metrics(
+        self, column: Column, metric: Metrics, column_results: Dict, **kwargs
+    ):
+        """Given a list of metrics, compute the given results
+        and returns the values
+
+        Args:
+            column: the column to compute the metrics against
+            metric: list of metrics to compute
+            column_results: computed values for the column
+        Returns:
+            dictionary of results
         """
-        table_name, table_type = table_name_and_type
         try:
-
-            table_constraints = None
-            salesforce_objects = self.client.restful(
-                f"sobjects/{table_name}/describe/",
-                params=None,
-            )
-            columns = self.get_columns(salesforce_objects["fields"])
-            table_request = CreateTableRequest(
-                name=table_name,
-                tableType=table_type,
-                description="",
-                columns=columns,
-                tableConstraints=table_constraints,
-                databaseSchema=self.context.database_schema.fullyQualifiedName,
-            )
-            yield table_request
-            self.register_record(table_request=table_request)
-
+            return metric(column).df_fn(column_results, self.dfs)
         except Exception as exc:
-            error = f"Unexpected exception for table [{table_name}]: {exc}"
             logger.debug(traceback.format_exc())
-            logger.warning(error)
-            self.status.failed(table_name, error, traceback.format_exc())
+            logger.warning(f"Unexpected exception computing metrics: {exc}")
+            return None
 
-    def get_columns(self, salesforce_fields):
-        """
-        Method to handle column details
-        """
-        row_order = 1
-        columns = []
-        for column in salesforce_fields:
-            col_constraint = None
-            if column["nillable"]:
-                col_constraint = Constraint.NULL
-            elif not column["nillable"]:
-                col_constraint = Constraint.NOT_NULL
-            if column["unique"]:
-                col_constraint = Constraint.UNIQUE
-
-            columns.append(
-                Column(
-                    name=column["name"],
-                    description=column["label"],
-                    dataType=self.column_type(column["type"].upper()),
-                    dataTypeDisplay=column["type"],
-                    constraint=col_constraint,
-                    ordinalPosition=row_order,
-                    dataLength=column["length"],
-                )
-            )
-            row_order += 1
-        return columns
-
-    def column_type(self, column_type: str):
-        if column_type in {
-            "ID",
-            "PHONE",
-            "EMAIL",
-            "ENCRYPTEDSTRING",
-            "COMBOBOX",
-            "URL",
-            "TEXTAREA",
-            "ADDRESS",
-            "REFERENCE",
-        }:
-            return DataType.VARCHAR.value
-        return DataType.UNKNOWN.value
-
-    def yield_view_lineage(self) -> Optional[Iterable[AddLineageRequest]]:
-        yield from []
-
-    def yield_tag(self, schema_name: str) -> Iterable[OMetaTagAndClassification]:
-        pass
+    def get_all_metrics(
+        self,
+        metric_funcs: list,
+    ):
+        """get all profiler metrics"""
+
+        profile_results = {"table": {}, "columns": defaultdict(dict)}
+        metric_list = [
+            self.compute_metrics(*metric_func) for metric_func in metric_funcs
+        ]
+        for metric_result in metric_list:
+            profile, column, metric_type = metric_result
+            if profile:
+                if metric_type == MetricTypes.Table.value:
+                    profile_results["table"].update(profile)
+                if metric_type == MetricTypes.System.value:
+                    profile_results["system"] = profile
+                else:
+                    if profile:
+                        profile_results["columns"][column].update(
+                            {
+                                "name": column,
+                                "timestamp": datetime.now(tz=timezone.utc).timestamp(),
+                                **profile,
+                            }
+                        )
+        return profile_results
 
-    def standardize_table_name(  # pylint: disable=unused-argument
-        self, schema: str, table: str
-    ) -> str:
-        return table
+    @property
+    def table(self):
+        """OM Table entity"""
+        return self._table
+
+    def get_columns(self):
+        if self.dfs:
+            df = self.dfs[0]
+            return [
+                SQALikeColumn(
+                    column_name,
+                    Type(DatalakeSource.fetch_col_types(df, column_name)),
+                )
+                for column_name in df.columns
+            ]
+        return []
 
-    def prepare(self):
-        pass
+    def get_connection_client(self):
+        return get_connection(self.service_connection_config).client
 
     def close(self):
+        """Nothing to close with pandas"""
         pass
-
-    def test_connection(self) -> None:
-        test_connection_fn = get_test_connection_fn(self.service_connection)
-        test_connection_fn(self.client, self.service_connection)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sample_data.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sample_data.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,14 +7,15 @@
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Sample Data source ingestion
 """
+# pylint: disable=too-many-lines,too-many-statements
 import json
 import random
 import traceback
 from collections import namedtuple
 from datetime import datetime, timedelta, timezone
 from typing import Any, Dict, Iterable, List, Union
 
@@ -26,37 +27,32 @@
 from metadata.generated.schema.api.data.createDashboardDataModel import (
     CreateDashboardDataModelRequest,
 )
 from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
 from metadata.generated.schema.api.data.createDatabaseSchema import (
     CreateDatabaseSchemaRequest,
 )
-from metadata.generated.schema.api.data.createLocation import CreateLocationRequest
 from metadata.generated.schema.api.data.createMlModel import CreateMlModelRequest
 from metadata.generated.schema.api.data.createPipeline import CreatePipelineRequest
 from metadata.generated.schema.api.data.createTable import CreateTableRequest
 from metadata.generated.schema.api.data.createTableProfile import (
     CreateTableProfileRequest,
 )
 from metadata.generated.schema.api.data.createTopic import CreateTopicRequest
 from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.api.services.createStorageService import (
-    CreateStorageServiceRequest,
-)
 from metadata.generated.schema.api.teams.createRole import CreateRoleRequest
 from metadata.generated.schema.api.teams.createTeam import CreateTeamRequest
 from metadata.generated.schema.api.teams.createUser import CreateUserRequest
 from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
 from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
 from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.data.dashboard import Dashboard
 from metadata.generated.schema.entity.data.dashboardDataModel import DashboardDataModel
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
-from metadata.generated.schema.entity.data.location import Location
 from metadata.generated.schema.entity.data.mlmodel import (
     FeatureSource,
     MlFeature,
     MlHyperParameter,
     MlStore,
 )
 from metadata.generated.schema.entity.data.pipeline import Pipeline, PipelineStatus
@@ -73,17 +69,14 @@
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.services.dashboardService import DashboardService
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.messagingService import MessagingService
 from metadata.generated.schema.entity.services.mlmodelService import MlModelService
-from metadata.generated.schema.entity.services.objectstoreService import (
-    ObjectStoreService,
-)
 from metadata.generated.schema.entity.services.pipelineService import PipelineService
 from metadata.generated.schema.entity.services.storageService import StorageService
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
@@ -100,51 +93,32 @@
 from metadata.ingestion.models.tests_data import (
     OMetaTestCaseResultsSample,
     OMetaTestCaseSample,
     OMetaTestSuiteSample,
 )
 from metadata.ingestion.models.user import OMetaUserProfile
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.database_service import TableLocationLink
 from metadata.parsers.schema_parsers import (
     InvalidSchemaTypeException,
     schema_parser_config_registry,
 )
 from metadata.utils import fqn
+from metadata.utils.constants import UTF_8
 from metadata.utils.helpers import get_standard_chart_type
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 COLUMN_NAME = "Column"
 KEY_TYPE = "Key type"
 DATA_TYPE = "Data type"
 COL_DESCRIPTION = "Description"
 TableKey = namedtuple("TableKey", ["schema", "table_name"])
 
 
-def get_storage_service_or_create(service_json, metadata_config) -> StorageService:
-    """
-    Get an existing storage service or create a new one based on the config provided
-
-    To be refactored after cleaning Storage Services
-    """
-
-    metadata = OpenMetadata(metadata_config)
-    service: StorageService = metadata.get_by_name(
-        entity=StorageService, fqn=service_json["name"]
-    )
-    if service is not None:
-        return service
-    created_service = metadata.create_or_update(
-        CreateStorageServiceRequest(**service_json)
-    )
-    return created_service
-
-
 class InvalidSampleDataException(Exception):
     """
     Sample data is not valid to be ingested
     """
 
 
 def get_lineage_entity_ref(edge, metadata_config) -> EntityReference:
@@ -176,302 +150,302 @@
     :return:
     """
     return TableKey(schema=row["schema"], table_name=row["table_name"])
 
 
 class SampleDataSource(
     Source[Entity]
-):  # pylint: disable=too-many-instance-attributes,too-many-public-methods,disable=too-many-lines,
+):  # pylint: disable=too-many-instance-attributes,too-many-public-methods
     """
     Loads JSON data and prepares the required
     python objects to be sent to the Sink.
     """
 
     def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
-        # pylint: disable=too-many-statements
         super().__init__()
         self.config = config
         self.service_connection = config.serviceConnection.__root__.config
         self.metadata_config = metadata_config
         self.metadata = OpenMetadata(metadata_config)
         self.list_policies = []
 
         sample_data_folder = self.service_connection.connectionOptions.__root__.get(
             "sampleDataFolder"
         )
         if not sample_data_folder:
             raise InvalidSampleDataException(
                 "Cannot get sampleDataFolder from connection options"
             )
-
-        self.storage_service_json = json.load(
-            open(  # pylint: disable=consider-using-with
-                sample_data_folder + "/locations/service.json",
-                "r",
-                encoding="utf-8",
-            )
-        )
-        self.locations = json.load(
-            open(  # pylint: disable=consider-using-with
-                sample_data_folder + "/locations/locations.json",
-                "r",
-                encoding="utf-8",
-            )
-        )
-        self.storage_service = get_storage_service_or_create(
-            service_json=self.storage_service_json,
-            metadata_config=metadata_config,
-        )
-        self.glue_storage_service_json = json.load(
-            open(  # pylint: disable=consider-using-with
-                sample_data_folder + "/glue/storage_service.json",
-                "r",
-                encoding="utf-8",
-            )
-        )
         self.glue_database_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/glue/database_service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.glue_database = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/glue/database.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.glue_database_schema = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/glue/database_schema.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.glue_tables = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/glue/tables.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.glue_database_service = self.metadata.get_service_or_create(
             entity=DatabaseService,
             config=WorkflowSource(**self.glue_database_service_json),
         )
-        self.glue_storage_service = get_storage_service_or_create(
-            self.glue_storage_service_json,
-            metadata_config,
-        )
         self.database_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/datasets/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.database = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/datasets/database.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.database_schema = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/datasets/database_schema.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.tables = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/datasets/tables.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.database_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/datasets/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.database_service = self.metadata.get_service_or_create(
             entity=DatabaseService, config=WorkflowSource(**self.database_service_json)
         )
 
         self.kafka_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/topics/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.topics = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/topics/topics.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
 
         self.kafka_service = self.metadata.get_service_or_create(
             entity=MessagingService, config=WorkflowSource(**self.kafka_service_json)
         )
 
+        with open(
+            sample_data_folder + "/looker/service.json",
+            "r",
+            encoding=UTF_8,
+        ) as file:
+            self.looker_service = self.metadata.get_service_or_create(
+                entity=DashboardService,
+                config=WorkflowSource(**json.load(file)),
+            )
+
+        with open(
+            sample_data_folder + "/looker/charts.json",
+            "r",
+            encoding=UTF_8,
+        ) as file:
+            self.looker_charts = json.load(file)
+
+        with open(
+            sample_data_folder + "/looker/dashboards.json",
+            "r",
+            encoding=UTF_8,
+        ) as file:
+            self.looker_dashboards = json.load(file)
+
+        with open(
+            sample_data_folder + "/looker/dashboardDataModels.json",
+            "r",
+            encoding=UTF_8,
+        ) as file:
+            self.looker_models = json.load(file)
+
         self.dashboard_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/dashboards/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.charts = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/dashboards/charts.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.data_models = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/dashboards/dashboardDataModels.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.dashboards = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/dashboards/dashboards.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.dashboard_service = self.metadata.get_service_or_create(
             entity=DashboardService,
             config=WorkflowSource(**self.dashboard_service_json),
         )
 
         self.pipeline_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/pipelines/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.pipelines = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/pipelines/pipelines.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.pipeline_service = self.metadata.get_service_or_create(
             entity=PipelineService, config=WorkflowSource(**self.pipeline_service_json)
         )
         self.lineage = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/lineage/lineage.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.teams = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/teams/teams.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.users = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/users/users.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.model_service_json = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/models/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.model_service = self.metadata.get_service_or_create(
             entity=MlModelService,
             config=WorkflowSource(**self.model_service_json),
         )
 
-        self.object_service_json = json.load(
+        self.storage_service_json = json.load(
             open(  # pylint: disable=consider-using-with
-                sample_data_folder + "/objectcontainers/service.json",
+                sample_data_folder + "/storage/service.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
 
-        self.object_store_service = self.metadata.get_service_or_create(
-            entity=ObjectStoreService,
-            config=WorkflowSource(**self.object_service_json),
+        self.storage_service = self.metadata.get_service_or_create(
+            entity=StorageService,
+            config=WorkflowSource(**self.storage_service_json),
         )
 
         self.models = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/models/models.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
 
         self.containers = json.load(
             open(  # pylint: disable=consider-using-with
-                sample_data_folder + "/objectcontainers/containers.json",
+                sample_data_folder + "/storage/containers.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
 
         self.user_entity = {}
         self.table_tests = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/datasets/tableTests.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.pipeline_status = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/pipelines/pipelineStatus.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.profiles = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/profiler/tableProfile.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.tests_suites = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/tests/testSuites.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
         self.tests_case_results = json.load(
             open(  # pylint: disable=consider-using-with
                 sample_data_folder + "/tests/testCaseResults.json",
                 "r",
-                encoding="utf-8",
+                encoding=UTF_8,
             )
         )
 
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
@@ -484,21 +458,21 @@
 
     def prepare(self):
         pass
 
     def next_record(self) -> Iterable[Entity]:
         yield from self.ingest_teams()
         yield from self.ingest_users()
-        yield from self.ingest_locations()
         yield from self.ingest_glue()
         yield from self.ingest_tables()
         yield from self.ingest_topics()
         yield from self.ingest_charts()
         yield from self.ingest_data_models()
         yield from self.ingest_dashboards()
+        yield from self.ingest_looker()
         yield from self.ingest_pipelines()
         yield from self.ingest_lineage()
         yield from self.ingest_pipeline_status()
         yield from self.ingest_mlmodels()
         yield from self.ingest_containers()
         yield from self.ingest_profiles()
         yield from self.ingest_test_suite()
@@ -506,15 +480,14 @@
         yield from self.ingest_test_case_results()
 
     def ingest_teams(self):
         """
         Ingest sample teams
         """
         for team in self.teams["teams"]:
-
             team_to_ingest = CreateTeamRequest(
                 name=team["name"], teamType=team["teamType"]
             )
             if team["parent"] is not None:
                 parent_list_id = []
                 for parent in team["parent"]:
                     tries = 3
@@ -530,28 +503,14 @@
                     if parent_object:
                         parent_list_id.append(parent_object.id)
 
                 team_to_ingest.parents = parent_list_id
 
             yield team_to_ingest
 
-    def ingest_locations(self) -> Iterable[Location]:
-        for location in self.locations["locations"]:
-            location_ev = CreateLocationRequest(
-                name=location["name"],
-                path=location["path"],
-                displayName=location["displayName"],
-                description=location["description"],
-                locationType=location["locationType"],
-                service=EntityReference(
-                    id=self.storage_service.id, type="storageService"
-                ),
-            )
-            yield location_ev
-
     def ingest_glue(self):
         """
         Ingest Sample Data for glue database source
         """
         db = CreateDatabaseRequest(
             name=self.database["name"],
             description=self.database["description"],
@@ -597,42 +556,14 @@
                 databaseSchema=database_schema_object.fullyQualifiedName,
                 tableConstraints=table.get("tableConstraints"),
                 tableType=table["tableType"],
             )
             self.status.scanned(f"Table Scanned: {table_request.name.__root__}")
             yield table_request
 
-            location = CreateLocationRequest(
-                name=table["name"],
-                service=EntityReference(
-                    id=self.glue_storage_service.id, type="storageService"
-                ),
-            )
-            self.status.scanned(f"Location Scanned: {location.name}")
-            yield location
-
-            table_fqn = fqn.build(
-                self.metadata,
-                entity_type=Table,
-                service_name=self.database_service.name.__root__,
-                database_name=db.name.__root__,
-                schema_name=schema.name.__root__,
-                table_name=table_request.name.__root__,
-                skip_es_search=True,
-            )
-
-            location_fqn = fqn.build(
-                self.metadata,
-                entity_type=Location,
-                service_name=self.glue_storage_service.name.__root__,
-                location_name=location.name.__root__,
-            )
-            if table_fqn and location_fqn:
-                yield TableLocationLink(table_fqn=table_fqn, location_fqn=location_fqn)
-
     def ingest_tables(self):
         """
         Ingest Sample Tables
         """
 
         db = CreateDatabaseRequest(
             name=self.database["name"],
@@ -721,14 +652,121 @@
                     schemaType=topic["schemaType"],
                     schemaFields=schema_fields,
                 )
 
             self.status.scanned(f"Topic Scanned: {create_topic.name.__root__}")
             yield create_topic
 
+    def ingest_looker(self) -> Iterable[Entity]:
+        """
+        Looker sample data
+        """
+        for data_model in self.looker_models:
+            try:
+                data_model_ev = CreateDashboardDataModelRequest(
+                    name=data_model["name"],
+                    displayName=data_model["displayName"],
+                    description=data_model["description"],
+                    columns=data_model["columns"],
+                    dataModelType=data_model["dataModelType"],
+                    sql=data_model["sql"],
+                    serviceType=data_model["serviceType"],
+                    service=self.looker_service.fullyQualifiedName,
+                )
+                self.status.scanned(
+                    f"Data Model Scanned: {data_model_ev.name.__root__}"
+                )
+                yield data_model_ev
+            except ValidationError as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Unexpected exception ingesting chart [{data_model}]: {err}"
+                )
+
+        for chart in self.looker_charts:
+            try:
+                chart_ev = CreateChartRequest(
+                    name=chart["name"],
+                    displayName=chart["displayName"],
+                    description=chart["description"],
+                    chartType=get_standard_chart_type(chart["chartType"]),
+                    chartUrl=chart["chartUrl"],
+                    service=self.looker_service.fullyQualifiedName,
+                )
+                self.status.scanned(f"Chart Scanned: {chart_ev.name.__root__}")
+                yield chart_ev
+            except ValidationError as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(f"Unexpected exception ingesting chart [{chart}]: {err}")
+
+        for dashboard in self.looker_dashboards:
+            try:
+                dashboard_ev = CreateDashboardRequest(
+                    name=dashboard["name"],
+                    displayName=dashboard["displayName"],
+                    description=dashboard["description"],
+                    dashboardUrl=dashboard["dashboardUrl"],
+                    charts=dashboard["charts"],
+                    dataModels=dashboard.get("dataModels", None),
+                    service=self.looker_service.fullyQualifiedName,
+                )
+                self.status.scanned(f"Dashboard Scanned: {dashboard_ev.name.__root__}")
+                yield dashboard_ev
+            except ValidationError as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Unexpected exception ingesting dashboard [{dashboard}]: {err}"
+                )
+
+        orders_view = self.metadata.get_by_name(
+            entity=DashboardDataModel, fqn="sample_looker.model.orders_view"
+        )
+        operations_view = self.metadata.get_by_name(
+            entity=DashboardDataModel, fqn="sample_looker.model.operations_view"
+        )
+        orders_explore = self.metadata.get_by_name(
+            entity=DashboardDataModel, fqn="sample_looker.model.orders"
+        )
+        orders_dashboard = self.metadata.get_by_name(
+            entity=Dashboard, fqn="sample_looker.orders"
+        )
+
+        yield AddLineageRequest(
+            edge=EntitiesEdge(
+                fromEntity=EntityReference(
+                    id=orders_view.id.__root__, type="dashboardDataModel"
+                ),
+                toEntity=EntityReference(
+                    id=orders_explore.id.__root__, type="dashboardDataModel"
+                ),
+            )
+        )
+
+        yield AddLineageRequest(
+            edge=EntitiesEdge(
+                fromEntity=EntityReference(
+                    id=operations_view.id.__root__, type="dashboardDataModel"
+                ),
+                toEntity=EntityReference(
+                    id=orders_explore.id.__root__, type="dashboardDataModel"
+                ),
+            )
+        )
+
+        yield AddLineageRequest(
+            edge=EntitiesEdge(
+                fromEntity=EntityReference(
+                    id=orders_explore.id.__root__, type="dashboardDataModel"
+                ),
+                toEntity=EntityReference(
+                    id=orders_dashboard.id.__root__, type="dashboard"
+                ),
+            )
+        )
+
     def ingest_charts(self) -> Iterable[CreateChartRequest]:
         for chart in self.charts["charts"]:
             try:
                 chart_ev = CreateChartRequest(
                     name=chart["name"],
                     displayName=chart["displayName"],
                     description=chart["description"],
@@ -925,15 +963,15 @@
                     if parent_container_fqn
                     else None,
                     prefix=container["prefix"],
                     dataModel=container.get("dataModel"),
                     numberOfObjects=container.get("numberOfObjects"),
                     size=container.get("size"),
                     fileFormats=container.get("fileFormats"),
-                    service=self.object_store_service.fullyQualifiedName,
+                    service=self.storage_service.fullyQualifiedName,
                 )
                 yield container_request
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(f"Error ingesting Container [{container}]: {exc}")
 
     def ingest_users(self) -> Iterable[OMetaUserProfile]:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sample_usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sample_usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/singlestore/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/singlestore/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/singlestore/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/singlestore/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/connection.py`

 * *Files 0% similar despite different names*

```diff
@@ -96,15 +96,14 @@
 
 
 def get_connection(connection: SnowflakeConnection) -> Engine:
     """
     Create connection
     """
     if connection.privateKey:
-
         snowflake_private_key_passphrase = (
             connection.snowflakePrivatekeyPassphrase.get_secret_value()
             if connection.snowflakePrivatekeyPassphrase
             else ""
         )
 
         if not snowflake_private_key_passphrase:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/lineage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -316,15 +316,14 @@
         to get the names and pass the Regular type.
 
         This is useful for sources where we need fine-grained
         logic on how to handle table types, e.g., external, foreign,...
         """
 
         if self.config.serviceConnection.__root__.config.includeTempTables:
-
             return [
                 TableNameAndType(name=table_name)
                 for table_name in self.inspector.get_table_names(
                     schema=schema_name, include_temp_tables="True"
                 )
                 or []
             ]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/query_parser.py`

 * *Files 22% similar despite different names*

```diff
@@ -5,99 +5,65 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Snowflake Query parser module
+Vertica usage module
 """
 from abc import ABC
-from datetime import datetime
 from typing import Iterable
 
-from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
-    SnowflakeConnection,
+from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
+    VerticaConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.generated.schema.type.tableQuery import TableQuery
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.query_parser_source import QueryParserSource
-from metadata.ingestion.source.database.snowflake.queries import (
-    SNOWFLAKE_SESSION_TAG_QUERY,
-)
+from metadata.ingestion.source.database.vertica.queries import VERTICA_LIST_DATABASES
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
-SNOWFLAKE_ABORTED_CODE = "1969"
 
 
-class SnowflakeQueryParserSource(QueryParserSource, ABC):
+class VerticaQueryParserSource(QueryParserSource, ABC):
     """
-    Snowflake base for Usage and Lineage
+    Vertica lineage parser source.
+
+    Vertica V_MONITOR schema changes from database to database.
+    To allow the lineage to happen for all the ingested databases
+    we'll need to iterate over them.
     """
 
+    filters: str
+
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
+        """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: SnowflakeConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, SnowflakeConnection):
+        connection: VerticaConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, VerticaConnection):
             raise InvalidSourceException(
-                f"Expected SnowflakeConnection, but got {connection}"
+                f"Expected VerticaConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
-    def get_sql_statement(self, start_time: datetime, end_time: datetime) -> str:
-        """
-        returns sql statement to fetch query logs
-        """
-        return self.sql_stmt.format(
-            start_time=start_time,
-            end_time=end_time,
-            result_limit=self.config.sourceConfig.config.resultLimit,
-            filters=self.filters,
-        )
-
-    def set_session_query_tag(self) -> None:
-        """
-        Method to set query tag for current session
-        """
-        if self.service_connection.queryTag:
-            self.engine.execute(
-                SNOWFLAKE_SESSION_TAG_QUERY.format(
-                    query_tag=self.service_connection.queryTag
-                )
-            )
-
     def get_table_query(self) -> Iterable[TableQuery]:
         database = self.config.serviceConnection.__root__.config.database
         if database:
-            use_db_query = f"USE DATABASE {database}"
-            self.engine.execute(use_db_query)
-            self.set_session_query_tag()
             yield from super().get_table_query()
         else:
-            query = "SHOW DATABASES"
-            results = self.engine.execute(query)
+            results = self.engine.execute(VERTICA_LIST_DATABASES)
             for res in results:
                 row = list(res)
-                use_db_query = f"USE DATABASE {row[1]}"
-                self.engine.execute(use_db_query)
-                logger.info(f"Ingesting from database: {row[1]}")
-                self.config.serviceConnection.__root__.config.database = row[1]
+                logger.info(f"Ingesting from database: {row[0]}")
+                self.config.serviceConnection.__root__.config.database = row[0]
                 self.engine = get_connection(self.service_connection)
-                self.set_session_query_tag()
                 yield from super().get_table_query()
-
-    def get_database_name(self, data: dict) -> str:  # pylint: disable=arguments-differ
-        """
-        Method to get database name
-        """
-        if not data["database_name"] and self.service_connection.database:
-            return self.service_connection.database
-        return data["database_name"]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/snowflake/usage.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/snowflake/utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/oracle/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,222 +4,276 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Module to define overriden dialect methods
+Utils module to define overrided sqlalchamy methods 
 """
+# pylint: disable=protected-access,unused-argument
+import re
 
-import sqlalchemy.types as sqltypes
-from sqlalchemy import exc as sa_exc
-from sqlalchemy import util as sa_util
+from sqlalchemy import sql, util
+from sqlalchemy.dialects.oracle.base import FLOAT, INTEGER, INTERVAL, NUMBER, TIMESTAMP
 from sqlalchemy.engine import reflection
-from sqlalchemy.sql import text
-from sqlalchemy.types import FLOAT
+from sqlalchemy.sql import sqltypes
 
-from metadata.ingestion.source.database.snowflake.queries import (
-    SNOWFLAKE_GET_COMMENTS,
-    SNOWFLAKE_GET_SCHEMA_COLUMNS,
-    SNOWFLAKE_GET_TABLE_NAMES,
-    SNOWFLAKE_GET_VIEW_NAMES,
-    SNOWFLAKE_GET_WITHOUT_TRANSIENT_TABLE_NAMES,
+from metadata.ingestion.source.database.oracle.queries import (
+    GET_MATERIALIZED_VIEW_NAMES,
+    ORACLE_ALL_MATERIALIZED_VIEW_DEFINITIONS,
+    ORACLE_ALL_TABLE_COMMENTS,
+    ORACLE_ALL_VIEW_DEFINITIONS,
+    ORACLE_GET_COLUMNS,
+    ORACLE_GET_TABLE_NAMES,
+    ORACLE_IDENTITY_TYPE,
 )
 from metadata.utils.sqlalchemy_utils import (
-    get_display_datatype,
     get_table_comment_wrapper,
+    get_view_definition_wrapper,
 )
 
 
-def get_table_names_reflection(self, schema=None, **kw):
-    """Return all table names in referred to within a particular schema.
+@reflection.cache
+def get_table_comment(
+    self,
+    connection,
+    table_name: str,
+    schema: str = None,
+    resolve_synonyms=False,
+    dblink="",
+    **kw,
+):
+    return get_table_comment_wrapper(
+        self,
+        connection,
+        table_name=table_name.lower(),
+        schema=schema.lower() if schema else None,
+        query=ORACLE_ALL_TABLE_COMMENTS,
+    )
+
+
+@reflection.cache
+def get_view_definition(
+    self,
+    connection,
+    view_name: str,
+    schema: str = None,
+    resolve_synonyms=False,
+    dblink="",
+    **kw,
+):
+
+    return get_view_definition_wrapper(
+        self,
+        connection,
+        table_name=view_name.lower(),
+        schema=schema.lower() if schema else None,
+        query=ORACLE_ALL_VIEW_DEFINITIONS,
+    )
 
-    The names are expected to be real tables only, not views.
-    Views are instead returned using the
-    :meth:`_reflection.Inspector.get_view_names`
-    method.
 
+@reflection.cache
+def get_mview_definition_dialect(
+    self,
+    connection,
+    view_name: str,
+    schema: str = None,
+    resolve_synonyms=False,
+    dblink="",
+    **kw,
+):
 
-    :param schema: Schema name. If ``schema`` is left at ``None``, the
-        database's default schema is
-        used, else the named schema is searched.  If the database does not
-        support named schemas, behavior is undefined if ``schema`` is not
-        passed as ``None``.  For special quoting, use :class:`.quoted_name`.
+    return get_view_definition_wrapper(
+        self,
+        connection,
+        table_name=view_name.lower(),
+        schema=schema.lower() if schema else None,
+        query=ORACLE_ALL_MATERIALIZED_VIEW_DEFINITIONS,
+    )
 
-    .. seealso::
 
-        :meth:`_reflection.Inspector.get_sorted_table_and_fkc_names`
+def _get_col_type(
+    self, coltype, precision, scale, length, colname
+):  # pylint: disable=too-many-branches
+    raw_type = coltype
+    if coltype == "NUMBER":
+        if precision is None and scale == 0:
+            coltype = INTEGER()
+        else:
+            coltype = NUMBER(precision, scale)
+            if precision is not None:
+                if scale is not None:
+                    raw_type += f"({precision},{scale})"
+                else:
+                    raw_type += f"({precision})"
+
+    elif coltype == "FLOAT":
+        # TODO: support "precision" here as "binary_precision"
+        coltype = FLOAT()
+    elif coltype in ("VARCHAR2", "NVARCHAR2", "CHAR", "NCHAR"):
+        coltype = self.ischema_names.get(coltype)(length)
+        if length:
+            raw_type += f"({length})"
+    elif "WITH TIME ZONE" in coltype or "TIMESTAMP" in coltype:
+        coltype = TIMESTAMP(timezone=True)
+    elif "INTERVAL" in coltype:
+        coltype = INTERVAL()
+    else:
+        coltype = re.sub(r"\(\d+\)", "", coltype)
+        try:
+            coltype = self.ischema_names[coltype]
+        except KeyError:
+            util.warn(f"Did not recognize type '{coltype}' of column '{colname}'")
+            coltype = sqltypes.NULLTYPE
+    return coltype, raw_type
 
-        :attr:`_schema.MetaData.sorted_tables`
 
+# pylint: disable=too-many-locals
+@reflection.cache
+def get_columns(self, connection, table_name, schema=None, **kw):
     """
 
-    with self._operation_context() as conn:  # pylint: disable=protected-access
-        return self.dialect.get_table_names(
-            conn, schema, info_cache=self.info_cache, **kw
-        )
+    Dialect method overridden to add raw data type
 
+    kw arguments can be:
 
-def get_table_names(self, connection, schema, **kw):
+        oracle_resolve_synonyms
 
-    if kw.get("include_temp_tables"):
-        cursor = connection.execute(SNOWFLAKE_GET_TABLE_NAMES.format(schema))
-        result = [self.normalize_name(row[0]) for row in cursor]
-        return result
+        dblink
+
+    """
+    resolve_synonyms = kw.get("oracle_resolve_synonyms", False)
+    dblink = kw.get("dblink", "")
+    info_cache = kw.get("info_cache")
 
-    cursor = connection.execute(
-        SNOWFLAKE_GET_WITHOUT_TRANSIENT_TABLE_NAMES.format(schema)
+    (table_name, schema, dblink, _) = self._prepare_reflection_args(
+        connection,
+        table_name,
+        schema,
+        resolve_synonyms,
+        dblink,
+        info_cache=info_cache,
     )
-    result = [self.normalize_name(row[0]) for row in cursor]
-    return result
+    columns = []
 
+    char_length_col = "data_length"
+    if self._supports_char_length:
+        char_length_col = "char_length"
 
-def get_view_names(self, connection, schema, **kw):  # pylint: disable=unused-argument
-    cursor = connection.execute(SNOWFLAKE_GET_VIEW_NAMES.format(schema))
-    result = [self.normalize_name(row[0]) for row in cursor]
-    return result
+    identity_cols = "NULL as default_on_null, NULL as identity_options"
+    if self.server_version_info >= (12,):
+        identity_cols = ORACLE_IDENTITY_TYPE.format(dblink=dblink)
+
+    params = {"table_name": table_name}
+
+    text = ORACLE_GET_COLUMNS.format(
+        dblink=dblink, char_length_col=char_length_col, identity_cols=identity_cols
+    )
+    if schema is not None:
+        params["owner"] = schema
+        text += " AND col.owner = :owner "
+    text += " ORDER BY col.column_id"
+
+    cols = connection.execute(sql.text(text), params)
+
+    for row in cols:
+        colname = self.normalize_name(row[0])
+        length = row[2]
+        nullable = row[5] == "Y"
+        default = row[6]
+        generated = row[8]
+        default_on_nul = row[9]
+        identity_options = row[10]
+
+        coltype, raw_coltype = self._get_col_type(
+            row.data_type, row.data_precision, row.data_scale, length, colname
+        )
+
+        computed = None
+        if generated == "YES":
+            computed = {"sqltext": default}
+            default = None
+
+        identity = None
+        if identity_options is not None:
+            identity = self._parse_identity_options(identity_options, default_on_nul)
+            default = None
+
+        cdict = {
+            "name": colname,
+            "type": coltype,
+            "nullable": nullable,
+            "default": default,
+            "autoincrement": "auto",
+            "comment": row.comments,
+            "system_data_type": raw_coltype,
+        }
+        if row.column_name.lower() == row.column_name:
+            cdict["quote"] = True
+        if computed is not None:
+            cdict["computed"] = computed
+        if identity is not None:
+            cdict["identity"] = identity
+
+        columns.append(cdict)
+    return columns
 
 
 @reflection.cache
-def get_view_definition(  # pylint: disable=unused-argument
-    self, connection, view_name, schema=None, **kw
-):
+def get_table_names(self, connection, schema=None, **kw):
     """
-    Gets the view definition
+    Exclude the materialized views from regular table names
     """
-    schema = schema or self.default_schema_name
-    if schema:
-        cursor = connection.execute(
-            "SHOW /* sqlalchemy:get_view_definition */ VIEWS "
-            f"LIKE '{view_name}' IN {schema}"
-        )
-    else:
-        cursor = connection.execute(
-            "SHOW /* sqlalchemy:get_view_definition */ VIEWS " f"LIKE '{view_name}'"
+    schema = self.denormalize_name(schema or self.default_schema_name)
+
+    # note that table_names() isn't loading DBLINKed or synonym'ed tables
+    if schema is None:
+        schema = self.default_schema_name
+
+    tablespace = ""
+
+    if self.exclude_tablespaces:
+        exclude_tablespace = ", ".join([f"'{ts}'" for ts in self.exclude_tablespaces])
+        tablespace = (
+            "nvl(tablespace_name, 'no tablespace') "
+            f"NOT IN ({exclude_tablespace}) AND "
         )
-    n2i = self.__class__._map_name_to_idx(cursor)  # pylint: disable=protected-access
-    try:
-        ret = cursor.fetchone()
-        if ret:
-            return ret[n2i["text"]]
-    except Exception:
-        pass
-    return None
+    sql_str = ORACLE_GET_TABLE_NAMES.format(tablespace=tablespace)
 
+    cursor = connection.execute(sql.text(sql_str), {"owner": schema})
+    return [self.normalize_name(row[0]) for row in cursor]
 
-@reflection.cache
-def get_table_comment(
-    self, connection, table_name, schema=None, **kw
-):  # pylint: disable=unused-argument
-    return get_table_comment_wrapper(
-        self,
-        connection,
-        table_name=table_name,
-        schema=schema,
-        query=SNOWFLAKE_GET_COMMENTS,
-    )
 
+def get_mview_names(self, schema=None):
+    """Return all materialized view names in `schema`.
 
-@reflection.cache
-def get_unique_constraints(  # pylint: disable=unused-argument
-    self, connection, table_name, schema=None, **kw
-):
-    return []
+    :param schema: Optional, retrieve names from a non-default schema.
+        For special quoting, use :class:`.quoted_name`.
 
+    """
 
-def normalize_names(self, name):  # pylint: disable=unused-argument
-    return name
+    with self._operation_context() as conn:
+        return self.dialect.get_mview_names(conn, schema, info_cache=self.info_cache)
 
 
-# pylint: disable=too-many-locals,protected-access
 @reflection.cache
-def get_schema_columns(self, connection, schema, **kw):
-    """Get all columns in the schema, if we hit 'Information schema query returned too much data' problem return
-    None, as it is cacheable and is an unexpected return type for this function"""
-    ans = {}
-    current_database, _ = self._current_database_schema(connection, **kw)
-    full_schema_name = self._denormalize_quote_join(current_database, schema)
-    try:
-        schema_primary_keys = self._get_schema_primary_keys(
-            connection, full_schema_name, **kw
-        )
-        result = connection.execute(
-            text(SNOWFLAKE_GET_SCHEMA_COLUMNS),
-            {"table_schema": self.denormalize_name(schema)},
-        )
-    except sa_exc.ProgrammingError as p_err:
-        if p_err.orig.errno == 90030:
-            # This means that there are too many tables in the schema, we need to go more granular
-            return None  # None triggers _get_table_columns while staying cacheable
-        raise
-    for (
-        table_name,
-        column_name,
-        coltype,
-        character_maximum_length,
-        numeric_precision,
-        numeric_scale,
-        is_nullable,
-        column_default,
-        is_identity,
-        comment,
-        identity_start,
-        identity_increment,
-    ) in result:
-        table_name = self.normalize_name(table_name)
-        column_name = self.normalize_name(column_name)
-        if table_name not in ans:
-            ans[table_name] = []
-        if column_name.startswith("sys_clustering_column"):
-            continue  # ignoring clustering column
-        col_type = self.ischema_names.get(coltype, None)
-        col_type_kw = {}
-        if col_type is None:
-            sa_util.warn(
-                f"Did not recognize type '{coltype}' of column '{column_name}'"
-            )
-            col_type = sqltypes.NULLTYPE
-        else:
-            if issubclass(col_type, FLOAT):
-                col_type_kw["precision"] = numeric_precision
-                col_type_kw["decimal_return_scale"] = numeric_scale
-            elif issubclass(col_type, sqltypes.Numeric):
-                col_type_kw["precision"] = numeric_precision
-                col_type_kw["scale"] = numeric_scale
-            elif issubclass(col_type, (sqltypes.String, sqltypes.BINARY)):
-                col_type_kw["length"] = character_maximum_length
-
-        type_instance = col_type(**col_type_kw)
-
-        current_table_pks = schema_primary_keys.get(table_name)
-
-        ans[table_name].append(
-            {
-                "name": column_name,
-                "type": type_instance,
-                "nullable": is_nullable == "YES",
-                "default": column_default,
-                "autoincrement": is_identity == "YES",
-                "system_data_type": get_display_datatype(
-                    coltype,
-                    char_len=character_maximum_length,
-                    precision=numeric_precision,
-                    scale=numeric_scale,
-                ),
-                "comment": comment,
-                "primary_key": (
-                    column_name
-                    in schema_primary_keys[table_name]["constrained_columns"]
-                )
-                if current_table_pks
-                else False,
-            }
+def get_mview_names_dialect(self, connection, schema=None, **kw):
+    schema = self.denormalize_name(schema or self.default_schema_name)
+    sql_query = sql.text(GET_MATERIALIZED_VIEW_NAMES)
+    cursor = connection.execute(sql_query, {"owner": self.denormalize_name(schema)})
+    return [self.normalize_name(row[0]) for row in cursor]
+
+
+def get_mview_definition(self, mview_name, schema=None):
+    """Return definition for `mview_name`.
+
+    :param schema: Optional, retrieve names from a non-default schema.
+        For special quoting, use :class:`.quoted_name`.
+
+    """
+
+    with self._operation_context() as conn:
+        return self.dialect.get_mview_definition(
+            conn, mview_name, schema, info_cache=self.info_cache
         )
-        if is_identity == "YES":
-            ans[table_name][-1]["identity"] = {
-                "start": identity_start,
-                "increment": identity_increment,
-            }
-    return ans
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sql_column_handler.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sql_column_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -278,14 +278,15 @@
                         dataTypeDisplay=column.get(
                             "system_data_type", data_type_display
                         ),
                         dataLength=col_data_length,
                         constraint=col_constraint,
                         children=children,
                         arrayDataType=arr_data_type,
+                        ordinalPosition=column.get("ordinalPosition"),
                     )
                     if precision:
                         om_column.precision = precision[0]
                         om_column.scale = precision[1]
 
                 else:
                     col_obj = self._process_complex_col_type(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlalchemy_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlalchemy_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlite/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/connection.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,51 +15,55 @@
 from typing import Optional
 
 from sqlalchemy.engine import Engine
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.sqliteConnection import (
-    SQLiteConnection,
+from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
+    VerticaConnection,
 )
 from metadata.ingestion.connections.builders import (
     create_generic_db_connection,
     get_connection_args_common,
+    get_connection_url_common,
 )
 from metadata.ingestion.connections.test_connections import test_connection_db_common
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.ingestion.source.database.vertica.queries import (
+    VERTICA_LIST_DATABASES,
+    VERTICA_TEST_GET_QUERIES,
+)
 
 
-def get_connection_url(connection: SQLiteConnection) -> str:
-    database_mode = connection.databaseMode if connection.databaseMode else ":memory:"
-
-    return f"{connection.scheme.value}:///{database_mode}"
-
-
-def get_connection(connection: SQLiteConnection) -> Engine:
+def get_connection(connection: VerticaConnection) -> Engine:
     """
     Create connection
     """
     return create_generic_db_connection(
         connection=connection,
-        get_connection_url_fn=get_connection_url,
+        get_connection_url_fn=get_connection_url_common,
         get_connection_args_fn=get_connection_args_common,
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     engine: Engine,
-    service_connection: SQLiteConnection,
+    service_connection: VerticaConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
+    queries = {
+        "GetQueries": VERTICA_TEST_GET_QUERIES,
+        "GetDatabases": VERTICA_LIST_DATABASES,
+    }
     test_connection_db_common(
         metadata=metadata,
         engine=engine,
         service_connection=service_connection,
         automation_workflow=automation_workflow,
+        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/sqlite/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/sqlite/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -99,15 +99,15 @@
 
 
 def _get_columns(
     self, connection: Connection, table_name: str, schema: str = None, **__
 ) -> List[Dict[str, Any]]:
     # pylint: disable=protected-access
     schema = schema or self._get_default_schema_name(connection)
-    query = f"SHOW COLUMNS FROM {schema}.{table_name}"
+    query = f'SHOW COLUMNS FROM {schema}."{table_name}"'
 
     res = connection.execute(sql.text(query), schema=schema, table=table_name)
     columns = []
     for record in res:
         col_type = datatype.parse_sqltype(record.Type)
         column = {
             "name": record.Column,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/trino/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/trino/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/usage_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/usage_source.py`

 * *Files 7% similar despite different names*

```diff
@@ -13,37 +13,33 @@
 """
 import csv
 import traceback
 from abc import ABC
 from datetime import datetime, timedelta
 from typing import Iterable, Optional
 
-from sqlalchemy.engine import Engine
-
 from metadata.generated.schema.type.tableQuery import TableQueries, TableQuery
-from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.query_parser_source import QueryParserSource
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class UsageSource(QueryParserSource, ABC):
     """
     Base class for all usage ingestion.
 
     Parse a query log to extract a `TableQuery` object
     """
 
-    def get_table_query(self) -> Optional[Iterable[TableQuery]]:
+    def yield_table_queries_from_logs(self) -> Optional[Iterable[TableQuery]]:
         """
-        If queryLogFilePath available in config iterate through log file
-        otherwise execute the sql query to fetch TableQuery data
+        Method to handle the usage from query logs
         """
-        if self.config.sourceConfig.config.queryLogFilePath:
+        try:
             query_list = []
             with open(
                 self.config.sourceConfig.config.queryLogFilePath, "r", encoding="utf-8"
             ) as fin:
                 for record in csv.DictReader(fin):
                     query_dict = dict(record)
                     analysis_date = (
@@ -64,32 +60,41 @@
                             aborted=self.get_aborted_status(query_dict),
                             databaseName=self.get_database_name(query_dict),
                             serviceName=self.config.serviceName,
                             databaseSchema=self.get_schema_name(query_dict),
                         )
                     )
             yield TableQueries(queries=query_list)
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.warning(f"Failed to read queries form log file due to: {err}")
 
+    def get_table_query(self) -> Optional[Iterable[TableQuery]]:
+        """
+        If queryLogFilePath available in config iterate through log file
+        otherwise execute the sql query to fetch TableQuery data
+        """
+        if self.config.sourceConfig.config.queryLogFilePath:
+            yield from self.yield_table_queries_from_logs()
         else:
-            engine = get_connection(self.service_connection)
-            yield from self.yield_table_queries(engine)
+            yield from self.yield_table_queries()
 
-    def yield_table_queries(self, engine: Engine):
+    def yield_table_queries(self):
         """
         Given an Engine, iterate over the day range and
         query the results
         """
         daydiff = self.end - self.start
         for days in range(daydiff.days):
             logger.info(
                 f"Scanning query logs for {(self.start + timedelta(days=days)).date()} - "
                 f"{(self.start + timedelta(days=days + 1)).date()}"
             )
             try:
-                with engine.connect() as conn:
+                with self.engine.connect() as conn:
                     rows = conn.execute(
                         self.get_sql_statement(
                             start_time=self.start + timedelta(days=days),
                             end_time=self.start + timedelta(days=days + 1),
                         )
                     )
                     queries = []
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,60 +10,63 @@
 #  limitations under the License.
 
 """
 Source connection handler
 """
 from typing import Optional
 
-from sqlalchemy.engine import Engine
+from dagster_graphql import DagsterGraphQLClient
+from gql.transport.requests import RequestsHTTPTransport
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
-    VerticaConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.dagsterConnection import (
+    DagsterConnection,
 )
-from metadata.ingestion.connections.builders import (
-    create_generic_db_connection,
-    get_connection_args_common,
-    get_connection_url_common,
-)
-from metadata.ingestion.connections.test_connections import test_connection_db_common
+from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.vertica.queries import (
-    VERTICA_LIST_DATABASES,
-    VERTICA_TEST_GET_QUERIES,
-)
+from metadata.ingestion.source.pipeline.dagster.queries import TEST_QUERY_GRAPHQL
+from metadata.utils.helpers import clean_uri
 
 
-def get_connection(connection: VerticaConnection) -> Engine:
+def get_connection(connection: DagsterConnection) -> DagsterGraphQLClient:
     """
     Create connection
     """
-    return create_generic_db_connection(
-        connection=connection,
-        get_connection_url_fn=get_connection_url_common,
-        get_connection_args_fn=get_connection_args_common,
+    url = clean_uri(connection.host)
+    dagster_connection = DagsterGraphQLClient(
+        url,
+        transport=RequestsHTTPTransport(
+            url=f"{url}/graphql",
+            headers={"Dagster-Cloud-Api-Token": connection.token.get_secret_value()}
+            if connection.token
+            else None,
+            timeout=connection.timeout,
+        ),
     )
 
+    return dagster_connection
+
 
 def test_connection(
     metadata: OpenMetadata,
-    engine: Engine,
-    service_connection: VerticaConnection,
+    client: DagsterGraphQLClient,
+    service_connection: DagsterConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
-    queries = {
-        "GetQueries": VERTICA_TEST_GET_QUERIES,
-        "GetDatabases": VERTICA_LIST_DATABASES,
-    }
-    test_connection_db_common(
+
+    def custom_executor_for_pipeline():
+        client._execute(TEST_QUERY_GRAPHQL)  # pylint: disable=protected-access
+
+    test_fn = {"GetPipelines": custom_executor_for_pipeline}
+
+    test_connection_steps(
         metadata=metadata,
-        engine=engine,
-        service_connection=service_connection,
+        test_fn=test_fn,
+        service_fqn=service_connection.type.value,
         automation_workflow=automation_workflow,
-        queries=queries,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/lineage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/lineage.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,15 +18,14 @@
 )
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class VerticaLineageSource(VerticaQueryParserSource, LineageSource):
-
     sql_stmt = VERTICA_SQL_STATEMENT
 
     filters = "AND query_type in ('INSERT', 'UPDATE', 'QUERY', 'DDL')"
 
     database_field = "DBNAME()"
 
     schema_field = ""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -117,15 +117,14 @@
     name,
     format_type,
     default,
     nullable,
     schema,
     comment,
 ):
-
     # strip (*) from character varying(5), timestamp(5)
     # with time zone, geometry(POLYGON), etc.
     attype = re.sub(r"\(.*\)", "", format_type)
 
     charlen = re.search(r"\(([\d,]+)\)", format_type)
     if charlen:
         charlen = charlen.group(1)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/query_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/redpanda/metadata.py`

 * *Files 22% similar despite different names*

```diff
@@ -5,65 +5,32 @@
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
-Vertica usage module
+RedPanda source ingestion
 """
-from abc import ABC
-from typing import Iterable
-
-from metadata.generated.schema.entity.services.connections.database.verticaConnection import (
-    VerticaConnection,
+from metadata.generated.schema.entity.services.connections.messaging.redpandaConnection import (
+    RedpandaConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
-from metadata.generated.schema.type.tableQuery import TableQuery
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.connections import get_connection
-from metadata.ingestion.source.database.query_parser_source import QueryParserSource
-from metadata.ingestion.source.database.vertica.queries import VERTICA_LIST_DATABASES
-from metadata.utils.logger import ingestion_logger
-
-logger = ingestion_logger()
-
+from metadata.ingestion.source.messaging.common_broker_source import CommonBrokerSource
 
-class VerticaQueryParserSource(QueryParserSource, ABC):
-    """
-    Vertica lineage parser source.
-
-    Vertica V_MONITOR schema changes from database to database.
-    To allow the lineage to happen for all the ingested databases
-    we'll need to iterate over them.
-    """
-
-    filters: str
 
+class RedpandaSource(CommonBrokerSource):
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        """Create class instance"""
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: VerticaConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, VerticaConnection):
+        connection: RedpandaConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, RedpandaConnection):
             raise InvalidSourceException(
-                f"Expected VerticaConnection, but got {connection}"
+                f"Expected RedpandaConnection, but got {connection}"
             )
         return cls(config, metadata_config)
-
-    def get_table_query(self) -> Iterable[TableQuery]:
-        database = self.config.serviceConnection.__root__.config.database
-        if database:
-            yield from super().get_table_query()
-        else:
-            results = self.engine.execute(VERTICA_LIST_DATABASES)
-            for res in results:
-                row = list(res)
-                logger.info(f"Ingesting from database: {row[0]}")
-                self.config.serviceConnection.__root__.config.database = row[0]
-                self.engine = get_connection(self.service_connection)
-                yield from super().get_table_query()
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/database/vertica/usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/database/vertica/usage.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,15 +18,14 @@
 )
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 class VerticaUsageSource(VerticaQueryParserSource, UsageSource):
-
     sql_stmt = VERTICA_SQL_STATEMENT
 
     filters = "AND query_type NOT IN ('UTILITY', 'TRANSACTION', 'SHOW', 'SET')"
 
     database_field = "DBNAME()"
 
     schema_field = ""  # schema filtering not available
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/ldap_users.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/ldap_users.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/common_broker_source.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/common_broker_source.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kafka/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kafka/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kafka/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/kinesis/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/kinesis/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/messaging_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/messaging_service.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/messaging/redpanda/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/messaging/redpanda/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/openmetadata/metadata.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,33 +4,35 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""
-RedPanda source ingestion
-"""
-from metadata.generated.schema.entity.services.connections.messaging.redpandaConnection import (
-    RedpandaConnection,
-)
+"""Metadata source module"""
+
+
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.messaging.common_broker_source import CommonBrokerSource
+from metadata.ingestion.source.metadata.metadata import MetadataSource
+from metadata.utils.logger import ingestion_logger
+
+logger = ingestion_logger()
+
 
+class OpenmetadataSource(MetadataSource):
+    """Metadata source Class"""
 
-class RedpandaSource(CommonBrokerSource):
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: RedpandaConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, RedpandaConnection):
+        connection: OpenMetadataConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, OpenMetadataConnection):
             raise InvalidSourceException(
-                f"Expected RedpandaConnection, but got {connection}"
+                f"Expected OpenMetadataConnection, but got {connection}"
             )
         return cls(config, metadata_config)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -212,15 +212,15 @@
                     table_entity: Table = self.metadata.get_by_name(
                         entity=Table, fqn=table_fqn
                     )
                     yield CreateTableRequest(
                         name=table_entity.name,
                         tableType=table_entity.tableType,
                         description=table_entity.description,
-                        databaseSchema=table_entity.databaseSchema,
+                        databaseSchema=table_entity.databaseSchema.fullyQualifiedName,
                         tags=table_entity.tags,
                         columns=table_entity.columns,
                         owner=user_entity_ref,
                     )
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.error(f"Failed to create user entity [{user}]: {exc}")
@@ -269,15 +269,14 @@
             )
         except Exception as err:
             logger.error(f"Failed to Ingest database due to - {err}")
             logger.debug(traceback.format_exc())
 
     def _yield_create_database_schema(self, table):
         try:
-
             database_schema_request = CreateDatabaseSchemaRequest(
                 name=table["schema"],
                 database=self.database_object.fullyQualifiedName,
             )
             yield database_schema_request
             database_schema_fqn = fqn.build(
                 self.metadata,
@@ -312,15 +311,15 @@
                 )
             else:
                 columns_meta = zip(
                     table["column_names"],
                     [None] * len(table["column_names"]),
                     table["column_types"],
                 )
-            for (name, description, data_type) in columns_meta:
+            for name, description, data_type in columns_meta:
                 # Amundsen merges the length into type itself. Instead of making changes to our generic type builder
                 # we will do a type match and see if it matches any primitive types and return a type
                 data_type = self.get_type_primitive_type(data_type)
                 parsed_string = ColumnTypeParser._parse_datatype_string(  # pylint: disable=protected-access
                     data_type
                 )
                 parsed_string["name"] = name
@@ -446,15 +445,15 @@
         except Exception as exc:
             error = f"Failed to create dashboard entity [{dashboard}]: {exc}"
             logger.debug(traceback.format_exc())
             logger.warning(error)
             self.status.failed(dashboard["name"], error, traceback.format_exc())
 
     def create_chart_entity(self, dashboard):
-        for (name, chart_id, chart_type, url) in zip(
+        for name, chart_id, chart_type, url in zip(
             dashboard["chart_names"],
             dashboard["chart_ids"],
             dashboard["chart_types"],
             dashboard["chart_urls"],
         ):
             chart = CreateChartRequest(
                 name=chart_id,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/amundsen/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/amundsen/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/client.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,14 @@
 
 class AtlasClient:
     """
     Client to interact with Atlas apis
     """
 
     def __init__(self, config: AtlasConnection, raw_data: bool = False):
-
         self.config = config
         self.auth_token = generate_http_basic_token(
             config.username, config.password.get_secret_value()
         )
         client_config: ClientConfig = ClientConfig(
             base_url=config.hostPort,
             auth_header="Authorization",
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/atlas/connection.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,15 +34,14 @@
 
 def test_connection(
     metadata: OpenMetadata,
     client: AtlasClient,
     service_connection: AtlasConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
-
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     test_fn = {"CheckAccess": client.list_entities}
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/atlas/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/ometa/mixins/glossary_mixin.py`

 * *Files 21% similar despite different names*

```diff
@@ -4,462 +4,475 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
-"""
-Atlas source to extract metadata
 """
+Mixin class containing Glossaries specific methods
 
+To be used be OpenMetadata
+"""
+import json
 import traceback
-from dataclasses import dataclass
-from typing import Any, Dict, Iterable, List
+from typing import Optional, Type, TypeVar, Union
 
-from metadata.generated.schema.api.classification.createClassification import (
-    CreateClassificationRequest,
-)
-from metadata.generated.schema.api.classification.createTag import CreateTagRequest
-from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest
-from metadata.generated.schema.api.lineage.addLineage import AddLineageRequest
-from metadata.generated.schema.api.services.createDatabaseService import (
-    CreateDatabaseServiceRequest,
-)
-from metadata.generated.schema.api.services.createMessagingService import (
-    CreateMessagingServiceRequest,
-)
-from metadata.generated.schema.entity.classification.tag import Tag
-from metadata.generated.schema.entity.data.database import Database
-from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
-from metadata.generated.schema.entity.data.pipeline import Pipeline
-from metadata.generated.schema.entity.data.table import Column, Table
-from metadata.generated.schema.entity.data.topic import Topic
-from metadata.generated.schema.entity.services.connections.metadata.atlasConnection import (
-    AtlasConnection,
-)
-from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
-    OpenMetadataConnection,
-)
-from metadata.generated.schema.entity.services.databaseService import DatabaseService
-from metadata.generated.schema.entity.services.messagingService import MessagingService
-from metadata.generated.schema.metadataIngestion.workflow import (
-    Source as WorkflowSource,
+from pydantic import BaseModel
+
+from metadata.generated.schema.entity.data.glossary import Glossary
+from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
+from metadata.generated.schema.type import basic
+from metadata.ingestion.ometa.mixins.patch_mixin_utils import (
+    OMetaPatchMixinBase,
+    PatchField,
+    PatchOperation,
+    PatchPath,
+    PatchValue,
 )
-from metadata.generated.schema.type.entityLineage import EntitiesEdge
-from metadata.generated.schema.type.entityReference import EntityReference
-from metadata.generated.schema.type.tagLabel import TagLabel
-from metadata.ingestion.api.source import InvalidSourceException, Source
-from metadata.ingestion.models.ometa_classification import OMetaTagAndClassification
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
-from metadata.ingestion.source.database.column_type_parser import ColumnTypeParser
-from metadata.ingestion.source.metadata.atlas.client import AtlasClient
-from metadata.utils import fqn
-from metadata.utils.logger import ingestion_logger
-from metadata.utils.metadata_service_helper import SERVICE_TYPE_MAPPER
+from metadata.ingestion.ometa.utils import model_str
+from metadata.utils.logger import ometa_logger
 
-logger = ingestion_logger()
+logger = ometa_logger()
 
-ATLAS_TAG_CATEGORY = "AtlasMetadata"
-ATLAS_TABLE_TAG = "atlas_table"
+T = TypeVar("T", bound=BaseModel)
 
 
-@dataclass
-class AtlasSource(Source):
+class GlossaryMixin(OMetaPatchMixinBase):
     """
-    Atlas source class
+    OpenMetadata API methods related to Glossaries.
+
+    To be inherited by OpenMetadata
     """
 
-    config: WorkflowSource
-    atlas_client: AtlasClient
-    tables: Dict[str, Any]
-    topics: Dict[str, Any]
+    def create_glossary(self, glossaries_body):
+        """Method to create new Glossary
+        Args:
+            glossaries_body (Glossary): body of the request
+        """
+        resp = self.client.put(
+            path=self.get_suffix(Glossary), data=glossaries_body.json()
+        )
+        logger.info(f"Created a Glossary: {resp}")
+
+    def create_glossary_term(self, glossary_term_body):
+        """Method to create new Glossary Term
+        Args:
+            glossary_term_body (Glossary): body of the request
+        """
+        resp = self.client.put(
+            path=self.get_suffix(GlossaryTerm), data=glossary_term_body.json()
+        )
+        logger.info(f"Created a Glossary Term: {resp}")
 
-    def __init__(
+    def patch_glossary_term_parent(
         self,
-        config: WorkflowSource,
-        metadata_config: OpenMetadataConnection,
-    ):
-        super().__init__()
-        self.config = config
-        self.metadata_config = metadata_config
-        self.metadata = OpenMetadata(metadata_config)
-        self.service_connection = self.config.serviceConnection.__root__.config
-
-        self.atlas_client = get_connection(self.service_connection)
-        self.connection_obj = self.atlas_client
-        self.tables: Dict[str, Any] = {}
-        self.topics: Dict[str, Any] = {}
-
-        self.service = None
-        self.message_service = None
-        self.entity_types = {
-            "Table": {
-                self.service_connection.entity_type: {"db": "db", "column": "columns"}
-            },
-            "Topic": {"Topic": {"schema": "schema"}},
-        }
-        self.test_connection()
-
-    @classmethod
-    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-
-        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: AtlasConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, AtlasConnection):
-            raise InvalidSourceException(
-                f"Expected AtlasConnection, but got {connection}"
-            )
-        return cls(config, metadata_config)
+        entity_id: Union[str, basic.Uuid],
+        parent_fqn: Optional[str] = None,
+    ) -> Optional[GlossaryTerm]:
+        """
+        Given an GlossaryTerm ID, JSON PATCH the parent. If parent FQN is not provided, the parent is removed.
+        Args:
+            entity_id: the ID of the GlossaryTerm to be patched
+            parent_fqn: the fully qualified name of the new parent GlossaryTerm. If None the parent is removed.
+        Return:
+            the updated GlossaryTerm if all was valid, otherwise None
+        """
+        instance: GlossaryTerm = self._fetch_entity_if_exists(
+            entity=GlossaryTerm, entity_id=entity_id
+        )
+        if not instance:
+            return None
 
-    def prepare(self):
+        if parent_fqn is not None:
+            try:
+                res = self.client.patch(
+                    path=f"{self.get_suffix(GlossaryTerm)}/{model_str(entity_id)}",
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.ADD
+                                if instance.parent is None
+                                else PatchOperation.REPLACE,
+                                PatchField.PATH: PatchPath.PARENT,
+                                PatchField.VALUE: {
+                                    PatchValue.TYPE: self.get_suffix(GlossaryTerm),
+                                    PatchValue.FQN: model_str(parent_fqn),
+                                },
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH parent for GlossaryTerm [{model_str(entity_id)}]: {exc}"
+                )
+
+        else:
+            try:
+                res = self.client.patch(
+                    path=f"{self.get_suffix(GlossaryTerm)}/{model_str(entity_id)}",
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.REMOVE,
+                                PatchField.PATH: PatchPath.PARENT,
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH parent for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
+        return None
+
+    def patch_glossary_term_related_terms(
+        self,
+        entity_id: Union[str, basic.Uuid],
+        related_term_id: Optional[Union[str, basic.Uuid]] = None,
+    ) -> Optional[GlossaryTerm]:
         """
-        Not required to implement
+        Given an GlossaryTerm ID and a related term, JSON PATCH the related terms. If related term ID is not provided,
+        the last term is removed.
+        Args:
+            entity_id: the ID of the GlossaryTerm to be patched
+            related_term_id: the ID of the related term to be added. If the related term ID is None, the last term is
+                removed.
+        Return:
+            the updated GlossaryTerm
         """
+        instance: GlossaryTerm = self._fetch_entity_if_exists(
+            entity=GlossaryTerm, entity_id=entity_id
+        )
+        if not instance:
+            return None
 
-    def next_record(self):
-        for service in self.service_connection.databaseServiceName or []:
-            check_service = self.metadata.get_by_name(
-                entity=DatabaseService, fqn=service
-            )
-            if check_service:
-                for key in self.entity_types["Table"]:
-                    self.service = check_service
-                    self.tables[key] = self.atlas_client.list_entities()
-                    if self.tables.get(key, None):
-                        for key in self.tables:
-                            yield from self._parse_table_entity(key, self.tables[key])
-            else:
-                logger.warning(
-                    f"Cannot find service for {service} - type DatabaseService"
+        term_index: int = (
+            len(instance.relatedTerms.__root__) - 1 if instance.relatedTerms else 0
+        )
+
+        if related_term_id is None:
+            try:
+                res = self.client.patch(
+                    path=f"{self.get_suffix(GlossaryTerm)}/{model_str(entity_id)}",
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.REMOVE,
+                                PatchField.PATH: PatchPath.RELATED_TERMS.format(
+                                    index=term_index
+                                ),
+                            }
+                        ]
+                    ),
                 )
+                return GlossaryTerm(**res)
 
-        for service in self.service_connection.messagingServiceName or []:
-            check_service = self.metadata.get_by_name(
-                entity=MessagingService, fqn=service
-            )
-            if check_service:
-                for key in self.entity_types["Topic"]:
-                    self.message_service = check_service
-                    self.topics[key] = self.atlas_client.list_entities()
-                    if self.topics.get(key, None):
-                        for topic in self.topics:
-                            yield from self._parse_topic_entity(topic)
-            else:
-                logger.warning(
-                    f"Cannot find service for {service} - type MessagingService"
-                )
-
-    def close(self):
-        """
-        Not required to implement
-        """
-
-    def _parse_topic_entity(self, name):
-        for key in self.topics:
-            topic_entity = self.atlas_client.get_entity(self.topics[key])
-            tpc_entities = topic_entity["entities"]
-            for tpc_entity in tpc_entities:
-                try:
-                    tpc_attrs = tpc_entity["attributes"]
-                    topic_name = tpc_attrs["name"]
-
-                    topic_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Topic,
-                        service_name=self.message_service.id,
-                        topic_name=topic_name,
-                    )
-
-                    topic_object = self.metadata.get_by_name(
-                        entity=Topic, fqn=topic_fqn
-                    )
-
-                    if tpc_attrs.get("description") and topic_object:
-                        self.metadata.patch_description(
-                            entity=Topic,
-                            entity_id=topic_object.id,
-                            description=tpc_attrs["description"],
-                            force=True,
-                        )
-
-                    yield from self.ingest_lineage(tpc_entity["guid"], name)
-
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(
-                        f"Failed to parse topi entry [{topic_entity}]: {exc}"
-                    )
-
-    def _parse_table_entity(self, name, entity):  # pylint: disable=too-many-locals
-        for table in entity:
-            table_entity = self.atlas_client.get_entity(table)
-            tbl_entities = table_entity["entities"]
-            db_entity = None
-            for tbl_entity in tbl_entities:
-                try:
-
-                    tbl_attrs = tbl_entity["attributes"]
-                    db_entity = tbl_entity["relationshipAttributes"][
-                        self.entity_types["Table"][name]["db"]
-                    ]
-
-                    database_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Database,
-                        service_name=self.service.name.__root__,
-                        database_name=db_entity["displayText"],
-                    )
-                    database_object = self.metadata.get_by_name(
-                        entity=Database, fqn=database_fqn
-                    )
-                    if db_entity.get("description", None) and database_object:
-                        self.metadata.patch_description(
-                            entity=Database,
-                            entity_id=database_object.id,
-                            description=db_entity["description"],
-                            force=True,
-                        )
-
-                    database_schema_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=DatabaseSchema,
-                        service_name=self.service.name.__root__,
-                        database_name=db_entity["displayText"],
-                        schema_name=db_entity["displayText"],
-                    )
-                    database_schema_object = self.metadata.get_by_name(
-                        entity=DatabaseSchema, fqn=database_schema_fqn
-                    )
-
-                    if db_entity.get("description", None) and database_schema_object:
-                        self.metadata.patch_description(
-                            entity=DatabaseSchema,
-                            entity_id=database_schema_object.id,
-                            description=db_entity["description"],
-                            force=True,
-                        )
-
-                    yield self.create_tag()
-
-                    table_fqn = fqn.build(
-                        metadata=self.metadata,
-                        entity_type=Table,
-                        service_name=self.service.name.__root__,
-                        database_name=db_entity["displayText"],
-                        schema_name=db_entity["displayText"],
-                        table_name=tbl_attrs["name"],
-                    )
-
-                    table_object = self.metadata.get_by_name(
-                        entity=Table, fqn=table_fqn
-                    )
-
-                    if table_object:
-                        if tbl_attrs.get("description", None):
-                            self.metadata.patch_description(
-                                entity_id=table_object.id,
-                                entity=Table,
-                                description=tbl_attrs["description"],
-                                force=True,
-                            )
-
-                        tag_fqn = fqn.build(
-                            self.metadata,
-                            entity_type=Tag,
-                            classification_name=ATLAS_TAG_CATEGORY,
-                            tag_name=ATLAS_TABLE_TAG,
-                        )
-
-                        self.metadata.patch_tag(
-                            entity=Table, entity_id=table_object.id, tag_fqn=tag_fqn
-                        )
-
-                    yield from self.ingest_lineage(tbl_entity["guid"], name)
-
-                except Exception as exc:
-                    logger.debug(traceback.format_exc())
-                    logger.warning(
-                        f"Failed to parse for database : {db_entity} - table {table}: {exc}"
-                    )
-
-    def get_tags(self):
-        tags = [
-            TagLabel(
-                tagFQN=fqn.build(
-                    self.metadata,
-                    Tag,
-                    tag_category_name=ATLAS_TAG_CATEGORY,
-                    tag_name=ATLAS_TABLE_TAG,
-                ),
-                labelType="Automated",
-                state="Suggested",
-                source="Classification",
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH parent for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
+        else:
+            related_term: GlossaryTerm = self.get_by_id(
+                entity=GlossaryTerm, entity_id=related_term_id, fields=["*"]
             )
-        ]
-        return tags
+            if related_term is None:
+                logger.error(
+                    f"Unable to PATCH GlossaryTerm [{entity_id}], unknown related term id [{related_term_id}]."
+                )
+                return None
 
-    def create_tag(self) -> OMetaTagAndClassification:
-        atlas_table_tag = OMetaTagAndClassification(
-            classification_request=CreateClassificationRequest(
-                name=ATLAS_TAG_CATEGORY,
-                description="Tags associates with atlas entities",
-            ),
-            tag_request=CreateTagRequest(
-                classification=ATLAS_TAG_CATEGORY,
-                name=ATLAS_TABLE_TAG,
-                description="Atlas Cluster Tag",
-            ),
+            try:
+                res = self.client.patch(
+                    path=f"{self.get_suffix(GlossaryTerm)}/{model_str(entity_id)}",
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.ADD,
+                                PatchField.PATH: PatchPath.RELATED_TERMS.format(
+                                    index=term_index + 1
+                                ),
+                                PatchField.VALUE: {
+                                    PatchValue.DISPLAY_NAME: related_term.displayName,
+                                    PatchValue.ID: model_str(related_term_id),
+                                    PatchValue.NAME: model_str(related_term.name),
+                                    PatchValue.TYPE: PatchValue.GLOSSARY_TERM,
+                                },
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH parent for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
+        return None
+
+    def patch_reviewers(
+        self,
+        entity: Union[Type[Glossary], Type[GlossaryTerm]],
+        entity_id: Union[str, basic.Uuid],
+        reviewer_id: Optional[Union[str, basic.Uuid]] = None,
+    ) -> Optional[T]:
+        """
+        Update the reviewers of a Glossary or GlossaryTerm. If the reviewer_id is included, the reviewer is added; if
+        not, the last reviewer is removed.
+        Args
+            entity (T): Entity Type - Glossary or GlossaryTerm
+            entity_id: ID of the entity to be updated
+            reviewer_id: ID of the user to added as a reviewer or None to remove the last reviewer
+        Return
+            The updated entity
+        """
+        instance: Union[Glossary, GlossaryTerm] = self._fetch_entity_if_exists(
+            entity=entity, entity_id=entity_id
         )
-        return atlas_table_tag
+        if not instance:
+            return None
+
+        index: int = (
+            len(instance.reviewers)
+            if entity == Glossary
+            else len(instance.reviewers.__root__)
+        )
+
+        if reviewer_id is not None:
+            try:
+                res = self.client.patch(
+                    path=f"{self.get_suffix(entity)}/{model_str(entity_id)}",
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.ADD,
+                                PatchField.PATH: PatchPath.REVIEWERS.format(
+                                    index=index
+                                ),
+                                PatchField.VALUE: {
+                                    PatchValue.ID: model_str(reviewer_id),
+                                    PatchValue.TYPE: PatchValue.USER,
+                                },
+                            }
+                        ]
+                    ),
+                )
+                return entity(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH reviewers for {entity.__class__.__name__} [{entity_id}]: {exc}"
+                )
+
+        else:
+            if index == 0:
+                logger.debug(
+                    f"Unable to remove reviewer from {entity.__class__.__name__} [{entity_id}]. No current reviewers."
+                )
+                return None
 
-    def _parse_table_columns(self, table_response, tbl_entity, name) -> List[Column]:
-        om_cols = []
-        col_entities = tbl_entity["relationshipAttributes"][
-            self.entity_types["Table"][name]["column"]
-        ]
-        referred_entities = table_response["referredEntities"]
-        ordinal_pos = 1
-        for col in col_entities:
             try:
-                col_guid = col["guid"]
-                col_ref_entity = referred_entities[col_guid]
-                column = col_ref_entity["attributes"]
-                col_data_length = "1"
-                om_column = Column(
-                    name=column["name"],
-                    description=column.get("comment", None),
-                    dataType=ColumnTypeParser.get_column_type(
-                        column["dataType"].upper()
-                    ),
-                    dataTypeDisplay=column["dataType"],
-                    dataLength=col_data_length,
-                    ordinalPosition=ordinal_pos,
+                res = self.client.patch(
+                    path=f"{self.get_suffix(entity)}/{model_str(entity_id)}",
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.REMOVE,
+                                PatchField.PATH: PatchPath.REVIEWERS.format(
+                                    index=index - 1
+                                ),
+                            }
+                        ]
+                    ),
                 )
-                om_cols.append(om_column)
+                return entity(**res)
+
             except Exception as exc:
                 logger.debug(traceback.format_exc())
-                logger.warning(f"Error parsing column [{col}]: {exc}")
-                continue
-        return om_cols
-
-    def get_database_entity(self, database_name: str) -> Database:
-        return CreateDatabaseRequest(
-            name=database_name,
-            service=self.service.fullyQualifiedName,
-        )
+                logger.error(
+                    f"Error trying to PATCH reviewers for {entity.__class__.__name__} [{entity_id}]: {exc}"
+                )
+
+        return None
 
-    def ingest_lineage(self, source_guid, name) -> Iterable[AddLineageRequest]:
+    def patch_glossary_term_synonyms(
+        self,
+        entity_id: Union[str, basic.Uuid],
+        synonym: Optional[str] = None,
+    ) -> Optional[GlossaryTerm]:
         """
-        Fetch and ingest lineage
+        Update the synonyms of a Glossary Term via PATCH
+        Params
+            entity_id: entity ID of the Glossary Term
+            synonym: if provided, the synonym is added. If None, the last synonym is removed.
+        Return
+            the updated entity
         """
-        lineage_response = self.atlas_client.get_lineage(source_guid)
-        lineage_relations = lineage_response["relations"]
-        tbl_entity = self.atlas_client.get_entity(lineage_response["baseEntityGuid"])
-        for key in tbl_entity["referredEntities"].keys():
-            if not tbl_entity["entities"][0]["relationshipAttributes"].get(
-                self.entity_types["Table"][name]["db"]
-            ):
-                continue
-            db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
-                self.entity_types["Table"][name]["db"]
-            ]
-            if not tbl_entity["referredEntities"].get(key):
-                continue
-            table_name = tbl_entity["referredEntities"][key]["relationshipAttributes"][
-                "table"
-            ]["displayText"]
-            from_fqn = fqn.build(
-                self.metadata,
-                entity_type=Table,
-                service_name=self.config.serviceName,
-                database_name=db_entity["displayText"],
-                schema_name=db_entity["displayText"],
-                table_name=table_name,
-            )
-            from_entity_ref = self.get_lineage_entity_ref(
-                from_fqn, self.metadata_config, "table"
-            )
-            for edge in lineage_relations:
-                if (
-                    lineage_response["guidEntityMap"][edge["toEntityId"]]["typeName"]
-                    == "processor"
-                ):
-                    continue
-
-                tbl_entity = self.atlas_client.get_entity(edge["toEntityId"])
-                for key in tbl_entity["referredEntities"]:
-                    db_entity = tbl_entity["entities"][0]["relationshipAttributes"][
-                        self.entity_types["Table"][name]["db"]
-                    ]
-
-                    db = self.get_database_entity(db_entity["displayText"])
-                    table_name = tbl_entity["referredEntities"][key][
-                        "relationshipAttributes"
-                    ]["table"]["displayText"]
-                    to_fqn = fqn.build(
-                        self.metadata,
-                        entity_type=Table,
-                        service_name=self.config.serviceName,
-                        database_name=db.name.__root__,
-                        schema_name=db_entity["displayText"],
-                        table_name=table_name,
-                    )
-                    to_entity_ref = self.get_lineage_entity_ref(
-                        to_fqn, self.metadata_config, "table"
-                    )
-                    yield from self.yield_lineage(from_entity_ref, to_entity_ref)
-
-    def get_database_service(self):
-        service = self.metadata.create_or_update(
-            CreateDatabaseServiceRequest(
-                name=SERVICE_TYPE_MAPPER.get("hive")["service_name"],
-                displayName=f"{self.config.serviceName}_database",
-                serviceType=SERVICE_TYPE_MAPPER.get("hive")["service_name"],
-                connection=SERVICE_TYPE_MAPPER["hive"]["connection"],
-            )
+        instance: GlossaryTerm = self._fetch_entity_if_exists(
+            entity=GlossaryTerm, entity_id=entity_id
         )
-        if service is not None:
-            return service
-        logger.error("Failed to create a service with name detlaLake")
+        if not instance:
+            return None
+
+        index: int = len(instance.synonyms)
+
+        if synonym is not None:
+            try:
+                res = self.client.patch(
+                    path=PatchPath.GLOSSARY_TERMS.format(
+                        entity_id=model_str(entity_id)
+                    ),
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.ADD,
+                                PatchField.PATH: PatchPath.SYNONYMS.format(index=index),
+                                PatchField.VALUE: synonym,
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH synonyms for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
+        else:
+            if index == 0:
+                logger.debug(
+                    f"Unable to remove synonyms from GlossaryTerm [{entity_id}]. No current synonyms."
+                )
+                return None
+
+            try:
+                res = self.client.patch(
+                    path=PatchPath.GLOSSARY_TERMS.format(
+                        entity_id=model_str(entity_id)
+                    ),
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.REMOVE,
+                                PatchField.PATH: PatchPath.SYNONYMS.format(
+                                    index=index - 1
+                                ),
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH synonyms for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
         return None
 
-    def get_message_service(self):
-        service = self.metadata.create_or_update(
-            CreateMessagingServiceRequest(
-                name=SERVICE_TYPE_MAPPER.get("kafka")["service_name"],
-                displayName=f"{self.config.serviceName}_messaging",
-                serviceType=SERVICE_TYPE_MAPPER.get("kafka")["service_name"],
-                connection=SERVICE_TYPE_MAPPER.get("kafka")["connection"],
-            )
+    def patch_glossary_term_references(
+        self,
+        entity_id: Union[str, basic.Uuid],
+        reference_name: Optional[str] = None,
+        reference_endpoint: Optional[str] = None,
+    ) -> Optional[GlossaryTerm]:
+        """
+        Update the references of a GlossaryTerm. If reference_name and reference_endpoint are None, the last reference
+        is removed.
+        Params
+            entity_id: ID of the GlossaryTerm
+            reference_name: the name of the reference
+            reference_endpoint: the valid URI endpoint of the reference
+        Return
+            updated GlossaryTerm
+        """
+        instance: GlossaryTerm = self._fetch_entity_if_exists(
+            entity=GlossaryTerm, entity_id=entity_id
         )
-        if service is not None:
-            return service
-        logger.error("Failed to create a service with name kafka")
-        return None
+        if not instance:
+            return None
 
-    def yield_lineage(self, from_entity_ref, to_entity_ref):
-        if from_entity_ref and to_entity_ref and from_entity_ref != to_entity_ref:
-            lineage = AddLineageRequest(
-                edge=EntitiesEdge(fromEntity=from_entity_ref, toEntity=to_entity_ref)
+        index: int = len(instance.references)
+
+        if bool(reference_name) ^ bool(reference_endpoint):
+            logger.debug(
+                f"Unable to PATCH references from GlossaryTerm [{entity_id}]: reference_name and references."
             )
-            yield lineage
+            return None
 
-    def get_lineage_entity_ref(
-        self, to_fqn, metadata_config, entity_type
-    ) -> EntityReference:
-        metadata = OpenMetadata(metadata_config)
-        if entity_type == "table":
-            table = metadata.get_by_name(entity=Table, fqn=to_fqn)
-            if table:
-                return EntityReference(id=table.id.__root__, type="table")
-        if entity_type == "pipeline":
-            pipeline = metadata.get_by_name(entity=Pipeline, fqn=to_fqn)
-            if pipeline:
-                return EntityReference(id=pipeline.id.__root__, type="pipeline")
-        return None
+        if reference_name is not None:
+            try:
+                res = self.client.patch(
+                    path=PatchPath.GLOSSARY_TERMS.format(
+                        entity_id=model_str(entity_id)
+                    ),
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.ADD,
+                                PatchField.PATH: PatchPath.REFERENCES.format(
+                                    index=index
+                                ),
+                                PatchField.VALUE: {
+                                    PatchValue.ENDPOINT: reference_endpoint,
+                                    PatchValue.NAME: reference_name,
+                                },
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
 
-    def test_connection(self) -> None:
-        test_connection_fn = get_test_connection_fn(self.service_connection)
-        test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH references for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
+        else:
+            if index == 0:
+                logger.debug(
+                    f"Unable to remove reference from GlossaryTerm [{entity_id}]. No current references."
+                )
+                return None
+
+            try:
+                res = self.client.patch(
+                    path=PatchPath.GLOSSARY_TERMS.format(
+                        entity_id=model_str(entity_id)
+                    ),
+                    data=json.dumps(
+                        [
+                            {
+                                PatchField.OPERATION: PatchOperation.REMOVE,
+                                PatchField.PATH: PatchPath.REFERENCES.format(
+                                    index=index - 1
+                                ),
+                            }
+                        ]
+                    ),
+                )
+                return GlossaryTerm(**res)
+
+            except Exception as exc:
+                logger.debug(traceback.format_exc())
+                logger.error(
+                    f"Error trying to PATCH references for GlossaryTerm [{entity_id}]: {exc}"
+                )
+
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/metadata.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,27 +11,35 @@
 """Metadata source module"""
 import traceback
 from typing import Iterable
 
 from metadata.generated.schema.entity.classification.classification import (
     Classification,
 )
+from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.data.dashboard import Dashboard
 from metadata.generated.schema.entity.data.glossary import Glossary
 from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
 from metadata.generated.schema.entity.data.mlmodel import MlModel
 from metadata.generated.schema.entity.data.pipeline import Pipeline
+from metadata.generated.schema.entity.data.query import Query
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.data.topic import Topic
 from metadata.generated.schema.entity.policies.policy import Policy
+from metadata.generated.schema.entity.services.connections.metadata.metadataESConnection import (
+    MetadataESConnection,
+)
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.services.databaseService import DatabaseService
 from metadata.generated.schema.entity.services.messagingService import MessagingService
+from metadata.generated.schema.entity.services.objectstoreService import (
+    ObjectStoreService,
+)
 from metadata.generated.schema.entity.services.pipelineService import PipelineService
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.common import Entity
@@ -54,15 +62,17 @@
         config: WorkflowSource,
         metadata_config: OpenMetadataConnection,
     ):
         super().__init__()
         self.config = config
         self.metadata_config = metadata_config
         self.metadata = OpenMetadata(metadata_config)
-        self.service_connection = config.serviceConnection.__root__.config
+        self.service_connection: MetadataESConnection = (
+            config.serviceConnection.__root__.config
+        )
         self.wrote_something = False
         self.tables = None
         self.topics = None
 
     def prepare(self):
         pass
 
@@ -157,14 +167,32 @@
 
         if self.service_connection.includePipelineServices:
             yield from self.fetch_entities(
                 entity_class=PipelineService,
                 fields=["owner"],
             )
 
+        if self.service_connection.includeContainers:
+            yield from self.fetch_entities(
+                entity_class=Container,
+                fields=["owner"],
+            )
+
+        if self.service_connection.includeStorageServices:
+            yield from self.fetch_entities(
+                entity_class=ObjectStoreService,
+                fields=["owner"],
+            )
+
+        if self.service_connection.includeQueries:
+            yield from self.fetch_entities(
+                entity_class=Query,
+                fields=["owner"],
+            )
+
     def fetch_entities(self, entity_class, fields):
         """
         Args:
             entity_class: class of the entities to be fetched
             fields: fields that must be additionally fetched
 
         Returns:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/metadata/metadata_elasticsearch/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/metadata/openmetadata/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/test_suite_protocol.py`

 * *Files 25% similar despite different names*

```diff
@@ -4,35 +4,41 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""Metadata source module"""
 
+"""
+Interfaces with database for all database engine
+supporting sqlalchemy abstraction layer
+"""
 
-from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
-    OpenMetadataConnection,
-)
-from metadata.generated.schema.metadataIngestion.workflow import (
-    Source as WorkflowSource,
+from abc import ABC, abstractmethod
+from typing import Optional, Union
+
+from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
+    DatalakeConnection,
 )
-from metadata.ingestion.api.source import InvalidSourceException
-from metadata.ingestion.source.metadata.metadata import MetadataSource
-from metadata.utils.logger import ingestion_logger
-
-logger = ingestion_logger()
-
-
-class OpenmetadataSource(MetadataSource):
-    """Metadata source Class"""
-
-    @classmethod
-    def create(cls, config_dict, metadata_config: OpenMetadataConnection):
-        config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: OpenMetadataConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, OpenMetadataConnection):
-            raise InvalidSourceException(
-                f"Expected OpenMetadataConnection, but got {connection}"
-            )
-        return cls(config, metadata_config)
+from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
+from metadata.generated.schema.tests.basic import TestCaseResult
+from metadata.generated.schema.tests.testCase import TestCase
+from metadata.ingestion.ometa.ometa_api import OpenMetadata
+
+
+class TestSuiteProtocol(ABC):
+    """Protocol interface for the processor"""
+
+    @abstractmethod
+    def __init__(
+        self,
+        ometa_client: OpenMetadata = None,
+        service_connection_config: Union[DatabaseConnection, DatalakeConnection] = None,
+    ):
+        """Required attribute for the interface"""
+        raise NotImplementedError
+
+    @abstractmethod
+    def run_test_case(self, test_case: TestCase) -> Optional[TestCaseResult]:
+        """run column data quality tests"""
+        raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlflow/connection.py`

 * *Files 0% similar despite different names*

```diff
@@ -38,15 +38,14 @@
 
 def test_connection(
     metadata: OpenMetadata,
     client: MlflowClient,
     service_connection: MlflowConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
-
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     test_fn = {"GetModels": client.list_registered_models}
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlflow/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py`

 * *Files 24% similar despite different names*

```diff
@@ -4,212 +4,193 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-"""ml flow source module"""
+"""SageMaker source module"""
 
-import ast
-import json
 import traceback
-from typing import Iterable, List, Optional, Tuple, cast
+from typing import Iterable, List, Optional
 
-from mlflow.entities import RunData
-from mlflow.entities.model_registry import ModelVersion, RegisteredModel
-from pydantic import ValidationError
+from pydantic import BaseModel, Extra, Field, ValidationError
 
 from metadata.generated.schema.api.data.createMlModel import CreateMlModelRequest
 from metadata.generated.schema.entity.data.mlmodel import (
-    FeatureType,
     MlFeature,
     MlHyperParameter,
     MlStore,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.connections.mlmodel.mlflowConnection import (
-    MlflowConnection,
+from metadata.generated.schema.entity.services.connections.mlmodel.sageMakerConnection import (
+    SageMakerConnection,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.tagLabel import TagLabel
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.mlmodel.mlmodel_service import MlModelServiceSource
 from metadata.utils.filters import filter_by_mlmodel
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class MlflowSource(MlModelServiceSource):
+class SageMakerModel(BaseModel):
+    class Config:
+        extra = Extra.forbid
+
+    name: str = Field(..., description="Model name", title="Model Name")
+    arn: str = Field(..., description="Model ARN in AWS account", title="Model ARN")
+    creation_timestamp: str = Field(
+        ...,
+        description="Timestamp of model creation in ISO format",
+        title="Creation Timestamp",
+    )
+
+
+class SagemakerSource(MlModelServiceSource):
     """
-    Source implementation to ingest MLFlow data.
+    Source implementation to ingest SageMaker data.
 
-    We will iterate on the registered ML Models
+    We will iterate on the ML Models
     and prepare an iterator of CreateMlModelRequest
     """
 
+    def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
+        super().__init__(config, metadata_config)
+        self.sagemaker = self.connection.client
+
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: MlflowConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, MlflowConnection):
+        connection: SageMakerConnection = config.serviceConnection.__root__.config
+        if not isinstance(connection, SageMakerConnection):
             raise InvalidSourceException(
-                f"Expected MlFlowConnection, but got {connection}"
+                f"Expected SageMakerConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
     def get_mlmodels(  # pylint: disable=arguments-differ
         self,
-    ) -> Iterable[Tuple[RegisteredModel, ModelVersion]]:
+    ) -> Iterable[SageMakerModel]:
         """
-        List and filters models from the registry
+        List and filters models
         """
-        for model in cast(RegisteredModel, self.client.list_registered_models()):
-            if filter_by_mlmodel(
-                self.source_config.mlModelFilterPattern, mlmodel_name=model.name
-            ):
-                self.status.filter(
-                    model.name,
-                    "MlModel name pattern not allowed",
-                )
-                continue
-
-            # Get the latest version
-            latest_version: Optional[ModelVersion] = next(
-                (
-                    ver
-                    for ver in model.latest_versions
-                    if ver.last_updated_timestamp == model.last_updated_timestamp
-                ),
-                None,
-            )
-            if not latest_version:
-                self.status.failed(model.name, "Invalid version")
-                continue
+        args, has_more_models, models = {"MaxResults": 100}, True, []
+        try:
+            while has_more_models:
+                response = self.sagemaker.list_models(**args)
+                models.append(response["Models"])
+                has_more_models = response.get("NextToken")
+                args["NextToken"] = response.get("NextToken")
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Failed to fetch models list - {err}")
 
-            yield model, latest_version
+        for model in models:
+            try:
+                if filter_by_mlmodel(
+                    self.source_config.mlModelFilterPattern,
+                    mlmodel_name=model["ModelName"],
+                ):
+                    self.status.filter(
+                        model["ModelName"],
+                        "MlModel name pattern not allowed",
+                    )
+                    continue
+                yield SageMakerModel(
+                    name=model["ModelName"],
+                    arn=model["ModelArn"],
+                    creation_timestamp=model["CreationTime"].isoformat(),
+                )
+            except ValidationError as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Validation error while creating SageMakerModel from model details - {err}"
+                )
+            except Exception as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Wild error while creating SageMakerModel from model details - {err}"
+                )
+            continue
 
     def _get_algorithm(self) -> str:  # pylint: disable=arguments-differ
-        logger.info("Setting algorithm with default value `mlmodel` for Mlflow")
+        logger.info(
+            "Setting algorithm to default value of `mlmodel` for SageMaker Model"
+        )
         return "mlmodel"
 
     def yield_mlmodel(  # pylint: disable=arguments-differ
-        self, model_and_version: Tuple[RegisteredModel, ModelVersion]
+        self, model: SageMakerModel
     ) -> Iterable[CreateMlModelRequest]:
         """
         Prepare the Request model
         """
-        model, latest_version = model_and_version
         self.status.scanned(model.name)
 
-        run = self.client.get_run(latest_version.run_id)
-
         mlmodel_request = CreateMlModelRequest(
             name=model.name,
-            description=model.description,
             algorithm=self._get_algorithm(),  # Setting this to a constant
-            mlHyperParameters=self._get_hyper_params(run.data),
-            mlFeatures=self._get_ml_features(
-                run.data, latest_version.run_id, model.name
-            ),
-            mlStore=self._get_ml_store(latest_version),
+            mlStore=self._get_ml_store(model.name),
             service=self.context.mlmodel_service.fullyQualifiedName,
         )
         yield mlmodel_request
         self.register_record(mlmodel_request=mlmodel_request)
 
-    def _get_hyper_params(  # pylint: disable=arguments-differ
+    def _get_ml_store(  # pylint: disable=arguments-differ
         self,
-        data: RunData,
-    ) -> Optional[List[MlHyperParameter]]:
+        model_name: str,
+    ) -> Optional[MlStore]:
         """
-        Get the hyper parameters from the parameters
-        logged in the run data object.
+        Get the Ml Store for the model
         """
         try:
-            if data.params:
-                return [
-                    MlHyperParameter(name=param[0], value=param[1])
-                    for param in data.params.items()
-                ]
+            model_info = self.sagemaker.describe_model(ModelName=model_name)
+            return MlStore(imageRepository=model_info["PrimaryContainer"]["Image"])
         except ValidationError as err:
             logger.debug(traceback.format_exc())
             logger.warning(
-                f"Validation error adding hyper parameters from RunData: {data} - {err}"
+                f"Validation error adding the MlModel store from model description: {model_name} - {err}"
             )
         except Exception as err:
             logger.debug(traceback.format_exc())
             logger.warning(
-                f"Wild error adding hyper parameters from RunData: {data} - {err}"
+                f"Wild error adding the MlModel store from model description: {model_name} - {err}"
             )
-
         return None
 
-    def _get_ml_store(  # pylint: disable=arguments-differ
-        self,
-        version: ModelVersion,
-    ) -> Optional[MlStore]:
-        """
-        Get the Ml Store from the model version object
-        """
+    def _get_tags(self, model_arn: str) -> Optional[List[TagLabel]]:
         try:
-            if version.source:
-                return MlStore(storage=version.source)
+            tags = self.sagemaker.list_tags(ResourceArn=model_arn)["Tags"]
+            return [
+                TagLabel(
+                    tagFQN=tag["Key"],
+                    description=tag["Value"],
+                    source="Classification",
+                    labelType="Propagated",
+                    state="Confirmed",
+                )
+                for tag in tags
+            ]
         except ValidationError as err:
             logger.debug(traceback.format_exc())
             logger.warning(
-                f"Validation error adding the MlModel store from ModelVersion: {version} - {err}"
+                f"Validation error adding TagLabel from model tags: {model_arn} - {err}"
             )
         except Exception as err:
             logger.debug(traceback.format_exc())
             logger.warning(
-                f"Wild error adding the MlModel store from ModelVersion: {version} - {err}"
+                f"Wild error adding TagLabel from model tags: {model_arn} - {err}"
             )
         return None
 
-    def _get_ml_features(  # pylint: disable=arguments-differ
-        self, data: RunData, run_id: str, model_name: str
-    ) -> Optional[List[MlFeature]]:
-        """
-        The RunData object comes with stringified `tags`.
-        Let's transform those and try to extract the `signature`
-        information
-        """
-        if data.tags:
-            try:
-                props = json.loads(data.tags["mlflow.log-model.history"])
-                latest_props = next(
-                    (prop for prop in props if prop["run_id"] == run_id), None
-                )
-                if not latest_props:
-                    reason = f"Cannot find the run ID properties for {run_id}"
-                    logger.warning(reason)
-                    self.status.warning(model_name, reason)
-                    return None
-
-                if latest_props.get("signature") and latest_props["signature"].get(
-                    "inputs"
-                ):
-
-                    features = ast.literal_eval(latest_props["signature"]["inputs"])
+    def _get_hyper_params(self, *args, **kwargs) -> Optional[List[MlHyperParameter]]:
+        pass
 
-                    return [
-                        MlFeature(
-                            name=feature["name"],
-                            dataType=FeatureType.categorical
-                            if feature["type"] == "string"
-                            else FeatureType.numerical,
-                        )
-                        for feature in features
-                    ]
-
-            except Exception as exc:  # pylint: disable=broad-except
-                logger.debug(traceback.format_exc())
-                reason = f"Cannot extract properties from RunData: {exc}"
-                logger.warning(reason)
-                self.status.warning(model_name, reason)
-
-        return None
+    def _get_ml_features(self, *args, **kwargs) -> Optional[List[MlFeature]]:
+        pass
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/mlmodel_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/mlmodel_service.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/mlmodel/sagemaker/connection.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,15 +34,14 @@
 
 def test_connection(
     metadata: OpenMetadata,
     client,
     service_connection: SageMakerConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
-
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     test_fn = {"GetModels": client.list_models}
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/objectstore_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/storage_service.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,96 +15,96 @@
 from typing import Any, Iterable
 
 from metadata.generated.schema.api.data.createContainer import CreateContainerRequest
 from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.objectstoreService import (
-    ObjectStoreConnection,
-    ObjectStoreService,
+from metadata.generated.schema.entity.services.storageService import (
+    StorageConnection,
+    StorageService,
 )
-from metadata.generated.schema.metadataIngestion.objectstoreServiceMetadataPipeline import (
-    ObjectStoreServiceMetadataPipeline,
+from metadata.generated.schema.metadataIngestion.storageServiceMetadataPipeline import (
+    StorageServiceMetadataPipeline,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import Source, SourceStatus
 from metadata.ingestion.api.topology_runner import TopologyRunnerMixin
 from metadata.ingestion.models.topology import (
     NodeStage,
     ServiceTopology,
     TopologyNode,
     create_source_context,
 )
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
-from metadata.ingestion.source.objectstore.s3.connection import S3ObjectStoreClient
+from metadata.ingestion.source.storage.s3.connection import S3ObjectStoreClient
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
-class ObjectStoreServiceTopology(ServiceTopology):
+class StorageServiceTopology(ServiceTopology):
 
     root = TopologyNode(
         producer="get_services",
         stages=[
             NodeStage(
-                type_=ObjectStoreService,
+                type_=StorageService,
                 context="objectstore_service",
                 processor="yield_create_request_objectstore_service",
                 overwrite=False,
                 must_return=True,
             ),
         ],
         children=["container"],
     )
 
     container = TopologyNode(
         producer="get_containers",
         stages=[
             NodeStage(
                 type_=Container,
-                context="containers",
+                context="container",
                 processor="yield_create_container_requests",
                 consumer=["objectstore_service"],
                 nullable=True,
             )
         ],
     )
 
 
-class ObjectStoreServiceSource(TopologyRunnerMixin, Source, ABC):
+class StorageServiceSource(TopologyRunnerMixin, Source, ABC):
     """
     Base class for Object Store Services.
     It implements the topology and context.
     """
 
-    source_config: ObjectStoreServiceMetadataPipeline
+    source_config: StorageServiceMetadataPipeline
     config: WorkflowSource
     metadata: OpenMetadata
     # Big union of types we want to fetch dynamically
-    service_connection: ObjectStoreConnection.__fields__["config"].type_
+    service_connection: StorageConnection.__fields__["config"].type_
 
-    topology = ObjectStoreServiceTopology()
+    topology = StorageServiceTopology()
     context = create_source_context(topology)
 
     def __init__(
         self,
         config: WorkflowSource,
         metadata_config: OpenMetadataConnection,
     ):
         super().__init__()
         self.config = config
         self.metadata_config = metadata_config
         self.metadata = OpenMetadata(metadata_config)
         self.service_connection = self.config.serviceConnection.__root__.config
-        self.source_config: ObjectStoreServiceMetadataPipeline = (
+        self.source_config: StorageServiceMetadataPipeline = (
             self.config.sourceConfig.config
         )
         self.connection: S3ObjectStoreClient = get_connection(self.service_connection)
 
         # Flag the connection for the test connection
         self.connection_obj = self.connection
         self.test_connection()
@@ -135,9 +135,9 @@
 
     def test_connection(self) -> None:
         test_connection_fn = get_test_connection_fn(self.service_connection)
         test_connection_fn(self.metadata, self.connection_obj, self.service_connection)
 
     def yield_create_request_objectstore_service(self, config: WorkflowSource):
         yield self.metadata.get_create_service_from_source(
-            entity=ObjectStoreService, config=config
+            entity=StorageService, config=config
         )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/s3/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,45 +21,44 @@
 
 from botocore.client import BaseClient
 
 from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.objectstore.s3ObjectStoreConnection import (
-    S3StoreConnection,
+from metadata.generated.schema.entity.services.connections.storage.s3Connection import (
+    S3Connection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 
 
 @dataclass
 class S3ObjectStoreClient:
     s3_client: BaseClient
     cloudwatch_client: BaseClient
 
 
-def get_connection(connection: S3StoreConnection) -> S3ObjectStoreClient:
+def get_connection(connection: S3Connection) -> S3ObjectStoreClient:
     """
     Returns 2 clients - the s3 client and the cloudwatch client needed for total nr of objects and total size
     """
     aws_client = AWSClient(connection.awsConfig)
     return S3ObjectStoreClient(
         s3_client=aws_client.get_client(service_name="s3"),
         cloudwatch_client=aws_client.get_client(service_name="cloudwatch"),
     )
 
 
 def test_connection(
     metadata: OpenMetadata,
     client: S3ObjectStoreClient,
-    service_connection: S3StoreConnection,
+    service_connection: S3Connection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
-
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     test_fn = {
         "ListBuckets": client.s3_client.list_buckets,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/objectstore/s3/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/storage/s3/metadata.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,43 +10,48 @@
 #  limitations under the License.
 """S3 object store extraction metadata"""
 import json
 import secrets
 import traceback
 from datetime import datetime, timedelta
 from enum import Enum
-from typing import Iterable, List, Optional
+from typing import Dict, Iterable, List, Optional
 
 from pandas import DataFrame
-from pydantic import Extra, Field, ValidationError
-from pydantic.main import BaseModel
+from pydantic import ValidationError
 
 from metadata.generated.schema.api.data.createContainer import CreateContainerRequest
 from metadata.generated.schema.entity.data import container
-from metadata.generated.schema.entity.data.container import ContainerDataModel
+from metadata.generated.schema.entity.data.container import (
+    Container,
+    ContainerDataModel,
+)
 from metadata.generated.schema.entity.data.table import Column
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
-from metadata.generated.schema.entity.services.connections.objectstore.s3ObjectStoreConnection import (
-    S3StoreConnection,
+from metadata.generated.schema.entity.services.connections.storage.s3Connection import (
+    S3Connection,
 )
-from metadata.generated.schema.metadataIngestion.objectstore.containerMetadataConfig import (
+from metadata.generated.schema.metadataIngestion.storage.containerMetadataConfig import (
     MetadataEntry,
-    ObjectStoreContainerConfig,
+    StorageContainerConfig,
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
+from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.source.database.datalake.metadata import DatalakeSource
 from metadata.ingestion.source.database.datalake.models import DatalakeColumnWrapper
-from metadata.ingestion.source.objectstore.objectstore_service import (
-    ObjectStoreServiceSource,
+from metadata.ingestion.source.storage.s3.models import (
+    S3BucketResponse,
+    S3ContainerDetails,
 )
+from metadata.ingestion.source.storage.storage_service import StorageServiceSource
 from metadata.utils.filters import filter_by_container
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 S3_CLIENT_ROOT_RESPONSE = "Contents"
 OPENMETADATA_TEMPLATE_FILE_NAME = "openmetadata.json"
@@ -54,109 +59,70 @@
 
 
 class S3Metric(Enum):
     NUMBER_OF_OBJECTS = "NumberOfObjects"
     BUCKET_SIZE_BYTES = "BucketSizeBytes"
 
 
-class S3BucketResponse(BaseModel):
-    """
-    Class modelling a response received from s3_client.list_buckets operation
-    """
-
-    class Config:
-        extra = Extra.forbid
-
-    name: str = Field(..., description="Bucket name", title="Bucket Name", alias="Name")
-    creation_date: datetime = Field(
-        ...,
-        description="Timestamp of Bucket creation in ISO format",
-        title="Creation Timestamp",
-        alias="CreationDate",
-    )
-
-
-class S3ContainerDetails(BaseModel):
-    """Class mapping container details used to create the container requests"""
-
-    class Config:
-        extra = Extra.forbid
-
-    name: str = Field(..., description="Bucket name", title="Bucket Name")
-    prefix: str = Field(..., description="Prefix for the container", title="Prefix")
-    number_of_objects: float = Field(
-        ..., description="Total nr. of objects", title="Nr. of objects"
-    )
-    size: float = Field(
-        ...,
-        description="Total size in bytes of all objects",
-        title="Total size(bytes) of objects",
-    )
-    file_formats: Optional[List[container.FileFormat]] = Field(
-        ..., description="File formats", title="File formats"
-    )
-    data_model: Optional[ContainerDataModel] = Field(
-        ..., description="Data Model of the container", title="Data Model"
-    )
-    creation_date: str = Field(
-        ...,
-        description="Timestamp of Bucket creation in ISO format",
-        title="Creation Timestamp",
-    )
-
-
-class S3Source(ObjectStoreServiceSource):
+class S3Source(StorageServiceSource):
     """
     Source implementation to ingest S3 buckets data.
     """
 
     def __init__(self, config: WorkflowSource, metadata_config: OpenMetadataConnection):
         super().__init__(config, metadata_config)
         self.s3_client = self.connection.s3_client
         self.cloudwatch_client = self.connection.cloudwatch_client
 
+        self._bucket_cache: Dict[str, Container] = {}
+
     @classmethod
     def create(cls, config_dict, metadata_config: OpenMetadataConnection):
         config: WorkflowSource = WorkflowSource.parse_obj(config_dict)
-        connection: S3StoreConnection = config.serviceConnection.__root__.config
-        if not isinstance(connection, S3StoreConnection):
+        connection: S3Connection = config.serviceConnection.__root__.config
+        if not isinstance(connection, S3Connection):
             raise InvalidSourceException(
                 f"Expected S3StoreConnection, but got {connection}"
             )
         return cls(config, metadata_config)
 
     def get_containers(self) -> Iterable[S3ContainerDetails]:
         bucket_results = self.fetch_buckets()
 
         for bucket_response in bucket_results:
             try:
+
+                # We always try to generate the parent container (the bucket)
+                yield self._generate_unstructured_container(
+                    bucket_response=bucket_response
+                )
+                self._bucket_cache[bucket_response.name] = self.context.container
+
                 metadata_config = self._load_metadata_file(
                     bucket_name=bucket_response.name
                 )
                 if metadata_config:
                     for metadata_entry in metadata_config.entries:
                         logger.info(
                             f"Extracting metadata from path {metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)} "
                             f"and generating structured container"
                         )
                         structured_container: Optional[
                             S3ContainerDetails
                         ] = self._generate_container_details(
                             bucket_response=bucket_response,
                             metadata_entry=metadata_entry,
+                            parent=EntityReference(
+                                id=self._bucket_cache[bucket_response.name].id.__root__,
+                                type="container",
+                            ),
                         )
                         if structured_container:
                             yield structured_container
-                else:
-                    logger.info(
-                        f"No metadata found for bucket {bucket_response.name}, generating unstructured container.."
-                    )
-                    yield self._generate_unstructured_container(
-                        bucket_response=bucket_response
-                    )
+
             except ValidationError as err:
                 error = f"Validation error while creating Container from bucket details - {err}"
                 logger.debug(traceback.format_exc())
                 logger.warning(error)
                 self.status.failed(bucket_response.name, error, traceback.format_exc())
             except Exception as err:
                 error = (
@@ -165,52 +131,70 @@
                 logger.debug(traceback.format_exc())
                 logger.warning(error)
                 self.status.failed(bucket_response.name, error, traceback.format_exc())
 
     def yield_create_container_requests(
         self, container_details: S3ContainerDetails
     ) -> Iterable[CreateContainerRequest]:
-
         yield CreateContainerRequest(
             name=container_details.name,
             prefix=container_details.prefix,
             numberOfObjects=container_details.number_of_objects,
             size=container_details.size,
             dataModel=container_details.data_model,
             service=self.context.objectstore_service.fullyQualifiedName,
+            parent=container_details.parent,
         )
 
     def _generate_container_details(
-        self, bucket_response: S3BucketResponse, metadata_entry: MetadataEntry
+        self,
+        bucket_response: S3BucketResponse,
+        metadata_entry: MetadataEntry,
+        parent: Optional[EntityReference] = None,
     ) -> Optional[S3ContainerDetails]:
         bucket_name = bucket_response.name
         sample_key = self._get_sample_file_path(
             bucket_name=bucket_name, metadata_entry=metadata_entry
         )
         # if we have a sample file to fetch a schema from
         if sample_key:
-            columns = self.extract_column_definitions(bucket_name, sample_key)
+
+            columns = self._get_columns(
+                bucket_name=bucket_name,
+                sample_key=sample_key,
+                metadata_entry=metadata_entry,
+            )
             if columns:
                 return S3ContainerDetails(
-                    name=f"{bucket_name}.{metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)}",
+                    name=metadata_entry.dataPath.strip(S3_KEY_SEPARATOR),
                     prefix=f"{S3_KEY_SEPARATOR}{metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)}",
                     creation_date=bucket_response.creation_date.isoformat(),
                     number_of_objects=self._fetch_metric(
                         bucket_name=bucket_name, metric=S3Metric.NUMBER_OF_OBJECTS
                     ),
                     size=self._fetch_metric(
                         bucket_name=bucket_name, metric=S3Metric.BUCKET_SIZE_BYTES
                     ),
                     file_formats=[container.FileFormat(metadata_entry.structureFormat)],
                     data_model=ContainerDataModel(
                         isPartitioned=metadata_entry.isPartitioned, columns=columns
                     ),
+                    parent=parent,
                 )
         return None
 
+    def _get_columns(
+        self, bucket_name: str, sample_key: str, metadata_entry: MetadataEntry
+    ) -> Optional[List[Column]]:
+        """
+        Get the columns from the file and partition information
+        """
+        extracted_cols = self.extract_column_definitions(bucket_name, sample_key)
+        return (metadata_entry.partitionColumns or []) + (extracted_cols or [])
+
     def extract_column_definitions(
         self, bucket_name: str, sample_key: str
     ) -> List[Column]:
         client_args = self.service_connection.awsConfig
         data_structure_details = DatalakeSource.get_s3_files(
             self.s3_client,
             key=sample_key,
@@ -284,30 +268,28 @@
         except Exception:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Failed fetching metric {metric.value} for bucket {bucket_name}, returning 0"
             )
         return 0
 
-    def _load_metadata_file(
-        self, bucket_name: str
-    ) -> Optional[ObjectStoreContainerConfig]:
+    def _load_metadata_file(self, bucket_name: str) -> Optional[StorageContainerConfig]:
         """
         Load the metadata template file from the root of the bucket, if it exists
         """
         if self._is_metadata_file_present(bucket_name=bucket_name):
             try:
                 logger.info(
                     f"Found metadata template file at - s3://{bucket_name}/{OPENMETADATA_TEMPLATE_FILE_NAME}"
                 )
                 response_object = self.s3_client.get_object(
                     Bucket=bucket_name, Key=OPENMETADATA_TEMPLATE_FILE_NAME
                 )
                 content = json.load(response_object["Body"])
-                metadata_config = ObjectStoreContainerConfig.parse_obj(content)
+                metadata_config = StorageContainerConfig.parse_obj(content)
                 return metadata_config
             except Exception as exc:
                 logger.debug(traceback.format_exc())
                 logger.warning(
                     f"Failed loading metadata file s3://{bucket_name}/{OPENMETADATA_TEMPLATE_FILE_NAME}-{exc}"
                 )
         return None
@@ -333,22 +315,21 @@
             ),
             file_formats=[],  # TODO should we fetch some random files by extension here? Would it be valuable info?
             data_model=None,
         )
 
     @staticmethod
     def _get_sample_file_prefix(metadata_entry: MetadataEntry) -> Optional[str]:
+        """
+        Return a prefix if we have structure data to read
+        """
         result = f"{metadata_entry.dataPath.strip(S3_KEY_SEPARATOR)}"
         if not metadata_entry.structureFormat:
             logger.warning(f"Ignoring un-structured metadata entry {result}")
             return None
-        if metadata_entry.isPartitioned and metadata_entry.partitionColumn:
-            result = (
-                f"{result}/{metadata_entry.partitionColumn.strip(S3_KEY_SEPARATOR)}"
-            )
         return result
 
     def _get_sample_file_path(
         self, bucket_name: str, metadata_entry: MetadataEntry
     ) -> Optional[str]:
         """
         Given a bucket and a metadata entry, returns the full path key to a file which can then be used to infer schema
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/client.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,29 +14,30 @@
 import json
 from typing import List
 
 from metadata.generated.schema.entity.services.connections.pipeline.airbyteConnection import (
     AirbyteConnection,
 )
 from metadata.ingestion.ometa.client import REST, APIError, ClientConfig
+from metadata.utils.constants import AUTHORIZATION_HEADER, NO_ACCESS_TOKEN
 from metadata.utils.credentials import generate_http_basic_token
 
 
 class AirbyteClient:
     """
     Client handling API communication with Airbyte
     """
 
     def __init__(self, config: AirbyteConnection):
         self.config = config
         client_config: ClientConfig = ClientConfig(
             base_url=self.config.hostPort,
             api_version="api/v1",
-            auth_header="Authorization",
-            auth_token=lambda: ("no_token", 0),
+            auth_header=AUTHORIZATION_HEADER,
+            auth_token=lambda: (NO_ACCESS_TOKEN, 0),
         )
         if self.config.username:
             client_config.auth_token_mode = "Basic"
             client_config.auth_token = lambda: (
                 generate_http_basic_token(
                     self.config.username, self.config.password.get_secret_value()
                 ),
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/connection.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,15 +34,14 @@
 
 def test_connection(
     metadata: OpenMetadata,
     client: AirbyteClient,
     service_connection: AirbyteConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
-
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     test_fn = {"GetPipelines": client.list_workspaces}
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airbyte/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airbyte/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,14 +37,15 @@
 )
 from metadata.generated.schema.type.entityLineage import EntitiesEdge, LineageDetails
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
 from metadata.utils import fqn
+from metadata.utils.helpers import clean_uri
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 STATUS_MAP = {
     "cancelled": StatusType.Failed,
@@ -99,15 +100,16 @@
     ) -> Iterable[CreatePipelineRequest]:
         """
         Convert a Connection into a Pipeline Entity
         :param pipeline_details: pipeline_details object from airbyte
         :return: Create Pipeline request with tasks
         """
         connection_url = (
-            f"/workspaces/{pipeline_details.workspace.get('workspaceId')}"
+            f"{clean_uri(self.service_connection.hostPort)}/workspaces"
+            f"/{pipeline_details.workspace.get('workspaceId')}"
             f"/connections/{pipeline_details.connection.get('connectionId')}"
         )
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.connection.get("connectionId"),
             displayName=pipeline_details.connection.get("name"),
             description="",
             pipelineUrl=connection_url,
@@ -134,15 +136,14 @@
 
         for job in self.client.list_jobs(
             pipeline_details.connection.get("connectionId")
         ):
             if not job or not job.get("attempts"):
                 continue
             for attempt in job["attempts"]:
-
                 task_status = [
                     TaskStatus(
                         name=str(pipeline_details.connection.get("connectionId")),
                         executionStatus=STATUS_MAP.get(
                             attempt["status"].lower(), StatusType.Pending
                         ).value,
                         startTime=attempt.get("createdAt"),
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py`

 * *Files 6% similar despite different names*

```diff
@@ -58,18 +58,22 @@
 
 we'll join the keys and get [
     XLet(inlets=[A], outlets=[B]),
     XLet(inlets=[X], outlets=[Y, Z]),
 ]
 and we'll treat this as independent sets of lineage
 """
+import logging
+import traceback
 from typing import Dict, List, Optional, Set
 
 from pydantic import BaseModel
 
+logger = logging.getLogger("airflow.task")
+
 INLETS_ATTR = "_inlets"
 OUTLETS_ATTR = "_outlets"
 
 
 class XLets(BaseModel):
     """
     Group inlets and outlets from all tasks in a DAG
@@ -108,42 +112,58 @@
     We expect xlets to have the following structure:
     [{'tables': ['FQN']}]
 
     :param operator: task to get xlets from
     :param xlet_mode: get inlet or outlet
     :return: list of tables FQN
     """
-    xlet = getattr(operator, xlet_mode)
+    xlet = getattr(operator, xlet_mode) if hasattr(operator, xlet_mode) else []
     xlet_data = parse_xlets(xlet)
 
     if not xlet_data:
-        operator.log.debug(f"Not finding proper {xlet_mode} in task {operator.task_id}")
+        logger.debug(f"Not finding proper {xlet_mode} in task {operator.task_id}")
 
     else:
-        operator.log.info(f"Found {xlet_mode} {xlet_data} in task {operator.task_id}")
+        logger.info(f"Found {xlet_mode} {xlet_data} in task {operator.task_id}")
 
     return xlet_data
 
 
 def get_xlets_from_dag(dag: "DAG") -> List[XLets]:
     """
     Fill the inlets and outlets of the Pipeline by iterating
     over all its tasks
     """
     _inlets = {}
     _outlets = {}
 
     # First, grab all the inlets and outlets from all tasks grouped by keys
     for task in dag.tasks:
-        _inlets.update(
-            get_xlets_from_operator(operator=task, xlet_mode=INLETS_ATTR) or []
-        )
-        _outlets.update(
-            get_xlets_from_operator(operator=task, xlet_mode=OUTLETS_ATTR) or []
-        )
+        try:
+            _inlets.update(
+                get_xlets_from_operator(
+                    operator=task,
+                    xlet_mode=INLETS_ATTR if hasattr(task, INLETS_ATTR) else "inlets",
+                )
+                or []
+            )
+            _outlets.update(
+                get_xlets_from_operator(
+                    operator=task,
+                    xlet_mode=OUTLETS_ATTR if hasattr(task, INLETS_ATTR) else "outlets",
+                )
+                or []
+            )
+
+        except Exception as exc:
+            error_msg = (
+                f"Error while getting inlets and outlets for task - {task} - {exc}"
+            )
+            logger.error(error_msg)
+            logger.error(traceback.format_exc())
 
     # We expect to have the same keys in both inlets and outlets dicts
     # We will then iterate over the inlet keys to build the list of XLets
     return [
         XLets(inlets=set(value), outlets=set(_outlets[key]))
         for key, value in _inlets.items()
         if value and _outlets.get(key)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/airflow/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/metadata.py`

 * *Files 3% similar despite different names*

```diff
@@ -41,16 +41,20 @@
 )
 from metadata.generated.schema.type.entityLineage import EntitiesEdge, LineageDetails
 from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.connections.session import create_and_bind_session
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.source.pipeline.airflow.lineage_parser import get_xlets_from_dag
+from metadata.ingestion.source.pipeline.airflow.models import (
+    AirflowDag,
+    AirflowDagDetails,
+)
 from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
-from metadata.utils.helpers import datetime_to_ts
+from metadata.utils.helpers import clean_uri, datetime_to_ts
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 STATUS_MAP = {
     "success": StatusType.Successful.value,
     "failed": StatusType.Failed.value,
@@ -260,80 +264,123 @@
         )
 
         for serialized_dag in self.session.query(
             SerializedDagModel.dag_id,
             json_data_column,
             SerializedDagModel.fileloc,
         ).all():
-            yield OMSerializedDagDetails(
-                dag_id=serialized_dag[0],
-                data=serialized_dag[1],
-                fileloc=serialized_dag[2],
-            )
+            try:
+                data = serialized_dag[1]["dag"]
+                dag = AirflowDagDetails(
+                    dag_id=serialized_dag[0],
+                    fileloc=serialized_dag[2],
+                    data=AirflowDag(**serialized_dag[1]),
+                    max_active_runs=data.get("max_active_runs", None),
+                    description=data.get("_description", None),
+                    start_date=data.get("start_date", None),
+                    tasks=data.get("tasks", []),
+                    owners=data.get("default_args", [])["__var"].get("email", [])
+                    if data.get("default_args")
+                    else None,
+                )
+                yield dag
+            except ValidationError as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(
+                    f"Error building pydantic model for {serialized_dag} - {err}"
+                )
+            except Exception as err:
+                logger.debug(traceback.format_exc())
+                logger.warning(f"Wild error yielding dag {serialized_dag} - {err}")
 
     def get_pipeline_name(self, pipeline_details: SerializedDAG) -> str:
         """
         Get Pipeline Name
         """
         return pipeline_details.dag_id
 
     @staticmethod
-    def get_tasks_from_dag(dag: SerializedDAG) -> List[Task]:
+    def get_tasks_from_dag(dag: AirflowDagDetails, host_port: str) -> List[Task]:
         """
         Obtain the tasks from a SerializedDAG
         :param dag: SerializedDAG
         :return: List of tasks
         """
         return [
             Task(
                 name=task.task_id,
                 description=task.doc_md,
-                # Just the suffix
-                taskUrl=f"/taskinstance/list/?flt1_dag_id_equals={dag.dag_id}&_flt_3_task_id={task.task_id}",
+                taskUrl=(
+                    f"{clean_uri(host_port)}/taskinstance/list/"
+                    f"?flt1_dag_id_equals={dag.dag_id}&_flt_3_task_id={task.task_id}"
+                ),
                 downstreamTasks=list(task.downstream_task_ids),
-                taskType=task.task_type,
                 startDate=task.start_date.isoformat() if task.start_date else None,
                 endDate=task.end_date.isoformat() if task.end_date else None,
             )
             for task in cast(Iterable[BaseOperator], dag.tasks)
         ]
 
     @staticmethod
     def _build_dag(data: Any) -> SerializedDAG:
         """
         Use the queried data to fetch the DAG
         :param data: from SQA query
         :return: SerializedDAG
         """
-
         if isinstance(data, dict):
             return SerializedDAG.from_dict(data)
 
         return SerializedDAG.from_json(data)
 
+    def get_user_details(self, email) -> Optional[EntityReference]:
+        user = self.metadata.get_user_by_email(email=email)
+        if user:
+            return EntityReference(id=user.id.__root__, type="user")
+        return None
+
+    def get_owner(self, owners) -> Optional[EntityReference]:
+        try:
+            if isinstance(owners, str) and owners:
+                return self.get_user_details(email=owners)
+
+            if isinstance(owners, List) and owners:
+                for owner in owners or []:
+                    return self.get_user_details(email=owner)
+
+            logger.debug(f"No user for found for email {owners} in OMD")
+        except Exception as exc:
+            logger.warning(f"Error while getting details of user {owners} - {exc}")
+        return None
+
     def yield_pipeline(
-        self, pipeline_details: OMSerializedDagDetails
+        self, pipeline_details: AirflowDagDetails
     ) -> Iterable[CreatePipelineRequest]:
         """
         Convert a DAG into a Pipeline Entity
         :param pipeline_details: SerializedDAG from airflow metadata DB
         :return: Create Pipeline request with tasks
         """
 
         try:
-            dag: SerializedDAG = self._build_dag(pipeline_details.data)
+
             pipeline_request = CreatePipelineRequest(
                 name=pipeline_details.dag_id,
-                description=dag.description,
-                pipelineUrl=f"/tree?dag_id={dag.dag_id}",  # Just the suffix
-                concurrency=dag.concurrency,
+                description=pipeline_details.description,
+                pipelineUrl=f"{clean_uri(self.service_connection.hostPort)}/tree?dag_id={pipeline_details.dag_id}",
+                concurrency=pipeline_details.max_active_runs,
                 pipelineLocation=pipeline_details.fileloc,
-                startDate=dag.start_date.isoformat() if dag.start_date else None,
-                tasks=self.get_tasks_from_dag(dag),
+                startDate=pipeline_details.start_date.isoformat()
+                if pipeline_details.start_date
+                else None,
+                tasks=self.get_tasks_from_dag(
+                    pipeline_details, self.service_connection.hostPort
+                ),
                 service=self.context.pipeline_service.fullyQualifiedName.__root__,
+                owner=self.get_owner(pipeline_details.owners),
             )
             yield pipeline_request
             self.register_record(pipeline_request=pipeline_request)
         except TypeError as err:
             logger.debug(traceback.format_exc())
             logger.warning(
                 f"Error building DAG information from {pipeline_details}. There might be Airflow version"
@@ -345,25 +392,25 @@
                 f"Error building pydantic model for {pipeline_details} - {err}"
             )
         except Exception as err:
             logger.debug(traceback.format_exc())
             logger.warning(f"Wild error ingesting pipeline {pipeline_details} - {err}")
 
     @staticmethod
-    def parse_xlets(xlet: List[Any]) -> Optional[List[str]]:
+    def parse_xlets(xlet: Optional[List[Any]]) -> Optional[List[str]]:
         """
         Parse airflow xlets for 2.1.4. E.g.,
 
         [{'__var': {'tables': ['sample_data.ecommerce_db.shopify.fact_order']},
         '__type': 'dict'}]
 
         :param xlet: airflow v2 xlet dict
         :return: table FQN list or None
         """
-        if len(xlet) and isinstance(xlet[0], dict):
+        if xlet and len(xlet) and isinstance(xlet[0], dict):
             tables = xlet[0].get("__var").get("tables")
             if tables and isinstance(tables, list):
                 return tables
 
         return None
 
     def get_inlets(self, task: BaseOperator) -> Optional[List[str]]:
@@ -391,29 +438,29 @@
             return self.parse_xlets(outlets)
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Error trying to parse outlets: {exc}")
             return None
 
     def yield_pipeline_lineage_details(
-        self, pipeline_details: OMSerializedDagDetails
+        self, pipeline_details: AirflowDagDetails
     ) -> Optional[Iterable[AddLineageRequest]]:
         """
         Parse xlets and add lineage between Pipelines and Tables
         :param pipeline_details: SerializedDAG from airflow metadata DB
         :return: Lineage from inlets and outlets
         """
-        dag: SerializedDAG = self._build_dag(pipeline_details.data)
+
         lineage_details = LineageDetails(
             pipeline=EntityReference(
                 id=self.context.pipeline.id.__root__, type="pipeline"
             )
         )
 
-        xlets = get_xlets_from_dag(dag=dag)
+        xlets = get_xlets_from_dag(dag=pipeline_details) if pipeline_details else []
         for xlet in xlets:
             for from_fqn in xlet.inlets or []:
                 from_entity = self.metadata.get_by_name(entity=Table, fqn=from_fqn)
                 if from_entity:
                     for to_fqn in xlet.outlets or []:
                         to_entity = self.metadata.get_by_name(entity=Table, fqn=to_fqn)
                         if to_entity:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,61 +8,49 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-from typing import Optional
 
-from dagster_graphql import DagsterGraphQLClient
-from gql.transport.requests import RequestsHTTPTransport
+from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.dagsterConnection import (
-    DagsterConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.databricksPipelineConnection import (
+    DatabricksPipelineConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.dagster.queries import TEST_QUERY_GRAPHQL
+from metadata.ingestion.source.database.databricks.client import DatabricksClient
 
 
-def get_connection(connection: DagsterConnection) -> DagsterGraphQLClient:
+def get_connection(connection: DatabricksPipelineConnection) -> DatabricksClient:
     """
     Create connection
     """
-    url = connection.host
-    dagster_connection = DagsterGraphQLClient(
-        url,
-        transport=RequestsHTTPTransport(
-            url=f"{url}/graphql",
-            headers={"Dagster-Cloud-Api-Token": connection.token.get_secret_value()}
-            if connection.token
-            else None,
-        ),
-    )
-
-    return dagster_connection
+    return DatabricksClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: DagsterGraphQLClient,
-    service_connection: DagsterConnection,
+    client: DatabricksClient,
+    service_connection: DatabricksPipelineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
     def custom_executor_for_pipeline():
-        client._execute(TEST_QUERY_GRAPHQL)  # pylint: disable=protected-access
+        result = client.list_jobs()
+        return list(result)
 
     test_fn = {"GetPipelines": custom_executor_for_pipeline}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_fqn=service_connection.type.value,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/dagster/queries.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/dagster/queries.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/dashboard/metabase/connection.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,51 +8,49 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
-
 from typing import Optional
 
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.databricksPipelineConnection import (
-    DatabricksPipelineConnection,
+from metadata.generated.schema.entity.services.connections.dashboard.metabaseConnection import (
+    MetabaseConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.database.databricks.client import DatabricksClient
+from metadata.ingestion.source.dashboard.metabase.client import MetabaseClient
 
 
-def get_connection(connection: DatabricksPipelineConnection) -> DatabricksClient:
+def get_connection(connection: MetabaseConnection) -> MetabaseClient:
     """
     Create connection
     """
-    return DatabricksClient(connection)
+    return MetabaseClient(connection)
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: DatabricksClient,
-    service_connection: DatabricksPipelineConnection,
+    client: MetabaseClient,
+    service_connection: MetabaseConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    def custom_executor_for_pipeline():
-        result = client.list_jobs()
-        return list(result)
+    def custom_executor():
+        return client.get_dashboards_list()
 
-    test_fn = {"GetPipelines": custom_executor_for_pipeline}
+    test_fn = {"GetDashboards": custom_executor}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_fqn=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py`

 * *Files 0% similar despite different names*

```diff
@@ -177,15 +177,14 @@
                 dependent_tasks[task["task_key"]] = [v["task_key"] for v in depends_on]
             else:
                 dependent_tasks[task["task_key"]] = None
 
         return dependent_tasks
 
     def yield_pipeline_status(self, pipeline_details) -> Iterable[OMetaPipelineStatus]:
-
         for job_id in self.context.job_id_list:
             try:
                 runs = self.client.get_job_runs(job_id=job_id)
                 for attempt in runs:
                     for task_run in attempt["tasks"]:
                         task_status = []
                         task_status.append(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/domopipeline/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/domopipeline/metadata.py`

 * *Files 1% similar despite different names*

```diff
@@ -110,27 +110,24 @@
 
     def yield_pipeline_lineage_details(
         self, pipeline_details
     ) -> Optional[Iterable[AddLineageRequest]]:
         return
 
     def yield_pipeline_status(self, pipeline_details) -> OMetaPipelineStatus:
-
         pipeline_id = pipeline_details.get("id")
         if not pipeline_id:
             logger.debug(
                 f"Could not extract ID from {pipeline_details} while getting status."
             )
             return None
 
         runs = self.connection.get_runs(pipeline_id)
         try:
-
             for run in runs or []:
-
                 start_time = run["beginTime"] // 1000 if run.get("beginTime") else None
                 end_time = run["endTime"] // 1000 if run.get("endTime") else None
                 run_state = run.get("state", "Pending")
 
                 task_status = TaskStatus(
                     name=pipeline_id,
                     executionStatus=STATUS_MAP.get(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,46 +8,47 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Source connection handler
 """
+
 from typing import Optional
 
+from metadata.clients.aws_client import AWSClient
 from metadata.generated.schema.entity.automations.workflow import (
     Workflow as AutomationWorkflow,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.fivetranConnection import (
-    FivetranConnection,
+from metadata.generated.schema.entity.services.connections.pipeline.gluePipelineConnection import (
+    GluePipelineConnection,
 )
 from metadata.ingestion.connections.test_connections import test_connection_steps
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.pipeline.fivetran.client import FivetranClient
 
 
-def get_connection(connection: FivetranConnection) -> FivetranClient:
+def get_connection(connection: GluePipelineConnection):
     """
     Create connection
     """
-    return FivetranClient(connection)
+    return AWSClient(connection.awsConfig).get_glue_client()
 
 
 def test_connection(
     metadata: OpenMetadata,
-    client: FivetranClient,
-    service_connection: FivetranConnection,
+    client,
+    service_connection: GluePipelineConnection,
     automation_workflow: Optional[AutomationWorkflow] = None,
 ) -> None:
     """
     Test connection. This can be executed either as part
     of a metadata workflow or during an Automation Workflow
     """
 
-    test_fn = {"GetPipelines": client.list_groups}
+    test_fn = {"GetPipelines": client.list_workflows}
 
     test_connection_steps(
         metadata=metadata,
         test_fn=test_fn,
         service_fqn=service_connection.type.value,
         automation_workflow=automation_workflow,
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/fivetran/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/fivetran/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,49 +6,48 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Source connection handler
+Validator for column values to not match regex test case
 """
 
 from typing import Optional
 
-from metadata.clients.aws_client import AWSClient
-from metadata.generated.schema.entity.automations.workflow import (
-    Workflow as AutomationWorkflow,
+from metadata.data_quality.validations.column.base.columnValuesToNotMatchRegex import (
+    BaseColumnValuesToNotMatchRegexValidator,
 )
-from metadata.generated.schema.entity.services.connections.pipeline.gluePipelineConnection import (
-    GluePipelineConnection,
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
+    PandasValidatorMixin,
 )
-from metadata.ingestion.connections.test_connections import test_connection_steps
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.sqa_like_column import SQALikeColumn
 
 
-def get_connection(connection: GluePipelineConnection):
-    """
-    Create connection
-    """
-    return AWSClient(connection.awsConfig).get_glue_client()
-
-
-def test_connection(
-    metadata: OpenMetadata,
-    client,
-    service_connection: GluePipelineConnection,
-    automation_workflow: Optional[AutomationWorkflow] = None,
-) -> None:
-    """
-    Test connection. This can be executed either as part
-    of a metadata workflow or during an Automation Workflow
-    """
-
-    test_fn = {"GetPipelines": client.list_workflows}
-
-    test_connection_steps(
-        metadata=metadata,
-        test_fn=test_fn,
-        service_fqn=service_connection.type.value,
-        automation_workflow=automation_workflow,
-    )
+class ColumnValuesToNotMatchRegexValidator(
+    BaseColumnValuesToNotMatchRegexValidator, PandasValidatorMixin
+):
+    """Validator for column values to not match regex test case"""
+
+    def _get_column_name(self) -> SQALikeColumn:
+        """Get column name from the test case entity link
+
+        Returns:
+            SQALikeColumn: column
+        """
+        return self.get_column_name(
+            self.test_case.entityLink.__root__,
+            self.runner,
+        )
+
+    def _run_results(
+        self, metric: Metrics, column: SQALikeColumn, **kwargs
+    ) -> Optional[int]:
+        """compute result of the test case
+
+        Args:
+            metric: metric
+            column: column
+        """
+        return self.run_dataframe_results(self.runner, metric, column, **kwargs)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/client.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/client.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/connection.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/connection.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/nifi/metadata.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/nifi/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 )
 from metadata.generated.schema.metadataIngestion.workflow import (
     Source as WorkflowSource,
 )
 from metadata.ingestion.api.source import InvalidSourceException
 from metadata.ingestion.models.pipeline_status import OMetaPipelineStatus
 from metadata.ingestion.source.pipeline.pipeline_service import PipelineServiceSource
+from metadata.utils.helpers import clean_uri
 from metadata.utils.logger import ingestion_logger
 
 logger = ingestion_logger()
 
 
 PROCESS_GROUP_FLOW = "processGroupFlow"
 BREADCRUMB = "breadcrumb"
@@ -109,15 +110,15 @@
         that form the Pipeline
         """
         try:
             return [
                 Task(
                     name=processor.id_,
                     displayName=processor.name,
-                    taskUrl=processor.uri.replace(self.service_connection.hostPort, ""),
+                    taskUrl=f"{clean_uri(self.service_connection.hostPort)}{processor.uri}",
                     taskType=processor.type_,
                     downstreamTasks=self._get_downstream_tasks_from(
                         source_id=processor.id_,
                         connections=pipeline_details.connections,
                     ),
                 )
                 for processor in pipeline_details.processors
@@ -136,17 +137,15 @@
         Convert a Connection into a Pipeline Entity
         :param pipeline_details: pipeline_details object from Nifi
         :return: Create Pipeline request with tasks
         """
         pipeline_request = CreatePipelineRequest(
             name=pipeline_details.id_,
             displayName=pipeline_details.name,
-            pipelineUrl=pipeline_details.uri.replace(
-                self.service_connection.hostPort, ""
-            ),
+            pipelineUrl=f"{clean_uri(self.service_connection.hostPort)}{pipeline_details.uri}",
             tasks=self._get_tasks_from_details(pipeline_details),
             service=self.context.pipeline_service.fullyQualifiedName.__root__,
         )
         yield pipeline_request
         self.register_record(pipeline_request=pipeline_request)
 
     def yield_pipeline_status(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/pipeline/pipeline_service.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/pipeline_service.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/source/sqa_types.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/sqa_types.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/ingestion/stage/table_usage.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/stage/table_usage.py`

 * *Files 7% similar despite different names*

```diff
@@ -21,15 +21,14 @@
 
 from metadata.config.common import ConfigModel
 from metadata.generated.schema.api.data.createQuery import CreateQueryRequest
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.generated.schema.entity.teams.user import User
-from metadata.generated.schema.type.entityReference import EntityReference
 from metadata.generated.schema.type.queryParserData import QueryParserData
 from metadata.generated.schema.type.tableUsageCount import TableUsageCount
 from metadata.ingestion.api.stage import Stage
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.utils.constants import UTF_8
 from metadata.utils.logger import ingestion_logger
 
@@ -82,26 +81,15 @@
         logger.info(f"Creating the directory to store staging data in {location}")
         location.mkdir(parents=True, exist_ok=True)
 
     def _get_user_entity(self, username: str):
         if username:
             user = self.metadata.get_by_name(entity=User, fqn=username)
             if user:
-                return [
-                    EntityReference(
-                        id=user.id,
-                        type="user",
-                        name=user.name.__root__,
-                        fullyQualifiedName=user.fullyQualifiedName.__root__,
-                        description=user.description,
-                        displayName=user.displayName,
-                        deleted=user.deleted,
-                        href=user.href,
-                    )
-                ]
+                return [user.fullyQualifiedName.__root__]
         return []
 
     def _add_sql_query(self, record, table):
         if self.table_queries.get((table, record.date)):
             self.table_queries[(table, record.date)].append(
                 CreateQueryRequest(
                     query=record.sql,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/datalake/datalake_test_suite_interface.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py`

 * *Files 22% similar despite different names*

```diff
@@ -6,83 +6,71 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Interfaces with database for all database engine
-supporting sqlalchemy abstraction layer
+Validator for column values sum to be between test case
 """
-from datetime import datetime, timezone
-from typing import Optional
 
-from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
-    DatalakeConnection,
+import traceback
+from abc import abstractmethod
+from typing import Union
+
+from sqlalchemy import Column
+
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
+from metadata.generated.schema.tests.basic import (
+    TestCaseResult,
+    TestCaseStatus,
+    TestResultValue,
 )
-from metadata.generated.schema.tests.basic import TestCaseResult
-from metadata.generated.schema.tests.testCase import TestCase
-from metadata.generated.schema.tests.testDefinition import TestDefinition
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.interfaces.datalake.mixins.pandas_mixin import PandasInterfaceMixin
-from metadata.interfaces.test_suite_protocol import TestSuiteProtocol
-from metadata.test_suite.validations.validator import Validator
-from metadata.utils.importer import import_test_case_class
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.logger import test_suite_logger
+from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
+SUM = "sum"
 
-class DataLakeTestSuiteInterface(TestSuiteProtocol, PandasInterfaceMixin):
-    """
-    Sequential interface protocol for testSuite and Profiler. This class
-    implements specific operations needed to run profiler and test suite workflow
-    against a Datalake source.
-    """
-
-    def __init__(
-        self,
-        ometa_client: OpenMetadata = None,
-        service_connection_config: DatalakeConnection = None,
-        table_entity=None,
-        df=None,
-    ):
-        self.table_entity = table_entity
-        self.df = df
-        self.ometa_client = ometa_client
-        self.service_connection_config = service_connection_config
-
-    def run_test_case(
-        self,
-        test_case: TestCase,
-    ) -> Optional[TestCaseResult]:
-        """Run table tests where platformsTest=OpenMetadata
 
-        Args:
-            test_case: test case object to execute
+class BaseColumnValuesSumToBeBetweenValidator(BaseTestValidator):
+    """Validator for column values sum to be between test case"""
+
+    def run_validation(self) -> TestCaseResult:
+        """Run validation for the given test case
 
         Returns:
-            TestCaseResult object
+            TestCaseResult:
         """
-
         try:
-            TestHandler = import_test_case_class(  # pylint: disable=invalid-name
-                self.ometa_client.get_by_id(
-                    TestDefinition, test_case.testDefinition.id
-                ).entityType.value,
-                "pandas",
-                test_case.testDefinition.fullyQualifiedName,
+            column: Union[SQALikeColumn, Column] = self._get_column_name()
+            res = self._run_results(Metrics.SUM, column)
+        except (ValueError, RuntimeError) as exc:
+            msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
+            logger.debug(traceback.format_exc())
+            logger.warning(msg)
+            return self.get_test_case_result_object(
+                self.execution_date,
+                TestCaseStatus.Aborted,
+                msg,
+                [TestResultValue(name=SUM, value=None)],
             )
 
-            test_handler = TestHandler(
-                self.df,
-                test_case=test_case,
-                execution_date=datetime.now(tz=timezone.utc).timestamp(),
-            )
-
-            return Validator(validator_obj=test_handler).validate()
-        except Exception as err:
-            logger.error(
-                f"Error executing {test_case.testDefinition.fullyQualifiedName} - {err}"
-            )
+        min_bound = self.get_min_bound("minValueForColSum")
+        max_bound = self.get_max_bound("maxValueForColSum")
 
-            raise RuntimeError(err)
+        return self.get_test_case_result_object(
+            self.execution_date,
+            self.get_test_case_status(min_bound <= res <= max_bound),
+            f"Found sum={res} vs. the expected min={min_bound}, max={max_bound}.",
+            [TestResultValue(name=SUM, value=str(res))],
+        )
+
+    @abstractmethod
+    def _get_column_name(self):
+        raise NotImplementedError
+
+    @abstractmethod
+    def _run_results(self, metric: Metrics, column: Union[SQALikeColumn, Column]):
+        raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/mixins/sqa_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/mixins/sqalchemy/sqa_mixin.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,14 +16,17 @@
 
 
 from typing import Optional
 
 from sqlalchemy import Column, MetaData, inspect
 from sqlalchemy.orm import DeclarativeMeta
 
+from metadata.generated.schema.entity.services.connections.database.databricksConnection import (
+    DatabricksConnection,
+)
 from metadata.generated.schema.entity.services.connections.database.snowflakeConnection import (
     SnowflakeType,
 )
 from metadata.ingestion.source.connections import get_connection
 from metadata.ingestion.source.database.snowflake.queries import (
     SNOWFLAKE_SESSION_TAG_QUERY,
 )
@@ -76,10 +79,23 @@
         ):
             session.execute(
                 SNOWFLAKE_SESSION_TAG_QUERY.format(
                     query_tag=self.service_connection_config.queryTag
                 )
             )
 
+    def set_catalog(self, session) -> None:
+        """Set catalog for the session. Right now only databricks requires it
+
+        Args:
+            session (Session): sqa session object
+        """
+        if isinstance(self.service_connection_config, DatabricksConnection):
+            bind = session.get_bind()
+            bind.execute(
+                "USE CATALOG %(catalog)s;",
+                {"catalog": self.service_connection_config.catalog},
+            ).first()
+
     def close(self):
         """close session"""
         self.session.close()
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/sqalchemy/sqa_test_suite_interface.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,28 +17,28 @@
 from datetime import datetime, timezone
 from typing import Optional, Union
 
 from sqlalchemy import MetaData
 from sqlalchemy.orm import DeclarativeMeta
 from sqlalchemy.orm.util import AliasedClass
 
+from metadata.data_quality.interface.test_suite_protocol import TestSuiteProtocol
+from metadata.data_quality.validations.validator import Validator
 from metadata.generated.schema.entity.data.table import PartitionProfilerConfig, Table
 from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
 from metadata.generated.schema.tests.basic import TestCaseResult
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testDefinition import TestDefinition
 from metadata.ingestion.connections.session import create_and_bind_session
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection
-from metadata.interfaces.sqalchemy.mixins.sqa_mixin import SQAInterfaceMixin
-from metadata.interfaces.test_suite_protocol import TestSuiteProtocol
+from metadata.mixins.sqalchemy.sqa_mixin import SQAInterfaceMixin
 from metadata.profiler.api.models import ProfileSampleConfig
-from metadata.profiler.profiler.runner import QueryRunner
-from metadata.profiler.profiler.sampler import Sampler
-from metadata.test_suite.validations.validator import Validator
+from metadata.profiler.processor.runner import QueryRunner
+from metadata.profiler.processor.sampler import Sampler
 from metadata.utils.constants import TEN_MIN
 from metadata.utils.importer import import_test_case_class
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.timeout import cls_timeout
 
 logger = test_suite_logger()
 
@@ -64,14 +64,15 @@
         self.ometa_client = ometa_client
         self.table_entity = table_entity
         self.service_connection_config = service_connection_config
         self.session = create_and_bind_session(
             get_connection(self.service_connection_config)
         )
         self.set_session_tag(self.session)
+        self.set_catalog(self.session)
 
         self._table = self._convert_table_to_orm_object(sqa_metadata_obj)
 
         self.profile_sample_config = profile_sample_config
         self.table_sample_query = table_sample_query
         self.table_partition_config = (
             table_partition_config if not self.table_sample_query else None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/interfaces/test_suite_protocol.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/runner/core.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,39 +6,40 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Interfaces with database for all database engine
-supporting sqlalchemy abstraction layer
+Main class to run data tests
 """
 
-from abc import ABC, abstractmethod
-from typing import Optional, Union
 
-from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
-    DatalakeConnection,
-)
-from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
-from metadata.generated.schema.tests.basic import TestCaseResult
+from metadata.data_quality.interface.test_suite_protocol import TestSuiteProtocol
+from metadata.data_quality.runner.models import TestCaseResultResponse
 from metadata.generated.schema.tests.testCase import TestCase
-from metadata.ingestion.ometa.ometa_api import OpenMetadata
+from metadata.utils.logger import test_suite_logger
 
+logger = test_suite_logger()
 
-class TestSuiteProtocol(ABC):
-    """Protocol interface for the processor"""
 
-    @abstractmethod
-    def __init__(
-        self,
-        ometa_client: OpenMetadata = None,
-        service_connection_config: Union[DatabaseConnection, DatalakeConnection] = None,
-    ):
-        """Required attribute for the interface"""
-        raise NotImplementedError
-
-    @abstractmethod
-    def run_test_case(self, test_case: TestCase) -> Optional[TestCaseResult]:
-        """run column data quality tests"""
-        raise NotImplementedError
+class DataTestsRunner:
+    """class to execute the test validation"""
+
+    def __init__(self, test_runner_interface: TestSuiteProtocol):
+        self.test_runner_interace = test_runner_interface
+
+    def run_and_handle(self, test_case: TestCase):
+        """run and handle test case validation"""
+        logger.info(
+            f"Executing test case {test_case.name.__root__} "
+            f"for entity {self.test_runner_interace.table_entity.fullyQualifiedName.__root__}"
+        )
+        test_result = self.test_runner_interace.run_test_case(
+            test_case,
+        )
+
+        if test_result:
+            return TestCaseResultResponse(
+                testCaseResult=test_result, testCase=test_case
+            )
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/json_schema_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/json_schema_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/protobuf_parser.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/protobuf_parser.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/parsers/schema_parsers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/parsers/schema_parsers.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/api/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/api/models.py`

 * *Files 5% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     ColumnProfilerConfig,
     PartitionProfilerConfig,
     ProfileSampleType,
     Table,
     TableData,
 )
 from metadata.generated.schema.type.basic import FullyQualifiedEntityName
-from metadata.profiler.profiler.models import ProfilerDef
+from metadata.profiler.processor.models import ProfilerDef
 
 
 class ColumnConfig(ConfigModel):
     """Column config for profiler"""
 
     excludeColumns: Optional[List[str]]
     includeColumns: Optional[List[ColumnProfilerConfig]]
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/api/workflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/api/workflow.py`

 * *Files 0% similar despite different names*

```diff
@@ -55,24 +55,24 @@
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
 from metadata.ingestion.source.connections import get_connection, get_test_connection_fn
 from metadata.profiler.api.models import (
     ProfilerProcessorConfig,
     ProfilerResponse,
     TableConfig,
 )
-from metadata.profiler.metrics.registry import Metrics
-from metadata.profiler.profiler.core import Profiler
-from metadata.profiler.profiler.default import DefaultProfiler, get_default_metrics
-from metadata.profiler.profiler.interface.pandas.pandas_profiler_interface import (
+from metadata.profiler.interface.pandas.pandas_profiler_interface import (
     PandasProfilerInterface,
 )
-from metadata.profiler.profiler.interface.profiler_protocol import ProfilerProtocol
-from metadata.profiler.profiler.interface.sqlalchemy.sqa_profiler_interface import (
+from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
+from metadata.profiler.interface.sqlalchemy.sqa_profiler_interface import (
     SQAProfilerInterface,
 )
+from metadata.profiler.metrics.registry import Metrics
+from metadata.profiler.processor.core import Profiler
+from metadata.profiler.processor.default import DefaultProfiler, get_default_metrics
 from metadata.utils import fqn
 from metadata.utils.class_helper import (
     get_service_class_from_service_type,
     get_service_type_from_source_type,
 )
 from metadata.utils.filters import filter_by_database, filter_by_schema, filter_by_table
 from metadata.utils.importer import get_sink
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/distinct_ratio.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/distinct_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/duplicate_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/duplicate_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/ilike_ratio.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/ilike_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/iqr.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/iqr.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/like_ratio.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/like_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/non_parametric_skew.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/non_parametric_skew.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/null_ratio.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/null_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/composed/unique_ratio.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/composed/unique_ratio.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/core.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/core.py`

 * *Files 0% similar despite different names*

```diff
@@ -65,15 +65,14 @@
     new_hist = add_props(bins=5)(Metrics.HISTOGRAM.value)
 
     new_hist will still be a class, so we can safely pass it
     to the profiler to be initialized for all the columns.
     """
 
     def inner(cls):
-
         # Create a new cls instance to avoid updating the original ref
         # In these scenarios, deepcopy(cls) just returns a pointer
         # to the same reference
         _new_cls = type("_new_cls", cls.__bases__, dict(cls.__dict__))
         _orig = cls.__init__
 
         def _new_init(self, *args, **kw):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/hybrid/histogram.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/hybrid/histogram.py`

 * *Files 0% similar despite different names*

```diff
@@ -205,18 +205,18 @@
             if i < len(bins) - 1
             else self._format_bin_labels(bins[i])
             for i in range(len(bins))
         ]
 
         bins.append(np.inf)  # add the last bin
 
-        frequencies = None
+        frequencies = np.zeros(num_bins)
 
         for df in dfs:
-            if not frequencies:
+            if not frequencies.any():
                 frequencies = (
                     pd.cut(df[self.col.name], bins, right=False).value_counts().values
                 )  # right boundary is exclusive
                 continue
             frequencies += (
                 pd.cut(df[self.col.name], bins, right=False).value_counts().values
             )  # right boundary is exclusive
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/registry.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/column_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/column_count.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,26 +21,26 @@
 from sqlalchemy.orm import DeclarativeMeta
 from sqlalchemy.sql.functions import FunctionElement
 
 from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
 from metadata.profiler.orm.registry import Dialects
 
 
-class ColunCountFn(FunctionElement):
+class ColumnCountFn(FunctionElement):
     name = __qualname__
     inherit_cache = CACHE
 
 
-@compiles(ColunCountFn)
+@compiles(ColumnCountFn)
 def _(element, compiler, **kw):
     return compiler.process(element.clauses, **kw)
 
 
-@compiles(ColunCountFn, Dialects.IbmDbSa)
-@compiles(ColunCountFn, Dialects.Db2)
+@compiles(ColumnCountFn, Dialects.IbmDbSa)
+@compiles(ColumnCountFn, Dialects.Db2)
 def _(element, compiler, **kw):
     """Returns column count for db2 database and handles casting variables.
     If casting is not provided for variables, db2 throws error.
     """
     proc = compiler.process(element.clauses, **kw)
     return f"CAST({proc} AS BIGINT)"
 
@@ -76,16 +76,14 @@
     @_label
     def fn(self):
         """sqlalchemy function"""
         if not hasattr(self, "table"):
             raise AttributeError(
                 "Column Count requires a table to be set: add_props(table=...)(Metrics.COLUMN_COUNT)"
             )
-        return ColunCountFn(literal(len(inspect(self.table).c)))
+        return ColumnCountFn(literal(len(inspect(self.table).c)))
 
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """dataframe function"""
         from pandas import DataFrame  # pylint: disable=import-outside-toplevel
 
-        df = cast(DataFrame, df)
-
-        return len(df.columns)
+        return len(cast(DataFrame, dfs[0]).columns)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/column_names.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/column_names.py`

 * *Files 13% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 Table Column Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 import sqlalchemy
 from sqlalchemy import inspect, literal
 from sqlalchemy.ext.compiler import compiles
 from sqlalchemy.orm import DeclarativeMeta
 from sqlalchemy.sql.functions import FunctionElement
 
@@ -80,13 +79,9 @@
             raise AttributeError(
                 "Column Count requires a table to be set: add_props(table=...)(Metrics.COLUMN_COUNT)"
             )
 
         col_names = ",".join(inspect(self.table).c.keys())
         return ColunNameFn(literal(col_names, type_=sqlalchemy.types.String))
 
-    def df_fn(self, df=None):
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        df = cast(DataFrame, df)
-
-        return df.columns.values.tolist()
+    def df_fn(self, dfs=None):
+        return dfs[0].columns.values.tolist()
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/count.py`

 * *Files 7% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 from sqlalchemy import column, func
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
@@ -40,21 +39,17 @@
         return int
 
     @_label
     def fn(self):
         """sqlalchemy function"""
         return func.count(column(self.col.name))
 
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """pandas function"""
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        df = cast(DataFrame, df)
-
         try:
-            return len(df[self.col.name])
+            return sum(df[self.col.name].count() for df in dfs)
         except Exception as err:
             logger.debug(
                 f"Don't know how to process type {self.col.type} when computing Count"
             )
             logger.error(err)
             return 0
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/count_in_set.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/count_in_set.py`

 * *Files 9% similar despite different names*

```diff
@@ -59,20 +59,20 @@
             return SumFn(case([(column(self.col.name).in_(set_values), 1)], else_=0))
 
         except Exception as exc:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.warning(f"Error trying to run countInSet for {self.col.name}: {exc}")
             return None
 
-    def df_fn(self, df):
+    def df_fn(self, dfs=None):
         """pandas function"""
         if not hasattr(self, "values"):
             raise AttributeError(
                 "CountInSet requires a set of values to be validate: add_props(values=...)(Metrics.COUNT_IN_SET)"
             )
 
         try:
-            return df[self.col.name][df[self.col.name].isin(list(self.values))].count()
+            return sum(sum(df[self.col.name].isin(self.values)) for df in dfs)
         except Exception as exc:  # pylint: disable=broad-except
             logger.debug(traceback.format_exc())
             logger.warning(f"Error trying to run countInSet for {self.col.name}: {exc}")
             return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/distinct_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/distinct_count.py`

 * *Files 8% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 Distinct Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 from sqlalchemy import column, distinct, func
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
@@ -39,20 +38,21 @@
     def metric_type(self):
         return int
 
     @_label
     def fn(self):
         return func.count(distinct(column(self.col.name)))
 
-    def df_fn(self, df=None):
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        df = cast(DataFrame, df)
+    def df_fn(self, dfs=None):
+        from collections import Counter  # pylint: disable=import-outside-toplevel
 
         try:
-            return len(set(df[self.col.name].values.tolist()))
+            counter = Counter()
+            for df in dfs:
+                counter.update(df[self.col.name].dropna().to_list())
+            return len(counter.keys())
         except Exception as err:
             logger.debug(
-                f"Don't know how to process type {self.col.type} "
+                f"Don't know how to process type {self.col.type}"
                 f"when computing Distinct Count.\n Error: {err}"
             )
             return 0
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/ilike_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/ilike_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/like_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/like_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/max.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/max.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 Max Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 from sqlalchemy import column, func
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.registry import is_date_time, is_quantifiable
 
 
@@ -36,21 +35,12 @@
     @_label
     def fn(self):
         """sqlalchemy function"""
         if (not is_quantifiable(self.col.type)) and (not is_date_time(self.col.type)):
             return None
         return func.max(column(self.col.name))
 
-    # pylint: disable=import-outside-toplevel
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """pandas function"""
-        from pandas import DataFrame
-
-        df = cast(DataFrame, df)
-
         if is_quantifiable(self.col.type) or is_date_time(self.col.type):
-            return (
-                df[self.col.name].max()
-                if not isinstance(df[self.col.name].max(), list)
-                else df[self.col.name].apply(max).max()
-            )
+            return max((df[self.col.name].max() for df in dfs))
         return 0
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/max_length.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/max_length.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 MAX_LENGTH Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 from sqlalchemy import column, func
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.length import LenFn
 from metadata.profiler.orm.registry import is_concatenable
 from metadata.utils.logger import profiler_logger
@@ -54,23 +53,21 @@
 
         logger.debug(
             f"Don't know how to process type {self.col.type} when computing MAX_LENGTH"
         )
         return None
 
     # pylint: disable=import-outside-toplevel
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """dataframe function"""
-        import pandas as pd
         from numpy import vectorize
 
-        df = cast(pd.DataFrame, df)  # satisfy mypy
-
+        length_vectorize_func = vectorize(len)
         if self._is_concatenable():
-            length_vector_fn = vectorize(len)
-            return length_vector_fn(
-                df[self.col.name][~df[self.col.name].isnull()]
-            ).max()
+            return max(
+                length_vectorize_func(df[self.col.name].dropna().astype(str)).max()
+                for df in dfs
+            )
         logger.debug(
             f"Don't know how to process type {self.col.type} when computing MAX_LENGTH"
         )
-        return 0
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/mean.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/min_length.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,89 +6,69 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-AVG Metric definition
+MIN_LENGTH Metric definition
 """
 # pylint: disable=duplicate-code
 
 
-from typing import cast
-
 from sqlalchemy import column, func
-from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.sql.functions import GenericFunction
 
-from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
+from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import Dialects, is_concatenable, is_quantifiable
+from metadata.profiler.orm.registry import is_concatenable
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
-# pylint: disable=invalid-name
-class avg(GenericFunction):
-    name = "avg"
-    inherit_cache = CACHE
-
-
-@compiles(avg, Dialects.ClickHouse)
-def _(element, compiler, **kw):
-    """Handle case for empty table. If empty, clickhouse returns NaN"""
-    proc = compiler.process(element.clauses, **kw)
-    return f"if(isNaN(avg({proc})), null, avg({proc}))"
 
-
-class Mean(StaticMetric):
+class MinLength(StaticMetric):
     """
-    AVG Metric
+    MIN_LENGTH Metric
 
-    Given a column, return the AVG value.
+    Given a column, return the MIN LENGTH value.
 
-    - For a quantifiable value, return the usual AVG
-    - For a concatenable (str, text...) return the AVG length
+    Only works for concatenable types
     """
 
     @classmethod
     def name(cls):
-        return "mean"
+        return "minLength"
 
     @property
     def metric_type(self):
-        return float
+        return int
+
+    def _is_concatenable(self):
+        return is_concatenable(self.col.type)
 
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if is_quantifiable(self.col.type):
-            return func.avg(column(self.col.name))
-
-        if is_concatenable(self.col.type):
-            return func.avg(LenFn(column(self.col.name)))
+        if self._is_concatenable():
+            return func.min(LenFn(column(self.col.name)))
 
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MEAN"
+            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
         )
         return None
 
     # pylint: disable=import-outside-toplevel
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """dataframe function"""
         from numpy import vectorize
-        from pandas import DataFrame
-
-        df = cast(DataFrame, df)  # satisfy mypy
-
-        if is_quantifiable(self.col.type):
-            return df[self.col.name].mean()
 
-        if is_concatenable(self.col.type):
-            length_vector_fn = vectorize(len)
-            return length_vector_fn(df[self.col.name]).mean()
+        length_vectorize_func = vectorize(len)
 
-        logger.warning(
-            f"Don't know how to process type {self.col.type} when computing MEAN"
+        if self._is_concatenable():
+            return min(
+                length_vectorize_func(df[self.col.name].dropna().astype(str)).min()
+                for df in dfs
+            )
+        logger.debug(
+            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
         )
-        return 0
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/min.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/min.py`

 * *Files 13% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 Min Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 from sqlalchemy import column, func
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.registry import is_date_time, is_quantifiable
 
 
@@ -36,21 +35,12 @@
     @_label
     def fn(self):
         """sqlalchemy function"""
         if (not is_quantifiable(self.col.type)) and (not is_date_time(self.col.type)):
             return None
         return func.min(column(self.col.name))
 
-    # pylint: disable=import-outside-toplevel
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """pandas function"""
-        from pandas import DataFrame
-
-        df = cast(DataFrame, df)
-
         if is_quantifiable(self.col.type) or is_date_time(self.col.type):
-            return (
-                df[self.col.name].min()
-                if not isinstance(df[self.col.name].min(), list)
-                else df[self.col.name].apply(max).max()
-            )
+            return min((df[self.col.name].min() for df in dfs))
         return 0
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/min_length.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/first_quartile.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,71 +6,81 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-MIN_LENGTH Metric definition
+First Quartile definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
+from typing import List, cast
 
-from sqlalchemy import column, func
+from sqlalchemy import column
 
 from metadata.profiler.metrics.core import StaticMetric, _label
-from metadata.profiler.orm.functions.length import LenFn
-from metadata.profiler.orm.registry import is_concatenable
+from metadata.profiler.orm.functions.median import MedianFn
+from metadata.profiler.orm.registry import is_quantifiable
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
-class MinLength(StaticMetric):
+class FirstQuartile(StaticMetric):
     """
-    MIN_LENGTH Metric
+    First Quartile Metric
 
-    Given a column, return the MIN LENGTH value.
+    Given a column, return the first quartile value.
 
-    Only works for concatenable types
+    - For a quantifiable value, return first quartile value
     """
 
     @classmethod
     def name(cls):
-        return "minLength"
+        return "firstQuartile"
+
+    @classmethod
+    def is_window_metric(cls):
+        return True
 
     @property
     def metric_type(self):
-        return int
-
-    def _is_concatenable(self):
-        return is_concatenable(self.col.type)
+        return float
 
     @_label
     def fn(self):
         """sqlalchemy function"""
-        if self._is_concatenable():
-            return func.min(LenFn(column(self.col.name)))
+        if is_quantifiable(self.col.type):
+            return MedianFn(column(self.col.name), self.col.table.name, 0.25)
 
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
+            f"Don't know how to process type {self.col.type} when computing First Quartile"
         )
         return None
 
-    # pylint: disable=import-outside-toplevel
-    def df_fn(self, df=None):
-        """dataframe function"""
-        from numpy import vectorize
-        from pandas import DataFrame
-
-        df = cast(DataFrame, df)  # satisfy mypy
-
-        if self._is_concatenable():
-            length_vector_fn = vectorize(len)
-            return length_vector_fn(
-                df[self.col.name][~df[self.col.name].isnull()]
-            ).min()
+    def df_fn(self, dfs=None):
+        """Dataframe function"""
+        # pylint: disable=import-outside-toplevel
+        import pandas as pd
+
+        df = cast(List[pd.DataFrame], dfs)
+
+        if is_quantifiable(self.col.type):
+            # we can't compute the first quartile unless we have
+            # the entire set. Median of Medians could be used
+            # though it would required set to be sorted before hand
+            try:
+                df = pd.concat(dfs)
+            except MemoryError:
+                logger.error(
+                    f"Unable to compute Median for {self.col.name} due to memory constraints."
+                    f"We recommend using a smaller sample size or partitionning."
+                )
+                return None
+            # check if nan
+            first_quartile = df[self.col.name].quantile(0.25, interpolation="midpoint")
+            return None if pd.isnull(first_quartile) else first_quartile
         logger.debug(
-            f"Don't know how to process type {self.col.type} when computing MIN_LENGTH"
+            f"Don't know how to process type {self.col.type} when computing First Quartile"
         )
-        return 0
+        return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/not_like_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/not_like_count.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/not_regexp_match_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/not_regexp_match_count.py`

 * *Files 12% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 """
 # pylint: disable=duplicate-code
 
 from sqlalchemy import case, column, not_
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
+from metadata.profiler.orm.registry import is_concatenable
 
 
 class NotRegexCount(StaticMetric):
     """
     NOT_REGEX_COUNT Metric
 
     Given a column, and an expression, return the number of
@@ -37,31 +38,40 @@
     def name(cls):
         return "notRegexCount"
 
     @property
     def metric_type(self):
         return int
 
+    def _is_concatenable(self):
+        return is_concatenable(self.col.type)
+
     @_label
     def fn(self):
         """sqlalchemy function"""
         if not hasattr(self, "expression"):
             raise AttributeError(
                 "Not Regex Count requires an expression to be set: add_props(expression=...)(Metrics.NOT_REGEX_COUNT)"
             )
         return SumFn(
             case(
                 [(not_(column(self.col.name).regexp_match(self.expression)), 0)],
                 else_=1,
             )
         )
 
-    def df_fn(self, df):
+    def df_fn(self, dfs):
         """pandas function"""
         if not hasattr(self, "expression"):
             raise AttributeError(
                 "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
             )
-
-        return df[self.col.name][
-            df[self.col.name].str.contains(self.expression)
-        ].count()
+        if self._is_concatenable():
+            return sum(
+                df[self.col.name][
+                    df[self.col.name].astype(str).str.contains(self.expression)
+                ].count()
+                for df in dfs
+            )
+        raise TypeError(
+            f"Don't know how to process type {self.col.type} when computing Not RegExp Match Count"
+        )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/null_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/null_count.py`

 * *Files 13% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 #  limitations under the License.
 
 """
 Null Count Metric definition
 """
 # pylint: disable=duplicate-code
 
-from typing import cast
 
 from sqlalchemy import case, column
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
 
 
@@ -46,13 +45,10 @@
         return int
 
     @_label
     def fn(self):
         """sqlalchemy function"""
         return SumFn(case([(column(self.col.name).is_(None), 1)], else_=0))
 
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """pandas function"""
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        df = cast(DataFrame, df)
-        return df[self.col.name].isnull().sum()
+        return sum(df[self.col.name].isnull().sum() for df in dfs)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/regexp_match_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/regexp_match_count.py`

 * *Files 22% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 """
 # pylint: disable=duplicate-code
 
 from sqlalchemy import case, column
 
 from metadata.profiler.metrics.core import StaticMetric, _label
 from metadata.profiler.orm.functions.sum import SumFn
+from metadata.profiler.orm.registry import is_concatenable
 
 
 class RegexCount(StaticMetric):
     """
     REGEX_COUNT Metric
 
     Given a column, and an expression, return the number of
@@ -37,29 +38,38 @@
     def name(cls):
         return "regexCount"
 
     @property
     def metric_type(self):
         return int
 
+    def _is_concatenable(self):
+        return is_concatenable(self.col.type)
+
     @_label
     def fn(self):
         """sqlalchemy function"""
         if not hasattr(self, "expression"):
             raise AttributeError(
                 "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
             )
         return SumFn(
             case([(column(self.col.name).regexp_match(self.expression), 1)], else_=0)
         )
 
-    def df_fn(self, df):
+    def df_fn(self, dfs):
         """pandas function"""
 
         if not hasattr(self, "expression"):
             raise AttributeError(
                 "Regex Count requires an expression to be set: add_props(expression=...)(Metrics.REGEX_COUNT)"
             )
-
-        return df[self.col.name][
-            df[self.col.name].str.contains(self.expression)
-        ].count()
+        if self._is_concatenable():
+            return sum(
+                df[self.col.name][
+                    df[self.col.name].astype(str).str.contains(self.expression)
+                ].count()
+                for df in dfs
+            )
+        raise TypeError(
+            f"Don't know how to process type {self.col.type} when computing RegExp Match Count"
+        )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/row_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/uuid.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,51 +6,58 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Table Count Metric definition
+Expand sqlalchemy types to map them to OpenMetadata DataType
 """
-# pylint: disable=duplicate-code
+# pylint: disable=duplicate-code,abstract-method
+import traceback
+from uuid import UUID
 
-from typing import cast
+from sqlalchemy.sql.sqltypes import String, TypeDecorator
 
-from sqlalchemy import func
+from metadata.utils.logger import profiler_logger
 
-from metadata.profiler.metrics.core import StaticMetric, _label
+logger = profiler_logger()
 
 
-class RowCount(StaticMetric):
+class UUIDString(TypeDecorator):
     """
-    ROW_NUMBER Metric
-
-    Count all rows on a table
+    Convert Python bytestring to string with hexadecimal digits and back for storage.
     """
 
-    @classmethod
-    def name(cls):
-        return "rowCount"
-
-    @classmethod
-    def is_col_metric(cls) -> bool:
-        """
-        Mark the class as a Table Metric
-        """
-        return False
+    impl = String
+    cache_ok = True
 
     @property
-    def metric_type(self):
-        return int
+    def python_type(self):
+        return str
 
-    @_label
-    def fn(self):
-        """sqlalchemy function"""
-        return func.count()
-
-    def df_fn(self, df=None):
-        """pandas function"""
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
+    @staticmethod
+    def validate(value: str) -> UUID:
+        """
+        Make sure the data is of correct type
+        """
+        try:
+            return UUID(value)
+        except ValueError as err:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Error converting value [{value}] to UUID: {err}")
+            raise err
+
+    def process_result_value(self, value: str, dialect):
+        """This is executed during result retrieval
+
+        Args:
+            value: database record
+            dialect: database dialect
+        Returns:
+            hex string representation of the byte value
+        """
+        return value
 
-        df = cast(DataFrame, df)
-        return len(df.index)
+    def process_literal_param(self, value, dialect):
+        self.validate(value)
+        return value
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/stddev.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/stddev.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 
 """
 Population Standard deviation Metric definition
 """
 
 # Keep SQA docs style defining custom constructs
 # pylint: disable=consider-using-f-string,duplicate-code
-from typing import cast
+
 
 from sqlalchemy import column
 from sqlalchemy.ext.compiler import compiles
 from sqlalchemy.sql.functions import FunctionElement
 
 from metadata.profiler.metrics.core import CACHE, StaticMetric, _label
 from metadata.profiler.orm.registry import Dialects, is_quantifiable
@@ -86,23 +86,26 @@
 
         logger.debug(
             f"{self.col} has type {self.col.type}, which is not listed as quantifiable."
             + " We won't compute STDDEV for it."
         )
         return None
 
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """pandas function"""
         import pandas as pd  # pylint: disable=import-outside-toplevel
 
-        df = cast(pd.DataFrame, df)
-
         if is_quantifiable(self.col.type):
-            stddev = df[self.col.name].std()
-            if pd.isnull(stddev):
+            try:
+                return pd.concat(df[self.col.name] for df in dfs).std()
+            except MemoryError:
+                logger.error(
+                    f"Unable to compute distinctCount for {self.col.name} due to memory constraints."
+                    f"We recommend using a smaller sample size or partitionning."
+                )
                 return None
-            return stddev
+
         logger.debug(
             f"{self.col.name} has type {self.col.type}, which is not listed as quantifiable."
             + " We won't compute STDDEV for it."
         )
         return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/sum.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/sum.py`

 * *Files 12% similar despite different names*

```diff
@@ -38,16 +38,12 @@
     def fn(self):
         """sqlalchemy function"""
         if is_quantifiable(self.col.type):
             return SumFn(column(self.col.name))
 
         return None
 
-    def df_fn(self, df):
+    def df_fn(self, dfs=None):
         """pandas function"""
         if is_quantifiable(self.col.type):
-            return (
-                df[self.col.name].sum()
-                if not isinstance(df[self.col.name].sum(), list)
-                else df[self.col.name].apply(sum).sum()
-            )
+            return sum(df[self.col.name].sum() for df in dfs)
         return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/static/unique_count.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/unique_count.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Unique Count Metric definition
 """
-from typing import Optional, cast
+from typing import Optional
 
 from sqlalchemy import column, func
 from sqlalchemy.orm import DeclarativeMeta, Session
 
 from metadata.profiler.metrics.core import QueryMetric
 from metadata.profiler.orm.registry import NOT_COMPUTE
 from metadata.utils.logger import profiler_logger
@@ -61,23 +61,24 @@
             .group_by(col)
             .having(func.count(col) == 1)  # Values that appear only once
         )
 
         only_once_cte = only_once.cte("only_once")
         return session.query(func.count().label(self.name())).select_from(only_once_cte)
 
-    def df_fn(self, df=None):
+    def df_fn(self, dfs=None):
         """
         Build the Unique Count metric
         """
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        df = cast(DataFrame, df)
+        from collections import Counter  # pylint: disable=import-outside-toplevel
 
         try:
-            return df[self.col.name].nunique()
+            counter = Counter()
+            for df in dfs:
+                counter.update(df[self.col.name].dropna().to_list())
+            return len([key for key, value in counter.items() if value == 1])
         except Exception as err:
             logger.debug(
                 f"Don't know how to process type {self.col.type}"
-                f"when computing Distinct Count.\n Error: {err}"
+                f"when computing Unique Count.\n Error: {err}"
             )
             return 0
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/system/system.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/system/system.py`

 * *Files 5% similar despite different names*

```diff
@@ -36,14 +36,16 @@
 DML_OPERATION_MAP = {
     "INSERT": "INSERT",
     "MERGE": "UPDATE",
     "UPDATE": "UPDATE",
     "DELETE": "DELETE",
 }
 
+SYSTEM_QUERY_RESULT_CACHE = {}
+
 
 @valuedispatch
 def get_system_metrics_for_dialect(
     dialect: str,
     session: Session,
     table: DeclarativeMeta,
     *args,
@@ -110,24 +112,29 @@
 
     metric_results: List[Dict] = []
     QueryResult = namedtuple(
         "QueryResult",
         "query_type,timestamp,destination_table,dml_statistics",
     )
 
-    cursor_jobs = session.execute(text(jobs))
-    rows_jobs = [
-        QueryResult(
-            row.statement_type,
-            row.start_time,
-            row.destination_table,
-            row.dml_statistics,
-        )
-        for row in cursor_jobs.fetchall()
-    ]
+    try:
+        # we'll try to get the cached data first
+        rows_jobs = kwargs["cache"][Dialects.BigQuery]["rows_jobs"]
+    except KeyError:
+        cursor_jobs = session.execute(text(jobs))
+        rows_jobs = [
+            QueryResult(
+                row.statement_type,
+                row.start_time,
+                row.destination_table,
+                row.dml_statistics,
+            )
+            for row in cursor_jobs.fetchall()
+        ]
+        SYSTEM_QUERY_RESULT_CACHE[Dialects.BigQuery] = {"rows_jobs": rows_jobs}
 
     for row_jobs in rows_jobs:
         if (
             row_jobs.destination_table.get("project_id") == session.get_bind().url.host
             and row_jobs.destination_table.get("dataset_id")
             == table.__table_args__["schema"]
             and row_jobs.destination_table.get("table_id") == table.__tablename__
@@ -200,15 +207,15 @@
             INNER JOIN  pg_catalog.svv_table_info sti ON si.tbl = sti.table_id
             INNER JOIN pg_catalog.stl_querytext sq ON si.query = sq.query
         WHERE
             sti."database" = '{session.get_bind().url.database}' AND
             sti."schema" = '{table.__table_args__["schema"]}' AND
             sti."table" = '{table.__tablename__}' AND
             "rows" != 0 AND
-            DATE(starttime) = CURRENT_DATE - 1
+            DATE(starttime) >= CURRENT_DATE - 1
         GROUP BY 2,3,4,5,6
         ORDER BY 6 desc
         """
     )
 
     stl_insert = dedent(
         f"""
@@ -224,15 +231,15 @@
             INNER JOIN  pg_catalog.svv_table_info sti ON si.tbl = sti.table_id
             INNER JOIN pg_catalog.stl_querytext sq ON si.query = sq.query
         WHERE
             sti."database" = '{session.get_bind().url.database}' AND
             sti."schema" = '{table.__table_args__["schema"]}' AND
             sti."table" = '{table.__tablename__}' AND
             "rows" != 0 AND
-            DATE(starttime) = CURRENT_DATE - 1
+            DATE(starttime) >= CURRENT_DATE - 1
         GROUP BY 2,3,4,5,6
         ORDER BY 6 desc
         """
     )
 
     metric_results: List[Dict] = []
     QueryResult = namedtuple(
@@ -348,31 +355,35 @@
     """
 
     QueryResult = namedtuple(
         "QueryResult",
         "query_id,database_name,schema_name,query_text,query_type,timestamp",
     )
 
-    rows = []
-
-    # limit of results is 10K. We'll query range of 1 hours to make sure we
-    # get all the necessary data.
-    rows = session.execute(text(information_schema_query_history)).fetchall()
-
-    query_results = [
-        QueryResult(
-            row.query_id,
-            row.database_name.lower() if row.database_name else None,
-            row.schema_name.lower() if row.schema_name else None,
-            sqlparse.parse(row.query_text)[0],
-            row.query_type,
-            row.start_time,
-        )
-        for row in rows
-    ]
+    try:
+        # we'll try to get the cached data first
+        rows = kwargs["cache"][Dialects.Snowflake]["rows"]
+        query_results = kwargs["cache"][Dialects.Snowflake]["query_results"]
+    except KeyError:
+        rows = session.execute(text(information_schema_query_history)).fetchall()
+        query_results = [
+            QueryResult(
+                row.query_id,
+                row.database_name.lower() if row.database_name else None,
+                row.schema_name.lower() if row.schema_name else None,
+                sqlparse.parse(row.query_text)[0],
+                row.query_type,
+                row.start_time,
+            )
+            for row in rows
+        ]
+        SYSTEM_QUERY_RESULT_CACHE[Dialects.Snowflake] = {
+            "rows": rows,
+            "query_results": query_results,
+        }
 
     for query_result in query_results:
         query_text = query_result.query_text
         identifier = next(
             (
                 query_el
                 for query_el in query_text.tokens
@@ -455,10 +466,11 @@
         conn_config = kwargs.get("conn_config")
 
         system_metrics = get_system_metrics_for_dialect(
             session.get_bind().dialect.name,
             session=session,
             table=self.table,
             conn_config=conn_config,
+            cache=SYSTEM_QUERY_RESULT_CACHE,
         )
 
         return system_metrics
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/first_quartile.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/registry.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,80 +6,103 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-First Quartile definition
+Registry definition.
+
+A Registry is a "smarter" Enum, which we'll
+use to control what classes we have available to
+use, validating that they are of proper type,
+and we can easily access the desired values.
 """
-# pylint: disable=duplicate-code
 
-from typing import List, cast
+from enum import Enum
+from typing import Optional
 
-from sqlalchemy import column
+from sqlalchemy.sql.sqltypes import TypeDecorator
 
-from metadata.profiler.metrics.core import StaticMetric, _label
-from metadata.profiler.orm.functions.median import MedianFn
-from metadata.profiler.orm.registry import is_quantifiable
-from metadata.utils.logger import profiler_logger
+from metadata.profiler.metrics.core import Metric
 
-logger = profiler_logger()
 
+class MetricRegistry(Enum):
+    """
+    Lets us __call__ values.
+    Used for our profiler registries of metrics.
 
-class FirstQuartile(StaticMetric):
+    Instead of:
+    - StaticMetrics.MIN.value(col)
+    We can use:
+    - StaticMetrics.MIN(col)
     """
-    First Quartile Metric
 
-    Given a column, return the first quartile value.
+    def __init__(self, metric):
+        if not issubclass(metric, Metric):
+            raise TypeError(
+                "Only Metrics can be part of the Metric Registry,"
+                + f" but found {type(metric)} instead."
+            )
+        self.metric = metric
+
+    def __call__(self, *args, **kwargs):
+        """
+        Allow to __init__ the mapped class directly
+
+        We run this as Metrics.MIN(col)
+        """
+        return self.value(*args, **kwargs)
 
-    - For a quantifiable value, return first quartile value
-    """
+    @property
+    def name(  # pylint: disable=function-redefined, invalid-overridden-method
+        self,
+    ) -> str:
+        """
+        Override the default `name` on Enums
+        to use the mapped class name instead.
+
+        name is a classmethod on Metrics, so
+        we do not need to __init__ here.
+
+        We run this as Metrics.MIN.name
+        """
+        return self.value.name()
+
+    def __str__(self):
+        return self.value.name()
 
     @classmethod
-    def name(cls):
-        return "firstQuartile"
+    def get(cls, key: str) -> Optional[Metric]:
+        """
+        Safely retrieve an element
+        from the Registry.
+
+        Added at class level. Execute from Metrics directly,
+        e.g., Metrics.get("MIN")
+        """
+        try:
+            return cls[key].value
+        except KeyError:
+            return None
 
     @classmethod
-    def is_window_metric(cls):
-        return True
+    def init(cls, key: str, *args, **kwargs):
+        try:
+            return cls[key](*args, **kwargs)
+        except KeyError:
+            return None
 
-    @property
-    def metric_type(self):
-        return float
 
-    @_label
-    def fn(self):
-        """sqlalchemy function"""
-        if is_quantifiable(self.col.type):
-            return MedianFn(column(self.col.name), self.col.table.name, 0.25)
-
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing First Quartile"
-        )
-        return None
-
-    def df_fn(self, dfs=None):
-        """Dataframe function"""
-        # pylint: disable=import-outside-toplevel
-        import numpy as np
-        import pandas as pd
-
-        df = cast(List[pd.DataFrame], dfs)
-
-        if is_quantifiable(self.col.type):
-            # we can't compute the first quartile unless we have
-            # the entire set. Median of Medians could be used
-            # though it would required set to be sorted before hand
-            try:
-                df = pd.concat(dfs)
-            except MemoryError:
-                logger.error(
-                    f"Unable to compute Median for {self.col.name} due to memory constraints."
-                    f"We recommend using a smaller sample size or partitionning."
-                )
-                return None
-            return np.percentile(df[self.col.name], 25)
-        logger.debug(
-            f"Don't know how to process type {self.col.type} when computing First Quartile"
-        )
-        return None
+class TypeRegistry(Enum):
+    """
+    Used to validate that we are passing proper
+    TypeDecorators to our Type Registry
+    """
+
+    def __init__(self, _type):
+        if not issubclass(_type, TypeDecorator):
+            raise TypeError(
+                "Only Metrics can be part of the Metric Registry,"
+                + f" but found {type(_type)} instead."
+            )
+        self._type = _type
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/median.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/median.py`

 * *Files 10% similar despite different names*

```diff
@@ -65,21 +65,20 @@
         dfs = cast(List[pd.DataFrame], dfs)
 
         if is_quantifiable(self.col.type):
             # we can't compute the median unless we have
             # the entire set. Median of Medians could be used
             # though it would required set to be sorted before hand
             try:
-                df = (
-                    pd.concat(dfs) if isinstance(dfs, list) else dfs
-                )  # workaround should be removed once #10351 is fixed
+                df = pd.concat(dfs)
             except MemoryError:
                 logger.error(
                     f"Unable to compute Median for {self.col.name} due to memory constraints."
                     f"We recommend using a smaller sample size or partitionning."
                 )
                 return None
-            return df[self.col.name].median()
+            median = df[self.col.name].median()
+            return None if pd.isnull(median) else median
         logger.debug(
             f"Don't know how to process type {self.col.type} when computing Median"
         )
         return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/metrics/window/third_quartile.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/window/third_quartile.py`

 * *Files 16% similar despite different names*

```diff
@@ -57,15 +57,14 @@
             f"Don't know how to process type {self.col.type} when computing Third Quartile"
         )
         return None
 
     def df_fn(self, dfs=None):
         """Dataframe function"""
         # pylint: disable=import-outside-toplevel
-        import numpy as np
         import pandas as pd
 
         df = cast(List[pd.DataFrame], dfs)
 
         if is_quantifiable(self.col.type):
             # we can't compute the median unless we have
             # the entire set. Median of Medians could be used
@@ -74,12 +73,14 @@
                 df = pd.concat(dfs)
             except MemoryError:
                 logger.error(
                     f"Unable to compute Median for {self.col.name} due to memory constraints."
                     f"We recommend using a smaller sample size or partitionning."
                 )
                 return None
-            return np.percentile(df[self.col.name], 75)
+            # check if nan
+            third_quartile = df[self.col.name].quantile(0.75, interpolation="midpoint")
+            return None if pd.isnull(third_quartile) else third_quartile
         logger.debug(
             f"Don't know how to process type {self.col.type} when computing Third Quartile"
         )
         return None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/converter.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/converter.py`

 * *Files 2% similar despite different names*

```diff
@@ -101,39 +101,54 @@
     """
     if table_service_type == databaseService.DatabaseServiceType.Snowflake:
         return True if not str(table_or_col).isupper() else None
 
     return None
 
 
+def check_if_should_quote_column_name(table_service_type) -> Optional[bool]:
+    """Check whether column name should be quoted when passed into the sql command build up.
+    This is important when a column name is the same as a reserve word and causes a sql error.
+
+    Args:
+        table_service_type: the main sql engine to determine if we should always quote.
+    Return: True or False
+    """
+
+    if table_service_type == databaseService.DatabaseServiceType.Hive:
+        return True
+
+    return None
+
+
 def build_orm_col(
     idx: int, col: Column, table_service_type, parent: Optional[str] = None
 ) -> sqlalchemy.Column:
     """
     Cook the ORM column from our metadata instance
     information.
 
     The first parsed column will be used arbitrarily
     as the PK, as SQLAlchemy forces us to specify
     at least one PK.
 
     As this is only used for INSERT/UPDATE/DELETE,
     there is no impact for our read-only purposes.
     """
-
     if parent:
         name = f"{parent}.{col.name.__root__}"
     else:
         name = col.name.__root__
 
     return sqlalchemy.Column(
         name=str(name),
         type_=map_types(col, table_service_type),
         primary_key=not bool(idx),  # The first col seen is used as PK
-        quote=check_snowflake_case_sensitive(table_service_type, col.name.__root__),
+        quote=check_if_should_quote_column_name(table_service_type)
+        or check_snowflake_case_sensitive(table_service_type, col.name.__root__),
         key=str(
             col.name.__root__
         ).lower(),  # Add lowercase column name as key for snowflake case sensitive columns
     )
 
 
 def get_columns(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/concat.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/concat.py`

 * *Files 2% similar despite different names*

```diff
@@ -34,14 +34,13 @@
     return "CONCAT(%s)" % compiler.process(element.clauses, **kw)
 
 
 @compiles(ConcatFn, Dialects.Redshift)
 @compiles(ConcatFn, Dialects.SQLite)
 @compiles(ConcatFn, Dialects.Vertica)
 def _(element, compiler, **kw):
-
     if len(element.clauses) < 2:
         raise ValueError("We need to concat at least two elements")
 
     concat = "||".join([compiler.process(elem, **kw) for elem in element.clauses])
 
     return concat
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/conn_test.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/conn_test.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/datetime.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/datetime.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/length.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/length.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/median.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/median.py`

 * *Files 20% similar despite different names*

```diff
@@ -69,29 +69,79 @@
     return "percentile_cont(%.1f) WITHIN GROUP (ORDER BY %s ASC) OVER()" % (
         percentile,
         col,
     )
 
 
 @compiles(MedianFn, Dialects.Hive)
-@compiles(MedianFn, Dialects.Impala)
 def _(elements, compiler, **kwargs):
     """Median computation for Hive"""
     col, _, percentile = [
         compiler.process(element, **kwargs) for element in elements.clauses
     ]
     return "percentile(cast(%s as BIGINT), %s)" % (col, percentile)
 
 
+@compiles(MedianFn, Dialects.Impala)
+def _(elements, compiler, **kwargs):
+    """Median computation for Impala
+    Median compution for Impala uses the appx_median function.
+    OM uses this median function to also compute first and third quartiles.
+    These calculations are not supported with a simple function inside Impala.
+    The if statement returns null when we are not looking for the .5 precentile
+    In Impala to get the first quartile a full SQL statement like this is necessary:
+        with ntiles as
+        (
+        select filesize, ntile(4) over (order by filesize) as quarter
+        from hdfs_files
+        )
+        , quarters as
+        (
+        select 1 as grp, max(filesize) as quartile_value, quarter
+            from ntiles
+        group by quarter
+        )
+        select max(case when quarter = 1 then quartile_value end) as first_q
+        , max(case when quarter = 2 then quartile_value end) as second_q
+        , max(case when quarter = 3 then quartile_value end) as third_q
+        , max(case when quarter = 4 then quartile_value end) as fourth_q
+        from quarters
+        group by grp
+        ;
+    """
+    col, _, percentile = [
+        compiler.process(element, **kwargs) for element in elements.clauses
+    ]
+    return "if(%s = .5, appx_median(%s), null)" % (percentile, col)
+
+
 @compiles(MedianFn, Dialects.MySQL)
 def _(elements, compiler, **kwargs):  # pylint: disable=unused-argument
-    """Median computation for MySQL currently not supported
-    Needs to be tackled in https://github.com/open-metadata/OpenMetadata/issues/6340
-    """
-    return "NULL"
+    """Median computation for MySQL"""
+    col = compiler.process(elements.clauses.clauses[0])
+    table = elements.clauses.clauses[1].value
+    percentile = elements.clauses.clauses[2].value
+
+    return """
+    (SELECT
+        {col}
+    FROM (
+        SELECT
+            t.{col}, 
+            ROW_NUMBER() OVER () AS row_num
+        FROM 
+            {table} t,
+            (SELECT @counter := COUNT(*) FROM {table}) t_count 
+        ORDER BY {col}
+        ) temp
+    WHERE temp.row_num = ROUND({percentile} * @counter)
+    )
+    """.format(
+        col=col, table=table, percentile=percentile
+    )
 
 
 @compiles(MedianFn, Dialects.SQLite)
 def _(elements, compiler, **kwargs):  # pylint: disable=unused-argument
     col = compiler.process(elements.clauses.clauses[0])
     table = elements.clauses.clauses[1].value
     percentile = elements.clauses.clauses[2].value
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/modulo.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/modulo.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/random_num.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/random_num.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/functions/sum.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/functions/sum.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/registry.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/registry.py`

 * *Files 0% similar despite different names*

```diff
@@ -97,14 +97,15 @@
     DataType.LONG.value,
 }
 
 CONCATENABLE_SET = {DataType.STRING.value, DataType.TEXT.value}
 
 DATATIME_SET = {DataType.DATETIME.value}
 
+
 # Now, let's define some helper methods to identify
 # the nature of an SQLAlchemy type
 def is_integer(_type) -> bool:
     """
     Check if sqlalchemy _type is derived from Integer
     """
     return issubclass(_type.__class__, Integer)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/bytea_to_string.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/bytea_to_string.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/hex_byte_string.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/orm/types/hex_byte_string.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/orm/types/uuid.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/client_version.py`

 * *Files 26% similar despite different names*

```diff
@@ -4,60 +4,48 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
-"""
-Expand sqlalchemy types to map them to OpenMetadata DataType
 """
-# pylint: disable=duplicate-code,abstract-method
-import traceback
-from uuid import UUID
-
-from sqlalchemy.sql.sqltypes import String, TypeDecorator
+Mixin class containing Server and client specific methods
 
-from metadata.utils.logger import profiler_logger
+To be used by OpenMetadata class
+"""
+import re
 
-logger = profiler_logger()
+try:
+    from importlib.metadata import version
+except ImportError:
+    from importlib_metadata import version
 
 
-class UUIDString(TypeDecorator):
+class VersionParsingException(Exception):
     """
-    Convert Python bytestring to string with hexadecimal digits and back for storage.
+    Used when we cannot parse version information from a string
     """
 
-    impl = String
-    cache_ok = True
-
-    @property
-    def python_type(self):
-        return str
 
-    @staticmethod
-    def validate(value: str) -> UUID:
-        """
-        Make sure the data is of correct type
-        """
-        try:
-            return UUID(value)
-        except ValueError as err:
-            logger.debug(traceback.format_exc())
-            logger.error(f"Error converting value [{value}] to UUID: {err}")
-            raise err
-
-    def process_result_value(self, value: str, dialect):
-        """This is executed during result retrieval
+def get_version_from_string(raw_version: str) -> str:
+    """
+    Given a raw version string, such as `0.10.1.dev0` or
+    `0.11.0-SNAPSHOT`, we should extract the major.minor.patch
+    :param raw_version: raw string with version info
+    :return: Clean version string
+    """
+    try:
+        return re.match(r"\d+.\d+.\d+", raw_version).group(0)
+    except AttributeError as err:
+        raise VersionParsingException(
+            f"Can't extract version from {raw_version}: {err}"
+        ) from err
 
-        Args:
-            value: database record
-            dialect: database dialect
-        Returns:
-            hex string representation of the byte value
-        """
-        return value
 
-    def process_literal_param(self, value, dialect):
-        self.validate(value)
-        return value
+def get_client_version() -> str:
+    """
+    Get openmetadata-ingestion module version
+    :return: client version
+    """
+    raw_version = version("openmetadata-ingestion")
+    return get_version_from_string(raw_version)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/core.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/core.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,28 +31,28 @@
     ColumnProfilerConfig,
     SystemProfile,
     TableData,
     TableProfile,
 )
 from metadata.ingestion.processor.pii import NERScanner
 from metadata.profiler.api.models import ProfilerResponse
+from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
 from metadata.profiler.metrics.core import (
     ComposedMetric,
     CustomMetric,
     HybridMetric,
     MetricTypes,
     QueryMetric,
     StaticMetric,
     SystemMetric,
     TMetric,
 )
 from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.metrics.static.row_count import RowCount
 from metadata.profiler.orm.registry import NOT_COMPUTE
-from metadata.profiler.profiler.interface.profiler_protocol import ProfilerProtocol
 from metadata.utils.logger import profiler_logger
 
 logger = profiler_logger()
 
 
 class MissingMetricException(Exception):
     """
@@ -392,29 +392,29 @@
             *[
                 (
                     metric,
                     MetricTypes.Query,
                     column,
                     self.table,
                 )
-                for column in self.columns
+                for column in columns
                 for metric in self.get_col_metrics(self.query_metrics, column)
             ],
             *[
                 (
                     [
                         metric
                         for metric in self.get_col_metrics(self.static_metrics, column)
                         if metric.is_window_metric()
                     ],
                     MetricTypes.Window,
                     column,
                     self.table,
                 )
-                for column in self.columns
+                for column in columns
             ],
         ]
 
         return column_metrics_for_thread_pool
 
     def profile_entity(self) -> None:
         """Get all the metrics for a given table"""
@@ -536,15 +536,14 @@
                 "metric2": ...,
             }
         }
 
         We need to transform it to TableProfile
         """
         try:
-
             # There are columns that we might have skipped from
             # computing metrics, if the type is not supported.
             # Let's filter those out.
             column_profile = [
                 ColumnProfile(
                     **self.column_results.get(
                         col.name
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/datalake_sampler.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/datalake_sampler.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,21 +8,18 @@
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 """
 Helper module to handle data sampling
 for the profiler
 """
-import math
-import random
 from typing import Any, Optional
 
-from metadata.generated.schema.entity.data.table import ProfileSampleType, TableData
+from metadata.generated.schema.entity.data.table import TableData
 from metadata.profiler.api.models import ProfileSampleConfig
-from metadata.utils.constants import CHUNKSIZE
 
 RANDOM_LABEL = "random"
 
 
 class DatalakeSampler:
     """
     Generates a sample of the data to not
@@ -44,59 +41,26 @@
         self.session = session
         self.table = table
         self._profile_sample_query = profile_sample_query
         self.sample_limit = 100
         self._sample_rows = None
 
     def _fetch_rows(self, data_frame):
-        from pandas import notnull  # pylint: disable=import-outside-toplevel
-
-        sampled_data_frame = data_frame.sample(
-            n=(int(self.profile_sample) or 100)
-            if self.profile_sample_type == ProfileSampleType.ROWS
-            else None,
-            frac=self.profile_sample
-            if self.profile_sample_type == ProfileSampleType.PERCENTAGE
-            else None,
-            random_state=random.randint(0, 100),
-            replace=True,
-        )
-        return (
-            sampled_data_frame.astype(object)
-            .where(
-                notnull(sampled_data_frame),
-                None,
-            )
-            .values.tolist()
-        )
+        return data_frame.dropna().values.tolist()
 
     def get_col_row(self, data_frame):
         """
         Fetches columns and rows from the data_frame
         """
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
         cols = []
-        chunk = None
-        if isinstance(data_frame, DataFrame):
-            return (
-                data_frame.columns.tolist(),
-                self._fetch_rows(data_frame),
-            )
-        chunk_limit = math.ceil(self.profile_sample / CHUNKSIZE)
-        cols = data_frame[0].columns.tolist()
         rows = []
-        for index, chunk in enumerate(data_frame):
-            if index >= chunk_limit:
+        cols = data_frame[0].columns.tolist()
+        # Sample Data should not exceed sample limit
+        for chunk in data_frame:
+            rows.extend(self._fetch_rows(chunk)[: self.sample_limit])
+            if len(rows) >= self.sample_limit:
                 break
-            rows.extend(self._fetch_rows(chunk))
         return cols, rows
 
     def fetch_dl_sample_data(self) -> TableData:
-        from pandas import DataFrame  # pylint: disable=import-outside-toplevel
-
-        cols, rows = self.get_col_row(
-            data_frame=self.table[0]
-            if not isinstance(self.table, DataFrame)
-            else self.table
-        )
+        cols, rows = self.get_col_row(data_frame=self.table)
         return TableData(columns=cols, rows=rows)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/default.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/default.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,18 +13,18 @@
 Default simple profiler to use
 """
 from typing import List, Optional
 
 from sqlalchemy.orm import DeclarativeMeta
 
 from metadata.generated.schema.entity.data.table import ColumnProfilerConfig
+from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
 from metadata.profiler.metrics.core import Metric, add_props
 from metadata.profiler.metrics.registry import Metrics
-from metadata.profiler.profiler.core import Profiler
-from metadata.profiler.profiler.interface.profiler_protocol import ProfilerProtocol
+from metadata.profiler.processor.core import Profiler
 
 
 def get_default_metrics(table: DeclarativeMeta) -> List[Metric]:
     return [
         # Table Metrics
         Metrics.ROW_COUNT.value,
         add_props(table=table)(Metrics.COLUMN_COUNT.value),
@@ -63,15 +63,14 @@
 
     def __init__(
         self,
         profiler_interface: ProfilerProtocol,
         include_columns: Optional[List[ColumnProfilerConfig]] = None,
         exclude_columns: Optional[List[str]] = None,
     ):
-
         _metrics = get_default_metrics(profiler_interface.table)
 
         super().__init__(
             *_metrics,
             profiler_interface=profiler_interface,
             include_columns=include_columns,
             exclude_columns=exclude_columns,
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/handle_partition.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/handle_partition.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/pandas/pandas_profiler_interface.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/sqlalchemy/sqa_profiler_interface.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,369 +10,542 @@
 #  limitations under the License.
 
 """
 Interfaces with database for all database engine
 supporting sqlalchemy abstraction layer
 """
 
+import concurrent.futures
+import threading
 import traceback
 from collections import defaultdict
 from datetime import datetime, timezone
 from typing import Dict, List
 
 from sqlalchemy import Column
+from sqlalchemy.exc import ProgrammingError
+from sqlalchemy.orm import scoped_session
 
-from metadata.generated.schema.entity.data.table import DataType, TableData
-from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
-    DatalakeConnection,
-)
+from metadata.generated.schema.entity.data.table import TableData
+from metadata.generated.schema.entity.services.databaseService import DatabaseConnection
 from metadata.ingestion.api.processor import ProfilerProcessorStatus
+from metadata.ingestion.connections.session import create_and_bind_thread_safe_session
 from metadata.ingestion.source.connections import get_connection
-from metadata.ingestion.source.database.datalake.metadata import (
-    DATALAKE_DATA_TYPES,
-    ometa_to_dataframe,
-)
-from metadata.interfaces.datalake.mixins.pandas_mixin import PandasInterfaceMixin
+from metadata.mixins.sqalchemy.sqa_mixin import SQAInterfaceMixin
+from metadata.profiler.interface.profiler_protocol import ProfilerProtocol
 from metadata.profiler.metrics.core import MetricTypes
 from metadata.profiler.metrics.registry import Metrics
-from metadata.profiler.profiler.datalake_sampler import DatalakeSampler
-from metadata.profiler.profiler.interface.profiler_protocol import ProfilerProtocol
+from metadata.profiler.metrics.static.mean import Mean
+from metadata.profiler.metrics.static.stddev import StdDev
+from metadata.profiler.metrics.static.sum import Sum
+from metadata.profiler.processor.runner import QueryRunner
+from metadata.profiler.processor.sampler import Sampler
+from metadata.utils.custom_thread_pool import CustomThreadPoolExecutor
 from metadata.utils.dispatch import valuedispatch
 from metadata.utils.logger import profiler_interface_registry_logger
-from metadata.utils.sqa_like_column import SQALikeColumn, Type
 
 logger = profiler_interface_registry_logger()
+thread_local = threading.local()
 
+OVERFLOW_ERROR_CODES = {
+    "snowflake": {100046, 100058},
+}
 
-class PandasProfilerInterface(ProfilerProtocol, PandasInterfaceMixin):
+
+def handle_query_exception(msg, exc, session):
+    """Handle exception for query runs"""
+    logger.debug(traceback.format_exc())
+    logger.warning(msg)
+    session.rollback()
+    raise RuntimeError(exc)
+
+
+class SQAProfilerInterface(ProfilerProtocol, SQAInterfaceMixin):
     """
     Interface to interact with registry supporting
     sqlalchemy.
     """
 
-    _profiler_type: str = DatalakeConnection.__name__
+    _profiler_type: str = DatabaseConnection.__name__
 
     def __init__(
         self,
         service_connection_config,
         ometa_client,
-        thread_count,
         entity,
         profile_sample_config,
         source_config,
         sample_query,
-        table_partition_config=None,
+        table_partition_config,
+        sqa_metadata=None,
+        timeout_seconds=43200,
+        thread_count=5,
         **kwargs,
     ):
         """Instantiate SQA Interface object"""
         self._thread_count = thread_count
         self.table_entity = entity
         self.ometa_client = ometa_client
         self.source_config = source_config
         self.service_connection_config = service_connection_config
-        self.client = get_connection(self.service_connection_config).client
         self.processor_status = ProfilerProcessorStatus()
-        self.processor_status.entity = (
-            self.table_entity.fullyQualifiedName.__root__
-            if self.table_entity.fullyQualifiedName
-            else None
-        )
+        try:
+            fqn = self.table_entity.fullyQualifiedName
+        except AttributeError:
+            self.processor_status.entity = None
+        else:
+            self.processor_status.entity = fqn.__root__ if fqn else None
+
+        self._table = self._convert_table_to_orm_object(sqa_metadata)
+
+        self.session_factory = self._session_factory(service_connection_config)
+        self.session = self.session_factory()
+        self.set_session_tag(self.session)
+        self.set_catalog(self.session)
+
         self.profile_sample_config = profile_sample_config
         self.profile_query = sample_query
-        self.table_partition_config = table_partition_config
-        self._table = entity
-        self.dfs = ometa_to_dataframe(
-            config_source=self.service_connection_config.configSource,
-            client=self.client,
-            table=self.table,
+        self.partition_details = (
+            table_partition_config if not self.profile_query else None
         )
-        if self.dfs and self.table_partition_config:
-            self.dfs = [self.get_partitioned_df(df) for df in self.dfs]
+
+        self.timeout_seconds = timeout_seconds
+
+    @property
+    def table(self):
+        return self._table
+
+    @staticmethod
+    def _session_factory(service_connection_config) -> scoped_session:
+        """Create thread safe session that will be automatically
+        garbage collected once the application thread ends
+        """
+        engine = get_connection(service_connection_config)
+        return create_and_bind_thread_safe_session(engine)
+
+    @staticmethod
+    def _compute_static_metrics_wo_sum(
+        metrics: List[Metrics],
+        runner: QueryRunner,
+        session,
+        column: Column,
+    ):
+        """If we catch an overflow error, we will try to compute the static
+        metrics without the sum, mean and stddev
+
+        Returns:
+            _type_: _description_
+        """
+        try:
+            row = runner.select_first_from_sample(
+                *[
+                    metric(column).fn()
+                    for metric in metrics
+                    if not metric.is_window_metric()
+                    and metric not in {Sum, StdDev, Mean}
+                ]
+            )
+            return dict(row)
+        except Exception as exc:
+            msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
+            handle_query_exception(msg, exc, session)
 
     @valuedispatch
     def _get_metrics(self, *args, **kwargs):
         """Generic getter method for metrics. To be used with
         specific dispatch methods
         """
         logger.warning("Could not get metric. No function registered.")
 
     # pylint: disable=unused-argument
     @_get_metrics.register(MetricTypes.Table.value)
     def _(
         self,
         metric_type: str,
         metrics: List[Metrics],
-        dfs: List,
+        runner: QueryRunner,
+        session,
         *args,
         **kwargs,
     ):
         """Given a list of metrics, compute the given results
         and returns the values
 
         Args:
             metrics: list of metrics to compute
         Returns:
             dictionnary of results
         """
-        import pandas as pd  # pylint: disable=import-outside-toplevel
-
         try:
-            row = []
-            for metric in metrics:
-                for df in dfs:
-                    row.append(
-                        metric().df_fn(df.astype(object).where(pd.notnull(df), None))
-                    )
+            row = runner.select_first_from_sample(
+                *[metric().fn() for metric in metrics]
+            )
+
             if row:
-                if isinstance(row, list):
-                    row_dict = {}
-                    for index, table_metric in enumerate(metrics):
-                        row_dict[table_metric.name()] = row[index]
-                    return row_dict
                 return dict(row)
             return None
 
         except Exception as exc:
             logger.debug(traceback.format_exc())
-            logger.warning(f"Error trying to compute profile for {exc}")
+            logger.warning(
+                f"Error trying to compute profile for {runner.table.__tablename__}: {exc}"  # type: ignore
+            )
+            session.rollback()
             raise RuntimeError(exc)
 
     # pylint: disable=unused-argument
     @_get_metrics.register(MetricTypes.Static.value)
     def _(
         self,
         metric_type: str,
         metrics: List[Metrics],
-        column,
-        dfs,
+        runner: QueryRunner,
+        session,
+        column: Column,
         *args,
         **kwargs,
     ):
         """Given a list of metrics, compute the given results
         and returns the values
 
         Args:
             column: the column to compute the metrics against
             metrics: list of metrics to compute
         Returns:
             dictionnary of results
         """
-        import pandas as pd  # pylint: disable=import-outside-toplevel
-
         try:
-            row = []
-            for metric in metrics:
-                for df in dfs:
-                    row.append(
-                        metric(column).df_fn(
-                            df.astype(object).where(pd.notnull(df), None)
-                        )
-                    )
-            row_dict = {}
-            for index, column_metric in enumerate(metrics):
-                row_dict[column_metric.name()] = row[index]
-            return row_dict
-        except Exception as exc:
-            logger.debug(
-                f"{traceback.format_exc()}\nError trying to compute profile for {exc}"
+            row = runner.select_first_from_sample(
+                *[
+                    metric(column).fn()
+                    for metric in metrics
+                    if not metric.is_window_metric()
+                ]
             )
-            raise RuntimeError(exc)
+            return dict(row)
+        except Exception as exc:
+            if (
+                isinstance(exc, ProgrammingError)
+                and exc.orig
+                and exc.orig.errno
+                in OVERFLOW_ERROR_CODES.get(session.bind.dialect.name)
+            ):
+                logger.info(
+                    f"Computing metrics without sum for {runner.table.__tablename__}.{column.name}"
+                )
+                return self._compute_static_metrics_wo_sum(
+                    metrics, runner, session, column
+                )
+
+            msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
+            handle_query_exception(msg, exc, session)
 
     # pylint: disable=unused-argument
     @_get_metrics.register(MetricTypes.Query.value)
     def _(
         self,
         metric_type: str,
-        metrics: Metrics,
-        column,
-        dfs,
-        *args,
-        **kwargs,
+        metric: Metrics,
+        runner: QueryRunner,
+        session,
+        column: Column,
+        sample,
     ):
         """Given a list of metrics, compute the given results
         and returns the values
 
         Args:
             column: the column to compute the metrics against
             metrics: list of metrics to compute
         Returns:
             dictionnary of results
         """
-        col_metric = None
-        for df in dfs:
-            col_metric = metrics(column).df_fn(df)
-        if not col_metric:
-            return None
-        return {metrics.name(): col_metric}
+        try:
+            col_metric = metric(column)
+            metric_query = col_metric.query(sample=sample, session=session)
+            if not metric_query:
+                return None
+            if col_metric.metric_type == dict:
+                results = runner.select_all_from_query(metric_query)
+                data = {k: [result[k] for result in results] for k in dict(results[0])}
+                return {metric.name(): data}
+
+            row = runner.select_first_from_query(metric_query)
+            return dict(row)
+        except Exception as exc:
+            msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
+            handle_query_exception(msg, exc, session)
 
     # pylint: disable=unused-argument
     @_get_metrics.register(MetricTypes.Window.value)
     def _(
         self,
         metric_type: str,
-        metrics: Metrics,
-        column,
+        metrics: List[Metrics],
+        runner: QueryRunner,
+        session,
+        column: Column,
         *args,
         **kwargs,
     ):
-        """
-        Given a list of metrics, compute the given results
+        """Given a list of metrics, compute the given results
         and returns the values
+
+        Args:
+            column: the column to compute the metrics against
+            metrics: list of metrics to compute
+        Returns:
+            dictionnary of results
         """
+        if not metrics:
+            return None
         try:
-            metric_values = {}
-            for metric in metrics:
-                metric_values[metric.name()] = metric(column).df_fn(self.dfs)
-            return metric_values if metric_values else None
+            row = runner.select_first_from_sample(
+                *[metric(column).fn() for metric in metrics]
+            )
         except Exception as exc:
-            logger.debug(traceback.format_exc())
-            logger.warning(f"Unexpected exception computing metrics: {exc}")
-            return None
+            if (
+                isinstance(exc, ProgrammingError)
+                and exc.orig
+                and exc.orig.errno
+                in OVERFLOW_ERROR_CODES.get(session.bind.dialect.name)
+            ):
+                logger.info(
+                    f"Skipping window metrics for {runner.table.__tablename__}.{column.name} due to overflow"
+                )
+                return None
+            msg = f"Error trying to compute profile for {runner.table.__tablename__}.{column.name}: {exc}"
+            handle_query_exception(msg, exc, session)
+        if row:
+            return dict(row)
+        return None
 
     @_get_metrics.register(MetricTypes.System.value)
     def _(
         self,
+        metric_type: str,
+        metric: Metrics,
+        runner: QueryRunner,
+        session,
         *args,
         **kwargs,
     ):
+        """Get system metric for tables
+
+        Args:
+            metric_type: type of metric
+            metrics: list of metrics to compute
+            session: SQA session object
+
+        Returns:
+            dictionnary of results
         """
-        Given a list of metrics, compute the given results
-        and returns the values
-        """
-        return None  # to be implemented
+        try:
+            rows = metric().sql(session, conn_config=self.service_connection_config)
+            return rows
+        except Exception as exc:
+            msg = f"Error trying to compute profile for {runner.table.__tablename__}: {exc}"
+            handle_query_exception(msg, exc, session)
 
-    def compute_metrics(
+    def _create_thread_safe_sampler(
+        self,
+        session,
+        table,
+    ):
+        """Create thread safe runner"""
+        if not hasattr(thread_local, "sampler"):
+            thread_local.sampler = Sampler(
+                session=session,
+                table=table,
+                profile_sample_config=self.profile_sample_config,
+                partition_details=self.partition_details,
+                profile_sample_query=self.profile_query,
+            )
+        return thread_local.sampler
+
+    def _create_thread_safe_runner(
+        self,
+        session,
+        table,
+        sample,
+    ):
+        """Create thread safe runner"""
+        if not hasattr(thread_local, "runner"):
+            thread_local.runner = QueryRunner(
+                session=session,
+                table=table,
+                sample=sample,
+                partition_details=self.partition_details,
+                profile_sample_query=self.profile_query,
+            )
+        return thread_local.runner
+
+    def compute_metrics_in_thread(
         self,
         metrics,
         metric_type,
         column,
         table,
     ):
         """Run metrics in processor worker"""
-        logger.debug(f"Running profiler for {table}")
-        try:
-            row = self._get_metrics(
-                metric_type.value,
-                metrics,
-                session=self.client,
-                dfs=self.dfs,
-                column=column,
+        logger.debug(
+            f"Running profiler for {table.__tablename__} on thread {threading.current_thread()}"
+        )
+        Session = self.session_factory  # pylint: disable=invalid-name
+        with Session() as session:
+            self.set_session_tag(session)
+            self.set_catalog(session)
+            sampler = self._create_thread_safe_sampler(
+                session,
+                table,
             )
-        except Exception as exc:
-            name = f"{column if column is not None else table}"
-            error = f"{name} metric_type.value: {exc}"
-            logger.error(error)
-            self.processor_status.failed_profiler(error, traceback.format_exc())
-            row = None
-        if column:
-            column = column.name
-        return row, column, metric_type.value
+            sample = sampler.random_sample()
+            runner = self._create_thread_safe_runner(
+                session,
+                table,
+                sample,
+            )
+
+            try:
+                row = self._get_metrics(
+                    metric_type.value,
+                    metrics,
+                    runner=runner,
+                    session=session,
+                    column=column,
+                    sample=sample,
+                )
+            except Exception as exc:
+                error = f"{column if column is not None else runner.table.__tablename__} metric_type.value: {exc}"
+                logger.error(error)
+                self.processor_status.failed_profiler(error, traceback.format_exc())
+                row = None
+
+            if column is not None:
+                column = column.name
+
+            return row, column, metric_type.value
+
+    # pylint: disable=use-dict-literal
+    def get_all_metrics(
+        self,
+        metric_funcs: list,
+    ):
+        """get all profiler metrics"""
+        logger.info(f"Computing metrics with {self._thread_count} threads.")
+        profile_results = {"table": dict(), "columns": defaultdict(dict)}
+        with CustomThreadPoolExecutor(max_workers=self._thread_count) as pool:
+            futures = [
+                pool.submit(
+                    self.compute_metrics_in_thread,
+                    *metric_func,
+                )
+                for metric_func in metric_funcs
+            ]
+
+            for future in futures:
+                if future.cancelled():
+                    continue
+
+                try:
+                    profile, column, metric_type = future.result(
+                        timeout=self.timeout_seconds
+                    )
+                    if metric_type != MetricTypes.System.value and not isinstance(
+                        profile, dict
+                    ):
+                        profile = dict()
+                    if metric_type == MetricTypes.Table.value:
+                        profile_results["table"].update(profile)
+                    elif metric_type == MetricTypes.System.value:
+                        profile_results["system"] = profile
+                    else:
+                        profile_results["columns"][column].update(
+                            {
+                                "name": column,
+                                "timestamp": datetime.now(tz=timezone.utc).timestamp(),
+                                **profile,
+                            }
+                        )
+                except concurrent.futures.TimeoutError as exc:
+                    pool.shutdown39(wait=True, cancel_futures=True)
+                    logger.debug(traceback.format_exc())
+                    logger.error(f"Operation was cancelled due to TimeoutError - {exc}")
+                    raise concurrent.futures.TimeoutError
+
+        return profile_results
 
     def fetch_sample_data(self, table) -> TableData:
         """Fetch sample data from database
 
         Args:
             table: ORM declarative table
 
         Returns:
             TableData: sample table data
         """
-        sampler = DatalakeSampler(
-            session=self.client,
-            table=self.dfs,
+        sampler = Sampler(
+            session=self.session,
+            table=table,
             profile_sample_config=self.profile_sample_config,
+            partition_details=self.partition_details,
             profile_sample_query=self.profile_query,
         )
-        return sampler.fetch_dl_sample_data()
+
+        # Only fetch columns that are in the table entity
+        # with struct columns we create a column for each field in the ORM table
+        # but we only want to fetch the columns that are in the table entity
+        sample_columns = [
+            column.name
+            for column in table.__table__.columns
+            if column.name in {col.name.__root__ for col in self.table_entity.columns}
+        ]
+
+        return sampler.fetch_sqa_sample_data(sample_columns)
 
     def get_composed_metrics(
         self, column: Column, metric: Metrics, column_results: Dict
     ):
         """Given a list of metrics, compute the given results
         and returns the values
 
         Args:
             column: the column to compute the metrics against
-            metric: list of metrics to compute
-            column_results: computed values for the column
+            metrics: list of metrics to compute
         Returns:
-            dictionary of results
+            dictionnary of results
         """
         try:
             return metric(column).fn(column_results)
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Unexpected exception computing metrics: {exc}")
+            self.session.rollback()
             return None
 
     def get_hybrid_metrics(
-        self, column: Column, metric: Metrics, column_results: Dict, **kwargs
+        self, column: Column, metric: Metrics, column_results: Dict, table, **kwargs
     ):
         """Given a list of metrics, compute the given results
         and returns the values
 
         Args:
             column: the column to compute the metrics against
-            metric: list of metrics to compute
-            column_results: computed values for the column
+            metrics: list of metrics to compute
         Returns:
-            dictionary of results
+            dictionnary of results
         """
+        sampler = Sampler(
+            session=self.session,
+            table=table,
+            profile_sample_config=self.profile_sample_config,
+            partition_details=self.partition_details,
+            profile_sample_query=self.profile_query,
+        )
+        sample = sampler.random_sample()
         try:
-            return metric(column).df_fn(column_results, self.dfs)
+            return metric(column).fn(sample, column_results, self.session)
         except Exception as exc:
             logger.debug(traceback.format_exc())
             logger.warning(f"Unexpected exception computing metrics: {exc}")
+            self.session.rollback()
             return None
-
-    def get_all_metrics(
-        self,
-        metric_funcs: list,
-    ):
-        """get all profiler metrics"""
-
-        profile_results = {"table": {}, "columns": defaultdict(dict)}
-        metric_list = [
-            self.compute_metrics(*metric_func) for metric_func in metric_funcs
-        ]
-        for metric_result in metric_list:
-            profile, column, metric_type = metric_result
-
-            if metric_type == MetricTypes.Table.value:
-                profile_results["table"].update(profile)
-            if metric_type == MetricTypes.System.value:
-                profile_results["system"] = profile
-            else:
-                if profile:
-                    profile_results["columns"][column].update(
-                        {
-                            "name": column,
-                            "timestamp": datetime.now(tz=timezone.utc).timestamp(),
-                            **profile,
-                        }
-                    )
-        return profile_results
-
-    @property
-    def table(self):
-        """OM Table entity"""
-        return self._table
-
-    def get_columns(self):
-        if self.dfs:
-            df = self.dfs[0]
-            return [
-                SQALikeColumn(
-                    column_name,
-                    Type(
-                        DATALAKE_DATA_TYPES.get(
-                            df[column_name].dtypes.name, DataType.STRING.value
-                        )
-                    ),
-                )
-                for column_name in df.columns
-            ]
-        return []
-
-    def close(self):
-        """Nothing to close with pandas"""
-        pass
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/interface/profiler_protocol.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/interface/profiler_protocol.py`

 * *Files 0% similar despite different names*

```diff
@@ -112,15 +112,14 @@
     @staticmethod
     def get_profile_sample_config(
         entity: Table,
         entity_config: Optional[TableConfig],
         source_config: DatabaseServiceProfilerPipeline,
     ) -> Optional[ProfileSampleConfig]:
         """_summary_
-
         Args:
             entity (Table): table entity object
             entity_config (Optional[TableConfig]): table config object from yaml/json file
         Returns:
             Optional[dict]: dict
         """
         if entity_config:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/runner.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/runner.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 """
 from typing import Dict, Optional, Union
 
 from sqlalchemy import text
 from sqlalchemy.orm import DeclarativeMeta, Query, Session
 from sqlalchemy.orm.util import AliasedClass
 
-from metadata.profiler.profiler.handle_partition import partition_filter_handler
+from metadata.profiler.processor.handle_partition import partition_filter_handler
 from metadata.utils.logger import query_runner_logger
 from metadata.utils.sqa_utils import get_query_filter_for_runner
 
 logger = query_runner_logger()
 
 
 class QueryRunner:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/profiler/sampler.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/processor/sampler.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     ProfileSampleType,
     TableData,
 )
 from metadata.profiler.api.models import ProfileSampleConfig
 from metadata.profiler.orm.functions.modulo import ModuloFn
 from metadata.profiler.orm.functions.random_num import RandomNumFn
 from metadata.profiler.orm.registry import Dialects
-from metadata.profiler.profiler.handle_partition import partition_filter_handler
+from metadata.profiler.processor.handle_partition import partition_filter_handler
 from metadata.utils.sqa_utils import (
     build_query_filter,
     dispatch_to_date_or_datetime,
     get_integer_range_filter,
     get_partition_col_type,
     get_value_filter,
 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/registry.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py`

 * *Files 25% similar despite different names*

```diff
@@ -6,103 +6,59 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Registry definition.
-
-A Registry is a "smarter" Enum, which we'll
-use to control what classes we have available to
-use, validating that they are of proper type,
-and we can easily access the desired values.
+Validator for column values to not match regex test case
 """
 
-from enum import Enum
 from typing import Optional
 
-from sqlalchemy.sql.sqltypes import TypeDecorator
-
-from metadata.profiler.metrics.core import Metric
-
-
-class MetricRegistry(Enum):
-    """
-    Lets us __call__ values.
-    Used for our profiler registries of metrics.
-
-    Instead of:
-    - StaticMetrics.MIN.value(col)
-    We can use:
-    - StaticMetrics.MIN(col)
-    """
-
-    def __init__(self, metric):
-        if not issubclass(metric, Metric):
-            raise TypeError(
-                "Only Metrics can be part of the Metric Registry,"
-                + f" but found {type(metric)} instead."
-            )
-        self.metric = metric
-
-    def __call__(self, *args, **kwargs):
-        """
-        Allow to __init__ the mapped class directly
-
-        We run this as Metrics.MIN(col)
-        """
-        return self.value(*args, **kwargs)
-
-    @property
-    def name(  # pylint: disable=function-redefined, invalid-overridden-method
-        self,
-    ) -> str:
-        """
-        Override the default `name` on Enums
-        to use the mapped class name instead.
-
-        name is a classmethod on Metrics, so
-        we do not need to __init__ here.
-
-        We run this as Metrics.MIN.name
-        """
-        return self.value.name()
-
-    def __str__(self):
-        return self.value.name()
+from sqlalchemy import Column, inspect
+from sqlalchemy.exc import CompileError
 
-    @classmethod
-    def get(cls, key: str) -> Optional[Metric]:
-        """
-        Safely retrieve an element
-        from the Registry.
-
-        Added at class level. Execute from Metrics directly,
-        e.g., Metrics.get("MIN")
+from metadata.data_quality.validations.column.base.columnValuesToNotMatchRegex import (
+    BaseColumnValuesToNotMatchRegexValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.logger import test_suite_logger
+
+logger = test_suite_logger()
+
+
+class ColumnValuesToNotMatchRegexValidator(
+    BaseColumnValuesToNotMatchRegexValidator, SQAValidatorMixin
+):
+    """Validator for column values to not match regex test case"""
+
+    def _get_column_name(self) -> Column:
+        """Get column name from the test case entity link
+
+        Returns:
+            SQALikeColumn: column
+        """
+        return self.get_column_name(
+            self.test_case.entityLink.__root__,
+            inspect(self.runner.table).c,
+        )
+
+    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
+        """compute result of the test case
+
+        Args:
+            metric: metric
+            column: column
         """
         try:
-            return cls[key].value
-        except KeyError:
-            return None
-
-    @classmethod
-    def init(cls, key: str, *args, **kwargs):
-        try:
-            return cls[key](*args, **kwargs)
-        except KeyError:
-            return None
-
-
-class TypeRegistry(Enum):
-    """
-    Used to validate that we are passing proper
-    TypeDecorators to our Type Registry
-    """
-
-    def __init__(self, _type):
-        if not issubclass(_type, TypeDecorator):
-            raise TypeError(
-                "Only Metrics can be part of the Metric Registry,"
-                + f" but found {type(_type)} instead."
+            return self.run_query_results(self.runner, metric, column, **kwargs)
+        except CompileError as err:
+            logger.warning(
+                f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
+            )
+            return self.run_query_results(
+                self.runner, Metrics.NOT_LIKE_COUNT, column, **kwargs
             )
-        self._type = _type
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/sink/file.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/sink/file.py`

 * *Files 0% similar despite different names*

```diff
@@ -53,15 +53,14 @@
 
     @classmethod
     def create(cls, config_dict: dict, _):
         config = FileSinkConfig.parse_obj(config_dict)
         return cls(config)
 
     def write_record(self, record: ProfilerResponse) -> None:
-
         if self.wrote_something:
             self.file.write("\n")
 
         self.file.write(f"Profile for: {record.table.fullyQualifiedName.__root__}\n")
         self.file.write(f"{record.profile.json()}\n")
 
         self.wrote_something = True
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/profiler/sink/metadata_rest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/sink/metadata_rest.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/api/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/api/models.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/api/workflow.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/api/workflow.py`

 * *Files 24% similar despite different names*

```diff
@@ -17,18 +17,31 @@
 
 import sys
 import traceback
 from copy import deepcopy
 from logging import Logger
 from typing import List, Optional, Set, Tuple
 
-from pydantic import ValidationError
+from antlr4.error.Errors import ParseCancellationException
+from pydantic import BaseModel, ValidationError
 from sqlalchemy import MetaData
 
 from metadata.config.common import WorkflowExecutionError
+from metadata.data_quality.api.models import (
+    TestCaseDefinition,
+    TestSuiteDefinition,
+    TestSuiteProcessorConfig,
+)
+from metadata.data_quality.interface.pandas.pandas_test_suite_interface import (
+    PandasTestSuiteInterface,
+)
+from metadata.data_quality.interface.sqlalchemy.sqa_test_suite_interface import (
+    SQATestSuiteInterface,
+)
+from metadata.data_quality.runner.core import DataTestsRunner
 from metadata.generated.schema.api.tests.createTestCase import CreateTestCaseRequest
 from metadata.generated.schema.api.tests.createTestSuite import CreateTestSuiteRequest
 from metadata.generated.schema.entity.data.table import PartitionProfilerConfig, Table
 from metadata.generated.schema.entity.services.connections.database.datalakeConnection import (
     DatalakeConnection,
 )
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
@@ -47,33 +60,41 @@
 from metadata.generated.schema.tests.testCase import TestCase
 from metadata.generated.schema.tests.testSuite import TestSuite
 from metadata.generated.schema.type.basic import FullyQualifiedEntityName
 from metadata.ingestion.api.parser import parse_workflow_config_gracefully
 from metadata.ingestion.api.processor import ProcessorStatus
 from metadata.ingestion.ometa.client_utils import create_ometa_client
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.ingestion.source.connections import get_connection
-from metadata.ingestion.source.database.datalake.metadata import ometa_to_dataframe
-from metadata.interfaces.datalake.datalake_test_suite_interface import (
-    DataLakeTestSuiteInterface,
-)
-from metadata.interfaces.sqalchemy.sqa_test_suite_interface import SQATestSuiteInterface
 from metadata.profiler.api.models import ProfileSampleConfig
-from metadata.test_suite.api.models import TestCaseDefinition, TestSuiteProcessorConfig
-from metadata.test_suite.runner.core import DataTestsRunner
 from metadata.utils import entity_link
 from metadata.utils.importer import get_sink
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.partition import get_partition_details
 from metadata.utils.workflow_output_handler import print_test_suite_status
 from metadata.workflow.workflow_status_mixin import WorkflowStatusMixin
 
 logger: Logger = test_suite_logger()
 
 
+class TestCaseToCreate(BaseModel):
+    """Test case to create"""
+
+    test_suite_name: str
+    test_case_name: str
+    entity_link: str
+
+    def __hash__(self):
+        """make this base model hashable on unique_name"""
+        return hash(f"{self.test_suite_name}.{self.test_case_name}")
+
+    def __str__(self) -> str:
+        """make this base model printable"""
+        return f"{self.test_suite_name}.{self.test_case_name}"
+
+
 class TestSuiteWorkflow(WorkflowStatusMixin):
     """workflow to run the test suite"""
 
     def __init__(self, config: OpenMetadataWorkflowConfig):
         """
         Instantiate test suite workflow class
 
@@ -103,15 +124,15 @@
         self.status = ProcessorStatus()
 
         if self.config.sink:
             self.sink = get_sink(
                 sink_type=self.config.sink.type,
                 sink_config=self.config.sink,
                 metadata_config=self.metadata_config,
-                from_="test_suite",
+                from_="data_quality",
             )
 
     @classmethod
     def create(cls, config_dict) -> TestSuiteWorkflow:
         """
         Instantiate a TestSuiteWorkflow object form a yaml or json config file
 
@@ -259,23 +280,20 @@
                 ometa_client=self.client,
                 sqa_metadata_obj=sqa_metadata_obj,
                 table_entity=table_entity,
                 profile_sample_config=profile_sample_config,
                 table_sample_query=table_sample_query,
                 table_partition_config=table_partition_config,
             )
-        return DataLakeTestSuiteInterface(
+        return PandasTestSuiteInterface(
             service_connection_config=service_connection_config,
             ometa_client=self.client,
-            df=ometa_to_dataframe(
-                service_connection_config.configSource,
-                get_connection(service_connection_config).client,
-                table_entity,
-            )[0],
+            profile_sample_config=profile_sample_config,
             table_entity=table_entity,
+            table_partition_config=table_partition_config,
         )
 
     def _create_data_tests_runner(self, sqa_interface):
         """create main object to run data test validation"""
         return DataTestsRunner(sqa_interface)
 
     def get_test_suite_entity_for_ui_workflow(self) -> Optional[List[TestSuite]]:
@@ -335,50 +353,145 @@
                 fields=["testSuite", "entityLink", "testDefinition"],
                 params={"testSuiteId": test_suite.id.__root__},
             )
             test_cases_entity.extend(test_case_entity_list.entities)
 
         return test_cases_entity
 
-    def get_test_case_from_cli_config(self) -> List[str]:
+    def get_test_case_from_cli_config(
+        self,
+    ) -> List[Tuple[TestCaseDefinition, TestSuiteDefinition]]:
         """Get all the test cases names defined in the CLI config file"""
         return [
             (test_case, test_suite)
             for test_suite in self.processor_config.testSuites
             for test_case in test_suite.testCases
         ]
 
+    def get_unique_test_case_name_in_config(self, test_cases_in_config: set) -> set:
+        """Get unique test case names in config. If a test case is created for the same entity
+        with the same name in different test suites, we only create one test case in the platform.
+        The other ones will be skipped.
+
+        Args:
+            test_cases_in_config (set): set of test cases in config
+
+        Returns:
+            set: set of unique test case names in config
+        """
+        seen = []
+        unique_test_case = []
+        for test_case in test_cases_in_config:
+            unique_test_case_name_in_entity = (
+                f"{test_case.test_case_name}/{test_case.entity_link}"
+            )
+            if unique_test_case_name_in_entity not in seen:
+                seen.append(unique_test_case_name_in_entity)
+                unique_test_case.append(test_case)
+                continue
+            logger.info(
+                f"Test case {test_case.test_case_name} for entity {test_case.entity_link}"
+                " was already defined in your profiler config file. Skipping it."
+            )
+
+        return set(unique_test_case)
+
+    def test_case_name_exists(self, test_case: TestCaseToCreate) -> bool:
+        """Check if a test case name already exists in the platform
+        for the same entity.
+
+        Args:
+            other (set): a set of platform test cases
+        Returns:
+            Optional["TestCaseToCreate"]
+        """
+        try:
+            entity_fqn = entity_link.get_table_or_column_fqn(test_case.entity_link)
+        except ValueError as exc:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Failed to get entity fqn: {exc}")
+            # we'll assume that the test case name is not unique
+            return True
+        except ParseCancellationException as err:
+            logger.debug(traceback.format_exc())
+            logger.error(f"Failed to parse: {test_case.entity_link}, err: {err}")
+            # we'll assume that the test case name is not unique
+            return True
+        test_case_fqn = f"{entity_fqn}.{test_case.test_case_name}"
+
+        test_case = self.metadata.get_by_name(
+            entity=TestCase,
+            fqn=test_case_fqn,
+            fields=["testDefinition", "testSuite"],
+        )
+
+        if not test_case:
+            return False
+        return True
+
     def compare_and_create_test_cases(
         self,
-        cli_config_test_cases_def: List[Tuple[TestCaseDefinition, TestSuite]],
+        cli_config_test_cases_def: List[Tuple[TestCaseDefinition, TestSuiteDefinition]],
         test_cases: List[TestCase],
     ) -> Optional[List[TestCase]]:
         """
         compare test cases defined in CLI config workflow with test cases
         defined on the server
 
         Args:
             cli_config_test_case_name: test cases defined in CLI workflow associated with its test suite
             test_cases: list of test cases entities fetch from the server using test suite names in the config file
         """
-        test_case_names_to_create = {
-            test_case_def[0].name for test_case_def in cli_config_test_cases_def
-        } - {test_case.name.__root__ for test_case in test_cases}
+        cli_test_cases = {
+            TestCaseToCreate(
+                test_suite_name=test_case_def[1].name,
+                test_case_name=test_case_def[0].name,
+                entity_link=test_case_def[0].entityLink.__root__,
+            )
+            for test_case_def in cli_config_test_cases_def
+        }
+        platform_test_cases = {
+            TestCaseToCreate(
+                test_suite_name=test_case.testSuite.name,
+                test_case_name=test_case.name.__root__,
+                entity_link=test_case.entityLink.__root__,
+            )
+            for test_case in test_cases
+        }
+
+        # we'll check the test cases defined in the CLI config file and not present in the platform
+        unique_test_cases_across_test_suites = cli_test_cases - platform_test_cases
+        # we'll check if we are creating a test case for an entity that already has the same name
+        unique_test_case_across_entities = {
+            test_case
+            for test_case in unique_test_cases_across_test_suites
+            if not self.test_case_name_exists(test_case)
+        }
+        test_case_names_to_create = self.get_unique_test_case_name_in_config(
+            unique_test_case_across_entities
+        )
 
         if not test_case_names_to_create:
             return None
 
         created_test_case = []
         for test_case_name_to_create in test_case_names_to_create:
             logger.info(f"Creating test case with name {test_case_name_to_create}")
             test_case_to_create, test_suite = next(
                 (
                     cli_config_test_case_def
                     for cli_config_test_case_def in cli_config_test_cases_def
-                    if cli_config_test_case_def[0].name == test_case_name_to_create
+                    if (
+                        cli_config_test_case_def[0].name
+                        == test_case_name_to_create.test_case_name
+                        and cli_config_test_case_def[0].entityLink.__root__
+                        == test_case_name_to_create.entity_link
+                        and cli_config_test_case_def[1].name
+                        == test_case_name_to_create.test_suite_name
+                    )
                 ),
                 (None, None),
             )
             try:
                 created_test_case.append(
                     self.metadata.create_or_update(
                         CreateTestCaseRequest(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/runner/models.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/readers/local.py`

 * *Files 27% similar despite different names*

```diff
@@ -4,21 +4,39 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-test case result response object
+Local Reader
 """
+import traceback
+from pathlib import Path
+
+from metadata.readers.base import Reader, ReadException
+from metadata.utils.constants import UTF_8
+from metadata.utils.logger import ingestion_logger
+
+logger = ingestion_logger()
+
 
-from pydantic import BaseModel
+class LocalReader(Reader):
+    """
+    Read files locally
+    """
 
-from metadata.generated.schema.tests.basic import TestCaseResult
-from metadata.generated.schema.tests.testCase import TestCase
+    def __init__(self, base_path: Path):
+        self.base_path = base_path
 
+    def read(self, path: str) -> str:
+        """
+        simple local reader
+        """
+        try:
+            with open(self.base_path / path, encoding=UTF_8) as file:
+                return file.read()
 
-class TestCaseResultResponse(BaseModel):
-    testCaseResult: TestCaseResult
-    testCase: TestCase
+        except Exception as err:
+            logger.debug(traceback.format_exc())
+            raise ReadException(f"Error reading file [{path}] locally: {err}")
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/sink/metadata_rest.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/sink/metadata_rest.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,22 +13,22 @@
 OpenMetadata REST Sink implementation for the ORM Profiler results
 """
 
 import traceback
 from typing import Optional
 
 from metadata.config.common import ConfigModel
+from metadata.data_quality.runner.models import TestCaseResultResponse
 from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (
     OpenMetadataConnection,
 )
 from metadata.ingestion.api.common import Entity
 from metadata.ingestion.api.sink import Sink
 from metadata.ingestion.ometa.client import APIError
 from metadata.ingestion.ometa.ometa_api import OpenMetadata
-from metadata.test_suite.runner.models import TestCaseResultResponse
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class MetadataRestSinkConfig(ConfigModel):
     api_endpoint: Optional[str] = None
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/base_test_handler.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/base_test_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.generated.schema.tests.testCase import TestCase, TestCaseParameterValue
-from metadata.profiler.profiler.runner import QueryRunner
+from metadata.profiler.processor.runner import QueryRunner
 
 T = TypeVar("T", bound=Callable)
 R = TypeVar("R")
 
 
 class BaseTestValidator(ABC):
     """Abstract class for test case handlers"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 MIN = "minValueLength"
 MAX = "maxValueLength"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 MAX = "max"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 MEAN = "mean"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 MEDIAN = "median"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 MIN = "min"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 STDDEV = "stddev"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesMissingCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py`

 * *Files 0% similar despite different names*

```diff
@@ -16,21 +16,21 @@
 import traceback
 from abc import abstractmethod
 from ast import literal_eval
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 NULL_COUNT = "nullCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py`

 * *Files 12% similar despite different names*

```diff
@@ -6,71 +6,78 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values sum to be between test case
+Validator for column values to match regex test case
 """
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
-SUM = "sum"
+LIKE_COUNT = "likeCount"
 
 
-class BaseColumnValuesSumToBeBetweenValidator(BaseTestValidator):
-    """Validator for column values sum to be between test case"""
+class BaseColumnValuesToMatchRegexValidator(BaseTestValidator):
+    """Validator for column values to match regex test case"""
 
     def run_validation(self) -> TestCaseResult:
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
+        regex: str = self.get_test_case_param_value(
+            self.test_case.parameterValues,  # type: ignore
+            "regex",
+            str,
+        )
         try:
             column: Union[SQALikeColumn, Column] = self._get_column_name()
-            res = self._run_results(Metrics.SUM, column)
+            count = self._run_results(Metrics.COUNT, column)
+            match_count = self._run_results(
+                Metrics.REGEX_COUNT, column, expression=regex
+            )
         except (ValueError, RuntimeError) as exc:
             msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
-                [TestResultValue(name=SUM, value=None)],
+                [TestResultValue(name=LIKE_COUNT, value=None)],
             )
 
-        min_bound = self.get_min_bound("minValueForColSum")
-        max_bound = self.get_max_bound("maxValueForColSum")
-
         return self.get_test_case_result_object(
             self.execution_date,
-            self.get_test_case_status(min_bound <= res <= max_bound),
-            f"Found sum={res} vs. the expected min={min_bound}, max={max_bound}.",
-            [TestResultValue(name=SUM, value=str(res))],
+            self.get_test_case_status(count == match_count),
+            f"Found {match_count} value(s) matching regex pattern vs {count} value(s) in the column.",
+            [TestResultValue(name=LIKE_COUNT, value=str(match_count))],
         )
 
     @abstractmethod
     def _get_column_name(self):
         raise NotImplementedError
 
     @abstractmethod
-    def _run_results(self, metric: Metrics, column: Union[SQALikeColumn, Column]):
+    def _run_results(
+        self, metric: Metrics, column: Union[SQALikeColumn, Column], **kwargs
+    ):
         raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,40 +11,54 @@
 
 """
 Validator for column values to be between test case
 """
 
 import traceback
 from abc import abstractmethod
-from datetime import datetime
+from datetime import date, datetime, time
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
 from metadata.profiler.orm.registry import is_date_time
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 from metadata.utils.time_utils import convert_timestamp
 
 logger = test_suite_logger()
 
 MIN = "min"
 MAX = "max"
 
 
 class BaseColumnValuesToBeBetweenValidator(BaseTestValidator):
     """Validator for column values to be between test case"""
 
+    def _convert_date_to_datetime(
+        self, date_object: date, time_converter: time
+    ) -> datetime:
+        """Convert date object to datetime object
+
+        Args:
+            date_object (date): date object
+            time_converter (time): time converter to use one of time.min or time.max
+
+        Returns:
+            datetime:
+        """
+        return datetime.combine(date_object, time_converter)
+
     def run_validation(self) -> TestCaseResult:
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
         try:
@@ -61,14 +75,19 @@
                 msg,
                 [
                     TestResultValue(name=MIN, value=None),
                     TestResultValue(name=MAX, value=None),
                 ],
             )
 
+        if type(min_res) is date:  # pylint: disable=unidiomatic-typecheck
+            min_res = self._convert_date_to_datetime(min_res, time.min)
+        if type(max_res) is date:  # pylint: disable=unidiomatic-typecheck
+            max_res = self._convert_date_to_datetime(max_res, time.max)
+
         min_bound = self.get_test_case_param_value(
             self.test_case.parameterValues,  # type: ignore
             "minValue",
             type_=datetime.fromtimestamp if is_date_time(column.type) else float,
             default=datetime.min if is_date_time(column.type) else float("-inf"),
             pre_processor=convert_timestamp if is_date_time(column.type) else None,
         )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,21 +16,21 @@
 import traceback
 from abc import abstractmethod
 from ast import literal_eval
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 ALLOWED_VALUE_COUNT = "allowedValueCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,21 +16,21 @@
 import traceback
 from abc import abstractmethod
 from ast import literal_eval
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.entity_link import get_table_fqn
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 COUNT_FORBIDDEN_VALUES = "countForbiddenValues"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py`

 * *Files 0% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 NULL_COUNT = "nullCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 VALUE_COUNT = "valueCount"
 UNIQUE_COUNT = "uniqueCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,78 +6,62 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to match regex test case
+Validator for table column count to be equal test case
 """
 
 import traceback
 from abc import abstractmethod
-from typing import Union
-
-from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
-from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
-LIKE_COUNT = "likeCount"
+COLUMN_COUNT = "columnCount"
 
 
-class BaseColumnValuesToMatchRegexValidator(BaseTestValidator):
-    """Validator for column values to match regex test case"""
+class BaseTableColumnCountToEqualValidator(BaseTestValidator):
+    """Validator for table column count to be equal test case"""
 
     def run_validation(self) -> TestCaseResult:
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
-        regex: str = self.get_test_case_param_value(
-            self.test_case.parameterValues,  # type: ignore
-            "regex",
-            str,
-        )
         try:
-            column: Union[SQALikeColumn, Column] = self._get_column_name()
-            count = self._run_results(Metrics.COUNT, column)
-            match_count = self._run_results(
-                Metrics.REGEX_COUNT, column, expression=regex
-            )
-        except (ValueError, RuntimeError) as exc:
+            count = self._run_results()
+        except Exception as exc:
             msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
-                [TestResultValue(name=LIKE_COUNT, value=None)],
+                [TestResultValue(name=COLUMN_COUNT, value=None)],
             )
 
+        expected_count = self.get_test_case_param_value(
+            self.test_case.parameterValues, "columnCount", int  # type: ignore
+        )
+
         return self.get_test_case_result_object(
             self.execution_date,
-            self.get_test_case_status(count == match_count),
-            f"Found {match_count} value(s) matching regex pattern vs {count} value(s) in the column.",
-            [TestResultValue(name=LIKE_COUNT, value=str(match_count))],
+            self.get_test_case_status(count == expected_count),
+            f"Found {count} columns vs. the expected {expected_count}",
+            [TestResultValue(name=COLUMN_COUNT, value=str(count))],
         )
 
     @abstractmethod
-    def _get_column_name(self):
-        raise NotImplementedError
-
-    @abstractmethod
-    def _run_results(
-        self, metric: Metrics, column: Union[SQALikeColumn, Column], **kwargs
-    ):
+    def _run_results(self):
         raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/base/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,21 +15,21 @@
 
 import traceback
 from abc import abstractmethod
 from typing import Union
 
 from sqlalchemy import Column
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 NOT_LIKE_COUNT = "notLikeCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,21 +12,21 @@
 """
 Validator for column value length to be between test case
 """
 
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueLengthsToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValueLengthsToBeBetween import (
     BaseColumnValueLengthsToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValueLengthsToBeBetweenValidator(
     BaseColumnValueLengthsToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column value lengths to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 """
 Validator for column value max to be between test case
 """
 
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMaxToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValueMaxToBeBetween import (
     BaseColumnValueMaxToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValueMaxToBeBetweenValidator(
     BaseColumnValueMaxToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column value max to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,21 +12,21 @@
 """
 Validator for column value mean to be between test case
 """
 
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMeanToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValueMeanToBeBetween import (
     BaseColumnValueMeanToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValueMeanToBeBetweenValidator(
     BaseColumnValueMeanToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column value mean to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column value median to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMedianToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValueMedianToBeBetween import (
     BaseColumnValueMedianToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValueMedianToBeBetweenValidator(
     BaseColumnValueMedianToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column value median to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,48 +4,48 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
-
 """
-Validator for column value min to be between test case
+Validator for column value max to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMinToBeBetween import (
-    BaseColumnValueMinToBeBetweenValidator,
+from sqlalchemy import Column, inspect
+
+from metadata.data_quality.validations.column.base.columnValueMaxToBeBetween import (
+    BaseColumnValueMaxToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
-    PandasValidatorMixin,
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.utils.sqa_like_column import SQALikeColumn
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValueMinToBeBetweenValidator(
-    BaseColumnValueMinToBeBetweenValidator, PandasValidatorMixin
+class ColumnValueMaxToBeBetweenValidator(
+    BaseColumnValueMaxToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column value min to be between test case"""
+    """Validator for column value max to be between test case"""
 
-    def _get_column_name(self) -> SQALikeColumn:
+    def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
-            SQALikeColumn: column
+            Column: _description_
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
-            self.runner,
+            inspect(self.runner.table).c,
         )
 
-    def _run_results(self, metric: Metrics, column: SQALikeColumn) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_dataframe_results(self.runner, metric, column)
+        return self.run_query_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column value stddev to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueStdDevToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValueStdDevToBeBetween import (
     BaseColumnValueStdDevToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValueStdDevToBeBetweenValidator(
     BaseColumnValueStdDevToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column value stddev to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesMissingCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column value missing count to be equal test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesMissingCount import (
+from metadata.data_quality.validations.column.base.columnValuesMissingCount import (
     BaseColumnValuesMissingCountValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValuesMissingCountValidator(
     BaseColumnValuesMissingCountValidator, PandasValidatorMixin
 ):
     """Validator for column value missing count to be equal test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column values sum to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesSumToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValuesSumToBeBetween import (
     BaseColumnValuesSumToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValuesSumToBeBetweenValidator(
     BaseColumnValuesSumToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column values sum to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column values to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValuesToBeBetween import (
     BaseColumnValuesToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValuesToBeBetweenValidator(
     BaseColumnValuesToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for column values to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column value to be in set test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeInSet import (
+from metadata.data_quality.validations.column.base.columnValuesToBeInSet import (
     BaseColumnValuesToBeInSetValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.logger import test_suite_logger
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 logger = test_suite_logger()
 
 
 class ColumnValuesToBeInSetValidator(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 
 """
 Validator for column value to be not in set test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeNotInSet import (
+from metadata.data_quality.validations.column.base.columnValuesToBeNotInSet import (
     BaseColumnValuesToBeNotInSetValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
 class ColumnValuesToBeNotInSetValidator(
     BaseColumnValuesToBeNotInSetValidator, PandasValidatorMixin
 ):
     """Validator for column value to be not in set test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,33 +6,33 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to be not null test case
+Validator for column values to be unique test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeNotNull import (
-    BaseColumnValuesToBeNotNullValidator,
+from metadata.data_quality.validations.column.base.columnValuesToBeUnique import (
+    BaseColumnValuesToBeUniqueValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
-class ColumnValuesToBeNotNullValidator(
-    BaseColumnValuesToBeNotNullValidator, PandasValidatorMixin
+class ColumnValuesToBeUniqueValidator(
+    BaseColumnValuesToBeUniqueValidator, PandasValidatorMixin
 ):
-    """Validator for column values to be not null test case"""
+    """Validator for column values to be unique test case"""
 
     def _get_column_name(self) -> SQALikeColumn:
         """Get column name from the test case entity link
 
         Returns:
             SQALikeColumn: column
         """
@@ -45,7 +45,13 @@
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
         return self.run_dataframe_results(self.runner, metric, column)
+
+    def _get_unique_count(
+        self, metric: Metrics, column: SQALikeColumn
+    ) -> Optional[int]:
+        """Get unique count of values"""
+        return self._run_results(metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,52 +6,48 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to be unique test case
+Validator for column values to match regex test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeUnique import (
-    BaseColumnValuesToBeUniqueValidator,
+from metadata.data_quality.validations.column.base.columnValuesToMatchRegex import (
+    BaseColumnValuesToMatchRegexValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
-class ColumnValuesToBeUniqueValidator(
-    BaseColumnValuesToBeUniqueValidator, PandasValidatorMixin
+class ColumnValuesToMatchRegexValidator(
+    BaseColumnValuesToMatchRegexValidator, PandasValidatorMixin
 ):
-    """Validator for column values to be unique test case"""
+    """Validator for column values to match regex test case"""
 
     def _get_column_name(self) -> SQALikeColumn:
         """Get column name from the test case entity link
 
         Returns:
             SQALikeColumn: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             self.runner,
         )
 
-    def _run_results(self, metric: Metrics, column: SQALikeColumn) -> Optional[int]:
+    def _run_results(
+        self, metric: Metrics, column: SQALikeColumn, **kwargs
+    ) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_dataframe_results(self.runner, metric, column)
-
-    def _get_unique_count(
-        self, metric: Metrics, column: SQALikeColumn
-    ) -> Optional[int]:
-        """Get unique count of values"""
-        return self._run_results(metric, column)
+        return self.run_dataframe_results(self.runner, metric, column, **kwargs)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py`

 * *Files 11% similar despite different names*

```diff
@@ -6,48 +6,47 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to match regex test case
+Validator for column values to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToMatchRegex import (
-    BaseColumnValuesToMatchRegexValidator,
+from sqlalchemy import Column, inspect
+
+from metadata.data_quality.validations.column.base.columnValuesToBeBetween import (
+    BaseColumnValuesToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
-    PandasValidatorMixin,
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.utils.sqa_like_column import SQALikeColumn
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValuesToMatchRegexValidator(
-    BaseColumnValuesToMatchRegexValidator, PandasValidatorMixin
+class ColumnValuesToBeBetweenValidator(
+    BaseColumnValuesToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column values to match regex test case"""
+    """Validator for column values to be between test case"""
 
-    def _get_column_name(self) -> SQALikeColumn:
+    def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
-            SQALikeColumn: column
+            Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
-            self.runner,
+            inspect(self.runner.table).c,
         )
 
-    def _run_results(
-        self, metric: Metrics, column: SQALikeColumn, **kwargs
-    ) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_dataframe_results(self.runner, metric, column, **kwargs)
+        return self.run_query_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/pandas/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py`

 * *Files 11% similar despite different names*

```diff
@@ -6,48 +6,46 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to not match regex test case
+Validator for column values to be not null test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToNotMatchRegex import (
-    BaseColumnValuesToNotMatchRegexValidator,
+from metadata.data_quality.validations.column.base.columnValuesToBeNotNull import (
+    BaseColumnValuesToBeNotNullValidator,
 )
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.sqa_like_column import SQALikeColumn
 
 
-class ColumnValuesToNotMatchRegexValidator(
-    BaseColumnValuesToNotMatchRegexValidator, PandasValidatorMixin
+class ColumnValuesToBeNotNullValidator(
+    BaseColumnValuesToBeNotNullValidator, PandasValidatorMixin
 ):
-    """Validator for column values to not match regex test case"""
+    """Validator for column values to be not null test case"""
 
     def _get_column_name(self) -> SQALikeColumn:
         """Get column name from the test case entity link
 
         Returns:
             SQALikeColumn: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             self.runner,
         )
 
-    def _run_results(
-        self, metric: Metrics, column: SQALikeColumn, **kwargs
-    ) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: SQALikeColumn) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_dataframe_results(self.runner, metric, column, **kwargs)
+        return self.run_dataframe_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,46 +6,47 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column value length to be between test case
+Validator for column values sum to be between test case
 """
 
-
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueLengthsToBeBetween import (
-    BaseColumnValueLengthsToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValuesSumToBeBetween import (
+    BaseColumnValuesSumToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValueLengthsToBeBetweenValidator(
-    BaseColumnValueLengthsToBeBetweenValidator, SQAValidatorMixin
+class ColumnValuesSumToBeBetweenValidator(
+    BaseColumnValuesSumToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column value length to be between test case"""
+    """Validator for column values sum to be between test case"""
 
     def _get_column_name(self) -> Column:
-        """get column name from the test case entity link
+        """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
     def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
         """compute result of the test case
 
         Args:
-            metric: metric for computation
+            metric: metric
             column: column
         """
         return self.run_query_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMaxToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py`

 * *Files 9% similar despite different names*

```diff
@@ -4,39 +4,42 @@
 #  You may obtain a copy of the License at
 #  http://www.apache.org/licenses/LICENSE-2.0
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
+
 """
-Validator for column value max to be between test case
+Validator for column value stddevv to be between test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMaxToBeBetween import (
-    BaseColumnValueMaxToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValueStdDevToBeBetween import (
+    BaseColumnValueStdDevToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValueMaxToBeBetweenValidator(
-    BaseColumnValueMaxToBeBetweenValidator, SQAValidatorMixin
+class ColumnValueStdDevToBeBetweenValidator(
+    BaseColumnValueStdDevToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column value max to be between test case"""
+    """Validator for column value stddev to be between test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
-            Column: _description_
+            Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
     def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMeanToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,19 +13,21 @@
 Validator for column value mean to be between test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMeanToBeBetween import (
+from metadata.data_quality.validations.column.base.columnValueMeanToBeBetween import (
     BaseColumnValueMeanToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
 
 
 class ColumnValueMeanToBeBetweenValidator(
     BaseColumnValueMeanToBeBetweenValidator, SQAValidatorMixin
 ):
     """Validator for column value mean to be between test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMedianToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,45 +6,48 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column value median to be between test case
+Validator for column value length to be between test case
 """
 
+
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMedianToBeBetween import (
-    BaseColumnValueMedianToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValueLengthsToBeBetween import (
+    BaseColumnValueLengthsToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValueMedianToBeBetweenValidator(
-    BaseColumnValueMedianToBeBetweenValidator, SQAValidatorMixin
+class ColumnValueLengthsToBeBetweenValidator(
+    BaseColumnValueLengthsToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column value median to be between test case"""
+    """Validator for column value length to be between test case"""
 
     def _get_column_name(self) -> Column:
-        """Get column name from the test case entity link
+        """get column name from the test case entity link
 
         Returns:
             Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
     def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
         """compute result of the test case
 
         Args:
-            metric: metric
+            metric: metric for computation
             column: column
         """
         return self.run_query_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueMinToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,32 +6,34 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column value min to be between test case
+Validator for column value median to be between test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueMinToBeBetween import (
-    BaseColumnValueMinToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValueMedianToBeBetween import (
+    BaseColumnValueMedianToBeBetweenValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValueMinToBeBetweenValidator(
-    BaseColumnValueMinToBeBetweenValidator, SQAValidatorMixin
+class ColumnValueMedianToBeBetweenValidator(
+    BaseColumnValueMedianToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column value min to be between test case"""
+    """Validator for column value median to be between test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py`

 * *Files 19% similar despite different names*

```diff
@@ -6,45 +6,46 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column value stddevv to be between test case
+Validator for column value min to be between test case
 """
 
 from typing import Optional
 
-from sqlalchemy import Column, inspect
-
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValueStdDevToBeBetween import (
-    BaseColumnValueStdDevToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValueMinToBeBetween import (
+    BaseColumnValueMinToBeBetweenValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
+    PandasValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.sqa_like_column import SQALikeColumn
 
 
-class ColumnValueStdDevToBeBetweenValidator(
-    BaseColumnValueStdDevToBeBetweenValidator, SQAValidatorMixin
+class ColumnValueMinToBeBetweenValidator(
+    BaseColumnValueMinToBeBetweenValidator, PandasValidatorMixin
 ):
-    """Validator for column value stddev to be between test case"""
+    """Validator for column value min to be between test case"""
 
-    def _get_column_name(self) -> Column:
+    def _get_column_name(self) -> SQALikeColumn:
         """Get column name from the test case entity link
 
         Returns:
-            Column: column
+            SQALikeColumn: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
-            inspect(self.runner.table).c,
+            self.runner,
         )
 
-    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: SQALikeColumn) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_query_results(self.runner, metric, column)
+        return self.run_dataframe_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesMissingCount.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,19 +13,21 @@
 Validator for column value missing count to be equal test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesMissingCount import (
+from metadata.data_quality.validations.column.base.columnValuesMissingCount import (
     BaseColumnValuesMissingCountValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class ColumnValuesMissingCountValidator(
     BaseColumnValuesMissingCountValidator, SQAValidatorMixin
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesSumToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py`

 * *Files 15% similar despite different names*

```diff
@@ -6,45 +6,47 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values sum to be between test case
+Validator for column value to be in set test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesSumToBeBetween import (
-    BaseColumnValuesSumToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValuesToBeInSet import (
+    BaseColumnValuesToBeInSetValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValuesSumToBeBetweenValidator(
-    BaseColumnValuesSumToBeBetweenValidator, SQAValidatorMixin
+class ColumnValuesToBeInSetValidator(
+    BaseColumnValuesToBeInSetValidator, SQAValidatorMixin
 ):
-    """Validator for column values sum to be between test case"""
+    """Validator for column value to be in set test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
-    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_query_results(self.runner, metric, column)
+        return self.run_query_results(self.runner, metric, column, **kwargs)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py`

 * *Files 17% similar despite different names*

```diff
@@ -6,32 +6,34 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to be between test case
+Validator for column value min to be between test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeBetween import (
-    BaseColumnValuesToBeBetweenValidator,
+from metadata.data_quality.validations.column.base.columnValueMinToBeBetween import (
+    BaseColumnValueMinToBeBetweenValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValuesToBeBetweenValidator(
-    BaseColumnValuesToBeBetweenValidator, SQAValidatorMixin
+class ColumnValueMinToBeBetweenValidator(
+    BaseColumnValueMinToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for column values to be between test case"""
+    """Validator for column value min to be between test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeInSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,32 +6,34 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column value to be in set test case
+Validator for column value to be not in set test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeInSet import (
-    BaseColumnValuesToBeInSetValidator,
+from metadata.data_quality.validations.column.base.columnValuesToBeNotInSet import (
+    BaseColumnValuesToBeNotInSetValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.metrics.registry import Metrics
 
 
-class ColumnValuesToBeInSetValidator(
-    BaseColumnValuesToBeInSetValidator, SQAValidatorMixin
+class ColumnValuesToBeNotInSetValidator(
+    BaseColumnValuesToBeNotInSetValidator, SQAValidatorMixin
 ):
-    """Validator for column value to be in set test case"""
+    """Validator for column value to be not in set test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeNotInSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,45 +6,50 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column value to be not in set test case
+Validator for column values to be not null test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeNotInSet import (
-    BaseColumnValuesToBeNotInSetValidator,
+from metadata.data_quality.validations.column.base.columnValuesToBeNotNull import (
+    BaseColumnValuesToBeNotNullValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.logger import test_suite_logger
+
+logger = test_suite_logger()
 
 
-class ColumnValuesToBeNotInSetValidator(
-    BaseColumnValuesToBeNotInSetValidator, SQAValidatorMixin
+class ColumnValuesToBeNotNullValidator(
+    BaseColumnValuesToBeNotNullValidator, SQAValidatorMixin
 ):
-    """Validator for column value to be not in set test case"""
+    """Validator for column values to be not null test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
-    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_query_results(self.runner, metric, column, **kwargs)
+        return self.run_query_results(self.runner, metric, column)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeNotNull.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,48 +6,59 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to be not null test case
+Validator for column values to match regex test case
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
+from sqlalchemy.exc import CompileError
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeNotNull import (
-    BaseColumnValuesToBeNotNullValidator,
+from metadata.data_quality.validations.column.base.columnValuesToMatchRegex import (
+    BaseColumnValuesToMatchRegexValidator,
+)
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class ColumnValuesToBeNotNullValidator(
-    BaseColumnValuesToBeNotNullValidator, SQAValidatorMixin
+class ColumnValuesToMatchRegexValidator(
+    BaseColumnValuesToMatchRegexValidator, SQAValidatorMixin
 ):
-    """Validator for column values to be not null test case"""
+    """Validator for column values to match regex test case"""
 
     def _get_column_name(self) -> Column:
         """Get column name from the test case entity link
 
         Returns:
             Column: column
         """
         return self.get_column_name(
             self.test_case.entityLink.__root__,
             inspect(self.runner.table).c,
         )
 
-    def _run_results(self, metric: Metrics, column: Column) -> Optional[int]:
+    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
         """compute result of the test case
 
         Args:
             metric: metric
             column: column
         """
-        return self.run_query_results(self.runner, metric, column)
+        try:
+            return self.run_query_results(self.runner, metric, column, **kwargs)
+        except CompileError as err:
+            logger.warning(
+                f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
+            )
+            return self.run_query_results(
+                self.runner, Metrics.LIKE_COUNT, column, **kwargs
+            )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeUnique.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py`

 * *Files 7% similar despite different names*

```diff
@@ -14,19 +14,21 @@
 """
 
 from typing import Optional
 
 from sqlalchemy import Column, inspect
 from sqlalchemy.orm.util import AliasedClass
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToBeUnique import (
+from metadata.data_quality.validations.column.base.columnValuesToBeUnique import (
     BaseColumnValuesToBeUniqueValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.profiler.metrics.registry import Metrics
 
 
 class ColumnValuesToBeUniqueValidator(
     BaseColumnValuesToBeUniqueValidator, SQAValidatorMixin
 ):
     """Validator for column values to be unique test case"""
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToMatchRegex.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py`

 * *Files 23% similar despite different names*

```diff
@@ -6,57 +6,39 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to match regex test case
+Validator for table column name to match set test case
 """
 
+
 from typing import Optional
 
-from sqlalchemy import Column, inspect
-from sqlalchemy.exc import CompileError
+from sqlalchemy import inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToMatchRegex import (
-    BaseColumnValuesToMatchRegexValidator,
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.data_quality.validations.table.base.tableColumnToMatchSet import (
+    BaseTableColumnToMatchSetValidator,
 )
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class ColumnValuesToMatchRegexValidator(
-    BaseColumnValuesToMatchRegexValidator, SQAValidatorMixin
+class TableColumnToMatchSetValidator(
+    BaseTableColumnToMatchSetValidator, SQAValidatorMixin
 ):
-    """Validator for column values to match regex test case"""
+    """Validator for table column name to match set test case"""
 
-    def _get_column_name(self) -> Column:
-        """Get column name from the test case entity link
-
-        Returns:
-            Column: column
-        """
-        return self.get_column_name(
-            self.test_case.entityLink.__root__,
-            inspect(self.runner.table).c,
-        )
-
-    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
-        """compute result of the test case
-
-        Args:
-            metric: metric
-            column: column
-        """
-        try:
-            return self.run_query_results(self.runner, metric, column, **kwargs)
-        except CompileError as err:
-            logger.warning(
-                f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
-            )
-            return self.run_query_results(
-                self.runner, Metrics.LIKE_COUNT, column, **kwargs
+    def _run_results(self) -> Optional[int]:
+        """compute result of the test case"""
+        names = inspect(self.runner.table).c
+        if not names:
+            raise ValueError(
+                f"Column names for test case {self.test_case.name} returned None"
             )
+        return names
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,57 +6,80 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for column values to not match regex test case
+Validator Mixin for SQA tests cases
 """
 
-from typing import Optional
+from typing import Any, List, Optional
 
-from sqlalchemy import Column, inspect
-from sqlalchemy.exc import CompileError
+from sqlalchemy import Column
+from sqlalchemy.exc import SQLAlchemyError
 
+from metadata.profiler.metrics.core import add_props
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.column.base.columnValuesToNotMatchRegex import (
-    BaseColumnValuesToNotMatchRegexValidator,
-)
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
+from metadata.profiler.processor.runner import QueryRunner
+from metadata.utils.entity_link import get_decoded_column
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class ColumnValuesToNotMatchRegexValidator(
-    BaseColumnValuesToNotMatchRegexValidator, SQAValidatorMixin
-):
-    """Validator for column values to not match regex test case"""
+class SQAValidatorMixin:
+    """Validator mixin for SQA test cases"""
 
-    def _get_column_name(self) -> Column:
-        """Get column name from the test case entity link
+    def get_column_name(self, entity_link: str, columns: List) -> Column:
+        """Given a column name get the column object
 
+        Args:
+            column_name (str): Column name
         Returns:
-            SQALikeColumn: column
+            Column: Column object
         """
-        return self.get_column_name(
-            self.test_case.entityLink.__root__,
-            inspect(self.runner.table).c,
+        column = get_decoded_column(entity_link)
+        column_obj = next(
+            (col for col in columns if col.name == column),
+            None,
         )
-
-    def _run_results(self, metric: Metrics, column: Column, **kwargs) -> Optional[int]:
-        """compute result of the test case
+        if column_obj is None:
+            raise ValueError(f"Cannot find column {column}")
+        return column_obj
+
+    def run_query_results(
+        self,
+        runner: QueryRunner,
+        metric: Metrics,
+        column: Optional[Column] = None,
+        **kwargs: Optional[Any],
+    ) -> Optional[int]:
+        """Run the metric query against the column
 
         Args:
-            metric: metric
-            column: column
+            runner (QueryRunner): runner object witj sqlalchemy session object
+            metric (Metrics): metric object
+            column (Column): column object
+            props_ (Optional[Any], optional): props to pass to metric object at runtime. Defaults to None.
+
+        Raises:
+            ValueError: error if no value is returned
+
+        Returns:
+            Any: value returned by the metric query
         """
+        metric_obj = add_props(**kwargs)(metric.value) if kwargs else metric.value
+        metric_fn = metric_obj(column).fn() if column is not None else metric_obj().fn()
+
         try:
-            return self.run_query_results(self.runner, metric, column, **kwargs)
-        except CompileError as err:
-            logger.warning(
-                f"Could not use `REGEXP` due to - {err}. Falling back to `LIKE`"
-            )
-            return self.run_query_results(
-                self.runner, Metrics.NOT_LIKE_COUNT, column, **kwargs
+            value = dict(runner.dispatch_query_select_first(metric_fn))  # type: ignore
+            res = value.get(metric.name)
+        except Exception as exc:
+            raise SQLAlchemyError(exc)
+
+        if res is None:
+            raise ValueError(
+                f"Query on table/column {column.name if column else ''} returned None"
             )
+
+        return res
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/mixins/pandas_validator_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,16 +22,17 @@
 from metadata.utils.entity_link import get_decoded_column
 from metadata.utils.sqa_like_column import SQALikeColumn, Type
 
 
 class PandasValidatorMixin:
     """Validator mixin for Pandas based test cases"""
 
-    def get_column_name(self, entity_link: str, df) -> SQALikeColumn:
-        column = df[get_decoded_column(entity_link)]
+    def get_column_name(self, entity_link: str, dfs) -> SQALikeColumn:
+        # we'll use the first dataframe chunk to get the column name.
+        column = dfs[0][get_decoded_column(entity_link)]
         _type = DATALAKE_DATA_TYPES.get(column.dtypes.name, DataType.STRING.value)
         sqa_like_column = SQALikeColumn(
             name=column.name,
             type=Type(_type),
         )
         sqa_like_column.type.__class__.__name__ = _type
         return sqa_like_column
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/mixins/sqa_validator_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,80 +6,92 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator Mixin for SQA tests cases
+Validator for table column to match set test case
 """
 
-from typing import Any, List, Optional
-
-from sqlalchemy import Column
-from sqlalchemy.exc import SQLAlchemyError
-
-from metadata.profiler.metrics.core import add_props
-from metadata.profiler.metrics.registry import Metrics
-from metadata.profiler.profiler.runner import QueryRunner
-from metadata.utils.entity_link import get_decoded_column
+import collections
+import traceback
+from abc import abstractmethod
+
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
+from metadata.generated.schema.tests.basic import (
+    TestCaseResult,
+    TestCaseStatus,
+    TestResultValue,
+)
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
+COLUMN_NAMES = "columnNames"
 
-class SQAValidatorMixin:
-    """Validator mixin for SQA test cases"""
 
-    def get_column_name(self, entity_link: str, columns: List) -> Column:
-        """Given a column name get the column object
+class BaseTableColumnToMatchSetValidator(BaseTestValidator):
+    """Validator for table column to match set test case"""
 
-        Args:
-            column_name (str): Column name
-        Returns:
-            Column: Column object
-        """
-        column = get_decoded_column(entity_link)
-        column_obj = next(
-            (col for col in columns if col.name == column),
-            None,
-        )
-        if column_obj is None:
-            raise ValueError(f"Cannot find column {column}")
-        return column_obj
-
-    def run_query_results(
-        self,
-        runner: QueryRunner,
-        metric: Metrics,
-        column: Optional[Column] = None,
-        **kwargs: Optional[Any],
-    ) -> Optional[int]:
-        """Run the metric query against the column
-
-        Args:
-            runner (QueryRunner): runner object witj sqlalchemy session object
-            metric (Metrics): metric object
-            column (Column): column object
-            props_ (Optional[Any], optional): props to pass to metric object at runtime. Defaults to None.
+    def compare(self, expected_names, actual_names) -> bool:
+        return collections.Counter(expected_names) == collections.Counter(actual_names)
 
-        Raises:
-            ValueError: error if no value is returned
+    def run_validation(self) -> TestCaseResult:
+        """Run validation for the given test case
 
         Returns:
-            Any: value returned by the metric query
+            TestCaseResult:
         """
-        metric_obj = add_props(**kwargs)(metric.value) if kwargs else metric.value
-        metric_fn = metric_obj(column).fn() if column is not None else metric_obj().fn()
-
         try:
-            value = dict(runner.dispatch_query_select_first(metric_fn))  # type: ignore
-            res = value.get(metric.name)
+            names = self._run_results()
         except Exception as exc:
-            raise SQLAlchemyError(exc)
-
-        if res is None:
-            raise ValueError(
-                f"Query on table/column {column.name if column else ''} returned None"
+            msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
+            logger.debug(traceback.format_exc())
+            logger.warning(msg)
+            return self.get_test_case_result_object(
+                self.execution_date,
+                TestCaseStatus.Aborted,
+                msg,
+                [TestResultValue(name=COLUMN_NAMES, value=None)],
             )
 
-        return res
+        expected_names = self.get_test_case_param_value(
+            self.test_case.parameterValues, "columnNames", str  # type: ignore
+        )
+
+        expected_names = (
+            [item.strip() for item in expected_names.split(",")]
+            if expected_names
+            else []
+        )
+
+        ordered = self.get_test_case_param_value(
+            self.test_case.parameterValues,  # type: ignore
+            "ordered",
+            bool,
+            default=False,
+        )
+
+        if ordered:
+            names_match = expected_names == names
+        else:
+            names_match = self.compare(expected_names, names)
+
+        status = self.get_test_case_status(names_match)
+        result_value = 1 if status == TestCaseStatus.Success else 0
+
+        result = (
+            f"Found {self.format_column_list(status, names)} column vs. "
+            f"the expected column names {self.format_column_list(status, expected_names)}."
+        )
+
+        return self.get_test_case_result_object(
+            self.execution_date,
+            status,
+            result,
+            [TestResultValue(name=COLUMN_NAMES, value=str(result_value))],
+        )
+
+    @abstractmethod
+    def _run_results(self):
+        raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,20 +12,20 @@
 """
 Validator for table column count to be between test case
 """
 
 import traceback
 from abc import abstractmethod
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 COLUMN_COUNT = "columnCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,62 +6,65 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column count to be equal test case
+Validator for table column nanme to exist test case
 """
 
 import traceback
 from abc import abstractmethod
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
-COLUMN_COUNT = "columnCount"
+COLUMN_NAMES_EXISTS = "columnNameExits"
 
 
-class BaseTableColumnCountToEqualValidator(BaseTestValidator):
-    """Validator for table column count to be equal test case"""
+class BaseTableColumnNameToExistValidator(BaseTestValidator):
+    """Validator for table column nanme to exist test case"""
 
     def run_validation(self) -> TestCaseResult:
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
         try:
-            count = self._run_results()
+            names = self._run_results()
         except Exception as exc:
             msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
-                [TestResultValue(name=COLUMN_COUNT, value=None)],
+                [TestResultValue(name=COLUMN_NAMES_EXISTS, value=None)],
             )
 
-        expected_count = self.get_test_case_param_value(
-            self.test_case.parameterValues, "columnCount", int  # type: ignore
+        name_to_exist = self.get_test_case_param_value(
+            self.test_case.parameterValues, "columnName", str  # type: ignore
         )
 
+        status = self.get_test_case_status(name_to_exist in names)
+        result_value = 1 if status == TestCaseStatus.Success else 0
+
         return self.get_test_case_result_object(
             self.execution_date,
-            self.get_test_case_status(count == expected_count),
-            f"Found {count} columns vs. the expected {expected_count}",
-            [TestResultValue(name=COLUMN_COUNT, value=str(count))],
+            status,
+            f"{name_to_exist} column expected vs {self.format_column_list(status, names)}",
+            [TestResultValue(name=COLUMN_NAMES_EXISTS, value=str(result_value))],
         )
 
     @abstractmethod
     def _run_results(self):
         raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnNameToExist.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,65 +6,66 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column nanme to exist test case
+Validator for table row count to equal test case
 """
 
 import traceback
 from abc import abstractmethod
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
+from metadata.profiler.metrics.registry import Metrics
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
-COLUMN_NAMES_EXISTS = "columnNameExits"
+ROW_COUNT = "rowCount"
 
 
-class BaseTableColumnNameToExistValidator(BaseTestValidator):
-    """Validator for table column nanme to exist test case"""
+class BaseTableRowCountToEqualValidator(BaseTestValidator):
+    """Validator for table row count to equal test case"""
 
     def run_validation(self) -> TestCaseResult:
         """Run validation for the given test case
 
         Returns:
             TestCaseResult:
         """
         try:
-            names = self._run_results()
-        except Exception as exc:
+            res = self._run_results(Metrics.ROW_COUNT)
+        except ValueError as exc:
             msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
-                [TestResultValue(name=COLUMN_NAMES_EXISTS, value=None)],
+                [TestResultValue(name=ROW_COUNT, value=None)],
             )
 
-        name_to_exist = self.get_test_case_param_value(
-            self.test_case.parameterValues, "columnName", str  # type: ignore
+        expected_count = self.get_test_case_param_value(
+            self.test_case.parameterValues,  # type: ignore
+            "value",
+            float,
+            default=float("-inf"),
         )
 
-        status = self.get_test_case_status(name_to_exist in names)
-        result_value = 1 if status == TestCaseStatus.Success else 0
-
         return self.get_test_case_result_object(
             self.execution_date,
-            status,
-            f"{name_to_exist} column expected vs {self.format_column_list(status, names)}",
-            [TestResultValue(name=COLUMN_NAMES_EXISTS, value=str(result_value))],
+            self.get_test_case_status(expected_count == res),
+            f"Found rowCount={res} rows vs. the expected {expected_count}",
+            [TestResultValue(name=ROW_COUNT, value=str(res))],
         )
 
     @abstractmethod
-    def _run_results(self):
+    def _run_results(self, metric: Metrics):
         raise NotImplementedError
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,92 +6,61 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column to match set test case
+Validator for table row inserted count to be between test case
 """
 
-import collections
-import traceback
-from abc import abstractmethod
-
-from metadata.generated.schema.tests.basic import (
-    TestCaseResult,
-    TestCaseStatus,
-    TestResultValue,
-)
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
-from metadata.utils.logger import test_suite_logger
-
-logger = test_suite_logger()
-
-COLUMN_NAMES = "columnNames"
-
-
-class BaseTableColumnToMatchSetValidator(BaseTestValidator):
-    """Validator for table column to match set test case"""
-
-    def compare(self, expected_names, actual_names) -> bool:
-        return collections.Counter(expected_names) == collections.Counter(actual_names)
-
-    def run_validation(self) -> TestCaseResult:
-        """Run validation for the given test case
-
-        Returns:
-            TestCaseResult:
-        """
-        try:
-            names = self._run_results()
-        except Exception as exc:
-            msg = f"Error computing {self.test_case.fullyQualifiedName}: {exc}"  # type: ignore
-            logger.debug(traceback.format_exc())
-            logger.warning(msg)
-            return self.get_test_case_result_object(
-                self.execution_date,
-                TestCaseStatus.Aborted,
-                msg,
-                [TestResultValue(name=COLUMN_NAMES, value=None)],
-            )
+from sqlalchemy import Column, text
 
-        expected_names = self.get_test_case_param_value(
-            self.test_case.parameterValues, "columnNames", str  # type: ignore
-        )
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.data_quality.validations.table.base.tableRowInsertedCountToBeBetween import (
+    BaseTableRowInsertedCountToBeBetweenValidator,
+)
+from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.sqa_utils import (
+    dispatch_to_date_or_datetime,
+    get_partition_col_type,
+)
 
-        expected_names = (
-            [item.strip() for item in expected_names.split(",")]
-            if expected_names
-            else []
-        )
 
-        ordered = self.get_test_case_param_value(
+class TableRowInsertedCountToBeBetweenValidator(
+    BaseTableRowInsertedCountToBeBetweenValidator, SQAValidatorMixin
+):
+    """Validator for table row inserted count to be between test case"""
+
+    def _get_column_name(self):
+        """returns the column name to be validated"""
+        return self.get_test_case_param_value(
             self.test_case.parameterValues,  # type: ignore
-            "ordered",
-            bool,
-            default=False,
+            "columnName",
+            Column,
         )
 
-        if ordered:
-            names_match = expected_names == names
-        else:
-            names_match = self.compare(expected_names, names)
-
-        status = self.get_test_case_status(names_match)
-        result_value = 1 if status == TestCaseStatus.Success else 0
-
-        result = (
-            f"Found {self.format_column_list(status, names)} column vs. "
-            f"the expected column names {self.format_column_list(status, expected_names)}."
-        )
+    def _run_results(self, column_name: str, range_type: str, range_interval: int):
+        """Execute the validation for the given test case
 
-        return self.get_test_case_result_object(
-            self.execution_date,
-            status,
-            result,
-            [TestResultValue(name=COLUMN_NAMES, value=str(result_value))],
+        Args:
+            column_name (str): column name
+            range_type (str): range type (DAY, HOUR, MONTH, YEAR)
+            range_interval (int): range interval
+        """
+        date_or_datetime_fn = dispatch_to_date_or_datetime(
+            range_interval,
+            text(range_type),
+            get_partition_col_type(column_name.name, self.runner.table.__table__.c),  # type: ignore
         )
 
-    @abstractmethod
-    def _run_results(self):
-        raise NotImplementedError
+        return dict(
+            self.runner.dispatch_query_select_first(
+                Metrics.ROW_COUNT.value().fn(),
+                query_filter_={
+                    "filters": [(column_name, "ge", date_or_datetime_fn)],
+                    "or_filter": False,
+                },
+            )  # type: ignore
+        ).get(Metrics.ROW_COUNT.name)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py`

 * *Files 14% similar despite different names*

```diff
@@ -13,20 +13,20 @@
 Validator for table custom SQL Query test case
 """
 
 import traceback
 from abc import abstractmethod
 from typing import cast
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 RESULT_ROW_COUNT = "resultRowCount"
 
 
@@ -54,21 +54,21 @@
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
                 [TestResultValue(name=RESULT_ROW_COUNT, value=None)],
             )
-
-        if len(rows) == 0:
+        len_rows = rows if isinstance(rows, int) else len(rows)
+        if len_rows == 0:
             status = TestCaseStatus.Success
             result_value = 0
         else:
             status = TestCaseStatus.Failed
-            result_value = len(rows)
+            result_value = len_rows
 
         return self.get_test_case_result_object(
             self.execution_date,
             status,
             f"Found {result_value} row(s). Test query is expected to return 0 row.",
             [TestResultValue(name=RESULT_ROW_COUNT, value=str(result_value))],
         )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,21 +12,21 @@
 """
 Validator for table row count to be between test case
 """
 
 import traceback
 from abc import abstractmethod
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
 from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 ROW_COUNT = "rowCount"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/base/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,20 +13,20 @@
 Validator for table row inserted count to be between test case
 """
 
 import traceback
 from abc import abstractmethod
 from typing import cast
 
+from metadata.data_quality.validations.base_test_handler import BaseTestValidator
 from metadata.generated.schema.tests.basic import (
     TestCaseResult,
     TestCaseStatus,
     TestResultValue,
 )
-from metadata.test_suite.validations.base_test_handler import BaseTestValidator
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 ROW_COUNT = "rowCount"
 
 
@@ -60,15 +60,15 @@
             range_interval = cast(int, range_interval)
             column_name = cast(str, column_name)
             range_type = cast(str, range_type)
 
             res = self._run_results(column_name, range_type, range_interval)
 
         except Exception as exc:
-            msg = f"Error computing {self.test_case.name} for {self.runner.table.__tablename__}: {exc}"  # type: ignore
+            msg = f"Error computing {self.test_case.name}: {exc}"  # type: ignore
             logger.debug(traceback.format_exc())
             logger.warning(msg)
             return self.get_test_case_result_object(
                 self.execution_date,
                 TestCaseStatus.Aborted,
                 msg,
                 [TestResultValue(name=ROW_COUNT, value=None)],
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py`

 * *Files 14% similar despite different names*

```diff
@@ -6,28 +6,29 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column count to be between test case
+Validator for table row count to be equal test case
 """
 
 from typing import Optional
 
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableColumnCountToBeBetween import (
-    BaseTableColumnCountToBeBetweenValidator,
+from metadata.data_quality.validations.table.base.tableRowCountToEqual import (
+    BaseTableRowCountToEqualValidator,
 )
+from metadata.profiler.metrics.registry import Metrics
 
 
-class TableColumnCountToBeBetweenValidator(
-    BaseTableColumnCountToBeBetweenValidator, PandasValidatorMixin
+class TableRowCountToEqualValidator(
+    BaseTableRowCountToEqualValidator, PandasValidatorMixin
 ):
-    """Validator for table column count to be between test case"""
+    """Validator for table row count to be equal test case"""
 
-    def _run_results(self) -> Optional[int]:
+    def _run_results(self, metric: Metrics) -> Optional[int]:
         """compute result of the test case"""
-        return len(self.runner.columns)
+        return self.run_dataframe_results(self.runner, metric)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,31 +6,36 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column count to be equal test case
+Validator for table column count to be between test case
 """
 
 from typing import Optional
 
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
-    PandasValidatorMixin,
+from sqlalchemy import inspect
+
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableColumnCountToEqual import (
-    BaseTableColumnCountToEqualValidator,
+from metadata.data_quality.validations.table.base.tableColumnCountToBeBetween import (
+    BaseTableColumnCountToBeBetweenValidator,
 )
-from metadata.utils.logger import test_suite_logger
-
-logger = test_suite_logger()
 
 
-class TableColumnCountToEqualValidator(
-    BaseTableColumnCountToEqualValidator, PandasValidatorMixin
+class TableColumnCountToBeBetweenValidator(
+    BaseTableColumnCountToBeBetweenValidator, SQAValidatorMixin
 ):
-    """Validator for table column count to be equal test case"""
+    """Validator for table column count to be between test case"""
 
     def _run_results(self) -> Optional[int]:
         """compute result of the test case"""
-        return len(self.runner.columns)
+        count = len(inspect(self.runner.table).c)
+        if not count:
+            raise ValueError(
+                f"Column Count for test case {self.test_case.name} returned None"
+            )
+
+        return count
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnNameToExist.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,32 +9,32 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Validator for table column name to exist test case
 """
 
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableColumnNameToExist import (
+from metadata.data_quality.validations.table.base.tableColumnNameToExist import (
     BaseTableColumnNameToExistValidator,
 )
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class TableColumnNameToExistValidator(
     BaseTableColumnNameToExistValidator, PandasValidatorMixin
 ):
     """Validator for table column name to exist test case"""
 
     def _run_results(self):
         """compute result of the test case"""
-        names = list(self.runner.columns)
+        names = list(self.runner[0].columns)
         if not names:
             raise ValueError(
                 f"Column names for test case {self.test_case.name} returned None"
             )
 
         return names
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,31 +10,31 @@
 #  limitations under the License.
 
 """
 Validator for table column name to match set test case
 """
 
 
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableColumnToMatchSet import (
+from metadata.data_quality.validations.table.base.tableColumnToMatchSet import (
     BaseTableColumnToMatchSetValidator,
 )
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
 class TableColumnToMatchSetValidator(
     BaseTableColumnToMatchSetValidator, PandasValidatorMixin
 ):
     """Validator table column name to match set test case"""
 
     def _run_results(self):
         """compute result of the test case"""
-        names = list(self.runner.columns)
+        names = list(self.runner[0].columns)
         if not names:
             raise ValueError(
                 f"Column names for test case {self.test_case.name} returned None"
             )
         return names
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,23 +9,29 @@
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
 Validator for table custom SQL Query test case
 """
 
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
-    PandasValidatorMixin,
+from sqlalchemy import text
+
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableCustomSQLQuery import (
+from metadata.data_quality.validations.table.base.tableCustomSQLQuery import (
     BaseTableCustomSQLQueryValidator,
 )
 
 
-class TableCustomSQLQueryValidator(
-    BaseTableCustomSQLQueryValidator, PandasValidatorMixin
-):
+class TableCustomSQLQueryValidator(BaseTableCustomSQLQueryValidator, SQAValidatorMixin):
     """Validator for table custom SQL Query test case"""
 
-    def _run_results(self, sql_expression: str):
+    def _run_results(self, sql_expression):
         """compute result of the test case"""
-        return self.runner.query(sql_expression)
+        try:
+            return self.runner._session.execute(  # pylint: disable=protected-access
+                text(sql_expression)
+            ).all()
+        except Exception as exc:
+            self.runner._session.rollback()  # pylint: disable=protected-access
+            raise exc
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,24 +11,27 @@
 
 """
 Validator for table row count to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
-    PandasValidatorMixin,
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableRowCountToBeBetween import (
+from metadata.data_quality.validations.table.base.tableRowCountToBeBetween import (
     BaseTableRowCountToBeBetweenValidator,
 )
+from metadata.profiler.metrics.registry import Metrics
+from metadata.utils.logger import test_suite_logger
+
+logger = test_suite_logger()
 
 
 class TableRowCountToBeBetweenValidator(
-    BaseTableRowCountToBeBetweenValidator, PandasValidatorMixin
+    BaseTableRowCountToBeBetweenValidator, SQAValidatorMixin
 ):
     """Validator for table row count to be between test case"""
 
     def _run_results(self, metric: Metrics) -> Optional[int]:
         """compute result of the test case"""
-        return self.run_dataframe_results(self.runner, metric)
+        return self.run_query_results(self.runner, metric)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableRowCountToEqual.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py`

 * *Files 10% similar despite different names*

```diff
@@ -6,29 +6,29 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table row count to be equal test case
+Validator for table row inserted count to be between test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
-    PandasValidatorMixin,
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableRowCountToEqual import (
+from metadata.data_quality.validations.table.base.tableRowCountToEqual import (
     BaseTableRowCountToEqualValidator,
 )
+from metadata.profiler.metrics.registry import Metrics
 
 
 class TableRowCountToEqualValidator(
-    BaseTableRowCountToEqualValidator, PandasValidatorMixin
+    BaseTableRowCountToEqualValidator, SQAValidatorMixin
 ):
-    """Validator for table row count to be equal test case"""
+    """Validator for table row inserted count to be between test case"""
 
     def _run_results(self, metric: Metrics) -> Optional[int]:
         """compute result of the test case"""
-        return self.run_dataframe_results(self.runner, metric)
+        return self.run_query_results(self.runner, metric)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/pandas/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py`

 * *Files 16% similar despite different names*

```diff
@@ -13,20 +13,23 @@
 Validator for table row inserted count to be between test case
 """
 
 from datetime import datetime
 
 from dateutil.relativedelta import relativedelta
 
-from metadata.test_suite.validations.mixins.pandas_validator_mixin import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
     PandasValidatorMixin,
 )
-from metadata.test_suite.validations.table.base.tableRowInsertedCountToBeBetween import (
+from metadata.data_quality.validations.table.base.tableRowInsertedCountToBeBetween import (
     BaseTableRowInsertedCountToBeBetweenValidator,
 )
+from metadata.utils.logger import test_suite_logger
+
+logger = test_suite_logger()
 
 
 class TableRowInsertedCountToBeBetweenValidator(
     BaseTableRowInsertedCountToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for table row inserted count to be between test case"""
 
@@ -67,9 +70,11 @@
 
         Args:
             column_name (str): column name
             range_type (str): range type (DAY, HOUR, MONTH, YEAR)
             range_interval (int): range interval
         """
         threshold_date = self._get_threshold_date(range_type, range_interval)
-
-        return len(self.runner.query(f"{column_name} >= {threshold_date}"))
+        return sum(
+            len(runner.query(f"{column_name} >= {threshold_date}"))
+            for runner in self.runner
+        )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py`

 * *Files 11% similar despite different names*

```diff
@@ -11,29 +11,23 @@
 
 """
 Validator for table column count to be between test case
 """
 
 from typing import Optional
 
-from sqlalchemy import inspect
-
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableColumnCountToBeBetween import (
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
+    PandasValidatorMixin,
+)
+from metadata.data_quality.validations.table.base.tableColumnCountToBeBetween import (
     BaseTableColumnCountToBeBetweenValidator,
 )
 
 
 class TableColumnCountToBeBetweenValidator(
-    BaseTableColumnCountToBeBetweenValidator, SQAValidatorMixin
+    BaseTableColumnCountToBeBetweenValidator, PandasValidatorMixin
 ):
     """Validator for table column count to be between test case"""
 
     def _run_results(self) -> Optional[int]:
         """compute result of the test case"""
-        count = len(inspect(self.runner.table).c)
-        if not count:
-            raise ValueError(
-                f"Column Count for test case {self.test_case.name} returned None"
-            )
-
-        return count
+        return len(self.runner[0].columns)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnCountToEqual.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,16 +13,18 @@
 Validator for table column count to be equal test case
 """
 
 from typing import Optional
 
 from sqlalchemy import inspect
 
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableColumnCountToEqual import (
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
+)
+from metadata.data_quality.validations.table.base.tableColumnCountToEqual import (
     BaseTableColumnCountToEqualValidator,
 )
 
 
 class TableColumnCountToEqualValidator(
     BaseTableColumnCountToEqualValidator, SQAValidatorMixin
 ):
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnNameToExist.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/profiler/metrics/static/row_count.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,35 +6,47 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column nanme to exist test case
+Table Count Metric definition
 """
+# pylint: disable=duplicate-code
 
-from sqlalchemy import inspect
 
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableColumnNameToExist import (
-    BaseTableColumnNameToExistValidator,
-)
-from metadata.utils.logger import test_suite_logger
-
-logger = test_suite_logger()
-
-
-class TableColumnNameToExistValidator(
-    BaseTableColumnNameToExistValidator, SQAValidatorMixin
-):
-    """Validator for table column nanme to exist test case"""
-
-    def _run_results(self):
-        """compute result of the test case"""
-        names = inspect(self.runner.table).c
-        if not names:
-            raise ValueError(
-                f"Column names for test case {self.test_case.name} returned None"
-            )
+from sqlalchemy import func
 
-        return names
+from metadata.profiler.metrics.core import StaticMetric, _label
+
+
+class RowCount(StaticMetric):
+    """
+    ROW_NUMBER Metric
+
+    Count all rows on a table
+    """
+
+    @classmethod
+    def name(cls):
+        return "rowCount"
+
+    @classmethod
+    def is_col_metric(cls) -> bool:
+        """
+        Mark the class as a Table Metric
+        """
+        return False
+
+    @property
+    def metric_type(self):
+        return int
+
+    @_label
+    def fn(self):
+        """sqlalchemy function"""
+        return func.count()
+
+    def df_fn(self, dfs=None):
+        """pandas function"""
+        return sum(len(df.index) for df in dfs)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableColumnToMatchSet.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py`

 * *Files 21% similar despite different names*

```diff
@@ -6,37 +6,35 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table column name to match set test case
+Validator for table custom SQL Query test case
 """
 
-
-from typing import Optional
-
-from sqlalchemy import inspect
-
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableColumnToMatchSet import (
-    BaseTableColumnToMatchSetValidator,
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
+    PandasValidatorMixin,
+)
+from metadata.data_quality.validations.table.base.tableCustomSQLQuery import (
+    BaseTableCustomSQLQueryValidator,
 )
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class TableColumnToMatchSetValidator(
-    BaseTableColumnToMatchSetValidator, SQAValidatorMixin
+class TableCustomSQLQueryValidator(
+    BaseTableCustomSQLQueryValidator, PandasValidatorMixin
 ):
-    """Validator for table column name to match set test case"""
+    """Validator for table custom SQL Query test case"""
 
-    def _run_results(self) -> Optional[int]:
+    def _run_results(self, sql_expression: str):
         """compute result of the test case"""
-        names = inspect(self.runner.table).c
-        if not names:
-            raise ValueError(
-                f"Column names for test case {self.test_case.name} returned None"
-            )
-        return names
+        return sum(  # pylint: disable=consider-using-generator
+            [
+                len(runner.query(sql_expression))
+                for runner in self.runner
+                if len(runner.query(sql_expression))
+            ]
+        )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableCustomSQLQuery.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,26 +6,29 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table custom SQL Query test case
+Validator for table row count to be between test case
 """
 
-from sqlalchemy import text
+from typing import Optional
 
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableCustomSQLQuery import (
-    BaseTableCustomSQLQueryValidator,
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
+    PandasValidatorMixin,
 )
+from metadata.data_quality.validations.table.base.tableRowCountToBeBetween import (
+    BaseTableRowCountToBeBetweenValidator,
+)
+from metadata.profiler.metrics.registry import Metrics
 
 
-class TableCustomSQLQueryValidator(BaseTableCustomSQLQueryValidator, SQAValidatorMixin):
-    """Validator for table custom SQL Query test case"""
+class TableRowCountToBeBetweenValidator(
+    BaseTableRowCountToBeBetweenValidator, PandasValidatorMixin
+):
+    """Validator for table row count to be between test case"""
 
-    def _run_results(self, sql_expression):
+    def _run_results(self, metric: Metrics) -> Optional[int]:
         """compute result of the test case"""
-        return self.runner._session.execute(  # pylint: disable=protected-access
-            text(sql_expression)
-        ).all()
+        return self.run_dataframe_results(self.runner, metric)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableRowCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py`

 * *Files 24% similar despite different names*

```diff
@@ -6,30 +6,31 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table row count to be between test case
+Validator for table column count to be equal test case
 """
 
 from typing import Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableRowCountToBeBetween import (
-    BaseTableRowCountToBeBetweenValidator,
+from metadata.data_quality.validations.mixins.pandas_validator_mixin import (
+    PandasValidatorMixin,
+)
+from metadata.data_quality.validations.table.base.tableColumnCountToEqual import (
+    BaseTableColumnCountToEqualValidator,
 )
 from metadata.utils.logger import test_suite_logger
 
 logger = test_suite_logger()
 
 
-class TableRowCountToBeBetweenValidator(
-    BaseTableRowCountToBeBetweenValidator, SQAValidatorMixin
+class TableColumnCountToEqualValidator(
+    BaseTableColumnCountToEqualValidator, PandasValidatorMixin
 ):
-    """Validator for table row count to be between test case"""
+    """Validator for table column count to be equal test case"""
 
-    def _run_results(self, metric: Metrics) -> Optional[int]:
+    def _run_results(self) -> Optional[int]:
         """compute result of the test case"""
-        return self.run_query_results(self.runner, metric)
+        return len(self.runner[0].columns)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableRowCountToEqual.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py`

 * *Files 20% similar despite different names*

```diff
@@ -6,27 +6,37 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table row inserted count to be between test case
+Validator for table column nanme to exist test case
 """
 
-from typing import Optional
+from sqlalchemy import inspect
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableRowCountToEqual import (
-    BaseTableRowCountToEqualValidator,
+from metadata.data_quality.validations.mixins.sqa_validator_mixin import (
+    SQAValidatorMixin,
 )
+from metadata.data_quality.validations.table.base.tableColumnNameToExist import (
+    BaseTableColumnNameToExistValidator,
+)
+from metadata.utils.logger import test_suite_logger
+
+logger = test_suite_logger()
 
 
-class TableRowCountToEqualValidator(
-    BaseTableRowCountToEqualValidator, SQAValidatorMixin
+class TableColumnNameToExistValidator(
+    BaseTableColumnNameToExistValidator, SQAValidatorMixin
 ):
-    """Validator for table row inserted count to be between test case"""
+    """Validator for table column nanme to exist test case"""
 
-    def _run_results(self, metric: Metrics) -> Optional[int]:
+    def _run_results(self):
         """compute result of the test case"""
-        return self.run_query_results(self.runner, metric)
+        names = inspect(self.runner.table).c
+        if not names:
+            raise ValueError(
+                f"Column names for test case {self.test_case.name} returned None"
+            )
+
+        return names
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/ingestion/source/pipeline/airflow/models.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,59 +6,63 @@
 #  Unless required by applicable law or agreed to in writing, software
 #  distributed under the License is distributed on an "AS IS" BASIS,
 #  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 #  See the License for the specific language governing permissions and
 #  limitations under the License.
 
 """
-Validator for table row inserted count to be between test case
+Tableau Source Model module
 """
 
-from sqlalchemy import Column, text
+from datetime import datetime
+from typing import Any, List, Optional
 
-from metadata.profiler.metrics.registry import Metrics
-from metadata.test_suite.validations.mixins.sqa_validator_mixin import SQAValidatorMixin
-from metadata.test_suite.validations.table.base.tableRowInsertedCountToBeBetween import (
-    BaseTableRowInsertedCountToBeBetweenValidator,
-)
-from metadata.utils.sqa_utils import (
-    dispatch_to_date_or_datetime,
-    get_partition_col_type,
-)
-
-
-class TableRowInsertedCountToBeBetweenValidator(
-    BaseTableRowInsertedCountToBeBetweenValidator, SQAValidatorMixin
-):
-    """Validator for table row inserted count to be between test case"""
-
-    def _get_column_name(self):
-        """returns the column name to be validated"""
-        return self.get_test_case_param_value(
-            self.test_case.parameterValues,  # type: ignore
-            "columnName",
-            Column,
-        )
-
-    def _run_results(self, column_name: str, range_type: str, range_interval: int):
-        """Execute the validation for the given test case
-
-        Args:
-            column_name (str): column name
-            range_type (str): range type (DAY, HOUR, MONTH, YEAR)
-            range_interval (int): range interval
-        """
-        date_or_datetime_fn = dispatch_to_date_or_datetime(
-            range_interval,
-            text(range_type),
-            get_partition_col_type(column_name.name, self.runner.table.__table__.c),  # type: ignore
-        )
-
-        return dict(
-            self.runner.dispatch_query_select_first(
-                Metrics.ROW_COUNT.value().fn(),
-                query_filter_={
-                    "filters": [(column_name, "ge", date_or_datetime_fn)],
-                    "or_filter": False,
-                },
-            )  # type: ignore
-        ).get(Metrics.ROW_COUNT.name)
+from pydantic import BaseModel, Extra, Field
+
+
+class AirflowBaseModel(BaseModel):
+    """
+    Tableau basic configurations
+    """
+
+    class Config:
+        extra = Extra.allow
+        arbitrary_types_allowed = True
+
+    dag_id: str
+
+
+class Task(BaseModel):
+    pool: Optional[str]
+    doc_md: Optional[str]
+    inlets: Optional[List[Any]] = Field(alias="_inlets")
+    task_id: str
+    outlets: Optional[List[Any]] = Field(alias="_outlets")
+    task_type: Optional[Any] = Field(alias="_task_type")
+    downstream_task_ids: List[str]
+    start_date: Optional[datetime]
+    end_date: Optional[datetime]
+
+
+class TaskList(BaseModel):
+    __root__: List[Task]
+
+
+class Dag(BaseModel):
+    fileloc: str
+    tags: Optional[List[str]]
+    start_date: float
+    _processor_dags_folder: str
+
+
+class AirflowDag(BaseModel):
+    dag: Optional[Dag]
+
+
+class AirflowDagDetails(AirflowBaseModel):
+    fileloc: str
+    data: AirflowDag
+    max_active_runs: Optional[int]
+    description: Optional[str]
+    start_date: Optional[datetime] = None
+    tasks: List[Task]
+    owners: Any
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/test_suite/validations/validator.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/data_quality/validations/validator.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/timer/repeated_timer.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/timer/repeated_timer.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/timer/workflow_reporter.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/timer/workflow_reporter.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/azure_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/azure_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/class_helper.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/class_helper.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,14 @@
         source_type = "metadataes"
     return source_type
 
 
 def _get_service_type_from(  # pylint: disable=inconsistent-return-statements
     service_subtype: str,
 ) -> ServiceType:
-
     for service_type in ServiceType:
         if service_subtype.lower() in [
             subtype.value.lower()
             for subtype in locate(
                 f"metadata.generated.schema.entity.services.{service_type.name.lower()}Service.{service_type.name}ServiceType"  # pylint: disable=line-too-long
             )
             or []
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/constants.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/constants.py`

 * *Files 12% similar despite different names*

```diff
@@ -24,7 +24,15 @@
     "caCerts": "ca_certs",
     "regionName": "region_name",
     "timeout": "timeout",
     "useAwsCredentials": "use_AWS_credentials",
     "useSSL": "use_ssl",
     "verifyCerts": "verify_certs",
 }
+
+QUERY_WITH_OM_VERSION = '/* {"app": "OpenMetadata"'
+
+QUERY_WITH_DBT = '/* {"app": "dbt"'
+
+AUTHORIZATION_HEADER = "Authorization"
+
+NO_ACCESS_TOKEN = "no_token"
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/credentials.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/credentials.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/custom_thread_pool.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/custom_thread_pool.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/dbt_config.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/dbt_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -189,29 +189,29 @@
             run_id = runs_data[0]["id"]
             try:
                 logger.debug("Requesting [dbt_catalog]")
                 dbt_catalog = client.get(
                     f"/accounts/{account_id}/runs/{run_id}/artifacts/{DBT_CATALOG_FILE_NAME}"
                 )
             except Exception as exc:
-                logger.info(
+                logger.debug(
                     f"dbt catalog file not found, skipping the catalog file: {exc}"
                 )
                 logger.debug(traceback.format_exc())
             logger.debug("Requesting [dbt_manifest]")
             dbt_manifest = client.get(
                 f"/accounts/{account_id}/runs/{run_id}/artifacts/{DBT_MANIFEST_FILE_NAME}"
             )
             try:
                 logger.debug("Requesting [dbt_run_results]")
                 dbt_run_results = client.get(
                     f"/accounts/{account_id}/runs/{run_id}/artifacts/{DBT_RUN_RESULTS_FILE_NAME}"
                 )
             except Exception as exc:
-                logger.info(
+                logger.debug(
                     f"dbt run_results file not found, skipping dbt tests: {exc}"
                 )
                 logger.debug(traceback.format_exc())
         if not dbt_manifest:
             raise DBTConfigException("Manifest file not found in DBT Cloud")
 
         return DbtFiles(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/dispatch.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/dispatch.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/elasticsearch.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/elasticsearch.py`

 * *Files 13% similar despite different names*

```diff
@@ -12,18 +12,23 @@
 Helper methods for ES
 """
 
 from typing import List, Optional, TypeVar
 
 from pydantic import BaseModel
 
+from metadata.generated.schema.analytics.reportData import ReportData
+from metadata.generated.schema.entity.classification.tag import Tag
+from metadata.generated.schema.entity.data.container import Container
 from metadata.generated.schema.entity.data.dashboard import Dashboard
 from metadata.generated.schema.entity.data.glossary import Glossary
+from metadata.generated.schema.entity.data.glossaryTerm import GlossaryTerm
 from metadata.generated.schema.entity.data.mlmodel import MlModel
 from metadata.generated.schema.entity.data.pipeline import Pipeline
+from metadata.generated.schema.entity.data.query import Query
 from metadata.generated.schema.entity.data.table import Table
 from metadata.generated.schema.entity.data.topic import Topic
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
 from metadata.utils.logger import utils_logger
 
 logger = utils_logger()
@@ -33,15 +38,22 @@
     Table.__name__: "table_search_index",
     Team.__name__: "team_search_index",
     User.__name__: "user_search_index",
     Dashboard.__name__: "dashboard_search_index",
     Topic.__name__: "topic_search_index",
     Pipeline.__name__: "pipeline_search_index",
     Glossary.__name__: "glossary_search_index",
+    GlossaryTerm.__name__: "glossary_search_index",
     MlModel.__name__: "mlmodel_search_index",
+    Tag.__name__: "tag_search_index",
+    Container.__name__: "container_search_index",
+    Query.__name__: "query_search_index",
+    ReportData.__name__: "entity_report_data_index",
+    "web_analytic_user_activity_report": "web_analytic_user_activity_report_data_index",
+    "web_analytic_entity_view_report": "web_analytic_entity_view_report_data_index",
 }
 
 
 def get_entity_from_es_result(
     entity_list: Optional[List[T]], fetch_multiple_entities: bool = False
 ) -> Optional[T]:
     """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/filters.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/filters.py`

 * *Files 6% similar despite different names*

```diff
@@ -219,7 +219,22 @@
     Include takes precedence over exclude
 
     :param container_filter_pattern: Container defining the container filtering logic
     :param container_name: container name
     :return: True for filtering, False otherwise
     """
     return _filter(container_filter_pattern, container_name)
+
+
+def filter_by_datamodel(
+    datamodel_filter_pattern: Optional[FilterPattern], datamodel_name: str
+) -> bool:
+    """
+    Return True if the models needs to be filtered, False otherwise
+
+    Include takes precedence over exclude
+
+    :param datamodel_filter_pattern: Model defining data model filtering logic
+    :param datamodel_name: data model name
+    :return: True for filtering, False otherwise
+    """
+    return _filter(datamodel_filter_pattern, datamodel_name)
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/fqn.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/fqn.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,17 +25,17 @@
 
 from metadata.antlr.split_listener import FqnSplitListener
 from metadata.generated.antlr.FqnLexer import FqnLexer
 from metadata.generated.antlr.FqnParser import FqnParser
 from metadata.generated.schema.entity.classification.tag import Tag
 from metadata.generated.schema.entity.data.chart import Chart
 from metadata.generated.schema.entity.data.dashboard import Dashboard
+from metadata.generated.schema.entity.data.dashboardDataModel import DashboardDataModel
 from metadata.generated.schema.entity.data.database import Database
 from metadata.generated.schema.entity.data.databaseSchema import DatabaseSchema
-from metadata.generated.schema.entity.data.location import Location
 from metadata.generated.schema.entity.data.mlmodel import MlModel
 from metadata.generated.schema.entity.data.pipeline import Pipeline
 from metadata.generated.schema.entity.data.table import Column, DataModel, Table
 from metadata.generated.schema.entity.data.topic import Topic
 from metadata.generated.schema.entity.teams.team import Team
 from metadata.generated.schema.entity.teams.user import User
 from metadata.generated.schema.tests.testCase import TestCase
@@ -271,38 +271,27 @@
     _: OpenMetadata,
     *,
     service_name: str,
     database_name: str,
     schema_name: str,
     model_name: str,
 ) -> str:
-
     return _build(service_name, database_name, schema_name, model_name)
 
 
 @fqn_build_registry.add(Pipeline)
 def _(
     _: OpenMetadata,
     *,
     service_name: str,
     pipeline_name: str,
 ) -> str:
     return _build(service_name, pipeline_name)
 
 
-@fqn_build_registry.add(Location)
-def _(
-    _: OpenMetadata,
-    *,
-    service_name: str,
-    location_name: str,
-) -> str:
-    return _build(service_name, location_name)
-
-
 @fqn_build_registry.add(Column)
 def _(
     _: OpenMetadata,  # ES Search not enabled for Columns
     *,
     service_name: str,
     database_name: str,
     schema_name: str,
@@ -397,14 +386,28 @@
         database_name,
         schema_name,
         table_name,
         test_case_name,
     )
 
 
+@fqn_build_registry.add(DashboardDataModel)
+def _(
+    _: OpenMetadata,  # ES Index not necessary for dashboard FQN building
+    *,
+    service_name: str,
+    data_model_name: str,
+) -> str:
+    if not service_name or not data_model_name:
+        raise FQNBuildingException(
+            f"Args should be informed, but got service=`{service_name}`, chart=`{data_model_name}``"
+        )
+    return _build(service_name, "model", data_model_name)
+
+
 def split_table_name(table_name: str) -> Dict[str, Optional[str]]:
     """
     Given a table name, try to extract database, schema and
     table info
     :param table_name: raw table name
     :return: dict with data
     """
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/gcs_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/gcs_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/helpers.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -339,7 +339,16 @@
     """
     if number == 0:
         return "0"
     units = ["", "K", "M", "B", "T"]
     constant_k = 1000.0
     magnitude = int(floor(log(abs(number), constant_k)))
     return f"{number / constant_k**magnitude:.2f}{units[magnitude]}"
+
+
+def clean_uri(uri: str) -> str:
+    """
+    if uri is like http://localhost:9000/
+    then remove the end / and
+    make it http://localhost:9000
+    """
+    return uri[:-1] if uri.endswith("/") else uri
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/importer.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/importer.py`

 * *Files 0% similar despite different names*

```diff
@@ -227,14 +227,14 @@
     Returns:
         Callable: test validator object
     """
     test_definition_class = (
         test_definition[0].upper() + test_definition[1:]
     )  # change test names to camel case
     return import_from_module(
-        "metadata.test_suite.validations.{}.{}.{}.{}Validator".format(
+        "metadata.data_quality.validations.{}.{}.{}.{}Validator".format(
             test_type.lower(),
             runner_type,
             test_definition,
             test_definition_class,
         )
     )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/logger.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/logger.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/lru_cache.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/lru_cache.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/metadata_service_helper.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/metadata_service_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,20 +74,15 @@
                     "oracleServiceName": {"title": "orcale_ser_name"}
                 },
             }
         },
     },
     "glue": {
         "service_name": "Glue",
-        "connection": {
-            "config": {
-                "awsConfig": "aws_config",
-                "storageServiceName": "glue_stroage_name",
-            }
-        },
+        "connection": {"config": {"awsConfig": "aws_config"}},
     },
     "snowflake": {
         "service_nmae": "Snowflake",
         "connection": {
             "config": {
                 "username": "randomName",
                 "account": "snow_fl_acco",
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/partition.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/partition.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/s3_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/s3_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/aws_based_secrets_manager.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/aws_based_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/aws_secrets_manager.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/aws_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/aws_ssm_secrets_manager.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/aws_ssm_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/external_secrets_manager.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/external_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/noop_secrets_manager.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/noop_secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/secrets_manager.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/secrets_manager.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/secrets/secrets_manager_factory.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/secrets/secrets_manager_factory.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/singleton.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/singleton.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/sqa_like_column.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/sqa_like_column.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/sqa_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/sqa_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -26,14 +26,15 @@
     DatetimeAddFn,
     TimestampAddFn,
 )
 from metadata.utils.logger import query_runner_logger
 
 logger = query_runner_logger()
 
+
 # pylint: disable=cell-var-from-loop
 def build_query_filter(
     filters: List[Tuple[Column, str, Any]], or_filter: bool = False
 ) -> Optional[BinaryExpression]:
     """Dynamically build query filter
 
     Args:
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/sqlalchemy_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/sqlalchemy_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/ssl_registry.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/ssl_registry.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/tag_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/tag_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/test_suite.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/test_suite.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/time_utils.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/time_utils.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/timeout.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/timeout.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/uuid_encoder.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/uuid_encoder.py`

 * *Files identical despite different names*

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/utils/workflow_output_handler.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/utils/workflow_output_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -70,14 +70,15 @@
     """
 
     INGEST = "ingest"
     PROFILE = "profile"
     TEST = "test"
     LINEAGE = "lineage"
     USAGE = "usage"
+    INSIGHT = "insight"
 
 
 EXAMPLES_WORKFLOW_PATH: Path = Path(__file__).parent / "../examples" / "workflows"
 
 URLS = {
     WorkflowType.INGEST: "https://docs.open-metadata.org/connectors/ingestion/workflows/metadata",
     WorkflowType.PROFILE: "https://docs.open-metadata.org/connectors/ingestion/workflows/profiler",
@@ -274,26 +275,26 @@
     Print the test suite workflow results
     Args:
         workflow (DataInsightWorkflow): workflow object
     """
     print_workflow_summary(
         workflow,
         processor=True,
-        processor_status=workflow.data_processor.get_status(),
+        processor_status=workflow.status,
     )
 
-    if workflow.data_processor.get_status().source_start_time:
+    if workflow.source.get_status().source_start_time:
         log_ansi_encoded_string(
-            message=f"Workflow finished in time {pretty_print_time_duration(time.time()-workflow.data_processor.get_status().source_start_time)} ",  # pylint: disable=line-too-long
+            message=f"Workflow finished in time {pretty_print_time_duration(time.time()-workflow.source.get_status().source_start_time)} ",  # pylint: disable=line-too-long
         )
 
     if workflow.result_status() == 1:
         log_ansi_encoded_string(message=WORKFLOW_FAILURE_MESSAGE)
     elif (
-        workflow.data_processor.get_status().warnings
+        workflow.source.get_status().warnings
         or workflow.status.warnings
         or (hasattr(workflow, "sink") and workflow.sink.get_status().warnings)
     ):
         log_ansi_encoded_string(message=WORKFLOW_WARNING_MESSAGE)
     else:
         log_ansi_encoded_string(message=WORKFLOW_SUCCESS_MESSAGE)
         log_ansi_encoded_string(
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/metadata/workflow/workflow_status_mixin.py` & `openmetadata-ingestion-1.0.0.0.dev1/src/metadata/workflow/workflow_status_mixin.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,15 +56,14 @@
     ) -> None:
         """
         Method to set the pipeline status of current ingestion pipeline
         """
 
         # if we don't have a related Ingestion Pipeline FQN, no status is set.
         if self.config.ingestionPipelineFQN:
-
             if state in (PipelineState.queued, PipelineState.running):
                 pipeline_status = PipelineStatus(
                     runId=self.run_id,
                     pipelineState=state,
                     startDate=datetime.now().timestamp() * 1000,
                     timestamp=datetime.now().timestamp() * 1000,
                 )
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/PKG-INFO` & `openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: openmetadata-ingestion
-Version: 1.0.0.0.dev0
+Version: 1.0.0.0.dev1
 Summary: Ingestion Framework for OpenMetadata
 Home-page: https://open-metadata.org/
 Author: OpenMetadata Committers
 License: Apache License 2.0
 Project-URL: Documentation, https://docs.open-metadata.org/
 Project-URL: Source, https://github.com/open-metadata/OpenMetadata
 Requires-Python: >=3.7
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/SOURCES.txt` & `openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/SOURCES.txt`

 * *Files 12% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 src/metadata/automations/runner.py
 src/metadata/cli/__init__.py
 src/metadata/cli/backup.py
 src/metadata/cli/dataquality.py
 src/metadata/cli/db_dump.py
 src/metadata/cli/docker.py
 src/metadata/cli/ingest.py
+src/metadata/cli/insight.py
 src/metadata/cli/openmetadata_dag_config_migration.py
 src/metadata/cli/openmetadata_imports_migration.py
 src/metadata/cli/profile.py
 src/metadata/cli/restore.py
 src/metadata/cli/utils.py
 src/metadata/clients/__init__.py
 src/metadata/clients/aws_client.py
@@ -47,28 +48,129 @@
 src/metadata/data_insight/processor/data_processor.py
 src/metadata/data_insight/processor/entity_report_data_processor.py
 src/metadata/data_insight/processor/web_analytic_report_data_processor.py
 src/metadata/data_insight/runner/kpi_runner.py
 src/metadata/data_insight/runner/run_result_registry.py
 src/metadata/data_insight/sink/__init__.py
 src/metadata/data_insight/sink/metadata_rest.py
+src/metadata/data_quality/__init__.py
+src/metadata/data_quality/api/__init__.py
+src/metadata/data_quality/api/models.py
+src/metadata/data_quality/api/workflow.py
+src/metadata/data_quality/interface/__init__.py
+src/metadata/data_quality/interface/test_suite_protocol.py
+src/metadata/data_quality/interface/pandas/__init__.py
+src/metadata/data_quality/interface/pandas/pandas_test_suite_interface.py
+src/metadata/data_quality/interface/sqlalchemy/__init__.py
+src/metadata/data_quality/interface/sqlalchemy/sqa_test_suite_interface.py
+src/metadata/data_quality/runner/__init__.py
+src/metadata/data_quality/runner/core.py
+src/metadata/data_quality/runner/models.py
+src/metadata/data_quality/sink/__init__.py
+src/metadata/data_quality/sink/metadata_rest.py
+src/metadata/data_quality/validations/__init__.py
+src/metadata/data_quality/validations/base_test_handler.py
+src/metadata/data_quality/validations/validator.py
+src/metadata/data_quality/validations/column/__init__.py
+src/metadata/data_quality/validations/column/base/__init__.py
+src/metadata/data_quality/validations/column/base/columnValueLengthsToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValueMaxToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValueMeanToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValueMedianToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValueMinToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValueStdDevToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValuesMissingCount.py
+src/metadata/data_quality/validations/column/base/columnValuesSumToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValuesToBeBetween.py
+src/metadata/data_quality/validations/column/base/columnValuesToBeInSet.py
+src/metadata/data_quality/validations/column/base/columnValuesToBeNotInSet.py
+src/metadata/data_quality/validations/column/base/columnValuesToBeNotNull.py
+src/metadata/data_quality/validations/column/base/columnValuesToBeUnique.py
+src/metadata/data_quality/validations/column/base/columnValuesToMatchRegex.py
+src/metadata/data_quality/validations/column/base/columnValuesToNotMatchRegex.py
+src/metadata/data_quality/validations/column/pandas/__init__.py
+src/metadata/data_quality/validations/column/pandas/columnValueLengthsToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValueMaxToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValueMeanToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValueMedianToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValueMinToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValueStdDevToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValuesMissingCount.py
+src/metadata/data_quality/validations/column/pandas/columnValuesSumToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToBeBetween.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToBeInSet.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotInSet.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToBeNotNull.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToBeUnique.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToMatchRegex.py
+src/metadata/data_quality/validations/column/pandas/columnValuesToNotMatchRegex.py
+src/metadata/data_quality/validations/column/sqlalchemy/__init__.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValueMinToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesMissingCount.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeBetween.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeInSet.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeNotNull.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToBeUnique.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToMatchRegex.py
+src/metadata/data_quality/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
+src/metadata/data_quality/validations/mixins/__init__.py
+src/metadata/data_quality/validations/mixins/pandas_validator_mixin.py
+src/metadata/data_quality/validations/mixins/sqa_validator_mixin.py
+src/metadata/data_quality/validations/table/__init__.py
+src/metadata/data_quality/validations/table/base/__init__.py
+src/metadata/data_quality/validations/table/base/tableColumnCountToBeBetween.py
+src/metadata/data_quality/validations/table/base/tableColumnCountToEqual.py
+src/metadata/data_quality/validations/table/base/tableColumnNameToExist.py
+src/metadata/data_quality/validations/table/base/tableColumnToMatchSet.py
+src/metadata/data_quality/validations/table/base/tableCustomSQLQuery.py
+src/metadata/data_quality/validations/table/base/tableRowCountToBeBetween.py
+src/metadata/data_quality/validations/table/base/tableRowCountToEqual.py
+src/metadata/data_quality/validations/table/base/tableRowInsertedCountToBeBetween.py
+src/metadata/data_quality/validations/table/pandas/__init__.py
+src/metadata/data_quality/validations/table/pandas/tableColumnCountToBeBetween.py
+src/metadata/data_quality/validations/table/pandas/tableColumnCountToEqual.py
+src/metadata/data_quality/validations/table/pandas/tableColumnNameToExist.py
+src/metadata/data_quality/validations/table/pandas/tableColumnToMatchSet.py
+src/metadata/data_quality/validations/table/pandas/tableCustomSQLQuery.py
+src/metadata/data_quality/validations/table/pandas/tableRowCountToBeBetween.py
+src/metadata/data_quality/validations/table/pandas/tableRowCountToEqual.py
+src/metadata/data_quality/validations/table/pandas/tableRowInsertedCountToBeBetween.py
+src/metadata/data_quality/validations/table/sqlalchemy/__init__.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableColumnCountToEqual.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableColumnNameToExist.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableColumnToMatchSet.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableCustomSQLQuery.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToBeBetween.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableRowCountToEqual.py
+src/metadata/data_quality/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
 src/metadata/examples/workflows/airbyte.yaml
 src/metadata/examples/workflows/airflow.yaml
 src/metadata/examples/workflows/amundsen.yaml
 src/metadata/examples/workflows/athena.yaml
+src/metadata/examples/workflows/athena_lineage.yaml
+src/metadata/examples/workflows/athena_usage.yaml
 src/metadata/examples/workflows/atlas.yaml
 src/metadata/examples/workflows/azuresql.yaml
 src/metadata/examples/workflows/bigquery.yaml
 src/metadata/examples/workflows/bigquery_lineage.yaml
 src/metadata/examples/workflows/bigquery_profiler.yaml
 src/metadata/examples/workflows/bigquery_usage.yaml
 src/metadata/examples/workflows/clickhouse.yaml
 src/metadata/examples/workflows/clickhouse_lineage.yaml
 src/metadata/examples/workflows/clickhouse_usage.yaml
 src/metadata/examples/workflows/dagster.yaml
+src/metadata/examples/workflows/data_insight.yaml
 src/metadata/examples/workflows/databricks.yaml
 src/metadata/examples/workflows/databricks_lineage.yaml
 src/metadata/examples/workflows/databricks_pipeline.yaml
 src/metadata/examples/workflows/databricks_usage.yaml
 src/metadata/examples/workflows/datalake.yaml
 src/metadata/examples/workflows/datalake_profiler.yaml
 src/metadata/examples/workflows/db2.yaml
@@ -160,15 +262,14 @@
 src/metadata/generated/schema/api/data/createContainer.py
 src/metadata/generated/schema/api/data/createDashboard.py
 src/metadata/generated/schema/api/data/createDashboardDataModel.py
 src/metadata/generated/schema/api/data/createDatabase.py
 src/metadata/generated/schema/api/data/createDatabaseSchema.py
 src/metadata/generated/schema/api/data/createGlossary.py
 src/metadata/generated/schema/api/data/createGlossaryTerm.py
-src/metadata/generated/schema/api/data/createLocation.py
 src/metadata/generated/schema/api/data/createMlModel.py
 src/metadata/generated/schema/api/data/createPipeline.py
 src/metadata/generated/schema/api/data/createQuery.py
 src/metadata/generated/schema/api/data/createTable.py
 src/metadata/generated/schema/api/data/createTableProfile.py
 src/metadata/generated/schema/api/data/createTopic.py
 src/metadata/generated/schema/api/data/restoreEntity.py
@@ -188,15 +289,14 @@
 src/metadata/generated/schema/api/policies/createPolicy.py
 src/metadata/generated/schema/api/services/__init__.py
 src/metadata/generated/schema/api/services/createDashboardService.py
 src/metadata/generated/schema/api/services/createDatabaseService.py
 src/metadata/generated/schema/api/services/createMessagingService.py
 src/metadata/generated/schema/api/services/createMetadataService.py
 src/metadata/generated/schema/api/services/createMlModelService.py
-src/metadata/generated/schema/api/services/createObjectStoreService.py
 src/metadata/generated/schema/api/services/createPipelineService.py
 src/metadata/generated/schema/api/services/createStorageService.py
 src/metadata/generated/schema/api/services/ingestionPipelines/__init__.py
 src/metadata/generated/schema/api/services/ingestionPipelines/createIngestionPipeline.py
 src/metadata/generated/schema/api/teams/__init__.py
 src/metadata/generated/schema/api/teams/createRole.py
 src/metadata/generated/schema/api/teams/createTeam.py
@@ -228,14 +328,15 @@
 src/metadata/generated/schema/auth/ssoAuth.py
 src/metadata/generated/schema/auth/tokenRefreshRequest.py
 src/metadata/generated/schema/configuration/__init__.py
 src/metadata/generated/schema/configuration/applicationConfiguration.py
 src/metadata/generated/schema/configuration/authConfig.py
 src/metadata/generated/schema/configuration/authenticationConfiguration.py
 src/metadata/generated/schema/configuration/authorizerConfiguration.py
+src/metadata/generated/schema/configuration/changeEventConfiguration.py
 src/metadata/generated/schema/configuration/elasticSearchConfiguration.py
 src/metadata/generated/schema/configuration/eventHandlerConfiguration.py
 src/metadata/generated/schema/configuration/fernetConfiguration.py
 src/metadata/generated/schema/configuration/jwtTokenConfiguration.py
 src/metadata/generated/schema/configuration/kafkaEventConfiguration.py
 src/metadata/generated/schema/configuration/ldapConfiguration.py
 src/metadata/generated/schema/configuration/pipelineServiceClientConfiguration.py
@@ -280,15 +381,14 @@
 src/metadata/generated/schema/entity/data/container.py
 src/metadata/generated/schema/entity/data/dashboard.py
 src/metadata/generated/schema/entity/data/dashboardDataModel.py
 src/metadata/generated/schema/entity/data/database.py
 src/metadata/generated/schema/entity/data/databaseSchema.py
 src/metadata/generated/schema/entity/data/glossary.py
 src/metadata/generated/schema/entity/data/glossaryTerm.py
-src/metadata/generated/schema/entity/data/location.py
 src/metadata/generated/schema/entity/data/metrics.py
 src/metadata/generated/schema/entity/data/mlmodel.py
 src/metadata/generated/schema/entity/data/pipeline.py
 src/metadata/generated/schema/entity/data/query.py
 src/metadata/generated/schema/entity/data/report.py
 src/metadata/generated/schema/entity/data/table.py
 src/metadata/generated/schema/entity/data/topic.py
@@ -299,25 +399,20 @@
 src/metadata/generated/schema/entity/policies/__init__.py
 src/metadata/generated/schema/entity/policies/filters.py
 src/metadata/generated/schema/entity/policies/policy.py
 src/metadata/generated/schema/entity/policies/accessControl/__init__.py
 src/metadata/generated/schema/entity/policies/accessControl/resourceDescriptor.py
 src/metadata/generated/schema/entity/policies/accessControl/resourcePermission.py
 src/metadata/generated/schema/entity/policies/accessControl/rule.py
-src/metadata/generated/schema/entity/policies/lifecycle/__init__.py
-src/metadata/generated/schema/entity/policies/lifecycle/deleteAction.py
-src/metadata/generated/schema/entity/policies/lifecycle/moveAction.py
-src/metadata/generated/schema/entity/policies/lifecycle/rule.py
 src/metadata/generated/schema/entity/services/__init__.py
 src/metadata/generated/schema/entity/services/dashboardService.py
 src/metadata/generated/schema/entity/services/databaseService.py
 src/metadata/generated/schema/entity/services/messagingService.py
 src/metadata/generated/schema/entity/services/metadataService.py
 src/metadata/generated/schema/entity/services/mlmodelService.py
-src/metadata/generated/schema/entity/services/objectstoreService.py
 src/metadata/generated/schema/entity/services/pipelineService.py
 src/metadata/generated/schema/entity/services/serviceType.py
 src/metadata/generated/schema/entity/services/storageService.py
 src/metadata/generated/schema/entity/services/connections/__init__.py
 src/metadata/generated/schema/entity/services/connections/connectionBasicType.py
 src/metadata/generated/schema/entity/services/connections/serviceConnection.py
 src/metadata/generated/schema/entity/services/connections/testConnectionDefinition.py
@@ -378,29 +473,29 @@
 src/metadata/generated/schema/entity/services/connections/metadata/metadataESConnection.py
 src/metadata/generated/schema/entity/services/connections/metadata/openMetadataConnection.py
 src/metadata/generated/schema/entity/services/connections/mlmodel/__init__.py
 src/metadata/generated/schema/entity/services/connections/mlmodel/customMlModelConnection.py
 src/metadata/generated/schema/entity/services/connections/mlmodel/mlflowConnection.py
 src/metadata/generated/schema/entity/services/connections/mlmodel/sageMakerConnection.py
 src/metadata/generated/schema/entity/services/connections/mlmodel/sklearnConnection.py
-src/metadata/generated/schema/entity/services/connections/objectstore/__init__.py
-src/metadata/generated/schema/entity/services/connections/objectstore/azureObjectStoreConnection.py
-src/metadata/generated/schema/entity/services/connections/objectstore/gcsObjectStoreConnection.py
-src/metadata/generated/schema/entity/services/connections/objectstore/s3ObjectStoreConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/__init__.py
 src/metadata/generated/schema/entity/services/connections/pipeline/airbyteConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/airflowConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/backendConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/customPipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/dagsterConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/databricksPipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/domoPipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/fivetranConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/gluePipelineConnection.py
 src/metadata/generated/schema/entity/services/connections/pipeline/nifiConnection.py
+src/metadata/generated/schema/entity/services/connections/storage/__init__.py
+src/metadata/generated/schema/entity/services/connections/storage/adlsConection.py
+src/metadata/generated/schema/entity/services/connections/storage/gcsConnection.py
+src/metadata/generated/schema/entity/services/connections/storage/s3Connection.py
 src/metadata/generated/schema/entity/services/ingestionPipelines/__init__.py
 src/metadata/generated/schema/entity/services/ingestionPipelines/ingestionPipeline.py
 src/metadata/generated/schema/entity/teams/__init__.py
 src/metadata/generated/schema/entity/teams/role.py
 src/metadata/generated/schema/entity/teams/team.py
 src/metadata/generated/schema/entity/teams/teamHierarchy.py
 src/metadata/generated/schema/entity/teams/user.py
@@ -423,53 +518,57 @@
 src/metadata/generated/schema/metadataIngestion/databaseServiceProfilerPipeline.py
 src/metadata/generated/schema/metadataIngestion/databaseServiceQueryLineagePipeline.py
 src/metadata/generated/schema/metadataIngestion/databaseServiceQueryUsagePipeline.py
 src/metadata/generated/schema/metadataIngestion/dbtPipeline.py
 src/metadata/generated/schema/metadataIngestion/messagingServiceMetadataPipeline.py
 src/metadata/generated/schema/metadataIngestion/metadataToElasticSearchPipeline.py
 src/metadata/generated/schema/metadataIngestion/mlmodelServiceMetadataPipeline.py
-src/metadata/generated/schema/metadataIngestion/objectstoreServiceMetadataPipeline.py
 src/metadata/generated/schema/metadataIngestion/pipelineServiceMetadataPipeline.py
+src/metadata/generated/schema/metadataIngestion/storageServiceMetadataPipeline.py
 src/metadata/generated/schema/metadataIngestion/testSuitePipeline.py
 src/metadata/generated/schema/metadataIngestion/workflow.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/__init__.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtBucketDetails.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtCloudConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtGCSConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtHttpConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtLocalConfig.py
 src/metadata/generated/schema/metadataIngestion/dbtconfig/dbtS3Config.py
-src/metadata/generated/schema/metadataIngestion/objectstore/__init__.py
-src/metadata/generated/schema/metadataIngestion/objectstore/containerMetadataConfig.py
+src/metadata/generated/schema/metadataIngestion/storage/__init__.py
+src/metadata/generated/schema/metadataIngestion/storage/containerMetadataConfig.py
 src/metadata/generated/schema/monitoring/__init__.py
 src/metadata/generated/schema/monitoring/eventMonitorProvider.py
 src/metadata/generated/schema/security/__init__.py
 src/metadata/generated/schema/security/securityConfiguration.py
 src/metadata/generated/schema/security/client/__init__.py
 src/metadata/generated/schema/security/client/auth0SSOClientConfig.py
 src/metadata/generated/schema/security/client/azureSSOClientConfig.py
 src/metadata/generated/schema/security/client/customOidcSSOClientConfig.py
 src/metadata/generated/schema/security/client/googleSSOClientConfig.py
 src/metadata/generated/schema/security/client/oktaSSOClientConfig.py
 src/metadata/generated/schema/security/client/openMetadataJWTClientConfig.py
 src/metadata/generated/schema/security/client/samlSSOClientConfig.py
 src/metadata/generated/schema/security/credentials/__init__.py
+src/metadata/generated/schema/security/credentials/accessTokenAuth.py
 src/metadata/generated/schema/security/credentials/awsCredentials.py
 src/metadata/generated/schema/security/credentials/azureCredentials.py
+src/metadata/generated/schema/security/credentials/basicAuth.py
 src/metadata/generated/schema/security/credentials/gcsCredentials.py
 src/metadata/generated/schema/security/credentials/gcsValues.py
+src/metadata/generated/schema/security/credentials/githubCredentials.py
 src/metadata/generated/schema/security/secrets/__init__.py
 src/metadata/generated/schema/security/secrets/secretsManagerConfiguration.py
 src/metadata/generated/schema/security/secrets/secretsManagerProvider.py
 src/metadata/generated/schema/security/ssl/__init__.py
 src/metadata/generated/schema/security/ssl/validateSSLClientConfig.py
 src/metadata/generated/schema/security/ssl/verifySSLConfig.py
 src/metadata/generated/schema/settings/__init__.py
-src/metadata/generated/schema/settings/eventPublisherJob.py
 src/metadata/generated/schema/settings/settings.py
+src/metadata/generated/schema/system/__init__.py
+src/metadata/generated/schema/system/eventPublisherJob.py
 src/metadata/generated/schema/tests/__init__.py
 src/metadata/generated/schema/tests/basic.py
 src/metadata/generated/schema/tests/customMetric.py
 src/metadata/generated/schema/tests/testCase.py
 src/metadata/generated/schema/tests/testDefinition.py
 src/metadata/generated/schema/tests/testSuite.py
 src/metadata/generated/schema/type/__init__.py
@@ -494,15 +593,14 @@
 src/metadata/generated/schema/type/jdbcConnection.py
 src/metadata/generated/schema/type/paging.py
 src/metadata/generated/schema/type/profile.py
 src/metadata/generated/schema/type/queryParserData.py
 src/metadata/generated/schema/type/reaction.py
 src/metadata/generated/schema/type/schedule.py
 src/metadata/generated/schema/type/schema.py
-src/metadata/generated/schema/type/storage.py
 src/metadata/generated/schema/type/tableQuery.py
 src/metadata/generated/schema/type/tableUsageCount.py
 src/metadata/generated/schema/type/tagLabel.py
 src/metadata/generated/schema/type/usageDetails.py
 src/metadata/generated/schema/type/usageRequest.py
 src/metadata/generated/schema/type/votes.py
 src/metadata/great_expectations/__init__.py
@@ -551,26 +649,26 @@
 src/metadata/ingestion/ometa/__init__.py
 src/metadata/ingestion/ometa/auth_provider.py
 src/metadata/ingestion/ometa/client.py
 src/metadata/ingestion/ometa/client_utils.py
 src/metadata/ingestion/ometa/credentials.py
 src/metadata/ingestion/ometa/models.py
 src/metadata/ingestion/ometa/ometa_api.py
-src/metadata/ingestion/ometa/patch.py
 src/metadata/ingestion/ometa/provider_registry.py
 src/metadata/ingestion/ometa/utils.py
 src/metadata/ingestion/ometa/mixins/__init__.py
 src/metadata/ingestion/ometa/mixins/dashboard_mixin.py
 src/metadata/ingestion/ometa/mixins/data_insight_mixin.py
 src/metadata/ingestion/ometa/mixins/es_mixin.py
 src/metadata/ingestion/ometa/mixins/glossary_mixin.py
 src/metadata/ingestion/ometa/mixins/ingestion_pipeline_mixin.py
 src/metadata/ingestion/ometa/mixins/lineage_mixin.py
 src/metadata/ingestion/ometa/mixins/mlmodel_mixin.py
 src/metadata/ingestion/ometa/mixins/patch_mixin.py
+src/metadata/ingestion/ometa/mixins/patch_mixin_utils.py
 src/metadata/ingestion/ometa/mixins/pipeline_mixin.py
 src/metadata/ingestion/ometa/mixins/query_mixin.py
 src/metadata/ingestion/ometa/mixins/role_policy_mixin.py
 src/metadata/ingestion/ometa/mixins/server_mixin.py
 src/metadata/ingestion/ometa/mixins/service_mixin.py
 src/metadata/ingestion/ometa/mixins/table_mixin.py
 src/metadata/ingestion/ometa/mixins/tests_mixin.py
@@ -581,19 +679,21 @@
 src/metadata/ingestion/processor/pii.py
 src/metadata/ingestion/processor/query_parser.py
 src/metadata/ingestion/sink/__init__.py
 src/metadata/ingestion/sink/elasticsearch.py
 src/metadata/ingestion/sink/file.py
 src/metadata/ingestion/sink/metadata_rest.py
 src/metadata/ingestion/sink/elasticsearch_mapping/__init__.py
+src/metadata/ingestion/sink/elasticsearch_mapping/container_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/dashboard_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/entity_report_data_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/glossary_term_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/mlmodel_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/pipeline_search_index_mapping.py
+src/metadata/ingestion/sink/elasticsearch_mapping/query_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/table_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/tag_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/team_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/topic_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/user_search_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_entity_view_report_data_index_mapping.py
 src/metadata/ingestion/sink/elasticsearch_mapping/web_analytic_user_activity_report_data_index_mapping.py
@@ -604,27 +704,33 @@
 src/metadata/ingestion/source/sqa_types.py
 src/metadata/ingestion/source/dashboard/__init__.py
 src/metadata/ingestion/source/dashboard/dashboard_service.py
 src/metadata/ingestion/source/dashboard/domodashboard/__init__.py
 src/metadata/ingestion/source/dashboard/domodashboard/connection.py
 src/metadata/ingestion/source/dashboard/domodashboard/metadata.py
 src/metadata/ingestion/source/dashboard/looker/__init__.py
+src/metadata/ingestion/source/dashboard/looker/columns.py
 src/metadata/ingestion/source/dashboard/looker/connection.py
 src/metadata/ingestion/source/dashboard/looker/metadata.py
+src/metadata/ingestion/source/dashboard/looker/models.py
+src/metadata/ingestion/source/dashboard/looker/parser.py
 src/metadata/ingestion/source/dashboard/metabase/__init__.py
+src/metadata/ingestion/source/dashboard/metabase/client.py
 src/metadata/ingestion/source/dashboard/metabase/connection.py
 src/metadata/ingestion/source/dashboard/metabase/metadata.py
+src/metadata/ingestion/source/dashboard/metabase/models.py
 src/metadata/ingestion/source/dashboard/mode/__init__.py
 src/metadata/ingestion/source/dashboard/mode/client.py
 src/metadata/ingestion/source/dashboard/mode/connection.py
 src/metadata/ingestion/source/dashboard/mode/metadata.py
 src/metadata/ingestion/source/dashboard/powerbi/__init__.py
 src/metadata/ingestion/source/dashboard/powerbi/client.py
 src/metadata/ingestion/source/dashboard/powerbi/connection.py
 src/metadata/ingestion/source/dashboard/powerbi/metadata.py
+src/metadata/ingestion/source/dashboard/powerbi/models.py
 src/metadata/ingestion/source/dashboard/quicksight/__init__.py
 src/metadata/ingestion/source/dashboard/quicksight/connection.py
 src/metadata/ingestion/source/dashboard/quicksight/metadata.py
 src/metadata/ingestion/source/dashboard/quicksight/models.py
 src/metadata/ingestion/source/dashboard/redash/__init__.py
 src/metadata/ingestion/source/dashboard/redash/client.py
 src/metadata/ingestion/source/dashboard/redash/connection.py
@@ -634,16 +740,18 @@
 src/metadata/ingestion/source/dashboard/superset/client.py
 src/metadata/ingestion/source/dashboard/superset/connection.py
 src/metadata/ingestion/source/dashboard/superset/db_source.py
 src/metadata/ingestion/source/dashboard/superset/metadata.py
 src/metadata/ingestion/source/dashboard/superset/mixin.py
 src/metadata/ingestion/source/dashboard/superset/queries.py
 src/metadata/ingestion/source/dashboard/tableau/__init__.py
+src/metadata/ingestion/source/dashboard/tableau/client.py
 src/metadata/ingestion/source/dashboard/tableau/connection.py
 src/metadata/ingestion/source/dashboard/tableau/metadata.py
+src/metadata/ingestion/source/dashboard/tableau/models.py
 src/metadata/ingestion/source/dashboard/tableau/queries.py
 src/metadata/ingestion/source/database/__init__.py
 src/metadata/ingestion/source/database/column_helpers.py
 src/metadata/ingestion/source/database/column_type_parser.py
 src/metadata/ingestion/source/database/common_db_source.py
 src/metadata/ingestion/source/database/database_service.py
 src/metadata/ingestion/source/database/lineage_source.py
@@ -651,15 +759,19 @@
 src/metadata/ingestion/source/database/sample_data.py
 src/metadata/ingestion/source/database/sample_usage.py
 src/metadata/ingestion/source/database/sql_column_handler.py
 src/metadata/ingestion/source/database/sqlalchemy_source.py
 src/metadata/ingestion/source/database/usage_source.py
 src/metadata/ingestion/source/database/athena/__init__.py
 src/metadata/ingestion/source/database/athena/connection.py
+src/metadata/ingestion/source/database/athena/lineage.py
 src/metadata/ingestion/source/database/athena/metadata.py
+src/metadata/ingestion/source/database/athena/models.py
+src/metadata/ingestion/source/database/athena/query_parser.py
+src/metadata/ingestion/source/database/athena/usage.py
 src/metadata/ingestion/source/database/azuresql/__init__.py
 src/metadata/ingestion/source/database/azuresql/connection.py
 src/metadata/ingestion/source/database/azuresql/metadata.py
 src/metadata/ingestion/source/database/bigquery/__init__.py
 src/metadata/ingestion/source/database/bigquery/connection.py
 src/metadata/ingestion/source/database/bigquery/lineage.py
 src/metadata/ingestion/source/database/bigquery/metadata.py
@@ -726,14 +838,15 @@
 src/metadata/ingestion/source/database/mysql/connection.py
 src/metadata/ingestion/source/database/mysql/metadata.py
 src/metadata/ingestion/source/database/mysql/utils.py
 src/metadata/ingestion/source/database/oracle/__init__.py
 src/metadata/ingestion/source/database/oracle/connection.py
 src/metadata/ingestion/source/database/oracle/metadata.py
 src/metadata/ingestion/source/database/oracle/queries.py
+src/metadata/ingestion/source/database/oracle/utils.py
 src/metadata/ingestion/source/database/pinotdb/__init__.py
 src/metadata/ingestion/source/database/pinotdb/connection.py
 src/metadata/ingestion/source/database/pinotdb/metadata.py
 src/metadata/ingestion/source/database/postgres/__init__.py
 src/metadata/ingestion/source/database/postgres/connection.py
 src/metadata/ingestion/source/database/postgres/lineage.py
 src/metadata/ingestion/source/database/postgres/metadata.py
@@ -814,29 +927,25 @@
 src/metadata/ingestion/source/mlmodel/mlmodel_service.py
 src/metadata/ingestion/source/mlmodel/mlflow/__init__.py
 src/metadata/ingestion/source/mlmodel/mlflow/connection.py
 src/metadata/ingestion/source/mlmodel/mlflow/metadata.py
 src/metadata/ingestion/source/mlmodel/sagemaker/__init__.py
 src/metadata/ingestion/source/mlmodel/sagemaker/connection.py
 src/metadata/ingestion/source/mlmodel/sagemaker/metadata.py
-src/metadata/ingestion/source/objectstore/__init__.py
-src/metadata/ingestion/source/objectstore/objectstore_service.py
-src/metadata/ingestion/source/objectstore/s3/__init__.py
-src/metadata/ingestion/source/objectstore/s3/connection.py
-src/metadata/ingestion/source/objectstore/s3/metadata.py
 src/metadata/ingestion/source/pipeline/__init__.py
 src/metadata/ingestion/source/pipeline/pipeline_service.py
 src/metadata/ingestion/source/pipeline/airbyte/__init__.py
 src/metadata/ingestion/source/pipeline/airbyte/client.py
 src/metadata/ingestion/source/pipeline/airbyte/connection.py
 src/metadata/ingestion/source/pipeline/airbyte/metadata.py
 src/metadata/ingestion/source/pipeline/airflow/__init__.py
 src/metadata/ingestion/source/pipeline/airflow/connection.py
 src/metadata/ingestion/source/pipeline/airflow/lineage_parser.py
 src/metadata/ingestion/source/pipeline/airflow/metadata.py
+src/metadata/ingestion/source/pipeline/airflow/models.py
 src/metadata/ingestion/source/pipeline/dagster/__init__.py
 src/metadata/ingestion/source/pipeline/dagster/connection.py
 src/metadata/ingestion/source/pipeline/dagster/metadata.py
 src/metadata/ingestion/source/pipeline/dagster/queries.py
 src/metadata/ingestion/source/pipeline/databrickspipeline/__init__.py
 src/metadata/ingestion/source/pipeline/databrickspipeline/connection.py
 src/metadata/ingestion/source/pipeline/databrickspipeline/metadata.py
@@ -850,36 +959,40 @@
 src/metadata/ingestion/source/pipeline/gluepipeline/__init__.py
 src/metadata/ingestion/source/pipeline/gluepipeline/connection.py
 src/metadata/ingestion/source/pipeline/gluepipeline/metadata.py
 src/metadata/ingestion/source/pipeline/nifi/__init__.py
 src/metadata/ingestion/source/pipeline/nifi/client.py
 src/metadata/ingestion/source/pipeline/nifi/connection.py
 src/metadata/ingestion/source/pipeline/nifi/metadata.py
+src/metadata/ingestion/source/storage/__init__.py
+src/metadata/ingestion/source/storage/storage_service.py
+src/metadata/ingestion/source/storage/s3/__init__.py
+src/metadata/ingestion/source/storage/s3/connection.py
+src/metadata/ingestion/source/storage/s3/metadata.py
+src/metadata/ingestion/source/storage/s3/models.py
 src/metadata/ingestion/stage/__init__.py
 src/metadata/ingestion/stage/table_usage.py
-src/metadata/interfaces/__init__.py
-src/metadata/interfaces/test_suite_protocol.py
-src/metadata/interfaces/datalake/__init__.py
-src/metadata/interfaces/datalake/datalake_test_suite_interface.py
-src/metadata/interfaces/datalake/mixins/__init__.py
-src/metadata/interfaces/datalake/mixins/pandas_mixin.py
-src/metadata/interfaces/sqalchemy/__init__.py
-src/metadata/interfaces/sqalchemy/sqa_test_suite_interface.py
-src/metadata/interfaces/sqalchemy/mixins/__init__.py
-src/metadata/interfaces/sqalchemy/mixins/sqa_mixin.py
+src/metadata/mixins/__init__.py
+src/metadata/mixins/pandas/__init__.py
+src/metadata/mixins/pandas/pandas_mixin.py
+src/metadata/mixins/sqalchemy/__init__.py
+src/metadata/mixins/sqalchemy/sqa_mixin.py
 src/metadata/parsers/__init__.py
 src/metadata/parsers/avro_parser.py
 src/metadata/parsers/json_schema_parser.py
 src/metadata/parsers/protobuf_parser.py
 src/metadata/parsers/schema_parsers.py
 src/metadata/profiler/__init__.py
 src/metadata/profiler/registry.py
 src/metadata/profiler/api/__init__.py
 src/metadata/profiler/api/models.py
 src/metadata/profiler/api/workflow.py
+src/metadata/profiler/interface/profiler_protocol.py
+src/metadata/profiler/interface/pandas/pandas_profiler_interface.py
+src/metadata/profiler/interface/sqlalchemy/sqa_profiler_interface.py
 src/metadata/profiler/metrics/__init__.py
 src/metadata/profiler/metrics/core.py
 src/metadata/profiler/metrics/registry.py
 src/metadata/profiler/metrics/composed/__init__.py
 src/metadata/profiler/metrics/composed/distinct_ratio.py
 src/metadata/profiler/metrics/composed/duplicate_count.py
 src/metadata/profiler/metrics/composed/ilike_ratio.py
@@ -927,120 +1040,29 @@
 src/metadata/profiler/orm/functions/modulo.py
 src/metadata/profiler/orm/functions/random_num.py
 src/metadata/profiler/orm/functions/sum.py
 src/metadata/profiler/orm/types/__init__.py
 src/metadata/profiler/orm/types/bytea_to_string.py
 src/metadata/profiler/orm/types/hex_byte_string.py
 src/metadata/profiler/orm/types/uuid.py
-src/metadata/profiler/profiler/__init__.py
-src/metadata/profiler/profiler/core.py
-src/metadata/profiler/profiler/datalake_sampler.py
-src/metadata/profiler/profiler/default.py
-src/metadata/profiler/profiler/handle_partition.py
-src/metadata/profiler/profiler/models.py
-src/metadata/profiler/profiler/runner.py
-src/metadata/profiler/profiler/sampler.py
-src/metadata/profiler/profiler/interface/profiler_protocol.py
-src/metadata/profiler/profiler/interface/pandas/pandas_profiler_interface.py
-src/metadata/profiler/profiler/interface/sqlalchemy/sqa_profiler_interface.py
+src/metadata/profiler/processor/__init__.py
+src/metadata/profiler/processor/core.py
+src/metadata/profiler/processor/datalake_sampler.py
+src/metadata/profiler/processor/default.py
+src/metadata/profiler/processor/handle_partition.py
+src/metadata/profiler/processor/models.py
+src/metadata/profiler/processor/runner.py
+src/metadata/profiler/processor/sampler.py
 src/metadata/profiler/sink/__init__.py
 src/metadata/profiler/sink/file.py
 src/metadata/profiler/sink/metadata_rest.py
-src/metadata/test_suite/__init__.py
-src/metadata/test_suite/api/__init__.py
-src/metadata/test_suite/api/models.py
-src/metadata/test_suite/api/workflow.py
-src/metadata/test_suite/runner/__init__.py
-src/metadata/test_suite/runner/core.py
-src/metadata/test_suite/runner/models.py
-src/metadata/test_suite/sink/__init__.py
-src/metadata/test_suite/sink/metadata_rest.py
-src/metadata/test_suite/validations/__init__.py
-src/metadata/test_suite/validations/base_test_handler.py
-src/metadata/test_suite/validations/validator.py
-src/metadata/test_suite/validations/column/__init__.py
-src/metadata/test_suite/validations/column/base/__init__.py
-src/metadata/test_suite/validations/column/base/columnValueLengthsToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValueMaxToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValueMeanToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValueMedianToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValueMinToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValueStdDevToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValuesMissingCount.py
-src/metadata/test_suite/validations/column/base/columnValuesSumToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValuesToBeBetween.py
-src/metadata/test_suite/validations/column/base/columnValuesToBeInSet.py
-src/metadata/test_suite/validations/column/base/columnValuesToBeNotInSet.py
-src/metadata/test_suite/validations/column/base/columnValuesToBeNotNull.py
-src/metadata/test_suite/validations/column/base/columnValuesToBeUnique.py
-src/metadata/test_suite/validations/column/base/columnValuesToMatchRegex.py
-src/metadata/test_suite/validations/column/base/columnValuesToNotMatchRegex.py
-src/metadata/test_suite/validations/column/pandas/__init__.py
-src/metadata/test_suite/validations/column/pandas/columnValueLengthsToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValueMaxToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValueMeanToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValueMedianToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValueMinToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValueStdDevToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValuesMissingCount.py
-src/metadata/test_suite/validations/column/pandas/columnValuesSumToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToBeBetween.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToBeInSet.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToBeNotInSet.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToBeNotNull.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToBeUnique.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToMatchRegex.py
-src/metadata/test_suite/validations/column/pandas/columnValuesToNotMatchRegex.py
-src/metadata/test_suite/validations/column/sqlalchemy/__init__.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValueLengthsToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValueMaxToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValueMeanToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValueMedianToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValueMinToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValueStdDevToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesMissingCount.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesSumToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeBetween.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeInSet.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeNotInSet.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeNotNull.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToBeUnique.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToMatchRegex.py
-src/metadata/test_suite/validations/column/sqlalchemy/columnValuesToNotMatchRegex.py
-src/metadata/test_suite/validations/mixins/__init__.py
-src/metadata/test_suite/validations/mixins/pandas_validator_mixin.py
-src/metadata/test_suite/validations/mixins/sqa_validator_mixin.py
-src/metadata/test_suite/validations/table/__init__.py
-src/metadata/test_suite/validations/table/base/__init__.py
-src/metadata/test_suite/validations/table/base/tableColumnCountToBeBetween.py
-src/metadata/test_suite/validations/table/base/tableColumnCountToEqual.py
-src/metadata/test_suite/validations/table/base/tableColumnNameToExist.py
-src/metadata/test_suite/validations/table/base/tableColumnToMatchSet.py
-src/metadata/test_suite/validations/table/base/tableCustomSQLQuery.py
-src/metadata/test_suite/validations/table/base/tableRowCountToBeBetween.py
-src/metadata/test_suite/validations/table/base/tableRowCountToEqual.py
-src/metadata/test_suite/validations/table/base/tableRowInsertedCountToBeBetween.py
-src/metadata/test_suite/validations/table/pandas/__init__.py
-src/metadata/test_suite/validations/table/pandas/tableColumnCountToBeBetween.py
-src/metadata/test_suite/validations/table/pandas/tableColumnCountToEqual.py
-src/metadata/test_suite/validations/table/pandas/tableColumnNameToExist.py
-src/metadata/test_suite/validations/table/pandas/tableColumnToMatchSet.py
-src/metadata/test_suite/validations/table/pandas/tableCustomSQLQuery.py
-src/metadata/test_suite/validations/table/pandas/tableRowCountToBeBetween.py
-src/metadata/test_suite/validations/table/pandas/tableRowCountToEqual.py
-src/metadata/test_suite/validations/table/pandas/tableRowInsertedCountToBeBetween.py
-src/metadata/test_suite/validations/table/sqlalchemy/__init__.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableColumnCountToBeBetween.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableColumnCountToEqual.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableColumnNameToExist.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableColumnToMatchSet.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableCustomSQLQuery.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableRowCountToBeBetween.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableRowCountToEqual.py
-src/metadata/test_suite/validations/table/sqlalchemy/tableRowInsertedCountToBeBetween.py
+src/metadata/readers/__init__.py
+src/metadata/readers/base.py
+src/metadata/readers/github.py
+src/metadata/readers/local.py
 src/metadata/timer/__init__.py
 src/metadata/timer/repeated_timer.py
 src/metadata/timer/workflow_reporter.py
 src/metadata/utils/__init__.py
 src/metadata/utils/azure_utils.py
 src/metadata/utils/class_helper.py
 src/metadata/utils/client_version.py
```

### Comparing `openmetadata-ingestion-1.0.0.0.dev0/src/openmetadata_ingestion.egg-info/requires.txt` & `openmetadata-ingestion-1.0.0.0.dev1/src/openmetadata_ingestion.egg-info/requires.txt`

 * *Files 6% similar despite different names*

```diff
@@ -1,134 +1,136 @@
+wheel~=0.38.4
 google>=3.0.0
-importlib-metadata~=4.12.0
-requests-aws4auth~=1.1
-boto3<2.0,>=1.20
-python-dateutil>=2.8.1
+pydantic~=1.10
+avro~=1.11
+python-jose~=3.3
 idna<3,>=2.5
-grpcio-tools>=1.47.2
+mypy_extensions>=0.4.3
+croniter~=1.3.0
+chardet==4.0.0
+requests-aws4auth~=1.1
 sqlalchemy<2,>=1.4.0
 tabulate==0.9.0
-email-validator>=1.0.3
-cryptography
-Jinja2>=2.11.3
-typing-inspect
-wheel~=0.38.4
-openmetadata-sqllineage==1.0.2
-antlr4-python3-runtime==4.9.2
-PyYAML
 setuptools~=65.6.3
-chardet==4.0.0
-python-jose~=3.3
+typing-inspect
+openmetadata-sqllineage>=1.0.3
+importlib-metadata~=4.12.0
+boto3<2.0,>=1.20
 cached-property==1.5.2
-typing-compat~=0.1.0
-mypy_extensions>=0.4.3
+antlr4-python3-runtime==4.9.2
+commonregex
 requests>=2.23
-croniter~=1.3.0
-jsonschema
 pymysql>=1.0.2
-pydantic~=1.10
-avro~=1.11
-commonregex
+PyYAML
+email-validator>=1.0.3
+python-dateutil>=2.8.1
+jsonschema
+typing-compat~=0.1.0
 google-auth>=1.33.0
+Jinja2>=2.11.3
+grpcio-tools>=1.47.2
+cryptography
 
 [airflow]
 apache-airflow==2.3.3
 
 [all]
-okta~=2.3
-boto3<2.0,>=1.20
-python-dateutil>=2.8.1
+dbt-artifacts-parser
+azure-identity~=1.12
+google-cloud-storage==1.43.0
+pymssql==2.2.5
+spacy==3.5.0
+thrift-sasl~=0.4
 idna<3,>=2.5
-clickhouse-sqlalchemy~=0.2
-tabulate==0.9.0
+croniter~=1.3.0
+requests-aws4auth~=1.1
+sqlalchemy-vertica[vertica-python]>=0.0.5
 GeoAlchemy2~=0.12
-elasticsearch==7.13.1
+gcsfs==2022.11.0
+okta~=2.3
+tabulate==0.9.0
+typing-inspect
+cachetools
+s3fs==0.4.2
+python_on_whales==0.55.0
+boto3<2.0,>=1.20
+pandas==1.3.5
+commonregex
+cx_Oracle<9,>=8.3.0
+pyodbc<5,>=4.0.35
+presto-types-parser>=0.0.2
+requests>=2.23
 sasl~=0.3
-neo4j~=5.3.0
-psycopg2-binary
-protobuf
-pymssql==2.2.5
-openmetadata-sqllineage==1.0.2
+pymysql>=1.0.2
+PyYAML
+pydomo~=0.3
 packaging==21.3
-sqlalchemy-vertica[vertica-python]>=0.0.5
-presidio-analyzer==2.2.32
 fastavro>=1.2.0
-mlflow-skinny~=1.30
-cached-property==1.5.2
-typing-compat~=0.1.0
+ldap3==2.9.1
 sqlalchemy-redshift~=0.8
-alembic~=1.10.2
-mypy_extensions>=0.4.3
-impyla~=0.18.0
-trino[sqlalchemy]
-pyarrow~=8.0
-dbt-artifacts-parser
-scikit-learn~=1.0
-croniter~=1.3.0
-spacy==3.5.0
-pydantic~=1.10
-clickhouse-driver~=0.2
-sqlalchemy-pytds~=0.3
-delta-spark~=2.2
-s3fs==0.4.2
-python_on_whales==0.55.0
+typing-compat~=0.1.0
+google-auth>=1.33.0
 azure-storage-blob~=12.14
-pinotdb~=0.3
+protobuf
+simple_salesforce==1.11.4
+python-snappy~=0.6.1
 thrift<1,>=0.13
-google>=3.0.0
-msal~=1.2
-importlib-metadata~=4.12.0
-requests-aws4auth~=1.1
-cx_Oracle<9,>=8.3.0
-tableau-api-lib~=0.1
-grpcio-tools>=1.47.2
-sqlalchemy<2,>=1.4.0
+psycopg2-binary
+pyarrow~=8.0
+google-cloud-datacatalog==3.6.2
 azure-storage-blob
-oracledb~=1.2
-presto-types-parser>=0.0.2
-simple_salesforce==1.11.4
-commonregex
-pydruid>=0.6.5
-email-validator>=1.0.3
-cryptography
+grpcio-tools>=1.47.2
 confluent_kafka==1.8.2
-Jinja2>=2.11.3
-adlfs>=2022.2.0
-gcsfs==2022.11.0
-typing-inspect
+clickhouse-sqlalchemy~=0.2
+sqlalchemy-pytds~=0.3
+delta-spark~=2.2
+mlflow-skinny~=1.30
 wheel~=0.38.4
-thrift-sasl~=0.4
+google>=3.0.0
+pydantic~=1.10
+pinotdb~=0.3
+avro~=1.11
+python-jose~=3.3
+pyhive~=0.6
 azure-identity
-antlr4-python3-runtime==4.9.2
-sqlalchemy-bigquery>=1.2.2
-google-cloud-storage==1.43.0
-pydomo~=0.3
-google-cloud
-cachetools
-PyYAML
+mypy_extensions>=0.4.3
+presidio-analyzer==2.2.32
+chardet==4.0.0
+alembic~=1.10.2
+neo4j~=5.3.0
+trino[sqlalchemy]
+sqlalchemy<2,>=1.4.0
+google-cloud-logging
 setuptools~=65.6.3
+openmetadata-sqllineage>=1.0.3
+importlib-metadata~=4.12.0
+google-cloud
+pydruid>=0.6.5
+cached-property==1.5.2
+looker-sdk>=22.20.0
+antlr4-python3-runtime==4.9.2
+tableau-api-lib~=0.1
+lkml~=1.3
+msal~=1.2
 sqlalchemy-databricks~=0.1
-ldap3==2.9.1
-chardet==4.0.0
-python-jose~=3.3
+email-validator>=1.0.3
+python-dateutil>=2.8.1
+snowflake-sqlalchemy~=1.4
 PyAthena[sqlalchemy]
-looker-sdk>=22.20.0
-pyhive~=0.6
-azure-identity~=1.12
-requests>=2.23
 jsonschema
-pymysql>=1.0.2
-avro~=1.11
-google-cloud-datacatalog==3.6.2
-google-cloud-logging
-snowflake-sqlalchemy~=1.4
-pandas==1.3.5
+elasticsearch==7.13.1
+sqlalchemy-bigquery>=1.2.2
+adlfs>=2022.2.0
+clickhouse-driver~=0.2
 dagster_graphql~=1.1
-google-auth>=1.33.0
-pyodbc<5,>=4.0.35
+Jinja2>=2.11.3
+impyla~=0.18.0
+oracledb~=1.2
+scikit-learn~=1.0
+cryptography
 
 [amundsen]
 neo4j~=5.3.0
 
 [athena]
 PyAthena[sqlalchemy]
 
@@ -137,116 +139,119 @@
 [azure-sso]
 msal~=1.2
 
 [azuresql]
 pyodbc<5,>=4.0.35
 
 [backup]
-azure-storage-blob
-boto3<2.0,>=1.20
 azure-identity
+boto3<2.0,>=1.20
+azure-storage-blob
 
 [base]
+wheel~=0.38.4
 google>=3.0.0
-importlib-metadata~=4.12.0
-requests-aws4auth~=1.1
-boto3<2.0,>=1.20
-python-dateutil>=2.8.1
+pydantic~=1.10
+avro~=1.11
+python-jose~=3.3
 idna<3,>=2.5
-grpcio-tools>=1.47.2
+mypy_extensions>=0.4.3
+croniter~=1.3.0
+chardet==4.0.0
+requests-aws4auth~=1.1
 sqlalchemy<2,>=1.4.0
 tabulate==0.9.0
-email-validator>=1.0.3
-cryptography
-Jinja2>=2.11.3
-typing-inspect
-wheel~=0.38.4
-openmetadata-sqllineage==1.0.2
-antlr4-python3-runtime==4.9.2
-PyYAML
 setuptools~=65.6.3
-chardet==4.0.0
-python-jose~=3.3
+typing-inspect
+openmetadata-sqllineage>=1.0.3
+importlib-metadata~=4.12.0
+boto3<2.0,>=1.20
 cached-property==1.5.2
-typing-compat~=0.1.0
-mypy_extensions>=0.4.3
+antlr4-python3-runtime==4.9.2
+commonregex
 requests>=2.23
-croniter~=1.3.0
-jsonschema
 pymysql>=1.0.2
-pydantic~=1.10
-avro~=1.11
-commonregex
+PyYAML
+email-validator>=1.0.3
+python-dateutil>=2.8.1
+jsonschema
+typing-compat~=0.1.0
 google-auth>=1.33.0
+Jinja2>=2.11.3
+grpcio-tools>=1.47.2
+cryptography
 
 [bigquery]
-pyarrow~=8.0
-sqlalchemy-bigquery>=1.2.2
 cachetools
+pyarrow~=8.0
 google-cloud-datacatalog==3.6.2
+sqlalchemy-bigquery>=1.2.2
 google-cloud-logging
 
 [clickhouse]
 clickhouse-driver~=0.2
 clickhouse-sqlalchemy~=0.2
 
 [dagster]
-pymysql>=1.0.2
-GeoAlchemy2~=0.12
 psycopg2-binary
+GeoAlchemy2~=0.12
+pymysql>=1.0.2
 dagster_graphql~=1.1
 
 [data-insight]
 elasticsearch==7.13.1
 
 [databricks]
 sqlalchemy-databricks~=0.1
 
 [datalake-azure]
+azure-storage-blob~=12.14
 pyarrow~=8.0
+azure-identity~=1.12
 boto3<2.0,>=1.20
 pandas==1.3.5
-azure-storage-blob~=12.14
-azure-identity~=1.12
 adlfs>=2022.2.0
+python-snappy~=0.6.1
 
 [datalake-gcs]
-google-cloud-storage==1.43.0
+gcsfs==2022.11.0
 pyarrow~=8.0
 boto3<2.0,>=1.20
 pandas==1.3.5
-gcsfs==2022.11.0
+google-cloud-storage==1.43.0
+python-snappy~=0.6.1
 
 [datalake-s3]
 pyarrow~=8.0
-s3fs==0.4.2
 boto3<2.0,>=1.20
 pandas==1.3.5
+python-snappy~=0.6.1
+s3fs==0.4.2
 
 [db2]
 ibm-db-sa~=0.3
 
 [dbt]
-google-cloud-storage==1.43.0
 dbt-artifacts-parser
 boto3<2.0,>=1.20
 google-cloud
+google-cloud-storage==1.43.0
 
 [deltalake]
 delta-spark~=2.2
 
 [dev]
-pylint
-black==22.3.0
 isort
-twine
-pycln
-pre-commit
 datamodel-code-generator==0.15.0
+pylint
+pycln
+twine
 docker
+pre-commit
+black==22.3.0
 
 [docker]
 python_on_whales==0.55.0
 
 [domo]
 pydomo~=0.3
 
@@ -262,40 +267,41 @@
 [glue]
 boto3<2.0,>=1.20
 
 [great-expectations]
 great-expectations~=0.16.0
 
 [hive]
-presto-types-parser>=0.0.2
-sasl~=0.3
-thrift<1,>=0.13
 pyhive~=0.6
 thrift-sasl~=0.4
+thrift<1,>=0.13
 impyla~=0.18.0
+presto-types-parser>=0.0.2
+sasl~=0.3
 
 [kafka]
-confluent_kafka==1.8.2
 fastavro>=1.2.0
+protobuf
 grpcio-tools>=1.47.2
+confluent_kafka==1.8.2
 avro~=1.11
-protobuf
 
 [kinesis]
 boto3<2.0,>=1.20
 
 [ldap-users]
 ldap3==2.9.1
 
 [looker]
+lkml~=1.3
 looker-sdk>=22.20.0
 
 [mlflow]
-alembic~=1.10.2
 mlflow-skinny~=1.30
+alembic~=1.10.2
 
 [mssql]
 sqlalchemy-pytds~=0.3
 
 [mssql-odbc]
 pyodbc<5,>=4.0.35
 
@@ -304,57 +310,57 @@
 
 [nifi]
 
 [okta]
 okta~=2.3
 
 [oracle]
-cx_Oracle<9,>=8.3.0
 oracledb~=1.2
+cx_Oracle<9,>=8.3.0
 
 [pii-processor]
 spacy==3.5.0
-presidio-analyzer==2.2.32
 pandas==1.3.5
+presidio-analyzer==2.2.32
 
 [pinotdb]
 pinotdb~=0.3
 
 [postgres]
-pymysql>=1.0.2
-GeoAlchemy2~=0.12
 psycopg2-binary
+GeoAlchemy2~=0.12
+pymysql>=1.0.2
 
 [powerbi]
 msal~=1.2
 
 [presto]
-presto-types-parser>=0.0.2
 pyhive~=0.6
+presto-types-parser>=0.0.2
 
 [pymssql]
 pymssql==2.2.5
 
 [quicksight]
 boto3<2.0,>=1.20
 
 [redash]
 packaging==21.3
 
 [redpanda]
-confluent_kafka==1.8.2
 fastavro>=1.2.0
+protobuf
 grpcio-tools>=1.47.2
+confluent_kafka==1.8.2
 avro~=1.11
-protobuf
 
 [redshift]
-sqlalchemy-redshift~=0.8
-GeoAlchemy2~=0.12
 psycopg2-binary
+GeoAlchemy2~=0.12
+sqlalchemy-redshift~=0.8
 
 [sagemaker]
 boto3<2.0,>=1.20
 
 [salesforce]
 simple_salesforce==1.11.4
 
@@ -369,20 +375,20 @@
 
 [superset]
 
 [tableau]
 tableau-api-lib~=0.1
 
 [test]
+pytest-order
 dbt-artifacts-parser
-apache-airflow==2.3.3
-great-expectations~=0.16.0
 coverage
+apache-airflow==2.3.3
 pytest-cov
-pytest-order
+great-expectations~=0.16.0
 pytest==7.0.0
 moto==4.0.8
 
 [trino]
 trino[sqlalchemy]
 
 [vertica]
```

