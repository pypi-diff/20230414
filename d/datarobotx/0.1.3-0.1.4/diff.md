# Comparing `tmp/datarobotx-0.1.3-py3-none-any.whl.zip` & `tmp/datarobotx-0.1.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,52 +1,51 @@
-Zip file size: 129068 bytes, number of entries: 50
--rw-r--r--  2.0 unx     1499 b- defN 23-Mar-29 18:11 datarobotx/__init__.py
--rw-r--r--  2.0 unx      271 b- defN 23-Apr-06 23:50 datarobotx/_version.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-29 18:11 datarobotx/py.typed
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-29 18:11 datarobotx/client/__init__.py
--rw-r--r--  2.0 unx     9337 b- defN 23-Apr-06 16:15 datarobotx/client/datasets.py
--rw-r--r--  2.0 unx    14621 b- defN 23-Apr-05 18:44 datarobotx/client/deployments.py
--rw-r--r--  2.0 unx     1002 b- defN 23-Apr-05 18:44 datarobotx/client/prediction_servers.py
--rw-r--r--  2.0 unx    24893 b- defN 23-Apr-06 16:15 datarobotx/client/projects.py
--rw-r--r--  2.0 unx     2420 b- defN 23-Apr-06 16:15 datarobotx/client/status.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-29 18:11 datarobotx/common/__init__.py
--rw-r--r--  2.0 unx     5980 b- defN 23-Apr-05 18:44 datarobotx/common/client.py
--rw-r--r--  2.0 unx     9598 b- defN 23-Apr-06 16:15 datarobotx/common/config.py
--rw-r--r--  2.0 unx     4588 b- defN 23-Apr-05 18:44 datarobotx/common/configurator.py
--rw-r--r--  2.0 unx   131191 b- defN 23-Apr-06 16:15 datarobotx/common/dr_config.py
--rw-r--r--  2.0 unx    11359 b- defN 23-Apr-06 16:15 datarobotx/common/logging.py
--rw-r--r--  2.0 unx     3980 b- defN 23-Apr-05 18:44 datarobotx/common/transformations.py
--rw-r--r--  2.0 unx    25868 b- defN 23-Apr-06 16:15 datarobotx/common/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-29 18:11 datarobotx/models/__init__.py
--rw-r--r--  2.0 unx     2754 b- defN 23-Apr-06 16:15 datarobotx/models/autoanomaly.py
--rw-r--r--  2.0 unx     3332 b- defN 23-Apr-06 16:15 datarobotx/models/autocluster.py
--rw-r--r--  2.0 unx     2465 b- defN 23-Apr-06 16:15 datarobotx/models/automl.py
--rw-r--r--  2.0 unx    11349 b- defN 23-Apr-06 19:53 datarobotx/models/autopilot.py
--rw-r--r--  2.0 unx     6784 b- defN 23-Apr-06 16:15 datarobotx/models/autots.py
--rw-r--r--  2.0 unx    14976 b- defN 23-Apr-06 16:15 datarobotx/models/colreduce.py
--rw-r--r--  2.0 unx    24227 b- defN 23-Apr-06 16:15 datarobotx/models/deploy.py
--rw-r--r--  2.0 unx    29169 b- defN 23-Apr-06 16:15 datarobotx/models/deployment.py
--rw-r--r--  2.0 unx     7869 b- defN 23-Apr-06 16:15 datarobotx/models/evaluation.py
--rw-r--r--  2.0 unx    16937 b- defN 23-Apr-06 16:15 datarobotx/models/featurediscovery.py
--rw-r--r--  2.0 unx     5593 b- defN 23-Apr-06 16:15 datarobotx/models/intraproject.py
--rw-r--r--  2.0 unx    26742 b- defN 23-Apr-06 16:15 datarobotx/models/model.py
--rw-r--r--  2.0 unx     9736 b- defN 23-Apr-06 16:15 datarobotx/models/selfdiscovery.py
--rw-r--r--  2.0 unx     4422 b- defN 23-Apr-06 16:15 datarobotx/models/share.py
--rw-r--r--  2.0 unx    20800 b- defN 23-Apr-06 16:15 datarobotx/models/sparkingest.py
--rw-r--r--  2.0 unx        0 b- defN 23-Mar-29 18:11 datarobotx/viz/__init__.py
--rw-r--r--  2.0 unx     4906 b- defN 23-Apr-06 16:15 datarobotx/viz/charts.py
--rw-r--r--  2.0 unx     6379 b- defN 23-Apr-05 18:44 datarobotx/viz/leaderboard.py
--rw-r--r--  2.0 unx     9407 b- defN 23-Apr-06 16:15 datarobotx/viz/modelcard.py
--rw-r--r--  2.0 unx     9508 b- defN 23-Apr-06 16:15 datarobotx/viz/viz.py
--rw-r--r--  2.0 unx      926 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/drx_button.html
--rw-r--r--  2.0 unx     1490 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/leaderboard.html
--rw-r--r--  2.0 unx      414 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/leaderboard.md
--rw-r--r--  2.0 unx     5433 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/model_card.html
--rw-r--r--  2.0 unx       48 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/model_card.md
--rw-r--r--  2.0 unx     3140 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/robot.svg
--rw-r--r--  2.0 unx     3135 b- defN 23-Mar-29 18:11 datarobotx/viz/templates/robot_large.svg
--rw-r--r--  2.0 unx     7555 b- defN 23-Apr-07 00:02 datarobotx-0.1.3.dist-info/LICENSE
--rw-r--r--  2.0 unx     4451 b- defN 23-Apr-07 00:02 datarobotx-0.1.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-07 00:02 datarobotx-0.1.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       11 b- defN 23-Apr-07 00:02 datarobotx-0.1.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4326 b- defN 23-Apr-07 00:02 datarobotx-0.1.3.dist-info/RECORD
-50 files, 494983 bytes uncompressed, 122164 bytes compressed:  75.3%
+Zip file size: 125855 bytes, number of entries: 49
+-rw-rw-r--  2.0 unx     1499 b- defN 23-Apr-14 19:28 datarobotx/__init__.py
+-rw-rw-r--  2.0 unx      271 b- defN 23-Apr-14 19:28 datarobotx/_version.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-14 19:28 datarobotx/py.typed
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-14 19:28 datarobotx/client/__init__.py
+-rw-rw-r--  2.0 unx     9402 b- defN 23-Apr-14 19:28 datarobotx/client/datasets.py
+-rw-rw-r--  2.0 unx    14621 b- defN 23-Apr-14 19:28 datarobotx/client/deployments.py
+-rw-rw-r--  2.0 unx     1002 b- defN 23-Apr-14 19:28 datarobotx/client/prediction_servers.py
+-rw-rw-r--  2.0 unx    24966 b- defN 23-Apr-14 19:28 datarobotx/client/projects.py
+-rw-rw-r--  2.0 unx     2420 b- defN 23-Apr-14 19:28 datarobotx/client/status.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-14 19:28 datarobotx/common/__init__.py
+-rw-rw-r--  2.0 unx     5980 b- defN 23-Apr-14 19:28 datarobotx/common/client.py
+-rw-rw-r--  2.0 unx     9306 b- defN 23-Apr-14 19:28 datarobotx/common/config.py
+-rw-rw-r--  2.0 unx     4588 b- defN 23-Apr-14 19:28 datarobotx/common/configurator.py
+-rw-rw-r--  2.0 unx   131191 b- defN 23-Apr-14 19:28 datarobotx/common/dr_config.py
+-rw-rw-r--  2.0 unx    10490 b- defN 23-Apr-14 19:28 datarobotx/common/logging.py
+-rw-rw-r--  2.0 unx     3980 b- defN 23-Apr-14 19:28 datarobotx/common/transformations.py
+-rw-rw-r--  2.0 unx    25914 b- defN 23-Apr-14 19:28 datarobotx/common/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-14 19:28 datarobotx/models/__init__.py
+-rw-rw-r--  2.0 unx     2754 b- defN 23-Apr-14 19:28 datarobotx/models/autoanomaly.py
+-rw-rw-r--  2.0 unx     3332 b- defN 23-Apr-14 19:28 datarobotx/models/autocluster.py
+-rw-rw-r--  2.0 unx     2465 b- defN 23-Apr-14 19:28 datarobotx/models/automl.py
+-rw-rw-r--  2.0 unx    11349 b- defN 23-Apr-14 19:28 datarobotx/models/autopilot.py
+-rw-rw-r--  2.0 unx     6784 b- defN 23-Apr-14 19:28 datarobotx/models/autots.py
+-rw-rw-r--  2.0 unx    14976 b- defN 23-Apr-14 19:28 datarobotx/models/colreduce.py
+-rw-rw-r--  2.0 unx    24227 b- defN 23-Apr-14 19:28 datarobotx/models/deploy.py
+-rw-rw-r--  2.0 unx    29549 b- defN 23-Apr-14 19:28 datarobotx/models/deployment.py
+-rw-rw-r--  2.0 unx     7869 b- defN 23-Apr-14 19:28 datarobotx/models/evaluation.py
+-rw-rw-r--  2.0 unx    16937 b- defN 23-Apr-14 19:28 datarobotx/models/featurediscovery.py
+-rw-rw-r--  2.0 unx     5593 b- defN 23-Apr-14 19:28 datarobotx/models/intraproject.py
+-rw-rw-r--  2.0 unx    26732 b- defN 23-Apr-14 19:28 datarobotx/models/model.py
+-rw-rw-r--  2.0 unx     9736 b- defN 23-Apr-14 19:28 datarobotx/models/selfdiscovery.py
+-rw-rw-r--  2.0 unx     4422 b- defN 23-Apr-14 19:28 datarobotx/models/share.py
+-rw-rw-r--  2.0 unx    20800 b- defN 23-Apr-14 19:28 datarobotx/models/sparkingest.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Apr-14 19:28 datarobotx/viz/__init__.py
+-rw-rw-r--  2.0 unx     4906 b- defN 23-Apr-14 19:28 datarobotx/viz/charts.py
+-rw-rw-r--  2.0 unx     6379 b- defN 23-Apr-14 19:28 datarobotx/viz/leaderboard.py
+-rw-rw-r--  2.0 unx     9407 b- defN 23-Apr-14 19:28 datarobotx/viz/modelcard.py
+-rw-rw-r--  2.0 unx     9508 b- defN 23-Apr-14 19:28 datarobotx/viz/viz.py
+-rw-rw-r--  2.0 unx      926 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/drx_button.html
+-rw-rw-r--  2.0 unx     1490 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/leaderboard.html
+-rw-rw-r--  2.0 unx      414 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/leaderboard.md
+-rw-rw-r--  2.0 unx     5433 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/model_card.html
+-rw-rw-r--  2.0 unx       48 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/model_card.md
+-rw-rw-r--  2.0 unx     3140 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/robot.svg
+-rw-rw-r--  2.0 unx     3135 b- defN 23-Apr-14 19:28 datarobotx/viz/templates/robot_large.svg
+-rw-rw-r--  2.0 unx     4733 b- defN 23-Apr-14 19:30 datarobotx-0.1.4.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-14 19:30 datarobotx-0.1.4.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-Apr-14 19:30 datarobotx-0.1.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     4235 b- defN 23-Apr-14 19:30 datarobotx-0.1.4.dist-info/RECORD
+49 files, 487012 bytes uncompressed, 119095 bytes compressed:  75.5%
```

## zipnote {}

```diff
@@ -129,23 +129,20 @@
 
 Filename: datarobotx/viz/templates/robot.svg
 Comment: 
 
 Filename: datarobotx/viz/templates/robot_large.svg
 Comment: 
 
-Filename: datarobotx-0.1.3.dist-info/LICENSE
+Filename: datarobotx-0.1.4.dist-info/METADATA
 Comment: 
 
-Filename: datarobotx-0.1.3.dist-info/METADATA
+Filename: datarobotx-0.1.4.dist-info/WHEEL
 Comment: 
 
-Filename: datarobotx-0.1.3.dist-info/WHEEL
+Filename: datarobotx-0.1.4.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobotx-0.1.3.dist-info/top_level.txt
-Comment: 
-
-Filename: datarobotx-0.1.3.dist-info/RECORD
+Filename: datarobotx-0.1.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## datarobotx/_version.py

```diff
@@ -6,8 +6,8 @@
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 #
-__version__ = "0.1.3"
+__version__ = "0.1.4"
```

## datarobotx/client/datasets.py

```diff
@@ -5,22 +5,24 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
+
+from __future__ import annotations
+
 import io
 import logging
 import re
 from typing import Any, cast, Dict, Optional, Tuple, Union
 
 import aiohttp
 import pandas as pd
-from pyspark.sql import DataFrame
 
 from datarobotx.client.status import await_status, poll
 from datarobotx.common.client import raise_value_error, session
 from datarobotx.common.config import context
 from datarobotx.common.utils import FilesSender, SparkSender
 
 logger = logging.getLogger("drx")
@@ -208,15 +210,17 @@
         if catalog_name is not None:
             resp_json = await resp.json()
             await patch_dataset(resp_json["catalogId"], name=catalog_name)
         return resp.headers["Location"]
 
 
 async def post_dataset_from_spark_df(
-    spark_df: DataFrame, name: str, max_rows: int
+    spark_df: "pyspark.DataFrame",  # type: ignore[name-defined] # noqa: F821
+    name: str,
+    max_rows: int,
 ) -> Dict[str, Any]:
     """Post to AI Catalog from a spark dataframe
 
     Upload either as a single HTTP post or as a multipart upload. Halts
     iteration on the spark partitions once max_rows has been reached
 
     .limit() in Spark can OOM as it collapses to a single partition
```

## datarobotx/client/projects.py

```diff
@@ -218,14 +218,16 @@
     """Retrieve list of models and associate properties for a given project id."""
     if params:
         params_encoded = urlencode(params)
         url = f"/projects/{pid}/models/?{params_encoded}"
     else:
         url = f"/projects/{pid}/models/"
     async with session.get(url, allow_redirects=False) as resp:
+        if resp.status != 200:
+            await raise_value_error(resp)
         json = await resp.json()
         return cast(Dict[str, Any], json)
 
 
 async def get_prediction_dataset(pid: str, ds_id: str) -> Dict[str, Any]:
     """Prediction dataset attributes"""
     url = f"/projects/{pid}/predictionDatasets/{ds_id}/"
```

## datarobotx/common/config.py

```diff
@@ -20,23 +20,24 @@
 
 _config_initialized = contextvars.ContextVar("config_initialized", default=False)
 _token = contextvars.ContextVar("token", default="")
 _endpoint = contextvars.ContextVar("endpoint", default="")
 _pred_server_id = contextvars.ContextVar("pred_server_id", default="")
 _rest_poll_interval = contextvars.ContextVar("rest_poll_interval", default=3)
 _max_wait = contextvars.ContextVar("max_wait", default=24 * 60 * 60)
-_deployments_use_rt_endpoint = contextvars.ContextVar("deployments_use_rt_endpoint", default=True)
 
 _theme = contextvars.ContextVar("theme", default="dark")
 
 _concurrency_poll_interval = contextvars.ContextVar("concurrency_poll_interval", default=0.2)
 
 _max_dr_ingest = contextvars.ContextVar("max_dr_ingest", default=5 * (10**9))
+
+# TODO: update to new DR-notebook-identifying env variable once NB-2599 complete
 _notebooks_remove_ipy_widgets = contextvars.ContextVar(
-    "notebooks_remove_ipy_widgets", default=False
+    "notebooks_remove_ipy_widgets", default="DATAROBOT_API_KEY_ID" in os.environ
 )
 
 
 # Context variable for the public entry point (function name) into threaded drx work
 drx_task_entry_point: contextvars.ContextVar[str] = contextvars.ContextVar("drx_task_entry_point")
 
 
@@ -250,23 +251,14 @@
             "token": self.token,
             "endpoint": self.endpoint,
             "pred_server_id": self.pred_server_id,
         }
         return d.__repr__()
 
     @property
-    def _deployments_use_rt_endpoint(self) -> bool:
-        """DataRobot API token"""
-        return _deployments_use_rt_endpoint.get()
-
-    @_deployments_use_rt_endpoint.setter
-    def _deployments_use_rt_endpoint(self, value: bool) -> None:
-        _deployments_use_rt_endpoint.set(value)
-
-    @property
     def theme(self) -> str:
         """Whether charts render in dark or light"""
         return _theme.get()
 
     @theme.setter
     def theme(self, value: str) -> None:
         _theme.set(value)
```

## datarobotx/common/logging.py

```diff
@@ -168,17 +168,14 @@
             s = self.format_html(record, s)
         else:
             s = self.format_terminal(record, s)
         return s
 
     def format_terminal(self, record: logging.LogRecord, s: str) -> str:
         """Format log messages for display in terminals (e.g. ANSI codes)"""
-        if not hasattr(record, "is_champion_msg"):
-            # Don't format champion hyperlink
-            s = self.format_terminal_hyperlinks(s)
 
         if record.exc_info is not None or hasattr(record, "is_stack_trace"):
             s = colored(f"{s}", "red", attrs=["bold"])
         elif record.levelno == logging.WARNING:
             s = colored("  - WARNING:", "red", attrs=["bold"]) + f" {s}"
         elif hasattr(record, "is_header"):
             s = colored("#", "blue", attrs=["bold"]) + colored(f" {s}", attrs=["bold"])
@@ -193,14 +190,15 @@
             for line in s.split("\n"):
                 lines += textwrap.wrap(
                     line,
                     width=80,
                     initial_indent="    ",
                     subsequent_indent="    ",
                     break_on_hyphens=False,
+                    break_long_words=False,
                 )
             lines[0] = colored("  - ", attrs=["bold"]) + lines[0][4:]
             s = "\n".join(lines)
         return s
 
     def format_html(self, record: logging.LogRecord, s: str) -> str:
         """Format log messages for display in notebook (e.g. HTML)"""
@@ -237,20 +235,14 @@
         html = (
             '<a target="_blank" rel="noopener noreferrer" '
             + 'style="color:DodgerBlue;text-decoration:underline" '
             + r'href="\2">\1</a>'
         )
         return re.sub(r"\[(.+?)\]\((.+?)\)", html, s)
 
-    @staticmethod
-    def format_terminal_hyperlinks(s: str) -> str:
-        """Turn markdown-style hyperlinks into HTML hyperlinks"""
-        ansi_str = colored(r"\1", attrs=["bold"]) + " " + colored(r"\2", attrs=["underline"])
-        return re.sub(r"\[(.+?)\]\((.+?)\)", ansi_str, s)
-
 
 try:
     import ipywidgets as widgets
 
     class IpywidgetsHandler(logging.Handler):
         """Log handler for Jupyter environments with ipywidgets"""
 
@@ -268,15 +260,15 @@
 async def refresh_bar(tqdm_bar: tqdm_typing.tqdm, delay: float = 0.5) -> None:  # type: ignore[type-arg]
     """Refreshes a bar until it is closed."""
     while not tqdm_bar.disable:
         tqdm_bar.refresh()
         await asyncio.sleep(delay)
 
 
-def setup_default_log_handler(force_terminal_output=False) -> logging.Handler:
+def setup_default_log_handler(force_terminal_output: bool = False) -> logging.Handler:
     """
     Initialize and configure the logging handler
 
     Parameters
     ----------
     force_terminal_output: bool
         If True, the default handler will always use the terminal output
@@ -285,27 +277,14 @@
     if not force_terminal_output and is_widgets_nb_env():
         handler: logging.Handler = IpywidgetsHandler()
         handler.setFormatter(DrxFormatter(as_html=True))
     else:
         handler = logging.StreamHandler()
         handler.setFormatter(DrxFormatter(as_html=False))
     # Unsubscribe from opt-in logging messages by default
-    handler.addFilter(filter=lambda x: int(not hasattr(x, "opt_in")))
+    handler.addFilter(filter=lambda x: bool(not hasattr(x, "opt_in")))
     logger.addHandler(handler)
     return handler
 
 
-def set_logging_to_terminal():
-    """
-    Set the default log handler to use the terminal output
-
-    This is required for DataRobot Notebooks while we wait for ipywidgets support
-    """
-    global default_handler  # pylint: disable=global-statement
-    global tqdm  # pylint: disable=global-statement
-    tqdm = drx_tqdm
-    logger.removeHandler(default_handler)  # pylint: disable=used-before-assignment
-    default_handler = setup_default_log_handler(force_terminal_output=True)
-
-
 tqdm = drx_tqdm_notebook if is_widgets_nb_env() else drx_tqdm
 default_handler = setup_default_log_handler()
```

## datarobotx/common/utils.py

```diff
@@ -35,28 +35,31 @@
     Dict,
     Generator,
     Generic,
     Iterator,
     List,
     Optional,
     Tuple,
+    TYPE_CHECKING,
     TypeVar,
     Union,
 )
 import zipfile
 
 import names_generator
 import pandas as pd
-from pyspark import Row
-import pyspark.sql
 
 from datarobotx.common.client import session
 from datarobotx.common.config import context, drx_task_entry_point, is_notebook_env
 from datarobotx.common.logging import get_widget_for_output, logging_output_widget, tqdm
 
+if TYPE_CHECKING:
+    from pyspark import Row
+    import pyspark.sql
+
 logger = logging.getLogger("drx")
 
 
 def create_task(coro: Coroutine) -> Task:  # type: ignore[type-arg]
     """Vestigial wrapper for asyncio.create_task"""
     return asyncio.create_task(coro)
```

## datarobotx/models/deployment.py

```diff
@@ -87,54 +87,64 @@
             raise ValueError("Could not extract a deployment id from the provided url.")
 
         return Deployment(deployment_id=deployment_id)
 
     def predict(
         self,
         X: pd.DataFrame,
+        batch_mode: bool = False,
         max_explanations: Union[int, str, None] = None,
     ) -> pd.DataFrame:
         """
         Make predictions on X asynchronously using the deployment
 
         Returns empty DataFrame which will be updated with results when complete
 
         Parameters
         ----------
         X: pd.DataFrame
             Data to make predictions on
+        batch_mode: bool (default=False)
+            If True, use batch mode for predictions
         max_explanations: int or 'all' (default=None)
             Number of explanations to return for each prediction.
             Note that 'all' is supported for deployments using SHAP models
             only.
 
         """
-        out = self._dispatch_predict(X, max_explanations=max_explanations)
+        out = self._dispatch_predict(X, batch_mode, max_explanations=max_explanations)
         assert isinstance(out, pd.DataFrame)
         return out
 
     def predict_proba(
-        self, X: pd.DataFrame, max_explanations: Union[int, str, None] = None
+        self,
+        X: pd.DataFrame,
+        batch_mode: bool = False,
+        max_explanations: Union[int, str, None] = None,
     ) -> pd.DataFrame:
         """
         Calculate class probabilities on X asynchronously using the deployment
 
         Returns empty DataFrame which will be updated with results when complete
 
         Parameters
         ----------
         X: pd.DataFrame
             Data to make predictions on
+        batch_mode: bool (default=False)
+            If True, use batch mode for predictions
         max_explanations: int or 'all' (default=None)
             Number of explanations to return for each prediction.
             Note that 'all' is supported for deployments using SHAP models
             only.
 
         """
-        out = self._dispatch_predict(X, max_explanations=max_explanations, class_probabilities=True)
+        out = self._dispatch_predict(
+            X, batch_mode, class_probabilities=True, max_explanations=max_explanations
+        )
         assert isinstance(out, pd.DataFrame)
         return out
 
     def predict_unstructured(self, X: Dict[str, Any]) -> Dict[str, Any]:
         """
         Make predictions with data asynchronously using the deployment
 
@@ -142,23 +152,24 @@
 
         Parameters
         ----------
         X: Dict[str, Any]
             Data to make predictions on
 
         """
-        out = self._dispatch_predict(X, unstructured=True)
+        out = self._dispatch_predict(X, batch_mode=False, unstructured=True)
         assert not isinstance(out, pd.DataFrame)
         return out
 
     def _dispatch_predict(
         self,
         X: Union[pd.DataFrame, Dict[str, Any]],
-        max_explanations: Union[int, str, None] = None,
+        batch_mode: bool,
         class_probabilities: bool = False,
+        max_explanations: Union[int, str, None] = None,
         unstructured: bool = False,
     ) -> Union[pd.DataFrame, Dict[str, Any]]:
         """Private method for dispatching predictions work
 
         Behavior depends on requested predictions type (structured vs. unstructured)
         and whether this deployment object has been initialized
         """
@@ -172,14 +183,15 @@
             logger.info("Waiting for deployment to be initialized...", extra={"is_header": True})
             while self._deployment_id is None:
                 time.sleep(context._concurrency_poll_interval)
 
         future = utils.create_task_new_thread(
             self._predict(
                 X,
+                batch_mode=batch_mode,
                 class_probabilities=class_probabilities,
                 max_explanations=max_explanations,
                 unstructured=unstructured,
             )
         )
 
         if not unstructured:
@@ -187,25 +199,25 @@
         else:
             result = utils.FutureDict(future=future)
         return result
 
     async def _predict(
         self,
         X: Union[pd.DataFrame, Dict[str, Any]],
+        batch_mode: bool,
         class_probabilities: bool = False,
         max_explanations: Union[int, str, None] = None,
         unstructured: bool = False,
     ) -> Union[pd.DataFrame, Dict[str, Any]]:
         deploy_info = await self._validate_predict_args(
             class_probabilities,
             max_explanations=max_explanations,
         )
-        use_rt = context._deployments_use_rt_endpoint
         if not unstructured:
-            if use_rt:
+            if not batch_mode:
                 preds_df = await deploy_client.post_predictions(
                     did=self._deployment_id,  # type: ignore[arg-type]
                     dr_key=deploy_info.dr_key,
                     pred_server_endpoint=deploy_info.endpoint,
                     payload=utils.prepare_df_upload(X),
                     max_explanations=max_explanations,  # type: ignore[arg-type]
                 )
```

## datarobotx/models/model.py

```diff
@@ -371,15 +371,15 @@
 
     @staticmethod
     def _log_starting_predictions(
         project_id: str, model_id: str, project_name: str, model_name: str
     ) -> None:
         """Log that predictions are about to be made"""
         eda_url = context._webui_base_url + f"/projects/{project_id}/eda"
-        model_url = context._webui_base_url + f"/projects/{project_id}/models/{model_id}/blueprint"
+        model_url = context._webui_base_url + f"/projects/{project_id}/models/{model_id}"
         msg = (
             "Making predictions with model "
             + f"[{model_name}]({model_url}) "
             + "from project "
             + f"[{project_name}]({eda_url})"
         )
         logger.info("Making predictions", extra={"is_header": True})
```

## Comparing `datarobotx-0.1.3.dist-info/METADATA` & `datarobotx-0.1.4.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobotx
-Version: 0.1.3
+Version: 0.1.4
 Summary: DataRobotX is a collection of DataRobot extensions
 Home-page: https://datarobot.github.io/drx
 Author: DataRobot
 Author-email: datarobotx@datarobot.com
 License: DataRobot Tool and Utility Agreement
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
@@ -16,15 +16,14 @@
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
-License-File: LICENSE
 Requires-Dist: aiohttp
 Requires-Dist: altair
 Requires-Dist: datarobot
 Requires-Dist: IPython
 Requires-Dist: ipywidgets
 Requires-Dist: names-generator
 Requires-Dist: pandas
@@ -71,37 +70,45 @@
 Requires-Dist: transformers ; extra == 'dev'
 Requires-Dist: scikit-learn ; extra == 'dev'
 Requires-Dist: pyspark ; extra == 'dev'
 Requires-Dist: mypy (==1.0.0) ; extra == 'dev'
 Provides-Extra: spark
 Requires-Dist: pyspark ; extra == 'spark'
 
-<img align="center" src="https://github.com/datarobot/drx/raw/master/doc/source/_static/logo.png" alt="drx">
+<img align="center" src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/logo.png" alt="drx">
+
+![PyPI](https://img.shields.io/pypi/v/datarobotx)
+![PyPI - Python Version](https://img.shields.io/pypi/pyversions/datarobotx)
+![PyPI - Format](https://img.shields.io/pypi/format/datarobotx)
 
 *DataRobotX* is a collection of DataRobot extensions designed to enhance the data science experience.
 
 Check out our [documentation](https://datarobot.github.io/drx) to get started.
 
 This package is released under the terms of the DataRobot Tool and Utility Agreement, which can be found on our [Legal](https://www.datarobot.com/legal/) page, along with our privacy policy and more.
 
 <table>
 <tbody><tr>
 <td><strong>Simple syntax</strong></td>
-<td><img src="https://github.com/datarobot/drx/raw/master/doc/source/_static/intro_gif.gif" alt="Intro"></td>
+<td><img src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/intro_gif.gif" alt="Intro"></td>
 </tr>
 <tr>
 <td><strong>Easy predictions</strong></td>
-<td><img src="https://github.com/datarobot/drx/raw/master/doc/source/_static/predictions_gif.gif" alt="Predict"></td>
+<td><img src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/predictions_gif.gif" alt="Predict"></td>
 </tr>
 <tr>
 <td><strong>Seamless async</strong></td>
-<td><img src="https://github.com/datarobot/drx/raw/master/doc/source/_static/async_gif.gif" alt="Async"></td>
+<td><img src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/async_gif.gif" alt="Async"></td>
+</tr>
+<tr>
+<td><strong>Dynamic widgets</strong></td>
+<td><img src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/EvaluateWidget.gif" alt="Widget"></td>
 </tr>
 <tr>
 <td><strong>Config helpers</strong></td>
-<td><img src="https://github.com/datarobot/drx/raw/master/doc/source/_static/code_complete_gif.gif" alt="Navigate"></td>
+<td><img src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/code_complete_gif.gif" alt="Navigate"></td>
 </tr>
 <tr>
 <td><strong>Extensions</strong></td>
-<td><img src="https://github.com/datarobot/drx/raw/master/doc/source/_static/self_discovery_gif.gif" alt="Extend"></td>
+<td><img src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/self_discovery_gif.gif" alt="Extend"></td>
 </tr>
 </tbody></table>
```

### html2text {}

```diff
@@ -1,27 +1,27 @@
-Metadata-Version: 2.1 Name: datarobotx Version: 0.1.3 Summary: DataRobotX is a
+Metadata-Version: 2.1 Name: datarobotx Version: 0.1.4 Summary: DataRobotX is a
 collection of DataRobot extensions Home-page: https://datarobot.github.io/drx
 Author: DataRobot Author-email: datarobotx@datarobot.com License: DataRobot
 Tool and Utility Agreement Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers Classifier: Intended Audience ::
 Science/Research Classifier: Topic :: Scientific/Engineering :: Artificial
 Intelligence Classifier: License :: Other/Proprietary License Classifier:
 Operating System :: OS Independent Classifier: Programming Language :: Python
 :: 3 Classifier: Programming Language :: Python :: 3 :: Only Classifier:
 Programming Language :: Python :: 3.7 Classifier: Programming Language ::
 Python :: 3.8 Classifier: Programming Language :: Python :: 3.9 Classifier:
 Programming Language :: Python :: 3.10 Requires-Python: >=3.7 Description-
-Content-Type: text/markdown License-File: LICENSE Requires-Dist: aiohttp
-Requires-Dist: altair Requires-Dist: datarobot Requires-Dist: IPython Requires-
-Dist: ipywidgets Requires-Dist: names-generator Requires-Dist: pandas Requires-
-Dist: PyYaml Requires-Dist: setuptools Requires-Dist: termcolor Requires-Dist:
-tqdm Provides-Extra: deploy Requires-Dist: cloudpickle ; extra == 'deploy'
-Requires-Dist: transformers ; extra == 'deploy' Requires-Dist: scikit-learn ;
-extra == 'deploy' Provides-Extra: dev Requires-Dist: flake8 (==5.0.4) ; extra
-== 'dev' Requires-Dist: pylint (==2.15.0) ; extra == 'dev' Requires-Dist: black
+Content-Type: text/markdown Requires-Dist: aiohttp Requires-Dist: altair
+Requires-Dist: datarobot Requires-Dist: IPython Requires-Dist: ipywidgets
+Requires-Dist: names-generator Requires-Dist: pandas Requires-Dist: PyYaml
+Requires-Dist: setuptools Requires-Dist: termcolor Requires-Dist: tqdm
+Provides-Extra: deploy Requires-Dist: cloudpickle ; extra == 'deploy' Requires-
+Dist: transformers ; extra == 'deploy' Requires-Dist: scikit-learn ; extra ==
+'deploy' Provides-Extra: dev Requires-Dist: flake8 (==5.0.4) ; extra == 'dev'
+Requires-Dist: pylint (==2.15.0) ; extra == 'dev' Requires-Dist: black
 (==22.8.0) ; extra == 'dev' Requires-Dist: isort (==5.10.1) ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev' Requires-Dist: pytest-sphinx ; extra ==
 'dev' Requires-Dist: pytest-asyncio ; extra == 'dev' Requires-Dist: vcrpy ;
 extra == 'dev' Requires-Dist: pytest-cov ; extra == 'dev' Requires-Dist: twine
 (>=1.11.0) ; extra == 'dev' Requires-Dist: setuptools ; extra == 'dev'
 Requires-Dist: wheel ; extra == 'dev' Requires-Dist: Sphinx (<5.1.0,>=4.3.0) ;
 extra == 'dev' Requires-Dist: furo (==2022.6.4.1) ; extra == 'dev' Requires-
@@ -34,18 +34,22 @@
 Dist: ipywidgets (==7.7.2) ; extra == 'dev' Requires-Dist: names-generator ;
 extra == 'dev' Requires-Dist: pandas ; extra == 'dev' Requires-Dist: PyYaml ;
 extra == 'dev' Requires-Dist: termcolor ; extra == 'dev' Requires-Dist: tqdm ;
 extra == 'dev' Requires-Dist: altair ; extra == 'dev' Requires-Dist:
 cloudpickle ; extra == 'dev' Requires-Dist: transformers ; extra == 'dev'
 Requires-Dist: scikit-learn ; extra == 'dev' Requires-Dist: pyspark ; extra ==
 'dev' Requires-Dist: mypy (==1.0.0) ; extra == 'dev' Provides-Extra: spark
-Requires-Dist: pyspark ; extra == 'spark' [drx] *DataRobotX* is a collection of
-DataRobot extensions designed to enhance the data science experience. Check out
-our [documentation](https://datarobot.github.io/drx) to get started. This
-package is released under the terms of the DataRobot Tool and Utility
-Agreement, which can be found on our [Legal](https://www.datarobot.com/legal/
-) page, along with our privacy policy and more.
+Requires-Dist: pyspark ; extra == 'spark' [drx] ![PyPI](https://img.shields.io/
+pypi/v/datarobotx) ![PyPI - Python Version](https://img.shields.io/pypi/
+pyversions/datarobotx) ![PyPI - Format](https://img.shields.io/pypi/format/
+datarobotx) *DataRobotX* is a collection of DataRobot extensions designed to
+enhance the data science experience. Check out our [documentation](https://
+datarobot.github.io/drx) to get started. This package is released under the
+terms of the DataRobot Tool and Utility Agreement, which can be found on our
+[Legal](https://www.datarobot.com/legal/) page, along with our privacy policy
+and more.
 Simple syntax    [Intro]
 Easy predictions [Predict]
 Seamless async   [Async]
+Dynamic widgets  [Widget]
 Config helpers   [Navigate]
 Extensions       [Extend]
```

## Comparing `datarobotx-0.1.3.dist-info/RECORD` & `datarobotx-0.1.4.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 datarobotx/__init__.py,sha256=METF-f1x8sWtCopTL0KK3WK579XhaAlUMAMCQJ41oew,1499
-datarobotx/_version.py,sha256=LG5meQ3hzs8NH53lS2cDS29-xlMFl34Wt6qwkR05ca4,271
+datarobotx/_version.py,sha256=Z7gXWDMU9DavPkBUzcxY6K6eXAz0T8hyA-niu2ecuaI,271
 datarobotx/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/client/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-datarobotx/client/datasets.py,sha256=C5cSXZmoGCg4o5PODNaeKEAFNYoPV0Zh11zb2cWRxeE,9337
+datarobotx/client/datasets.py,sha256=l1SoA0yl2Eqbb3pQNcoa9G_sj9MK_xObxt9pt7wvON8,9402
 datarobotx/client/deployments.py,sha256=A9NMf7X25mPH5tsu-r2J6_VjLtKiJlAZgiAVaOD77LI,14621
 datarobotx/client/prediction_servers.py,sha256=BS6Z1fT5ev6ldiIOxKZcuELGcT2k5scZ5j8VH7q3ads,1002
-datarobotx/client/projects.py,sha256=GGX4q5uQHW61V8EcZT0Uj1GJUDqgmb-B8SAizkSFft8,24893
+datarobotx/client/projects.py,sha256=85ytqyR3g219bULSvoi-P9gvp0_1-8y6EOutccLb5GE,24966
 datarobotx/client/status.py,sha256=ExkEDpnBbyQCemp2r4cptUMzwhM5o3F_febWq4qsmKI,2420
 datarobotx/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/common/client.py,sha256=bo3yz0KY7kSF8WZp4F8axYZRmpoFDU6iL3vPoSMgLHg,5980
-datarobotx/common/config.py,sha256=uFrY7dpINS4z_DTF-EeO5RzLq1b07Id_Q-tz8BDL_5U,9598
+datarobotx/common/config.py,sha256=fjXENYUFQAPmuKuzdWn8usDn3oJXRdF2VVUPLHn8KKg,9306
 datarobotx/common/configurator.py,sha256=vdTL4Esk_QYyWNg31EGap7PsBvhL7QizGDVr0FEKxSo,4588
 datarobotx/common/dr_config.py,sha256=RCFE7VEMFxmSFF2Ry2NvONC4g9xh98_QChuubgaq1OI,131191
-datarobotx/common/logging.py,sha256=qfkqsBxnC3NCd9mRoPLGEy7y52VQ6u31HLbQCizO6m8,11359
+datarobotx/common/logging.py,sha256=JjaBGsfTGwYej4FCC0jO3PBRnrQ3B5A6P7o70fgMPts,10490
 datarobotx/common/transformations.py,sha256=ZZpNBmgu1jZvfrNoVtwfLsvSvwGDE5WCTrKkfYosngA,3980
-datarobotx/common/utils.py,sha256=5IWaAetYBog4iQDuRHp8FfAiu8GSBMtWMqVrtswqX2A,25868
+datarobotx/common/utils.py,sha256=atcDRE8eMsPsBQ6oZXrjOPpUG8EY1KpEIThuvn6u7F0,25914
 datarobotx/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/models/autoanomaly.py,sha256=IkhJzilTX1NWiiiEQbVgnd9rWRHnFdeUtdv03kREDXc,2754
 datarobotx/models/autocluster.py,sha256=UBwcx1_oFrlXnae4oNx3poNybmq1bBh_MPnsPbF_46w,3332
 datarobotx/models/automl.py,sha256=K3tTFn84WVdLobzDBDvEnOtyXazrIcZ86y0hN7elSho,2465
 datarobotx/models/autopilot.py,sha256=v2WlNOM55nQuaRtmCaBsnXJqlFtsfg71EAUXKkIo7BI,11349
 datarobotx/models/autots.py,sha256=6qrCfIYg_TCjNdS6VNIl_140uVqIdjlx9nGOWPgDX2g,6784
 datarobotx/models/colreduce.py,sha256=Y0gXfziUEKOtOTMgGwprl4AZAAxtAvYT3zATuWHBJWg,14976
 datarobotx/models/deploy.py,sha256=RmVGf1VmHonBjThh_BfL2TVpzW_2QDCDQqr1I_27DJs,24227
-datarobotx/models/deployment.py,sha256=r8MFKwTc6CS2lZ6TGu5Tv5HWleFtIrAjqg6g7ab93CE,29169
+datarobotx/models/deployment.py,sha256=1RzHLHvqRNn9rScOsugU1bXxHYRGrMfgy0wecxvhfdk,29549
 datarobotx/models/evaluation.py,sha256=EWSMBjz2b0aOFgWBJro3zCFjtbjtQtMEwPxY3o1WTbE,7869
 datarobotx/models/featurediscovery.py,sha256=MvqXKQOKZcGNVF8lv5L1oh1JEF9bZNUmCG4vGZ1-_gk,16937
 datarobotx/models/intraproject.py,sha256=3B4d26qjwgq73HCkMLLh2m05Rtxe7mDhX1epmmFsWnM,5593
-datarobotx/models/model.py,sha256=0YL9Cp-H3_EkYHikxQtdIEPEfS-OBBgydJlRKID8CN4,26742
+datarobotx/models/model.py,sha256=vxI447Hh_LsUHiDdrHGr6mCxwBHcCHvP7j0_bPaCuac,26732
 datarobotx/models/selfdiscovery.py,sha256=4cJ-ADXcQ1ten0ZNHVp2KvKLZ8Jb8xKIT7Dj50laPgA,9736
 datarobotx/models/share.py,sha256=DHUu-V08sDS0uLEYI35_KXkX6PHbvyJJUdOss38Bsno,4422
 datarobotx/models/sparkingest.py,sha256=E6bjHRPYzAxMwaPCRQdUFbjWodfvrTha05dpCvW0sIo,20800
 datarobotx/viz/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/viz/charts.py,sha256=Kw9ZSnNbNmCXk-kYioNtaAPQoNrfIvJp0Xn-Qh_NNts,4906
 datarobotx/viz/leaderboard.py,sha256=Jp9du1peJ58gn-I6F4--K3TDQVoLpLQU_yeILM8sdr4,6379
 datarobotx/viz/modelcard.py,sha256=NqTrKeCMNfo9sIi05e_SANF-2ExVO1tX5FSKe1Ucf7s,9407
@@ -39,12 +39,11 @@
 datarobotx/viz/templates/drx_button.html,sha256=z2wsHdX64v0kbsleMCyo943QnEMGxi0RfHI34hyHMCg,926
 datarobotx/viz/templates/leaderboard.html,sha256=SYSaWRuNBXEzNcm0KE9TUbCo4KDHVB8wABbeQiiz3f8,1490
 datarobotx/viz/templates/leaderboard.md,sha256=QeFvLJVpLSJ1SwHF1cWdj46toXo_1ZD7OznMSG1Bx-k,414
 datarobotx/viz/templates/model_card.html,sha256=H2YKd4gNBGnaEMLCGeywx1DR6O3zvFM_ci9cxfvQ-p0,5433
 datarobotx/viz/templates/model_card.md,sha256=5Bdd7jKLEYY-hpeZ2I99C6sJwMqaeMS5jRbwLcmrCuc,48
 datarobotx/viz/templates/robot.svg,sha256=ZVruQowP1A7ri6frXGiCZnpOnY_KM8Osi66nMJAdyiM,3140
 datarobotx/viz/templates/robot_large.svg,sha256=lAbBH7yv151ngs7Y1jlMw8anqQ-PG77tzTJawLCrc3E,3135
-datarobotx-0.1.3.dist-info/LICENSE,sha256=gg5xMUPpkjU-hVhALB4x2e8kw2sx_KPb4VbV_4wGV4o,7555
-datarobotx-0.1.3.dist-info/METADATA,sha256=AxClWU6mvM0EdsNzDBl9vtnJXf44x0U5kf507wx2cqA,4451
-datarobotx-0.1.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-datarobotx-0.1.3.dist-info/top_level.txt,sha256=_lN2CPexvLnaRX18n5OTMk9KDKnkKNABD7EzuwyfUpI,11
-datarobotx-0.1.3.dist-info/RECORD,,
+datarobotx-0.1.4.dist-info/METADATA,sha256=4IwJf8mF1Nyn1xO3nrJ-5Thp9VUBtpC5bWvlwItHx7E,4733
+datarobotx-0.1.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+datarobotx-0.1.4.dist-info/top_level.txt,sha256=_lN2CPexvLnaRX18n5OTMk9KDKnkKNABD7EzuwyfUpI,11
+datarobotx-0.1.4.dist-info/RECORD,,
```

