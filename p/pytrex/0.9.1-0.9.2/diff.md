# Comparing `tmp/pytrex-0.9.1-py3-none-any.whl.zip` & `tmp/pytrex-0.9.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,23 @@
-Zip file size: 41459 bytes, number of entries: 21
--rw-r--r--  2.0 unx      132 b- defN 22-Nov-23 21:17 pytrex/__init__.py
--rw-r--r--  2.0 unx     3958 b- defN 22-Nov-23 21:17 pytrex/common.py
--rw-r--r--  2.0 unx     5002 b- defN 22-Nov-23 21:17 pytrex/text_opts.py
--rw-r--r--  2.0 unx     7979 b- defN 22-Nov-23 21:17 pytrex/trex_app.py
--rw-r--r--  2.0 unx     3751 b- defN 22-Nov-23 21:17 pytrex/trex_capture.py
--rw-r--r--  2.0 unx     1659 b- defN 22-Nov-23 21:17 pytrex/trex_object.py
--rw-r--r--  2.0 unx    11555 b- defN 22-Nov-23 21:17 pytrex/trex_port.py
--rw-r--r--  2.0 unx     2788 b- defN 22-Nov-23 21:17 pytrex/trex_statistics_view.py
--rw-r--r--  2.0 unx    62688 b- defN 22-Nov-23 21:17 pytrex/trex_stl_packet_builder_scapy.py
--rw-r--r--  2.0 unx    11857 b- defN 22-Nov-23 21:17 pytrex/trex_stream.py
--rw-r--r--  2.0 unx      969 b- defN 22-Nov-23 21:17 pytrex/zipmsg.py
--rw-r--r--  2.0 unx      109 b- defN 22-Nov-23 21:17 pytrex/api/__init__.py
--rw-r--r--  2.0 unx    10415 b- defN 22-Nov-23 21:17 pytrex/api/trex_event.py
--rw-r--r--  2.0 unx    11628 b- defN 22-Nov-23 21:17 pytrex/api/trex_stl_async_client.py
--rw-r--r--  2.0 unx     6860 b- defN 22-Nov-23 21:17 pytrex/api/trex_stl_conn.py
--rw-r--r--  2.0 unx     7559 b- defN 22-Nov-23 21:17 pytrex/api/trex_stl_jsonrpc_client.py
--rw-r--r--  2.0 unx        0 b- defN 22-Nov-23 21:17 pytrex/api/trex_stl_types.py
--rw-r--r--  2.0 unx     1159 b- defN 22-Nov-23 21:17 pytrex-0.9.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Nov-23 21:17 pytrex-0.9.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 22-Nov-23 21:17 pytrex-0.9.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1673 b- defN 22-Nov-23 21:17 pytrex-0.9.1.dist-info/RECORD
-21 files, 151840 bytes uncompressed, 38769 bytes compressed:  74.5%
+Zip file size: 41857 bytes, number of entries: 21
+-rw-rw-rw-  2.0 fat      140 b- defN 22-Nov-06 09:12 pytrex/__init__.py
+-rw-rw-rw-  2.0 fat     4152 b- defN 22-Nov-06 09:12 pytrex/common.py
+-rw-rw-rw-  2.0 fat     5218 b- defN 22-Nov-06 09:12 pytrex/text_opts.py
+-rw-rw-rw-  2.0 fat     8193 b- defN 23-Apr-14 06:35 pytrex/trex_app.py
+-rw-rw-rw-  2.0 fat     3849 b- defN 22-Nov-06 09:12 pytrex/trex_capture.py
+-rw-rw-rw-  2.0 fat     1581 b- defN 23-Apr-14 06:35 pytrex/trex_object.py
+-rw-rw-rw-  2.0 fat    11960 b- defN 23-Apr-14 06:35 pytrex/trex_port.py
+-rw-rw-rw-  2.0 fat     2864 b- defN 23-Apr-14 06:35 pytrex/trex_statistics_view.py
+-rw-rw-rw-  2.0 fat    64578 b- defN 23-Apr-14 06:35 pytrex/trex_stl_packet_builder_scapy.py
+-rw-rw-rw-  2.0 fat    12179 b- defN 23-Apr-14 06:35 pytrex/trex_stream.py
+-rw-rw-rw-  2.0 fat     1012 b- defN 22-Nov-06 09:12 pytrex/zipmsg.py
+-rw-rw-rw-  2.0 fat      112 b- defN 22-Nov-06 09:12 pytrex/api/__init__.py
+-rw-rw-rw-  2.0 fat    10736 b- defN 22-Nov-06 09:12 pytrex/api/trex_event.py
+-rw-rw-rw-  2.0 fat    12057 b- defN 22-Nov-06 09:12 pytrex/api/trex_stl_async_client.py
+-rw-rw-rw-  2.0 fat     7093 b- defN 22-Nov-06 09:12 pytrex/api/trex_stl_conn.py
+-rw-rw-rw-  2.0 fat     7798 b- defN 22-Nov-06 09:12 pytrex/api/trex_stl_jsonrpc_client.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-06 09:12 pytrex/api/trex_stl_types.py
+-rw-rw-rw-  2.0 fat     1207 b- defN 23-Apr-14 13:55 pytrex-0.9.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-14 13:55 pytrex-0.9.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        7 b- defN 23-Apr-14 13:55 pytrex-0.9.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     1674 b- defN 23-Apr-14 13:55 pytrex-0.9.2.dist-info/RECORD
+21 files, 156502 bytes uncompressed, 39167 bytes compressed:  75.0%
```

## zipnote {}

```diff
@@ -45,20 +45,20 @@
 
 Filename: pytrex/api/trex_stl_jsonrpc_client.py
 Comment: 
 
 Filename: pytrex/api/trex_stl_types.py
 Comment: 
 
-Filename: pytrex-0.9.1.dist-info/METADATA
+Filename: pytrex-0.9.2.dist-info/METADATA
 Comment: 
 
-Filename: pytrex-0.9.1.dist-info/WHEEL
+Filename: pytrex-0.9.2.dist-info/WHEEL
 Comment: 
 
-Filename: pytrex-0.9.1.dist-info/top_level.txt
+Filename: pytrex-0.9.2.dist-info/top_level.txt
 Comment: 
 
-Filename: pytrex-0.9.1.dist-info/RECORD
+Filename: pytrex-0.9.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pytrex/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-"""
-Cisco TRex OO API.
-"""
-from trafficgenerator import TgnError
-
-
-class TrexError(TgnError):
-    """Base class for Trex errors."""
+"""
+Cisco TRex OO API.
+"""
+from trafficgenerator import TgnError
+
+
+class TrexError(TgnError):
+    """Base class for Trex errors."""
```

## pytrex/common.py

 * *Ordering differences only*

```diff
@@ -1,194 +1,194 @@
-#!/usr/bin/python3
-
-import os
-import random
-import re
-import socket
-import string
-import sys
-import time
-
-try:
-    import pwd
-except ImportError:
-    import getpass
-
-    pwd = None
-
-using_python_3 = True if sys.version_info.major == 3 else False
-
-
-def get_current_user():
-    if pwd:
-        return pwd.getpwuid(os.geteuid()).pw_name
-    else:
-        return getpass.getuser()
-
-
-def user_input():
-    if using_python_3:
-        return eval(input())
-    else:
-        # using python version 2
-        return eval(input())
-
-
-class random_id_gen:
-    """
-    Emulated generator for creating a random chars id of specific length
-
-    :parameters:
-        length : int
-            the desired length of the generated id
-
-            default: 8
-
-    :return:
-        a random id with each next() request.
-    """
-
-    def __init__(self, length=8):
-        self.id_chars = string.ascii_lowercase + string.digits
-        self.length = length
-
-    def __next__(self):
-        return "".join(random.choice(self.id_chars) for _ in range(self.length))
-
-    # __next__ = next
-    next = __next__
-
-
-# try to get number from input, return None in case of fail
-def get_number(input):
-    try:
-        return int(input)
-    except Exception:
-        try:
-            return int(input)
-        except Exception:
-            return None
-
-
-def list_intersect(l1, l2):
-    return list([x for x in l1 if x in l2])
-
-
-# actually first list minus second
-
-
-def list_difference(l1, l2):
-    return list([x for x in l1 if x not in l2])
-
-
-# symmetric diff
-
-
-def list_xor(l1, l2):
-    return list(set(l1) ^ set(l2))
-
-
-def is_sub_list(l1, l2):
-    return set(l1) <= set(l2)
-
-
-# splits a timestamp in seconds to sec/usec
-
-
-def sec_split_usec(ts):
-    return int(ts), int((ts - int(ts)) * 1e6)
-
-
-# a simple passive timer
-class PassiveTimer(object):
-
-    # timeout_sec = None means forever
-    def __init__(self, timeout_sec):
-        if timeout_sec is not None:
-            self.expr_sec = time.time() + timeout_sec
-        else:
-            self.expr_sec = None
-
-    def has_expired(self):
-        # if no timeout was set - return always false
-        if self.expr_sec is None:
-            return False
-
-        return time.time() > self.expr_sec
-
-
-def is_valid_ipv4(addr):
-    try:
-        socket.inet_pton(socket.AF_INET, addr)
-        return True
-    except (socket.error, TypeError):
-        return False
-
-
-def is_valid_ipv6(addr):
-    try:
-        socket.inet_pton(socket.AF_INET6, addr)
-        return True
-    except (socket.error, TypeError):
-        return False
-
-
-def is_valid_mac(mac):
-    return bool(re.match("[0-9a-f]{2}([-:])[0-9a-f]{2}(\\1[0-9a-f]{2}){4}$", mac.lower()))
-
-
-def list_remove_dup(l):
-    tmp = list()
-
-    for x in l:
-        if x not in tmp:
-            tmp.append(x)
-
-    return tmp
-
-
-def bitfield_to_list(bf):
-    rc = []
-    bitpos = 0
-
-    while bf > 0:
-        if bf & 0x1:
-            rc.append(bitpos)
-        bitpos += 1
-        bf = bf >> 1
-
-    return rc
-
-
-def set_window_always_on_top(title):
-    # we need the GDK module, if not available - ignroe this command
-    try:
-        if sys.version_info < (3, 0):
-            from gtk import gdk
-        else:
-            # from gi.repository import Gdk as gdk
-            return
-
-    except ImportError:
-        return
-
-    # search the window and set it as above
-    root = gdk.get_default_root_window()
-
-    for id in root.property_get("_NET_CLIENT_LIST")[2]:
-        w = gdk.window_foreign_new(id)
-        if w:
-            name = w.property_get("WM_NAME")[2]
-            if title in name:
-                w.set_keep_above(True)
-                gdk.window_process_all_updates()
-                break
-
-
-def bitfield_to_str(bf):
-    lst = bitfield_to_list(bf)
-    return "-" if not lst else ", ".join([str(x) for x in lst])
-
-
-# https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/
-def natural_sorted_key(val):
-    return [int(c) if c.isdigit() else c for c in re.split(r"(\d+)", val)]
+#!/usr/bin/python3
+
+import os
+import random
+import re
+import socket
+import string
+import sys
+import time
+
+try:
+    import pwd
+except ImportError:
+    import getpass
+
+    pwd = None
+
+using_python_3 = True if sys.version_info.major == 3 else False
+
+
+def get_current_user():
+    if pwd:
+        return pwd.getpwuid(os.geteuid()).pw_name
+    else:
+        return getpass.getuser()
+
+
+def user_input():
+    if using_python_3:
+        return eval(input())
+    else:
+        # using python version 2
+        return eval(input())
+
+
+class random_id_gen:
+    """
+    Emulated generator for creating a random chars id of specific length
+
+    :parameters:
+        length : int
+            the desired length of the generated id
+
+            default: 8
+
+    :return:
+        a random id with each next() request.
+    """
+
+    def __init__(self, length=8):
+        self.id_chars = string.ascii_lowercase + string.digits
+        self.length = length
+
+    def __next__(self):
+        return "".join(random.choice(self.id_chars) for _ in range(self.length))
+
+    # __next__ = next
+    next = __next__
+
+
+# try to get number from input, return None in case of fail
+def get_number(input):
+    try:
+        return int(input)
+    except Exception:
+        try:
+            return int(input)
+        except Exception:
+            return None
+
+
+def list_intersect(l1, l2):
+    return list([x for x in l1 if x in l2])
+
+
+# actually first list minus second
+
+
+def list_difference(l1, l2):
+    return list([x for x in l1 if x not in l2])
+
+
+# symmetric diff
+
+
+def list_xor(l1, l2):
+    return list(set(l1) ^ set(l2))
+
+
+def is_sub_list(l1, l2):
+    return set(l1) <= set(l2)
+
+
+# splits a timestamp in seconds to sec/usec
+
+
+def sec_split_usec(ts):
+    return int(ts), int((ts - int(ts)) * 1e6)
+
+
+# a simple passive timer
+class PassiveTimer(object):
+
+    # timeout_sec = None means forever
+    def __init__(self, timeout_sec):
+        if timeout_sec is not None:
+            self.expr_sec = time.time() + timeout_sec
+        else:
+            self.expr_sec = None
+
+    def has_expired(self):
+        # if no timeout was set - return always false
+        if self.expr_sec is None:
+            return False
+
+        return time.time() > self.expr_sec
+
+
+def is_valid_ipv4(addr):
+    try:
+        socket.inet_pton(socket.AF_INET, addr)
+        return True
+    except (socket.error, TypeError):
+        return False
+
+
+def is_valid_ipv6(addr):
+    try:
+        socket.inet_pton(socket.AF_INET6, addr)
+        return True
+    except (socket.error, TypeError):
+        return False
+
+
+def is_valid_mac(mac):
+    return bool(re.match("[0-9a-f]{2}([-:])[0-9a-f]{2}(\\1[0-9a-f]{2}){4}$", mac.lower()))
+
+
+def list_remove_dup(l):
+    tmp = list()
+
+    for x in l:
+        if x not in tmp:
+            tmp.append(x)
+
+    return tmp
+
+
+def bitfield_to_list(bf):
+    rc = []
+    bitpos = 0
+
+    while bf > 0:
+        if bf & 0x1:
+            rc.append(bitpos)
+        bitpos += 1
+        bf = bf >> 1
+
+    return rc
+
+
+def set_window_always_on_top(title):
+    # we need the GDK module, if not available - ignroe this command
+    try:
+        if sys.version_info < (3, 0):
+            from gtk import gdk
+        else:
+            # from gi.repository import Gdk as gdk
+            return
+
+    except ImportError:
+        return
+
+    # search the window and set it as above
+    root = gdk.get_default_root_window()
+
+    for id in root.property_get("_NET_CLIENT_LIST")[2]:
+        w = gdk.window_foreign_new(id)
+        if w:
+            name = w.property_get("WM_NAME")[2]
+            if title in name:
+                w.set_keep_above(True)
+                gdk.window_process_all_updates()
+                break
+
+
+def bitfield_to_str(bf):
+    lst = bitfield_to_list(bf)
+    return "-" if not lst else ", ".join([str(x) for x in lst])
+
+
+# https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/
+def natural_sorted_key(val):
+    return [int(c) if c.isdigit() else c for c in re.split(r"(\d+)", val)]
```

## pytrex/text_opts.py

 * *Ordering differences only*

```diff
@@ -1,216 +1,216 @@
-import json
-import re
-import sys
-
-TEXT_CODES = {
-    "bold": {"start": "\x1b[1m", "end": "\x1b[22m"},
-    "cyan": {"start": "\x1b[36m", "end": "\x1b[39m"},
-    "blue": {"start": "\x1b[34m", "end": "\x1b[39m"},
-    "red": {"start": "\x1b[31m", "end": "\x1b[39m"},
-    "magenta": {"start": "\x1b[35m", "end": "\x1b[39m"},
-    "green": {"start": "\x1b[32m", "end": "\x1b[39m"},
-    "yellow": {"start": "\x1b[33m", "end": "\x1b[39m"},
-    "underline": {"start": "\x1b[4m", "end": "\x1b[24m"},
-}
-
-
-class TextCodesStripper:
-    keys = [re.escape(v["start"]) for k, v in list(TEXT_CODES.items())]
-    keys += [re.escape(v["end"]) for k, v in list(TEXT_CODES.items())]
-    pattern = re.compile("|".join(keys))
-
-    @staticmethod
-    def strip(s):
-        return re.sub(TextCodesStripper.pattern, "", s)
-
-
-def clear_formatting(s):
-    return TextCodesStripper.strip(s)
-
-
-def format_num(size, suffix="", compact=True, opts=None):
-    if opts is None:
-        opts = ()
-
-    txt = "NaN"
-
-    if type(size) == str:
-        return "N/A"
-
-    u = ""
-
-    if compact:
-        for unit in ["", "K", "M", "G", "T", "P"]:
-            if abs(size) < 1000.0:
-                u = unit
-                break
-            size /= 1000.0
-
-    if isinstance(size, float):
-        txt = "%3.2f" % (size)
-    else:
-        txt = "{:,}".format(size)
-
-    if u or suffix:
-        txt += " {:}{:}".format(u, suffix)
-
-    if isinstance(opts, tuple):
-        return format_text(txt, *opts)
-    else:
-        return format_text(txt, (opts))
-
-
-def format_time(t_sec):
-    if t_sec < 0:
-        return "infinite"
-
-    if t_sec == 0:
-        return "zero"
-
-    if t_sec < 1:
-        # low numbers
-        for unit in ["ms", "usec", "ns"]:
-            t_sec *= 1000.0
-            if t_sec >= 1.0:
-                return "{:,.2f} [{:}]".format(t_sec, unit)
-
-        return "NaN"
-
-    else:
-        # seconds
-        if t_sec < 60.0:
-            return "{:,.2f} [{:}]".format(t_sec, "sec")
-
-        # minutes
-        t_sec /= 60.0
-        if t_sec < 60.0:
-            return "{:,.2f} [{:}]".format(t_sec, "minutes")
-
-        # hours
-        t_sec /= 60.0
-        if t_sec < 24.0:
-            return "{:,.2f} [{:}]".format(t_sec, "hours")
-
-        # days
-        t_sec /= 24.0
-        return "{:,.2f} [{:}]".format(t_sec, "days")
-
-
-def format_percentage(size):
-    return "%0.2f %%" % (size)
-
-
-def bold(text):
-    return text_attribute(text, "bold")
-
-
-def cyan(text):
-    return text_attribute(text, "cyan")
-
-
-def blue(text):
-    return text_attribute(text, "blue")
-
-
-def red(text):
-    return text_attribute(text, "red")
-
-
-def magenta(text):
-    return text_attribute(text, "magenta")
-
-
-def green(text):
-    return text_attribute(text, "green")
-
-
-def yellow(text):
-    return text_attribute(text, "yellow")
-
-
-def underline(text):
-    return text_attribute(text, "underline")
-
-
-# apply attribute on each non-empty line
-
-
-def text_attribute(text, attribute):
-    return "\n".join(
-        [
-            "{start}{txt}{end}".format(start=TEXT_CODES[attribute]["start"], txt=line, end=TEXT_CODES[attribute]["end"])
-            if line
-            else ""
-            for line in str(text).split("\n")
-        ]
-    )
-
-
-FUNC_DICT = {
-    "blue": blue,
-    "bold": bold,
-    "green": green,
-    "yellow": yellow,
-    "cyan": cyan,
-    "magenta": magenta,
-    "underline": underline,
-    "red": red,
-}
-
-
-def __format_text_tty(text, *args):
-    return_string = text
-    for i in args:
-        func = FUNC_DICT.get(i)
-        if func:
-            return_string = func(return_string)
-
-    return return_string
-
-
-def __format_text_non_tty(text, *args):
-    return str(text)
-
-
-# choose according to stdout type
-format_text = __format_text_tty if sys.stdout.isatty() else __format_text_non_tty
-
-
-def format_threshold(value, red_zone, green_zone):
-    try:
-        if value >= red_zone[0] and value <= red_zone[1]:
-            return format_text("{0}".format(value), "red")
-
-        if value >= green_zone[0] and value <= green_zone[1]:
-            return format_text("{0}".format(value), "green")
-    except TypeError:
-        # if value is not comparable or not a number - skip this
-        pass
-
-    return "{0}".format(value)
-
-
-# pretty print for JSON
-
-
-def pretty_json(json_str, use_colors=True):
-    pretty_str = json.dumps(json.loads(json_str), indent=4, separators=(",", ": "), sort_keys=True)
-
-    if not use_colors:
-        return pretty_str
-
-    try:
-        # int numbers
-        pretty_str = re.sub(r"([ ]*:[ ]+)(\-?[1-9][0-9]*[^.])", r"\1{0}".format(blue(r"\2")), pretty_str)
-        # float
-        pretty_str = re.sub(r"([ ]*:[ ]+)(\-?[1-9][0-9]*\.[0-9]+)", r"\1{0}".format(magenta(r"\2")), pretty_str)
-        # # strings
-        #
-        pretty_str = re.sub(r'([ ]*:[ ]+)("[^"]*")', r"\1{0}".format(red(r"\2")), pretty_str)
-        pretty_str = re.sub(
-            r"('[^']*')", r"{0}\1{1}".format(TEXT_CODES["magenta"]["start"], TEXT_CODES["red"]["start"]), pretty_str
-        )
-    except Exception:
-        pass
-
-    return pretty_str
+import json
+import re
+import sys
+
+TEXT_CODES = {
+    "bold": {"start": "\x1b[1m", "end": "\x1b[22m"},
+    "cyan": {"start": "\x1b[36m", "end": "\x1b[39m"},
+    "blue": {"start": "\x1b[34m", "end": "\x1b[39m"},
+    "red": {"start": "\x1b[31m", "end": "\x1b[39m"},
+    "magenta": {"start": "\x1b[35m", "end": "\x1b[39m"},
+    "green": {"start": "\x1b[32m", "end": "\x1b[39m"},
+    "yellow": {"start": "\x1b[33m", "end": "\x1b[39m"},
+    "underline": {"start": "\x1b[4m", "end": "\x1b[24m"},
+}
+
+
+class TextCodesStripper:
+    keys = [re.escape(v["start"]) for k, v in list(TEXT_CODES.items())]
+    keys += [re.escape(v["end"]) for k, v in list(TEXT_CODES.items())]
+    pattern = re.compile("|".join(keys))
+
+    @staticmethod
+    def strip(s):
+        return re.sub(TextCodesStripper.pattern, "", s)
+
+
+def clear_formatting(s):
+    return TextCodesStripper.strip(s)
+
+
+def format_num(size, suffix="", compact=True, opts=None):
+    if opts is None:
+        opts = ()
+
+    txt = "NaN"
+
+    if type(size) == str:
+        return "N/A"
+
+    u = ""
+
+    if compact:
+        for unit in ["", "K", "M", "G", "T", "P"]:
+            if abs(size) < 1000.0:
+                u = unit
+                break
+            size /= 1000.0
+
+    if isinstance(size, float):
+        txt = "%3.2f" % (size)
+    else:
+        txt = "{:,}".format(size)
+
+    if u or suffix:
+        txt += " {:}{:}".format(u, suffix)
+
+    if isinstance(opts, tuple):
+        return format_text(txt, *opts)
+    else:
+        return format_text(txt, (opts))
+
+
+def format_time(t_sec):
+    if t_sec < 0:
+        return "infinite"
+
+    if t_sec == 0:
+        return "zero"
+
+    if t_sec < 1:
+        # low numbers
+        for unit in ["ms", "usec", "ns"]:
+            t_sec *= 1000.0
+            if t_sec >= 1.0:
+                return "{:,.2f} [{:}]".format(t_sec, unit)
+
+        return "NaN"
+
+    else:
+        # seconds
+        if t_sec < 60.0:
+            return "{:,.2f} [{:}]".format(t_sec, "sec")
+
+        # minutes
+        t_sec /= 60.0
+        if t_sec < 60.0:
+            return "{:,.2f} [{:}]".format(t_sec, "minutes")
+
+        # hours
+        t_sec /= 60.0
+        if t_sec < 24.0:
+            return "{:,.2f} [{:}]".format(t_sec, "hours")
+
+        # days
+        t_sec /= 24.0
+        return "{:,.2f} [{:}]".format(t_sec, "days")
+
+
+def format_percentage(size):
+    return "%0.2f %%" % (size)
+
+
+def bold(text):
+    return text_attribute(text, "bold")
+
+
+def cyan(text):
+    return text_attribute(text, "cyan")
+
+
+def blue(text):
+    return text_attribute(text, "blue")
+
+
+def red(text):
+    return text_attribute(text, "red")
+
+
+def magenta(text):
+    return text_attribute(text, "magenta")
+
+
+def green(text):
+    return text_attribute(text, "green")
+
+
+def yellow(text):
+    return text_attribute(text, "yellow")
+
+
+def underline(text):
+    return text_attribute(text, "underline")
+
+
+# apply attribute on each non-empty line
+
+
+def text_attribute(text, attribute):
+    return "\n".join(
+        [
+            "{start}{txt}{end}".format(start=TEXT_CODES[attribute]["start"], txt=line, end=TEXT_CODES[attribute]["end"])
+            if line
+            else ""
+            for line in str(text).split("\n")
+        ]
+    )
+
+
+FUNC_DICT = {
+    "blue": blue,
+    "bold": bold,
+    "green": green,
+    "yellow": yellow,
+    "cyan": cyan,
+    "magenta": magenta,
+    "underline": underline,
+    "red": red,
+}
+
+
+def __format_text_tty(text, *args):
+    return_string = text
+    for i in args:
+        func = FUNC_DICT.get(i)
+        if func:
+            return_string = func(return_string)
+
+    return return_string
+
+
+def __format_text_non_tty(text, *args):
+    return str(text)
+
+
+# choose according to stdout type
+format_text = __format_text_tty if sys.stdout.isatty() else __format_text_non_tty
+
+
+def format_threshold(value, red_zone, green_zone):
+    try:
+        if value >= red_zone[0] and value <= red_zone[1]:
+            return format_text("{0}".format(value), "red")
+
+        if value >= green_zone[0] and value <= green_zone[1]:
+            return format_text("{0}".format(value), "green")
+    except TypeError:
+        # if value is not comparable or not a number - skip this
+        pass
+
+    return "{0}".format(value)
+
+
+# pretty print for JSON
+
+
+def pretty_json(json_str, use_colors=True):
+    pretty_str = json.dumps(json.loads(json_str), indent=4, separators=(",", ": "), sort_keys=True)
+
+    if not use_colors:
+        return pretty_str
+
+    try:
+        # int numbers
+        pretty_str = re.sub(r"([ ]*:[ ]+)(\-?[1-9][0-9]*[^.])", r"\1{0}".format(blue(r"\2")), pretty_str)
+        # float
+        pretty_str = re.sub(r"([ ]*:[ ]+)(\-?[1-9][0-9]*\.[0-9]+)", r"\1{0}".format(magenta(r"\2")), pretty_str)
+        # # strings
+        #
+        pretty_str = re.sub(r'([ ]*:[ ]+)("[^"]*")', r"\1{0}".format(red(r"\2")), pretty_str)
+        pretty_str = re.sub(
+            r"('[^']*')", r"{0}\1{1}".format(TEXT_CODES["magenta"]["start"], TEXT_CODES["red"]["start"]), pretty_str
+        )
+    except Exception:
+        pass
+
+    return pretty_str
```

## pytrex/trex_app.py

```diff
@@ -1,225 +1,225 @@
-"""
-Classes and utilities that represents TRex GUI application.
-"""
-import logging
-import random
-import time
-from typing import Dict, List, Optional
-
-from trafficgenerator import ApiType, TgnApp
-
-from pytrex.api.trex_event import EventsHandler
-from pytrex.api.trex_stl_conn import Connection
-from pytrex.trex_object import TrexObject
-from pytrex.trex_port import TrexCaptureMode, TrexPort
-from pytrex.trex_statistics_view import TrexStreamStatistics
-
-logger = logging.getLogger("tgn.trex")
-
-
-class TrexApp(TgnApp):
-    """TrexApp object, equivalent to TRex application."""
-
-    def __init__(self, username: str, ip: str, port: int = 4501, async_port: int = 4500, virtual: bool = False) -> None:
-        """Start TRex application.
-
-        :param username: User name.
-        :param ip: TRex server IP.
-        :param port: TRex server RPC port.
-        :param async_port: Async port
-        :param virtual: ???
-        """
-        super().__init__(logger, ApiType.socket)
-        self.server = TrexServer(username, ip, port, async_port, virtual)
-
-
-class TrexServer(TrexObject):
-    """Represents single TRex server."""
-
-    def __init__(self, username: str, ip: str, port: int = 4501, async_port: int = 4500, virtual: bool = False) -> None:
-        """Start TRex application.
-
-        :param ip: TRex server IP.
-        :param port: TRex server RPC port.
-        :param async_port: Async port
-        :param username: User name.
-        :param virtual: ???
-        """
-        self.logger = logger
-        self.username = username
-        self.ip = ip
-        self.port = port
-        self.async_port = async_port
-        self.virtual = virtual
-        self.server = self
-        self.event_handler: EventsHandler = None
-        super().__init__(parent=None, objType="server", objRef="server")
-
-    def connect(self) -> None:
-        """Connect to the TRex server."""
-        self.event_handler = EventsHandler(self)
-        connection_info = {
-            "username": self.username,
-            "server": self.ip,
-            "sync_port": self.port,
-            "async_port": self.async_port,
-            "virtual": self.virtual,
-        }
-        self.api = Connection(connection_info, self.logger, self)
-        self.api.connect()
-        self.session_id = random.getrandbits(32)
-
-    def disconnect(self) -> None:
-        """Release all ports and disconnect from server."""
-        for port in self.ports.values():
-            port.release()
-        self.api.disconnect()
-
-    def reserve_ports(self, locations: List[int], force: bool = False, reset: bool = False) -> Dict[int, TrexPort]:
-        """Reserve ports.
-
-        TRex -> Port -> Acquire.
-
-        :param locations: list of ports locations in the form <port number> to reserve
-        :param force: True - take forcefully. False - fail if port is reserved by other user
-        :param reset: True - reset port, False - leave port configuration
-        :return: ports dictionary (location: object)
-        """
-        return_dict = {}
-        for location in locations:
-            TrexPort(parent=self, index=location).reserve(force, reset)
-            return_dict[location] = self.ports[location]
-        return return_dict
-
-    #
-    # Configuration
-    #
-
-    def get_system_info(self) -> dict:
-        return self.transmit("get_system_info", {})["result"]
-
-    def get_supported_cmds(self) -> dict:
-        return self.transmit("get_supported_cmds", {})["result"]
-
-    #
-    # Control
-    #
-
-    def clear_stats(self, *ports: TrexPort) -> None:
-        """Clear statistics on list of ports.
-
-        :param ports: list of ports to start traffic on, if empty, clear all ports.
-        """
-        for port in ports or self.ports.values():
-            port.clear_stats()
-        TrexStreamStatistics.clear_stats(self)
-
-    def start_transmit(self, blocking: bool = False, *ports: TrexPort) -> None:
-        """Start traffic on list of ports.
-
-        If possible, synchronize start of traffic, else, start.
-
-        :param blocking: if blocking - wait for transmit end, else - return after transmit starts.
-        :param ports: list of ports to start transmit on, if empty, start on all ports.
-        """
-        ports = ports if ports else self.ports.values()
-
-        synchronized = True
-        if not len(ports) % 2:
-            for port in ports:
-                if list(self.ports.values()).index(port) ^ 1 not in list(self.ports.keys()):
-                    synchronized = False
-        else:
-            synchronized = False
-
-        if synchronized:
-            save_level = self.logger.level
-            start_time = time.time()
-            rc = self.api.rpc.transmit("ping", api_class=None)
-            start_at_ts = rc["result"]["ts"] + max((time.time() - start_time), 0.5) * len(ports)
-            self.logger.level = save_level
-        else:
-            start_at_ts = 0
-
-        for port in ports:
-            port.start_at_ts = start_at_ts
-            port.start_transmit(blocking=False)
-
-        if blocking:
-            self.wait_transmit(*ports)
-
-    def stop_transmit(self, *ports: TrexPort) -> None:
-        """Stop traffic on list of ports.
-
-        :param ports: list of ports to stop transmit on, if empty, stop on all ports.
-        """
-        for port in ports or self.ports.values():
-            port.stop_transmit()
-
-    def wait_transmit(self, *ports: TrexPort) -> None:
-        """Wait for transmit end on list of ports.
-
-        :param ports: list of ports to wait for, if empty, wait for all ports.
-        """
-        for port in ports or self.ports.values():
-            port.wait_transmit()
-        time.sleep(4)
-
-    def clear_capture(self, *ports: TrexPort) -> None:
-        """Clear all existing capture IDs on list ports.
-
-        :param ports: list of ports to clear capture on, if empty, clear on all ports.
-        """
-        for port in ports or self.ports.values():
-            port.clear_capture(rx=True, tx=True)
-
-    def start_capture(
-        self, limit: int = 1000, mode: TrexCaptureMode = TrexCaptureMode.FIXED, bpf_filter: str = "", *ports: TrexPort
-    ) -> None:
-        """Start RX capture on list of ports.
-
-        :param limit: limit the total number of captrured packets (for all ports) memory requierment is O(9K * limit).
-        :param mode: when full, if fixed drop new packets, else (cyclic) drop old packets.
-        :param bpf_filter:  A Berkeley Packet Filter pattern. Only packets matching the filter will be captured.
-        :param ports: list of ports to start capture on, if empty, start on all ports.
-        """
-        ports = ports or self.ports.values()
-        for port in ports:
-            port.start_capture(
-                rx=True,
-                tx=False,
-                limit=int(limit / len(ports)),
-                mode=mode,
-                bpf_filter=bpf_filter,
-            )
-
-    def stop_capture(self, limit: int = 1000, output: Optional[str] = None, *ports) -> Dict[TrexPort, List]:
-        """Stop capture on list of ports.
-
-        :param limit: limit the total number of packets that will be read from the capture buffer of all ports.
-        :param output: prefix for the capture file name.
-            Capture files for each port will be stored in individual output file named 'prefix-{port ID}.txt'.
-        :param ports: list of ports to stop capture on, if empty, stop on all ports.
-        """
-        ports = ports if ports else list(self.ports.values())
-        packets = {}
-        for port in ports:
-            port_output = f"{output}-{port.id}.txt" if output else None
-            packets[port] = port.stop_capture(limit=int(limit / len(ports)), output=port_output)
-        return packets
-
-    #
-    # Properties
-    #
-
-    @property
-    def ports(self) -> Dict[int, TrexPort]:
-        """Return dictionary {index: TrexPort} of all ports."""
-        return {p.id: p for p in self.get_objects_by_type("port")}
-
-    #
-    # Private
-    #
-
-    def _get_api_h(self):
-        return self.api.get_api_h()
+"""
+Classes and utilities that represents TRex GUI application.
+"""
+import logging
+import random
+import time
+from typing import Dict, List, Optional
+
+from trafficgenerator import ApiType, TgnApp
+
+from pytrex.api.trex_event import EventsHandler
+from pytrex.api.trex_stl_conn import Connection
+from pytrex.trex_object import TrexObject
+from pytrex.trex_port import TrexCaptureMode, TrexPort
+from pytrex.trex_statistics_view import TrexStreamStatistics
+
+logger = logging.getLogger("tgn.trex")
+
+
+class TrexApp(TgnApp):
+    """TrexApp object, equivalent to TRex application."""
+
+    def __init__(self, username: str, ip: str, port: int = 4501, async_port: int = 4500, virtual: bool = False) -> None:
+        """Start TRex application.
+
+        :param username: User name.
+        :param ip: TRex server IP.
+        :param port: TRex server RPC port.
+        :param async_port: Async port
+        :param virtual: ???
+        """
+        super().__init__(logger, ApiType.socket)
+        self.server = TrexServer(username, ip, port, async_port, virtual)
+
+
+class TrexServer(TrexObject):
+    """Represents single TRex server."""
+
+    def __init__(self, username: str, ip: str, port: int = 4501, async_port: int = 4500, virtual: bool = False) -> None:
+        """Start TRex application.
+
+        :param ip: TRex server IP.
+        :param port: TRex server RPC port.
+        :param async_port: Async port
+        :param username: User name.
+        :param virtual: ???
+        """
+        self.logger = logger
+        self.username = username
+        self.ip = ip
+        self.port = port
+        self.async_port = async_port
+        self.virtual = virtual
+        self.server = self
+        self.event_handler: EventsHandler = None
+        super().__init__(parent=None, objType="server", objRef="server")
+
+    def connect(self) -> None:
+        """Connect to the TRex server."""
+        self.event_handler = EventsHandler(self)
+        connection_info = {
+            "username": self.username,
+            "server": self.ip,
+            "sync_port": self.port,
+            "async_port": self.async_port,
+            "virtual": self.virtual,
+        }
+        self.api = Connection(connection_info, self.logger, self)
+        self.api.connect()
+        self.session_id = random.getrandbits(32)
+
+    def disconnect(self) -> None:
+        """Release all ports and disconnect from server."""
+        for port in self.ports.values():
+            port.release()
+        self.api.disconnect()
+
+    def reserve_ports(self, locations: list[int], force: bool = False, reset: bool = False) -> dict[int, TrexPort]:
+        """Reserve ports.
+
+        TRex -> Port -> Acquire.
+
+        :param locations: list of ports locations in the form <port number> to reserve
+        :param force: True - take forcefully. False - fail if port is reserved by other user
+        :param reset: True - reset port, False - leave port configuration
+        :return: ports dictionary (location: object)
+        """
+        for location in locations:
+            TrexPort(server=self, index=location).reserve(force, reset)
+        return self.ports
+
+    #
+    # Configuration
+    #
+
+    def get_system_info(self) -> dict:
+        """Get system information from server."""
+        return self.transmit("get_system_info", {})["result"]
+
+    def get_supported_cmds(self) -> dict:
+        """Get supported commands from server."""
+        return self.transmit("get_supported_cmds", {})["result"]
+
+    #
+    # Control
+    #
+
+    def clear_stats(self, *ports: TrexPort) -> None:
+        """Clear statistics on list of ports.
+
+        :param ports: list of ports to start traffic on, if empty, clear all ports.
+        """
+        for port in ports or self.ports.values():
+            port.clear_stats()
+        TrexStreamStatistics.clear_stats(self)
+
+    def start_transmit(self, blocking: bool = False, *ports: TrexPort) -> None:
+        """Start traffic on list of ports.
+
+        If possible, synchronize start of traffic, else, start.
+
+        :param blocking: if blocking - wait for transmit end, else - return after transmit starts.
+        :param ports: list of ports to start transmit on, if empty, start on all ports.
+        """
+        ports = ports or self.ports.values()
+
+        synchronized = True
+        if not len(ports) % 2:
+            for port in ports:
+                if list(self.ports.values()).index(port) ^ 1 not in list(self.ports.keys()):
+                    synchronized = False
+        else:
+            synchronized = False
+
+        if synchronized:
+            save_level = self.logger.level
+            start_time = time.time()
+            rc = self.api.rpc.transmit("ping", api_class=None)
+            start_at_ts = rc["result"]["ts"] + max((time.time() - start_time), 0.5) * len(ports)
+            self.logger.level = save_level
+        else:
+            start_at_ts = 0
+
+        for port in ports:
+            port.start_at_ts = start_at_ts
+            port.start_transmit(blocking=False)
+
+        if blocking:
+            self.wait_transmit(*ports)
+
+    def stop_transmit(self, *ports: TrexPort) -> None:
+        """Stop traffic on list of ports.
+
+        :param ports: list of ports to stop transmit on, if empty, stop on all ports.
+        """
+        for port in ports or self.ports.values():
+            port.stop_transmit()
+
+    def wait_transmit(self, *ports: TrexPort) -> None:
+        """Wait for transmit end on list of ports.
+
+        :param ports: list of ports to wait for, if empty, wait for all ports.
+        """
+        for port in ports or self.ports.values():
+            port.wait_transmit()
+        time.sleep(4)
+
+    def clear_capture(self, *ports: TrexPort) -> None:
+        """Clear all existing capture IDs on list ports.
+
+        :param ports: list of ports to clear capture on, if empty, clear on all ports.
+        """
+        for port in ports or self.ports.values():
+            port.clear_capture(rx=True, tx=True)
+
+    def start_capture(
+        self, limit: int = 1000, mode: TrexCaptureMode = TrexCaptureMode.FIXED, bpf_filter: str = "", *ports: TrexPort
+    ) -> None:
+        """Start RX capture on list of ports.
+
+        :param limit: limit the total number of captrured packets (for all ports) memory requierment is O(9K * limit).
+        :param mode: when full, if fixed drop new packets, else (cyclic) drop old packets.
+        :param bpf_filter:  A Berkeley Packet Filter pattern. Only packets matching the filter will be captured.
+        :param ports: list of ports to start capture on, if empty, start on all ports.
+        """
+        ports = ports or self.ports.values()
+        for port in ports:
+            port.start_capture(
+                rx=True,
+                tx=False,
+                limit=int(limit / len(ports)),
+                mode=mode,
+                bpf_filter=bpf_filter,
+            )
+
+    def stop_capture(self, limit: int = 1000, output: Optional[str] = None, *ports) -> Dict[TrexPort, List]:
+        """Stop capture on list of ports.
+
+        :param limit: limit the total number of packets that will be read from the capture buffer of all ports.
+        :param output: prefix for the capture file name.
+            Capture files for each port will be stored in individual output file named 'prefix-{port ID}.txt'.
+        :param ports: list of ports to stop capture on, if empty, stop on all ports.
+        """
+        ports = ports or self.ports.values()
+        packets = {}
+        for port in ports:
+            port_output = f"{output}-{port.id}.txt" if output else None
+            packets[port] = port.stop_capture(limit=int(limit / len(ports)), output=port_output)
+        return packets
+
+    #
+    # Properties
+    #
+
+    @property
+    def ports(self) -> dict[int, TrexPort]:
+        """Return dictionary {index: TrexPort} of all ports."""
+        return {p.id: p for p in self.get_objects_by_type("port")}
+
+    #
+    # Private
+    #
+
+    def _get_api_h(self):
+        return self.api.get_api_h()
```

## pytrex/trex_capture.py

 * *Ordering differences only*

```diff
@@ -1,98 +1,98 @@
-import base64
-import binascii
-from enum import Enum
-from typing import List, Optional
-
-from scapy.layers.l2 import Ether
-
-from .trex_object import TrexObject
-
-
-class TrexCaptureMode(Enum):
-    FIXED = 0
-    CYCLIC = 1
-
-
-class TrexCapture(TrexObject):
-    """Per port capture operations."""
-
-    def __init__(self, parent) -> None:
-        super().__init__(parent=parent, objType="capture")
-        self.packets: List[dict] = []
-
-    def start(
-        self,
-        rx: Optional[bool] = True,
-        tx: Optional[bool] = False,
-        limit: Optional[int] = 1000,
-        mode: Optional[TrexCaptureMode] = TrexCaptureMode.FIXED,
-        bpf_filter: Optional[str] = "",
-    ) -> None:
-        """Start capture on list of ports.
-
-        :param rx: if rx, capture RX packets, else, do not capture
-        :param tx: if tx, capture TX packets, else, do not capture
-        :param limit: limit the total number of captured packets (RX and TX) memory requirement is O(9K * limit).
-        :param mode: when full, if fixed drop new packets, else (cyclic) drop old packets.
-        :param bpf_filter:  A Berkeley Packet Filter pattern. Only packets matching the filter will be captured.
-        """
-        params = {
-            "command": "start",
-            "limit": limit,
-            "mode": mode.name.lower(),
-            "rx": [self.parent.id] if rx else [],
-            "tx": [self.parent.id] if tx else [],
-            "filter": bpf_filter,
-        }
-        rc = self.transmit("capture", params=params)
-        self._data["index"] = rc["result"]["capture_id"]
-
-    def stop(self, limit: int = 1000, output: Optional[str] = None) -> List[dict]:
-        """Stop capture.
-
-        :param limit: limit the number of packets that will be read from the capture buffer.
-        :param output: full path to file where capture packets will be stored, if None - do not store packets in file.
-        """
-        params = {"command": "stop", "capture_id": self.id}
-        rc = self.transmit("capture", params=params)
-        pkt_count = rc["result"]["pkt_count"]
-        packets = self.fetch_capture_packets(min(limit, pkt_count), output)
-
-        params = {"command": "remove", "capture_id": self.id}
-        self.transmit("capture", params=params)
-
-        return packets
-
-    def fetch_capture_packets(self, limit: int = 1000, output: Optional[str] = None) -> List[dict]:
-        """Fetch packets from existing active capture.
-
-        :param limit: limit the number of packets that will be read from the capture buffer.
-        :param output: full path to file where capture packets will be stored, if None - do not store packets in file.
-        """
-        self.packets = []
-        pending = limit
-        while pending > 0:
-            params = {"command": "fetch", "capture_id": self.id, "pkt_limit": min(50, pending)}
-            rc = self.transmit("capture", params=params)
-
-            pkts = rc["result"]["pkts"]
-            pending = rc["result"]["pending"]
-            start_ts = rc["result"]["start_ts"]
-
-            # write packets
-            for pkt in pkts:
-                pkt["rel_ts"] = pkt["ts"] - start_ts
-                pkt["binary"] = base64.b64decode(pkt["binary"])
-                pkt["hex"] = binascii.hexlify(pkt["binary"])
-                pkt["scapy"] = Ether(pkt["binary"])
-                self.packets.append(pkt)
-
-        if output:
-            with open(output, "w+") as capture_file:
-                for packet in self.packets:
-                    str_packet = str(packet["hex"])[2:-1]
-                    capture_file.write("000000 ")
-                    capture_file.write(" ".join(a + b for a, b in zip(str_packet[::2], str_packet[1::2])))
-                    capture_file.write("\n")
-
-        return self.packets
+import base64
+import binascii
+from enum import Enum
+from typing import List, Optional
+
+from scapy.layers.l2 import Ether
+
+from .trex_object import TrexObject
+
+
+class TrexCaptureMode(Enum):
+    FIXED = 0
+    CYCLIC = 1
+
+
+class TrexCapture(TrexObject):
+    """Per port capture operations."""
+
+    def __init__(self, parent) -> None:
+        super().__init__(parent=parent, objType="capture")
+        self.packets: List[dict] = []
+
+    def start(
+        self,
+        rx: Optional[bool] = True,
+        tx: Optional[bool] = False,
+        limit: Optional[int] = 1000,
+        mode: Optional[TrexCaptureMode] = TrexCaptureMode.FIXED,
+        bpf_filter: Optional[str] = "",
+    ) -> None:
+        """Start capture on list of ports.
+
+        :param rx: if rx, capture RX packets, else, do not capture
+        :param tx: if tx, capture TX packets, else, do not capture
+        :param limit: limit the total number of captured packets (RX and TX) memory requirement is O(9K * limit).
+        :param mode: when full, if fixed drop new packets, else (cyclic) drop old packets.
+        :param bpf_filter:  A Berkeley Packet Filter pattern. Only packets matching the filter will be captured.
+        """
+        params = {
+            "command": "start",
+            "limit": limit,
+            "mode": mode.name.lower(),
+            "rx": [self.parent.id] if rx else [],
+            "tx": [self.parent.id] if tx else [],
+            "filter": bpf_filter,
+        }
+        rc = self.transmit("capture", params=params)
+        self._data["index"] = rc["result"]["capture_id"]
+
+    def stop(self, limit: int = 1000, output: Optional[str] = None) -> List[dict]:
+        """Stop capture.
+
+        :param limit: limit the number of packets that will be read from the capture buffer.
+        :param output: full path to file where capture packets will be stored, if None - do not store packets in file.
+        """
+        params = {"command": "stop", "capture_id": self.id}
+        rc = self.transmit("capture", params=params)
+        pkt_count = rc["result"]["pkt_count"]
+        packets = self.fetch_capture_packets(min(limit, pkt_count), output)
+
+        params = {"command": "remove", "capture_id": self.id}
+        self.transmit("capture", params=params)
+
+        return packets
+
+    def fetch_capture_packets(self, limit: int = 1000, output: Optional[str] = None) -> List[dict]:
+        """Fetch packets from existing active capture.
+
+        :param limit: limit the number of packets that will be read from the capture buffer.
+        :param output: full path to file where capture packets will be stored, if None - do not store packets in file.
+        """
+        self.packets = []
+        pending = limit
+        while pending > 0:
+            params = {"command": "fetch", "capture_id": self.id, "pkt_limit": min(50, pending)}
+            rc = self.transmit("capture", params=params)
+
+            pkts = rc["result"]["pkts"]
+            pending = rc["result"]["pending"]
+            start_ts = rc["result"]["start_ts"]
+
+            # write packets
+            for pkt in pkts:
+                pkt["rel_ts"] = pkt["ts"] - start_ts
+                pkt["binary"] = base64.b64decode(pkt["binary"])
+                pkt["hex"] = binascii.hexlify(pkt["binary"])
+                pkt["scapy"] = Ether(pkt["binary"])
+                self.packets.append(pkt)
+
+        if output:
+            with open(output, "w+") as capture_file:
+                for packet in self.packets:
+                    str_packet = str(packet["hex"])[2:-1]
+                    capture_file.write("000000 ")
+                    capture_file.write(" ".join(a + b for a, b in zip(str_packet[::2], str_packet[1::2])))
+                    capture_file.write("\n")
+
+        return self.packets
```

## pytrex/trex_object.py

```diff
@@ -1,56 +1,53 @@
-"""
-Base classes and utilities for all TRex objects.
-"""
-from typing import Dict, List, Type, Union
-
-from trafficgenerator.tgn_object import TgnObject
-
-
-class TrexObject(TgnObject):
-    """Base class for all Trex objects."""
-
-    def __init__(self, parent: Union["TrexObject", None], **data):
-        """Create TRex object."""
-        if parent:
-            self.username = parent.username
-            self.session_id = parent.session_id
-            self.server = parent.server
-            if parent.index:
-                data["objRef"] = f'{data["objType"]}/{parent.index}'
-                if "index" in data:
-                    data["objRef"] += f'{data["index"]}'
-            else:
-                data["objRef"] = f'{data["objType"]}/{data["index"]}'
-        super().__init__(parent, **data)
-
-    def transmit(self, method_name: str, params: dict = None) -> dict:
-        """Transmit object command.
-
-        :param method_name: RPC command.
-        :param params: command parameters.
-        """
-        return self.api.rpc.transmit(method_name, params, "core")
-
-    def transmit_batch(self, batch_list):
-        return self.api.rpc.transmit_batch(batch_list)
-
-    def get_name(self) -> str:
-        pass
-
-    def get_attributes(self) -> Dict[str, str]:
-        pass
-
-    def get_attribute(self, attribute: str) -> str:
-        pass
-
-    def get_children(self, *types: str) -> List[TgnObject]:
-        pass
-
-    def get_objects_from_attribute(self, attribute: str) -> List[TgnObject]:
-        pass
-
-    def get_obj_class(self, obj_type: str) -> Type[TgnObject]:
-        pass
-
-    def _create(self, **attributes: Dict[str, object]) -> str:
-        pass
+"""
+Base classes and utilities for all TRex objects.
+"""
+from typing import Dict, List, Type, Union
+
+from trafficgenerator.tgn_object import TgnObject
+
+
+class TrexObject(TgnObject):
+    """Base class for all Trex objects."""
+
+    def __init__(self, parent: Union["TrexObject", None], **data: str):
+        """Create TRex object."""
+        if parent:
+            self.username = parent.username
+            self.session_id = parent.session_id
+            self.server = parent.server
+            data["objRef"] = f"{parent.ref}/{data['objType']}"
+            if "index" in data:
+                data["objRef"] += data["index"]
+        super().__init__(parent, **data)
+
+    def transmit(self, method_name: str, params: dict = None) -> dict:
+        """Transmit object command.
+
+        :param method_name: RPC command.
+        :param params: command parameters.
+        """
+        return self.api.rpc.transmit(method_name, params, "core")
+
+    def transmit_batch(self, batch_list):
+        return self.api.rpc.transmit_batch(batch_list)
+
+    def get_name(self) -> str:
+        pass
+
+    def get_attributes(self) -> Dict[str, str]:
+        pass
+
+    def get_attribute(self, attribute: str) -> str:
+        pass
+
+    def get_children(self, *types: str) -> List[TgnObject]:
+        pass
+
+    def get_objects_from_attribute(self, attribute: str) -> List[TgnObject]:
+        pass
+
+    def get_obj_class(self, obj_type: str) -> Type[TgnObject]:
+        pass
+
+    def _create(self, **attributes: Dict[str, object]) -> str:
+        pass
```

## pytrex/trex_port.py

```diff
@@ -1,358 +1,358 @@
-"""
-Classes and utilities that represents TRex port.
-"""
-import re
-import time
-from copy import deepcopy
-from enum import Enum
-from typing import Dict, List, Optional
-
-from trafficgenerator import TgnError
-
-from pytrex.api import RpcCmdData
-from pytrex.trex_capture import TrexCapture, TrexCaptureMode
-from pytrex.trex_object import TrexObject
-from pytrex.trex_stream import TrexStream, TrexYamlLoader
-
-MASK_ALL = (1 << 64) - 1
-
-
-def decode_multiplier(val, allow_update=False, divide_count=1):
-
-    factor_table = {None: 1, "k": 1e3, "m": 1e6, "g": 1e9}
-    pattern = r"^(\d+(\.\d+)?)(((k|m|g)?(bpsl1|pps|bps))|%)?"
-
-    # do we allow updates ?  +/-
-    if not allow_update:
-        pattern += "$"
-        match = re.match(pattern, val)
-        op = None
-    else:
-        pattern += r"([\+\-])?$"
-        match = re.match(pattern, val)
-        if match:
-            op = match.group(7)
-        else:
-            op = None
-
-    result = {}
-
-    if not match:
-        return None
-
-    # value in group 1
-    value = float(match.group(1))
-
-    # decode unit as whole
-    unit = match.group(3)
-
-    # k,m,g
-    factor = match.group(5)
-
-    # type of multiplier
-    m_type = match.group(6)
-
-    # raw type(factor)
-    if not unit:
-        result["type"] = "raw"
-        result["value"] = value
-
-    # percentage
-    elif unit == "%":
-        result["type"] = "percentage"
-        result["value"] = value
-
-    elif m_type == "bps":
-        result["type"] = "bps"
-        result["value"] = value * factor_table[factor]
-
-    elif m_type == "pps":
-        result["type"] = "pps"
-        result["value"] = value * factor_table[factor]
-
-    elif m_type == "bpsl1":
-        result["type"] = "bpsl1"
-        result["value"] = value * factor_table[factor]
-
-    if op == "+":
-        result["op"] = "add"
-    elif op == "-":
-        result["op"] = "sub"
-    else:
-        result["op"] = "abs"
-
-    if result["op"] != "percentage":
-        result["value"] = result["value"] / divide_count
-
-    return result
-
-
-class PortState(Enum):
-    """Port states. Enum names are the port states used in RPC."""
-
-    DOWN = 0
-    IDLE = 1
-    STREAMS = 2
-    TX = 3
-    PAUSE = 4
-    PCAP_TX = 5
-
-
-class TrexPort(TrexObject):
-    """Represents TRex port."""
-
-    def __init__(self, parent, index):
-        """Create port object.
-
-        :param parent: parent chassis.
-        :param index: port index, zero based
-        """
-        super().__init__(parent=parent, objType="port", index=index)
-        self.mul = decode_multiplier("1", allow_update=False, divide_count=1)
-        self.duration = -1
-        self.force = False
-        self.mask = MASK_ALL
-        self.start_at_ts = 0.0
-        self.stat_names: dict = None
-        self.base_xstats: dict = None
-        self.base_stats: dict = None
-        self.statistics: dict = None
-        self.xstatistics: dict = None
-
-    def reserve(self, force: Optional[bool] = False, reset: Optional[bool] = False) -> None:
-        """Reserve port.
-
-        TRex -> Port -> [Force] Acquire.
-        Acquire returns port handler that must be used in subsequent port method calls.
-
-        :param force: True - take forcefully, False - fail if port is reserved by other user
-        :param reset: True - reset port, False - leave port configuration
-        """
-        params = {"port_id": int(self.index), "user": self.username, "session_id": self.session_id, "force": force}
-        self._data["objRef"] = self.api.rpc.transmit("acquire", params)["result"]
-        if reset:
-            self.reset()
-
-    def release(self):
-        """Release port.
-
-        TRex -> Port -> Release Acquire.
-        """
-        self.transmit("release")
-
-    def reset(self) -> None:
-        self.stop_transmit()
-        self.set_promiscuous_mode(enabled=True)
-        self.remove_all_streams()
-
-    #
-    # Configuration.
-    #
-
-    def get_status(self):
-        params = {"session_id": self.session_id}
-        rc = self.api.rpc.transmit("get_port_status", params)
-        return rc["result"]
-
-    def set_service_mode(self, enabled):
-        params = {"session_id": self.session_id, "enabled": enabled}
-        self.transmit("service", params)
-
-    def set_promiscuous_mode(self, enabled):
-        params = {"session_id": self.session_id, "attr": {"promiscuous": {"enabled": enabled}}}
-        self.transmit("set_port_attr", params)
-
-    #
-    # Streams.
-    #
-
-    def remove_all_streams(self) -> None:
-        self.del_objects_by_type("stream")
-        self.transmit("remove_all_streams")
-
-    def add_stream(self, name: str) -> TrexStream:
-        """Add stream with default configuration.
-
-        :param name: unique stream name
-        """
-        return TrexStream(self, index=len(self.streams), name=name)
-
-    def load_streams(self, yaml_file) -> None:
-        """Load streams from YAML file.
-
-        :param yaml_file: full path to yaml profile file.
-        """
-        yaml_loader = TrexYamlLoader(self, yaml_file)
-        yaml_loader.parse()
-
-    def save_streams(self, yaml_file):
-        """Save streams to YAML file.
-
-        :param yaml_file: full path to yaml profile file.
-        """
-        raise NotImplementedError()
-
-    def write_streams(self) -> None:
-        """Write all streams to server."""
-        self.transmit("remove_all_streams")
-        batch = []
-        for name, stream in self.streams.items():
-            stream_fields = deepcopy(stream.fields)
-            stream_id = list(self.streams.keys()).index(name) + 1
-            next_stream = stream_fields.pop("next_stream")
-            stream_fields["next_stream_id"] = list(self.streams.keys()).index(next_stream) + 1 if next_stream else -1
-
-            params = {"handler": self.ref, "port_id": self.id, "stream_id": stream_id, "stream": stream_fields}
-            cmd = RpcCmdData("add_stream", params, "core")
-            batch.append(cmd)
-
-        self.api.rpc.transmit_batch(batch)
-
-    #
-    # Control.
-    #
-
-    def get_port_state(self) -> PortState:
-        """Get port state from server."""
-        rc = self.transmit("get_port_status")
-        return PortState[rc["result"]["state"].upper()]
-
-    def is_transmitting(self) -> bool:
-        """Return True if port is transmitting, else return False."""
-        return self.get_port_state() in [PortState.TX, PortState.PCAP_TX]
-
-    def start_transmit(self, blocking: Optional[bool] = False) -> None:
-        """Start transmit.
-
-        :param blocking: if blockeing - wait for transmit end, else - return after transmit starts.
-        :return:
-        """
-        if self.get_port_state() == PortState.IDLE:
-            raise TgnError("unable to start traffic - no streams attached to port")
-
-        params = {
-            "mul": self.mul,
-            "duration": self.duration,
-            "force": self.force,
-            "core_mask": self.mask,
-            "start_at_ts": self.start_at_ts,
-        }
-        self.transmit("start_traffic", params)
-
-        if blocking:
-            self.wait_transmit()
-
-    def stop_transmit(self) -> None:
-        """Stop transmit."""
-        self.transmit("stop_traffic")
-        self.wait_transmit()
-
-    def wait_transmit(self) -> None:
-        """Wait until port finishes transmition."""
-        while self.is_transmitting():
-            time.sleep(1)
-
-    #
-    # Statistics.
-    #
-
-    def clear_stats(self) -> None:
-        """Get base counters values so read stats can subtract them from current counters values."""
-        values = self.transmit("get_port_xstats_values")["result"]
-        self.stat_names = self.transmit("get_port_xstats_names")["result"]
-        self.base_xstats = dict(zip(self.stat_names["xstats_names"], values["xstats_values"]))
-        self.base_stats = self.transmit("get_port_stats")["result"]
-        self.statistics = self.base_stats
-        self.xstatistics = self.base_xstats
-
-    def read_stats(self) -> dict:
-        """Read current counters values and adjust them based on base counters read before the test."""
-        self.statistics = self.transmit("get_port_stats")["result"]
-        for stat, value in self.statistics.items():
-            if not stat.endswith("ps"):
-                value -= self.base_stats[stat]
-            self.statistics[stat] = value
-        return self.statistics
-
-    def read_xstats(self) -> dict:
-        """Read current extended counters values and adjust them based on base counters read before the test."""
-        values = self.transmit("get_port_xstats_values")["result"]
-        self.xstatistics = dict(zip(self.stat_names["xstats_names"], values["xstats_values"]))
-        for stat, value in self.xstatistics.items():
-            self.statistics[stat] = value - self.base_xstats[stat]
-        return self.xstatistics
-
-    #
-    # Capture
-    #
-
-    def clear_capture(self, rx: Optional[bool] = True, tx: Optional[bool] = False) -> None:
-        """Clear existing capture IDs on the port.
-
-        :param rx: if rx, clear RX captures, else, do not clear
-        :param tx: if tx, clear TX captures, else, do not clear
-        """
-        rc = self.transmit("capture", {"command": "status"})
-        for capture in rc["result"]:
-            if rx and int(capture["filter"]["rx"]) - 1 == self.id or tx and int(capture["filter"]["tx"]) - 1 == self.id:
-                params = {"command": "remove", "capture_id": capture["id"]}
-                self.transmit("capture", params=params)
-
-    def start_capture(
-        self,
-        rx: Optional[bool] = True,
-        tx: Optional[bool] = False,
-        limit: Optional[int] = 1000,
-        mode: Optional[TrexCaptureMode] = TrexCaptureMode.FIXED,
-        bpf_filter: Optional[str] = "",
-    ) -> None:
-        """Start capture.
-
-        :param rx: if rx, capture RX packets, else, do not capture
-        :param tx: if tx, capture TX packets, else, do not capture
-        :param limit: limit the total number of captured packets (RX and TX) memory requierment is O(9K * limit).
-        :param mode: when full, if fixed drop new packets, else (cyclic) drop old packets.
-        :param bpf_filter:  A Berkeley Packet Filter pattern. Only packets matching the filter will be captured.
-        """
-        self.clear_capture(rx=rx, tx=tx)
-        self.set_service_mode(enabled=True)
-        self.capture.start(rx, tx, limit, mode, bpf_filter)
-
-    def stop_capture(self, limit: Optional[int] = 1000, output: Optional[str] = None) -> List[Dict]:
-        """Stop capture.
-
-        :param limit: limit the number of packets that will be read from the capture buffer.
-        :param output: full path to file where capture packets will be stored, if None - do not store packets in file.
-            You can run text2pcap on the resulted file and then open it with wireshark.
-        """
-        return self.capture.stop(limit, output)
-
-    #
-    # Low level.
-    #
-
-    def transmit(self, method_name: str, params: Optional[Dict] = None) -> dict:
-        """Transmit port RPC command.
-
-        :param method_name: RPC command
-        :param params: command parameters
-        """
-        params = params if params else {}
-        params["port_id"] = self.id
-        params["handler"] = self.ref
-        return super().transmit(method_name, params)
-
-    #
-    # Properties.
-    #
-
-    @property
-    def streams(self) -> Dict[str, "TrexPort"]:
-        return {s.name: s for s in self.get_objects_by_type("stream")}
-
-    @property
-    def capture(self) -> TrexCapture:
-        """Return TrexCapture object for the port."""
-        if not self.get_object_by_type("capture"):
-            TrexCapture(self)
-        return self.get_object_by_type("capture")
+"""
+Classes and utilities that represents TRex port.
+"""
+import re
+import time
+from copy import deepcopy
+from enum import Enum
+from pathlib import Path
+from typing import Dict, List, Optional
+
+from trafficgenerator import TgnError
+
+from pytrex.api import RpcCmdData
+from pytrex.trex_capture import TrexCapture, TrexCaptureMode
+from pytrex.trex_object import TrexObject
+from pytrex.trex_stream import TrexStream, TrexYamlLoader
+
+MASK_ALL = (1 << 64) - 1
+
+
+def decode_multiplier(val, allow_update=False, divide_count=1):
+    factor_table = {None: 1, "k": 1e3, "m": 1e6, "g": 1e9}
+    pattern = r"^(\d+(\.\d+)?)(((k|m|g)?(bpsl1|pps|bps))|%)?"
+
+    # do we allow updates ?  +/-
+    if not allow_update:
+        pattern += "$"
+        match = re.match(pattern, val)
+        op = None
+    else:
+        pattern += r"([\+\-])?$"
+        match = re.match(pattern, val)
+        if match:
+            op = match.group(7)
+        else:
+            op = None
+
+    result = {}
+
+    if not match:
+        return None
+
+    # value in group 1
+    value = float(match.group(1))
+
+    # decode unit as whole
+    unit = match.group(3)
+
+    # k,m,g
+    factor = match.group(5)
+
+    # type of multiplier
+    m_type = match.group(6)
+
+    # raw type(factor)
+    if not unit:
+        result["type"] = "raw"
+        result["value"] = value
+
+    # percentage
+    elif unit == "%":
+        result["type"] = "percentage"
+        result["value"] = value
+
+    elif m_type == "bps":
+        result["type"] = "bps"
+        result["value"] = value * factor_table[factor]
+
+    elif m_type == "pps":
+        result["type"] = "pps"
+        result["value"] = value * factor_table[factor]
+
+    elif m_type == "bpsl1":
+        result["type"] = "bpsl1"
+        result["value"] = value * factor_table[factor]
+
+    if op == "+":
+        result["op"] = "add"
+    elif op == "-":
+        result["op"] = "sub"
+    else:
+        result["op"] = "abs"
+
+    if result["op"] != "percentage":
+        result["value"] = result["value"] / divide_count
+
+    return result
+
+
+class PortState(Enum):
+    """Port states. Enum names are the port states used in RPC."""
+
+    DOWN = 0
+    IDLE = 1
+    STREAMS = 2
+    TX = 3
+    PAUSE = 4
+    PCAP_TX = 5
+
+
+class TrexPort(TrexObject):
+    """Represents TRex port."""
+
+    def __init__(self, server, index: int) -> None:
+        """Create port object.
+
+        :param server: Parent server.
+        :param index: Port index, zero based
+        """
+        super().__init__(parent=server, objType="port", index=str(index))
+        self.mul = decode_multiplier("1", allow_update=False, divide_count=1)
+        self.duration = -1
+        self.force = False
+        self.mask = MASK_ALL
+        self.start_at_ts = 0.0
+        self.stat_names: dict = None
+        self.base_xstats: dict = None
+        self.base_stats: dict = None
+        self.statistics: dict = None
+        self.xstatistics: dict = None
+
+    def reserve(self, force: Optional[bool] = False, reset: Optional[bool] = False) -> None:
+        """Reserve port.
+
+        TRex -> Port -> [Force] Acquire.
+        Acquire returns port handler that must be used in subsequent port method calls.
+
+        :param force: True - take forcefully, False - fail if port is reserved by other user
+        :param reset: True - reset port, False - leave port configuration
+        """
+        params = {"port_id": int(self.index), "user": self.username, "session_id": self.session_id, "force": force}
+        self._data["objRef"] = self.api.rpc.transmit("acquire", params)["result"]
+        if reset:
+            self.reset()
+
+    def release(self):
+        """Release port.
+
+        TRex -> Port -> Release Acquire.
+        """
+        self.transmit("release")
+
+    def reset(self) -> None:
+        self.stop_transmit()
+        self.set_promiscuous_mode(enabled=True)
+        self.remove_all_streams()
+
+    #
+    # Configuration.
+    #
+
+    def get_status(self):
+        params = {"session_id": self.session_id}
+        rc = self.api.rpc.transmit("get_port_status", params)
+        return rc["result"]
+
+    def set_service_mode(self, enabled):
+        params = {"session_id": self.session_id, "enabled": enabled}
+        self.transmit("service", params)
+
+    def set_promiscuous_mode(self, enabled):
+        params = {"session_id": self.session_id, "attr": {"promiscuous": {"enabled": enabled}}}
+        self.transmit("set_port_attr", params)
+
+    #
+    # Streams.
+    #
+
+    def remove_all_streams(self) -> None:
+        self.del_objects_by_type("stream")
+        self.transmit("remove_all_streams")
+
+    def add_stream(self, name: str) -> TrexStream:
+        """Add stream with default configuration.
+
+        :param name: unique stream name
+        """
+        return TrexStream(self, index=len(self.streams), name=name)
+
+    def load_streams(self, yaml_file: Path) -> None:
+        """Load streams from YAML file.
+
+        :param yaml_file: full path to yaml profile file.
+        """
+        yaml_loader = TrexYamlLoader(self, yaml_file)
+        yaml_loader.parse()
+
+    def save_streams(self, yaml_file):
+        """Save streams to YAML file.
+
+        :param yaml_file: full path to yaml profile file.
+        """
+        raise NotImplementedError()
+
+    def write_streams(self) -> None:
+        """Write all streams to server."""
+        self.transmit("remove_all_streams")
+        batch = []
+        for name, stream in self.streams.items():
+            stream_fields = deepcopy(stream.fields)
+            stream_id = list(self.streams.keys()).index(name) + 1
+            next_stream = stream_fields.pop("next_stream")
+            stream_fields["next_stream_id"] = list(self.streams.keys()).index(next_stream) + 1 if next_stream else -1
+
+            params = {"handler": self.ref, "port_id": self.id, "stream_id": stream_id, "stream": stream_fields}
+            cmd = RpcCmdData("add_stream", params, "core")
+            batch.append(cmd)
+
+        self.api.rpc.transmit_batch(batch)
+
+    #
+    # Control.
+    #
+
+    def get_port_state(self) -> PortState:
+        """Get port state from server."""
+        rc = self.transmit("get_port_status")
+        return PortState[rc["result"]["state"].upper()]
+
+    def is_transmitting(self) -> bool:
+        """Return True if port is transmitting, else return False."""
+        return self.get_port_state() in [PortState.TX, PortState.PCAP_TX]
+
+    def start_transmit(self, blocking: Optional[bool] = False) -> None:
+        """Start transmit.
+
+        :param blocking: if blockeing - wait for transmit end, else - return after transmit starts.
+        :return:
+        """
+        if self.get_port_state() == PortState.IDLE:
+            raise TgnError("unable to start traffic - no streams attached to port")
+
+        params = {
+            "mul": self.mul,
+            "duration": self.duration,
+            "force": self.force,
+            "core_mask": self.mask,
+            "start_at_ts": self.start_at_ts,
+        }
+        self.transmit("start_traffic", params)
+
+        if blocking:
+            self.wait_transmit()
+
+    def stop_transmit(self) -> None:
+        """Stop transmit."""
+        self.transmit("stop_traffic")
+        self.wait_transmit()
+
+    def wait_transmit(self) -> None:
+        """Wait until port finishes transmition."""
+        while self.is_transmitting():
+            time.sleep(1)
+
+    #
+    # Statistics.
+    #
+
+    def clear_stats(self) -> None:
+        """Get base counters values so read stats can subtract them from current counters values."""
+        values = self.transmit("get_port_xstats_values")["result"]
+        self.stat_names = self.transmit("get_port_xstats_names")["result"]
+        self.base_xstats = dict(zip(self.stat_names["xstats_names"], values["xstats_values"]))
+        self.base_stats = self.transmit("get_port_stats")["result"]
+        self.statistics = self.base_stats
+        self.xstatistics = self.base_xstats
+
+    def read_stats(self) -> dict:
+        """Read current counters values and adjust them based on base counters read before the test."""
+        self.statistics = self.transmit("get_port_stats")["result"]
+        for stat, value in self.statistics.items():
+            if not stat.endswith("ps"):
+                value -= self.base_stats[stat]
+            self.statistics[stat] = value
+        return self.statistics
+
+    def read_xstats(self) -> dict:
+        """Read current extended counters values and adjust them based on base counters read before the test."""
+        values = self.transmit("get_port_xstats_values")["result"]
+        self.xstatistics = dict(zip(self.stat_names["xstats_names"], values["xstats_values"]))
+        for stat, value in self.xstatistics.items():
+            self.statistics[stat] = value - self.base_xstats[stat]
+        return self.xstatistics
+
+    #
+    # Capture
+    #
+
+    def clear_capture(self, rx: Optional[bool] = True, tx: Optional[bool] = False) -> None:
+        """Clear existing capture IDs on the port.
+
+        :param rx: if rx, clear RX captures, else, do not clear
+        :param tx: if tx, clear TX captures, else, do not clear
+        """
+        rc = self.transmit("capture", {"command": "status"})
+        for capture in rc["result"]:
+            if rx and int(capture["filter"]["rx"]) - 1 == self.id or tx and int(capture["filter"]["tx"]) - 1 == self.id:
+                params = {"command": "remove", "capture_id": capture["id"]}
+                self.transmit("capture", params=params)
+
+    def start_capture(
+        self,
+        rx: Optional[bool] = True,
+        tx: Optional[bool] = False,
+        limit: Optional[int] = 1000,
+        mode: Optional[TrexCaptureMode] = TrexCaptureMode.FIXED,
+        bpf_filter: Optional[str] = "",
+    ) -> None:
+        """Start capture.
+
+        :param rx: if rx, capture RX packets, else, do not capture
+        :param tx: if tx, capture TX packets, else, do not capture
+        :param limit: limit the total number of captured packets (RX and TX) memory requierment is O(9K * limit).
+        :param mode: when full, if fixed drop new packets, else (cyclic) drop old packets.
+        :param bpf_filter:  A Berkeley Packet Filter pattern. Only packets matching the filter will be captured.
+        """
+        self.clear_capture(rx=rx, tx=tx)
+        self.set_service_mode(enabled=True)
+        self.capture.start(rx, tx, limit, mode, bpf_filter)
+
+    def stop_capture(self, limit: Optional[int] = 1000, output: Optional[str] = None) -> List[Dict]:
+        """Stop capture.
+
+        :param limit: limit the number of packets that will be read from the capture buffer.
+        :param output: full path to file where capture packets will be stored, if None - do not store packets in file.
+            You can run text2pcap on the resulted file and then open it with wireshark.
+        """
+        return self.capture.stop(limit, output)
+
+    #
+    # Low level.
+    #
+
+    def transmit(self, method_name: str, params: Optional[Dict] = None) -> dict:
+        """Transmit port RPC command.
+
+        :param method_name: RPC command
+        :param params: command parameters
+        """
+        params = params if params else {}
+        params["port_id"] = self.id
+        params["handler"] = self.ref
+        return super().transmit(method_name, params)
+
+    #
+    # Properties.
+    #
+
+    @property
+    def streams(self) -> dict[str, TrexStream]:
+        return {s.name: s for s in self.get_objects_by_type("stream")}
+
+    @property
+    def capture(self) -> TrexCapture:
+        """Return TrexCapture object for the port."""
+        if not self.get_object_by_type("capture"):
+            TrexCapture(self)
+        return self.get_object_by_type("capture")
```

## pytrex/trex_statistics_view.py

```diff
@@ -1,60 +1,60 @@
-from trafficgenerator.tgn_object import TgnSubStatsDict
-
-
-class TrexStatistics:
-    def __init__(self, server):
-        self.server = server
-        self.statistics = TgnSubStatsDict()
-
-
-class TrexPortStatistics(TrexStatistics):
-    def read(self):
-        self.statistics = TgnSubStatsDict()
-        for port in self.server.ports.values():
-            self.statistics[port] = port.read_stats()
-        return self.statistics
-
-
-class TrexStreamStatistics(TrexStatistics):
-    def __init__(self, server):
-        super().__init__(server)
-        rc = self.server.api.rpc.transmit("get_active_pgids")
-        self.ids = rc["result"]["ids"]["flow_stats"]
-        self.stream_id_to_stream = {}
-        for port in server.ports.values():
-            for stream in port.streams.values():
-                stream_id = stream.fields["flow_stats"]["stream_id"]
-                self.stream_id_to_stream[stream_id] = stream
-
-    @classmethod
-    def clear_stats(cls, server):
-        """Get base counters values so read stats can subtract them from current counters values."""
-        rc = server.api.rpc.transmit("get_active_pgids")
-        ids = rc["result"]["ids"]["flow_stats"]
-        cls.base_pgid_stats = server.api.rpc.transmit("get_pgid_stats", params={"pgids": ids})["result"]["flow_stats"]
-
-    def read(self):
-        """Read current counters values and adjust them based on base counters read before the test."""
-        rc = self.server.api.rpc.transmit("get_pgid_stats", params={"pgids": self.ids})
-        pgid_stats = rc["result"]["flow_stats"]
-        self.statistics = TgnSubStatsDict()
-        for pgid, stats in pgid_stats.items():
-            stream = self.stream_id_to_stream[int(pgid)]
-            self.statistics[stream] = {"tx": {}, "rx": {}}
-            tx_port = stream.parent
-            for name, values in stats.items():
-                if name.startswith("t"):
-                    value = values[str(tx_port.id)]
-                    if not name.endswith("s") and hasattr(self, "base_pgid_stats"):
-                        value -= self.base_pgid_stats[pgid][name][str(tx_port.id)]
-                    self.statistics[stream]["tx"][name] = value
-                else:
-                    for rx_port, value in values.items():
-                        if not name.endswith("s") and hasattr(self, "base_pgid_stats"):
-                            value -= self.base_pgid_stats[pgid][name][str(tx_port.id)]
-                        if value:
-                            rx_port = self.server.ports[int(rx_port)]
-                            if rx_port not in self.statistics[stream]["rx"]:
-                                self.statistics[stream]["rx"][rx_port] = {}
-                            self.statistics[stream]["rx"][rx_port][name] = value
-        return self.statistics
+from trafficgenerator.tgn_object import TgnSubStatsDict
+
+
+class TrexStatistics:
+    def __init__(self, server) -> None:
+        self.server = server
+        self.statistics = TgnSubStatsDict()
+
+
+class TrexPortStatistics(TrexStatistics):
+    def read(self):
+        self.statistics = TgnSubStatsDict()
+        for port in self.server.ports.values():
+            self.statistics[port] = port.read_stats()
+        return self.statistics
+
+
+class TrexStreamStatistics(TrexStatistics):
+    def __init__(self, server) -> None:
+        super().__init__(server)
+        rc = self.server.api.rpc.transmit("get_active_pgids")
+        self.ids = rc["result"]["ids"]["flow_stats"]
+        self.stream_id_to_stream = {}
+        for port in server.ports.values():
+            for stream in port.streams.values():
+                stream_id = stream.fields["flow_stats"]["stream_id"]
+                self.stream_id_to_stream[stream_id] = stream
+
+    @classmethod
+    def clear_stats(cls, server):
+        """Get base counters values so read stats can subtract them from current counters values."""
+        rc = server.api.rpc.transmit("get_active_pgids")
+        ids = rc["result"]["ids"]["flow_stats"]
+        cls.base_pgid_stats = server.api.rpc.transmit("get_pgid_stats", params={"pgids": ids})["result"]["flow_stats"]
+
+    def read(self):
+        """Read current counters values and adjust them based on base counters read before the test."""
+        rc = self.server.api.rpc.transmit("get_pgid_stats", params={"pgids": self.ids})
+        pgid_stats = rc["result"]["flow_stats"]
+        self.statistics = TgnSubStatsDict()
+        for pgid, stats in pgid_stats.items():
+            stream = self.stream_id_to_stream[int(pgid)]
+            self.statistics[stream] = {"tx": {}, "rx": {}}
+            tx_port = stream.parent
+            for name, values in stats.items():
+                if name.startswith("t"):
+                    value = values[str(tx_port.id)]
+                    if not name.endswith("s") and hasattr(self, "base_pgid_stats"):
+                        value -= self.base_pgid_stats[pgid][name][str(tx_port.id)]
+                    self.statistics[stream]["tx"][name] = value
+                else:
+                    for rx_port, value in values.items():
+                        if not name.endswith("s") and hasattr(self, "base_pgid_stats"):
+                            value -= self.base_pgid_stats[pgid][name][str(tx_port.id)]
+                        if value:
+                            rx_port = self.server.ports[int(rx_port)]
+                            if rx_port not in self.statistics[stream]["rx"]:
+                                self.statistics[stream]["rx"][rx_port] = {}
+                            self.statistics[stream]["rx"][rx_port][name] = value
+        return self.statistics
```

## pytrex/trex_stl_packet_builder_scapy.py

```diff
@@ -1,1960 +1,1954 @@
-import base64
-import copy
-import inspect
-import json
-import os
-import random
-import socket
-import struct
-
-import yaml
-from scapy.all import RawPcapReader, mac2str
-from scapy.layers.l2 import Ether
-from scapy.packet import NoPayload, Packet
-from scapy.utils import hexdump, wrpcap
-
-
-class CTRexPacketBuildException(Exception):
-    """
-    This is the general Packet Building error exception class.
-    """
-
-    def __init__(self, code, message):
-        self.code = code
-        self.message = message
-
-    def __str__(self):
-        return self.__repr__()
-
-    def __repr__(self):
-        return "[errcode:%r] %r" % (self.code, self.message)
-
-
-# ###############################################################################################
-
-
-def safe_ord(c):
-    if type(c) is str:
-        return ord(c)
-    elif type(c) is int:
-        return c
-    else:
-        raise TypeError("Cannot convert: {0} of type: {1}".format(c, type(c)))
-
-
-def _buffer_to_num(str_buffer):
-    res = 0
-    for i in str_buffer:
-        res = res << 8
-        res += safe_ord(i)
-    return res
-
-
-def ipv4_str_to_num(ipv4_buffer):
-    assert len(ipv4_buffer) == 4, "Size of ipv4_buffer is not 4"
-    return _buffer_to_num(ipv4_buffer)
-
-
-def mac_str_to_num(mac_buffer):
-    assert len(mac_buffer) == 6, "Size of mac_buffer is not 6"
-    return _buffer_to_num(mac_buffer)
-
-
-def int2mac(val):
-    mac_arr = []
-    for _ in range(6):
-        val, char = divmod(val, 256)
-        mac_arr.insert(0, "%02x" % char)
-    return ":".join(mac_arr)
-
-
-def int2ip(val):
-    ip_arr = []
-    for _ in range(4):
-        val, char = divmod(val, 256)
-        ip_arr.insert(0, "%s" % char)
-    return ".".join(ip_arr)
-
-
-def increase_mac(mac_str, val=1):
-    if ":" in mac_str:
-        mac_str = mac2str(mac_str)
-    mac_val = mac_str_to_num(mac_str)
-    return int2mac((mac_val + val) % (1 << 48))
-
-
-def increase_ip(ip_str, val=1):
-    ip_val = ipv4_str_to_num(is_valid_ipv4_ret(ip_str))
-    return int2ip((ip_val + val) % (1 << 32))
-
-
-# RFC 3513
-
-
-def generate_ipv6(mac_str, prefix="fe80"):
-    mac_arr = mac_str.split(":")
-    assert len(mac_arr) == 6, "mac should be in format of 11:22:33:44:55:66, got: %s" % mac_str
-    mac_arr[0] = "%x" % (int(mac_arr[0], 16) ^ 2)  # invert second bit
-    return "%s::%s%s:%sff:fe%s:%s%s" % tuple([prefix] + mac_arr[:3] + mac_arr[3:])
-
-
-# RFC 4291
-
-
-def generate_ipv6_solicited_node(mac_str):
-    mac_arr = mac_str.split(":")
-    assert len(mac_arr) == 6, "mac should be in format of 11:22:33:44:55:66, got: %s" % mac_str
-    return "ff02::1:ff%s:%s%s" % tuple(mac_arr[3:])
-
-
-# return full ipv6 ff02::1 -> ff02:0:0:0:0:0:0:1
-def expand_ipv6(addr):
-    addr_arr = addr.split(":")
-    if addr.startswith(":"):
-        addr_arr[0] = "0"
-    if addr.endswith(":"):
-        addr_arr[-1] = "0"
-    for i, e in enumerate(addr_arr):
-        if not e:
-            return ":".join(addr_arr[:i] + ["0"] * (9 - len(addr_arr)) + addr_arr[i + 1 :])
-    return ":".join(addr_arr)
-
-
-# return multicast mac based on ipv6 ff02::1 -> 33:33:00:00:00:01
-def multicast_mac_from_ipv6(addr):
-    addr = expand_ipv6(addr)
-    addr_arr = addr.split(":")
-    return "33:33:%02x:%02x:%02x:%02x" % (divmod(int(addr_arr[-2], 16), 256) + divmod(int(addr_arr[-1], 16), 256))
-
-
-def is_valid_ipv4_ret(ip_addr):
-    """
-    Return buffer in network order
-    """
-    if type(ip_addr) == bytes and len(ip_addr) == 4:
-        return ip_addr
-
-    if type(ip_addr) == int:
-        ip_addr = socket.inet_ntoa(struct.pack("!I", ip_addr))
-
-    try:
-        return socket.inet_pton(socket.AF_INET, ip_addr)
-    except AttributeError:  # no inet_pton here, sorry
-        return socket.inet_aton(ip_addr)
-    except socket.error:  # not a valid address
-        raise CTRexPacketBuildException(-10, "Not valid ipv4 format")
-
-
-def is_valid_ipv6_ret(ipv6_addr):
-    """
-    Return buffer in network order
-    """
-    if type(ipv6_addr) == bytes and len(ipv6_addr) == 16:
-        return ipv6_addr
-    try:
-        return socket.inet_pton(socket.AF_INET6, ipv6_addr)
-    except AttributeError:  # no inet_pton here, sorry
-        raise CTRexPacketBuildException(-10, "No inet_pton function available")
-    except Exception:
-        raise CTRexPacketBuildException(-10, "Not valid ipv6 format")
-
-
-class CTRexScriptsBase(object):
-    """
-    VM Script base class
-    """
-
-    def clone(self):
-        return copy.deepcopy(self)
-
-
-class CTRexScFieldRangeBase(CTRexScriptsBase):
-
-    FILED_TYPES = ["inc", "dec", "rand"]
-
-    def __init__(self, field_name, field_type):
-        super(CTRexScFieldRangeBase, self).__init__()
-        self.field_name = field_name
-        self.field_type = field_type
-        if self.field_type not in CTRexScFieldRangeBase.FILED_TYPES:
-            raise CTRexPacketBuildException(-12, "Field type should be in %s" % CTRexScFieldRangeBase.FILED_TYPES)
-
-
-class CTRexScFieldRangeValue(CTRexScFieldRangeBase):
-    """
-    Range of field values
-    """
-
-    def __init__(self, field_name, field_type, min_value, max_value):
-        super(CTRexScFieldRangeValue, self).__init__(field_name, field_type)
-        self.min_value = min_value
-        self.max_value = max_value
-        if min_value > max_value:
-            raise CTRexPacketBuildException(-12, "Invalid range: min is greater than max.")
-        if min_value == max_value:
-            raise CTRexPacketBuildException(-13, "Invalid range: min value is equal to max value.")
-
-
-class CTRexScIpv4SimpleRange(CTRexScFieldRangeBase):
-    """
-    Range of ipv4 ip
-    """
-
-    def __init__(self, field_name, field_type, min_ip, max_ip):
-        super(CTRexScIpv4SimpleRange, self).__init__(field_name, field_type)
-        self.min_ip = min_ip
-        self.max_ip = max_ip
-        mmin = ipv4_str_to_num(is_valid_ipv4_ret(min_ip))
-        mmax = ipv4_str_to_num(is_valid_ipv4_ret(max_ip))
-        if mmin > mmax:
-            raise CTRexPacketBuildException(-11, "CTRexScIpv4SimpleRange m_min ip is bigger than max")
-
-
-class CTRexScIpv4TupleGen(CTRexScriptsBase):
-    """
-    Range tuple
-    """
-
-    FLAGS_ULIMIT_FLOWS = 1
-
-    def __init__(self, min_ipv4, max_ipv4, num_flows=100000, min_port=1025, max_port=65535, flags=0):
-        super(CTRexScIpv4TupleGen, self).__init__()
-        self.min_ip = min_ipv4
-        self.max_ip = max_ipv4
-        mmin = ipv4_str_to_num(is_valid_ipv4_ret(min_ipv4))
-        mmax = ipv4_str_to_num(is_valid_ipv4_ret(max_ipv4))
-        if mmin > mmax:
-            raise CTRexPacketBuildException(-11, "CTRexScIpv4SimpleRange m_min ip is bigger than max")
-
-        self.num_flows = num_flows
-
-        self.min_port = min_port
-        self.max_port = max_port
-        self.flags = flags
-
-
-class CTRexScTrimPacketSize(CTRexScriptsBase):
-    """
-    Trim packet size. Field type is CTRexScFieldRangeBase.FILED_TYPES = ["inc","dec","rand"]
-    """
-
-    def __init__(self, field_type="rand", min_pkt_size=None, max_pkt_size=None):
-        super(CTRexScTrimPacketSize, self).__init__()
-        self.field_type = field_type
-        self.min_pkt_size = min_pkt_size
-        self.max_pkt_size = max_pkt_size
-        if max_pkt_size is not None and min_pkt_size is not None:
-            if min_pkt_size == max_pkt_size:
-                raise CTRexPacketBuildException(-11, "CTRexScTrimPacketSize min_pkt_size is the same as max_pkt_size ")
-
-            if min_pkt_size > max_pkt_size:
-                raise CTRexPacketBuildException(-11, "CTRexScTrimPacketSize min_pkt_size is bigger than max_pkt_size ")
-
-
-class STLScVmRaw(CTRexScriptsBase):
-    """
-    Raw instructions
-    """
-
-    def __init__(self, list_of_commands=None, split_by_field=None, cache_size=None):
-        """
-        Include a list of a basic instructions objects.
-
-        :parameters:
-             list_of_commands : list
-                list of instructions
-
-             split_by_field : string
-                by which field to split to threads
-
-             cache_size     : uint16_t
-                In case it is bigger than zero, FE results will be cached
-                - this will speedup of the program at the cost of limiting the
-                number of possible packets to the number of cache.
-                The cache size is limited to the pool size
-
-        The following example splits the generated traffic by "ip_src" variable.
-
-        .. code-block:: python
-
-            # Split by
-
-            # TCP SYN
-            base_pkt  = Ether()/IP(dst="48.0.0.1")/TCP(dport=80,flags="S")
-
-
-            # vm
-            vm = STLScVmRaw( [ STLVmFlowVar(name="ip_src",
-                                                  min_value="16.0.0.0",
-                                                  max_value="16.0.0.254",
-                                                  size=4, op="inc"),
-
-
-                               STLVmWrFlowVar(fv_name="ip_src", pkt_offset= "IP.src" ),
-
-                               STLVmFixIpv4(offset = "IP"), # fix checksum
-                              ]
-                             ,split_by_field = "ip_src",
-                             cache_size = 1000
-                           )
-
-        """
-
-        super(STLScVmRaw, self).__init__()
-        self.split_by_field = split_by_field
-        self.cache_size = cache_size
-
-        if list_of_commands is None:
-            self.commands = []
-        else:
-            self.commands = list_of_commands
-
-    def add_cmd(self, cmd):
-        self.commands.append(cmd)
-
-
-################################################################################################
-# VM raw instructions
-################################################################################################
-
-
-class CTRexVmInsBase(object):
-    """
-    Instruction base
-    """
-
-    def __init__(self, ins_type):
-        self.type = ins_type
-
-
-class CTRexVmInsFixIpv4(CTRexVmInsBase):
-    def __init__(self, offset):
-        super(CTRexVmInsFixIpv4, self).__init__("fix_checksum_ipv4")
-        self.pkt_offset = offset
-
-
-class CTRexVmInsFixHwCs(CTRexVmInsBase):
-    L4_TYPE_UDP = 11
-    L4_TYPE_TCP = 13
-
-    def __init__(self, l2_len, l3_len, l4_type):
-        super(CTRexVmInsFixHwCs, self).__init__("fix_checksum_hw")
-        self.l2_len = l2_len
-        self.l3_len = l3_len
-        self.l4_type = l4_type
-
-
-class CTRexVmInsFlowVar(CTRexVmInsBase):
-    # TBD add more validation tests
-
-    OPERATIONS = ["inc", "dec", "random"]
-    VALID_SIZES = [1, 2, 4, 8]
-
-    def __init__(self, fv_name, size, op, init_value, min_value, max_value, step):
-        super(CTRexVmInsFlowVar, self).__init__("flow_var")
-        self.name = fv_name
-        self.size = size
-        self.op = op
-        self.init_value = init_value
-        assert init_value >= 0, "init_value(%s) is negative" % init_value
-        self.min_value = min_value
-        assert min_value >= 0, "min_value(%s) is negative" % min_value
-        self.max_value = max_value
-        assert max_value >= 0, "max_value(%s) is negative" % max_value
-        self.step = step
-        assert step >= 0, "step(%s) is negative" % step
-
-
-class CTRexVmInsFlowVarRandLimit(CTRexVmInsBase):
-    # TBD add more validation tests
-
-    VALID_SIZES = [1, 2, 4, 8]
-
-    def __init__(self, fv_name, size, limit, seed, min_value, max_value):
-        super(CTRexVmInsFlowVarRandLimit, self).__init__("flow_var_rand_limit")
-        self.name = fv_name
-        self.size = size
-        self.limit = limit
-        assert limit >= 0, "limit(%s) is negative" % limit
-        self.seed = seed
-        self.min_value = min_value
-        assert min_value >= 0, "min_value(%s) is negative" % min_value
-        self.max_value = max_value
-        assert max_value >= 0, "max_value(%s) is negative" % max_value
-
-
-class CTRexVmInsWrFlowVar(CTRexVmInsBase):
-    def __init__(self, fv_name, pkt_offset, add_value=0, is_big_endian=True):
-        super(CTRexVmInsWrFlowVar, self).__init__("write_flow_var")
-        self.name = fv_name
-        self.pkt_offset = pkt_offset
-        self.add_value = add_value
-        self.is_big_endian = is_big_endian
-
-
-class CTRexVmInsWrMaskFlowVar(CTRexVmInsBase):
-    def __init__(self, fv_name, pkt_offset, pkt_cast_size, mask, shift, add_value, is_big_endian=True):
-        super(CTRexVmInsWrMaskFlowVar, self).__init__("write_mask_flow_var")
-        self.name = fv_name
-        self.pkt_offset = pkt_offset
-        self.pkt_cast_size = pkt_cast_size
-        self.mask = mask
-        self.shift = shift
-        self.add_value = add_value
-        self.is_big_endian = is_big_endian
-
-
-class CTRexVmInsTrimPktSize(CTRexVmInsBase):
-    def __init__(self, fv_name):
-        super(CTRexVmInsTrimPktSize, self).__init__("trim_pkt_size")
-        self.name = fv_name
-
-
-class CTRexVmInsTupleGen(CTRexVmInsBase):
-    def __init__(self, fv_name, ip_min, ip_max, port_min, port_max, limit_flows, flags=0):
-        super(CTRexVmInsTupleGen, self).__init__("tuple_flow_var")
-        self.name = fv_name
-        self.ip_min = ip_min
-        self.ip_max = ip_max
-        self.port_min = port_min
-        self.port_max = port_max
-        self.limit_flows = limit_flows
-        self.flags = flags
-
-
-# ###############################################################################################
-#
-class CTRexVmEngine(object):
-    def __init__(self):
-        """
-        Inlcude list of instructions.
-        """
-        super(CTRexVmEngine, self).__init__()
-        self.ins = []
-        self.split_by_var = ""
-        self.cache_size = 0
-
-    # return as json
-
-    def get_json(self):
-        inst_array = []
-        # dump it as dict
-        for obj in self.ins:
-            inst_array.append(obj.__dict__)
-
-        d = {"instructions": inst_array, "split_by_var": self.split_by_var}
-        if self.cache_size > 0:
-            d["cache"] = self.cache_size
-        return d
-
-    def add_ins(self, ins):
-        # assert issubclass(ins, CTRexVmInsBase)
-        self.ins.append(ins)
-
-    def dump(self):
-        cnt = 0
-        for obj in self.ins:
-            print(("ins", cnt))
-            cnt = cnt + 1
-            print((obj.__dict__))
-
-    def dump_bjson(self):
-        print((json.dumps(self.get_json(), sort_keys=True, indent=4)))
-
-    def dump_as_yaml(self):
-        print((yaml.dump(self.get_json(), default_flow_style=False)))
-
-
-# ###############################################################################################
-
-
-class CTRexScapyPktUtl(object):
-    def __init__(self, scapy_pkt):
-        self.pkt = scapy_pkt
-
-    def pkt_iter(self):
-        p = self.pkt
-        while True:
-            yield p
-            p = p.payload
-            if p is None or isinstance(p, NoPayload):
-                break
-
-    def get_list_iter(self):
-        list_iter = list(self.pkt_iter())
-        return list_iter
-
-    def get_pkt_layers(self):
-        """
-        Return string 'IP:UDP:TCP'
-        """
-        list_iter = self.get_list_iter()
-        l1 = [p.name for p in list_iter]
-        return ":".join(l1)
-
-    def _layer_offset(self, name, cnt=0):
-        """
-        Return offset of layer. Example: 'IP',1 returns offfset of layer ip:1
-        """
-        save_cnt = cnt
-        for pkt in self.pkt_iter():
-            if name in (pkt.name, pkt.__class__.__name__):
-                if cnt == 0:
-                    return (pkt, pkt._offset)
-                else:
-                    cnt = cnt - 1
-
-        raise CTRexPacketBuildException(-11, ("no layer %s-%d" % (name, save_cnt)))
-
-    def layer_offset(self, name, cnt=0):
-        """
-        Return offset of layer. Example: 'IP',1 returns offfset of layer ip:1
-        """
-        save_cnt = cnt
-        for pkt in self.pkt_iter():
-            if pkt.name == name:
-                if cnt == 0:
-                    return pkt._offset
-                else:
-                    cnt = cnt - 1
-
-        raise CTRexPacketBuildException(-11, ("no layer %s-%d" % (name, save_cnt)))
-
-    def get_field_offet(self, layer, layer_cnt, field_name):
-        """
-        Return offset of layer. Example: 'IP',1 returns offfset of layer ip:1
-        """
-        t = self._layer_offset(layer, layer_cnt)
-        l_offset = t[1]
-        layer_pkt = t[0]
-        # layer_pkt.dump_fields_offsets()
-
-        for f in layer_pkt.fields_desc:
-            if f.name == field_name:
-                return (l_offset + f._offset, f.get_size_bytes())
-
-        raise CTRexPacketBuildException(-11, "No layer %s-%d." % (field_name, layer_cnt))
-
-    def get_layer_offet_by_str(self, layer_des):
-        """
-         Return layer offset by string.
-
-        :parameters:
-
-         IP:0
-         IP:1
-         return offset
-
-
-        """
-        l1 = layer_des.split(":")
-        layer = ""
-        layer_cnt = 0
-
-        if len(l1) == 1:
-            layer = l1[0]
-        else:
-            layer = l1[0]
-            layer_cnt = int(l1[1])
-
-        return self.layer_offset(layer, layer_cnt)
-
-    def get_field_offet_by_str(self, field_des):
-        """
-        Return field_des(offset,size) layer:cnt.field
-        Example:
-        802|1Q.vlan get 802.1Q->valn replace | with .
-        IP.src
-        IP:0.src (first IP.src like IP.src)
-        Example: IP:1.src  for internal IP
-
-        Return(offset, size) as tuple.
-
-
-        """
-
-        s = field_des.split(".")
-        if len(s) != 2:
-            raise CTRexPacketBuildException(-11, ("Field desription should be layer:cnt.field Example: IP.src or IP:1.src"))
-
-        layer_ex = s[0].replace("|", ".")
-        field = s[1]
-
-        l1 = layer_ex.split(":")
-        layer = ""
-        layer_cnt = 0
-
-        if len(l1) == 1:
-            layer = l1[0]
-        else:
-            layer = l1[0]
-            layer_cnt = int(l1[1])
-
-        return self.get_field_offet(layer, layer_cnt, field)
-
-    def has_IPv4(self):
-        return self.pkt.has_layer("IP")
-
-    def has_IPv6(self):
-        return self.pkt.has_layer("IPv6")
-
-    def has_UDP(self):
-        return self.pkt.has_layer("UDP")
-
-
-################################################################################################
-
-
-class CTRexVmDescBase(object):
-    """
-    Instruction base
-    """
-
-    def __init__(self):
-        pass
-
-    def get_obj(self):
-        return self
-
-    def get_json(self):
-        return self.get_obj().__dict__
-
-    def dump_bjson(self):
-        print((json.dumps(self.get_json(), sort_keys=True, indent=4)))
-
-    def dump_as_yaml(self):
-        print((yaml.dump(self.get_json(), default_flow_style=False)))
-
-    def get_var_ref(self):
-        """
-        Virtual function returns a ref var name.
-        """
-        return None
-
-    def get_var_name(self):
-        """
-        Virtual function returns the varible name if it exists.
-        """
-        return None
-
-    def compile(self, parent):
-        """
-        Virtual function to take parent that has function name_to_offset.
-        """
-        pass
-
-
-def valid_fv_size(size):
-    if not (size in CTRexVmInsFlowVar.VALID_SIZES):
-        raise CTRexPacketBuildException(-11, ("Flow var has invalid size %d ") % size)
-
-
-def valid_fv_ops(op):
-    if not (op in CTRexVmInsFlowVar.OPERATIONS):
-        raise CTRexPacketBuildException(-11, ("Flow var has invalid op %s ") % op)
-
-
-def get_max_by_size(size):
-    d = {1: ((1 << 8) - 1), 2: ((1 << 16) - 1), 4: ((1 << 32) - 1), 8: 0xFFFFFFFFFFFFFFFF}
-    return d[size]
-
-
-def convert_val(val):
-    if type(val) is int:
-        return val
-    if type(val) == str:
-        return ipv4_str_to_num(is_valid_ipv4_ret(val))
-    raise CTRexPacketBuildException(-11, ("init val invalid %s ") % val)
-
-
-class STLVmFlowVar(CTRexVmDescBase):
-    def __init__(self, name, init_value=None, min_value=0, max_value=255, size=4, step=1, op="inc"):
-        """
-        Flow variable instruction. Allocates a variable on a stream context.
-        The size argument determines the variable size.
-        The operation can be inc, dec, and random.
-        For increment and decrement operations, can set the "step" size.
-        For all operations, can set initialization value, minimum and maximum value.
-
-        :parameters:
-             name : string
-                Name of the stream variable
-
-             init_value : int
-                Init value of the variable. If not specified, it will be min_value
-
-             min_value  : int
-                Min value
-
-             max_value  : int
-                Max value
-
-             size  : int
-                Number of bytes of the variable. Possible values: 1,2,4,8 for uint8_t,
-                uint16_t, uint32_t, uint64_t
-
-             step  : int
-                Step in case of "inc" or "dec" operations
-
-             op    : string
-                Possible values: "inc", "dec", "random"
-
-        .. code-block:: python
-
-            # Example1
-
-            # input
-            STLVmFlowVar(min_value=0, max_value=3, size=1,op="inc")
-
-            # output 0,1,2,3,0,1,2,3 ..
-
-            # input
-            STLVmFlowVar(min_value=0, max_value=3, size=1,op="dec")
-
-            # output 0,3,2,1,0,3,2,1 ..
-
-
-            # input
-            STLVmFlowVar(min_value=0, max_value=3, size=1,op="random")
-
-            # output 1,1,2,3,1,2,1,0 ..
-
-            # input
-            STLVmFlowVar(min_value=0, max_value=10, size=1,op="inc",step=3)
-
-            # output 0,3,6,9,0,3,6,9,0..
-
-
-        """
-        super(STLVmFlowVar, self).__init__()
-        self.name = name
-        self.size = size
-        valid_fv_size(size)
-        self.op = op
-        valid_fv_ops(op)
-
-        # choose default value for init val
-        if init_value is None:
-            init_value = max_value if op == "dec" else min_value
-
-        self.init_value = convert_val(init_value)
-        self.min_value = convert_val(min_value)
-        self.max_value = convert_val(max_value)
-        self.step = convert_val(step)
-
-        if self.min_value > self.max_value:
-            raise CTRexPacketBuildException(-11, ("max %d is lower than min %d ") % (self.max_value, self.min_value))
-
-    def get_obj(self):
-        return CTRexVmInsFlowVar(self.name, self.size, self.op, self.init_value, self.min_value, self.max_value, self.step)
-
-    def get_var_name(self):
-        return [self.name]
-
-
-class STLVmFlowVarRepeatableRandom(CTRexVmDescBase):
-    def __init__(self, name, size=4, limit=100, seed=None, min_value=0, max_value=None):
-        """
-        Flow variable instruction for repeatable random with limit number of generating numbers.
-        Allocates memory on a stream context.
-        The size argument determines the variable size. Could be 1,2,4 or 8
-
-        1. The maximum number of distinct values will  'limit'. There could be a case of repetition
-        2. The values will be repeated  after 'limit' number of values.
-
-        :parameters:
-             name : string
-                Name of the stream variable
-
-             size  : int
-                Number of bytes of the variable. Possible values: 1,2,4,8 for uint8_t,
-                uint16_t, uint32_t, uint64_t
-
-             limit  : int
-                The number of distinct repetable random number
-
-             seed   : int
-                For deterministic result, you can set this to a uint16_t number
-
-             min_value  : int
-                Min value
-
-             max_value  : int
-                Max value
-
-
-        .. code-block:: python
-
-            # Example1
-
-            # input , 1 byte or random with limit of 5
-            STLVmFlowVarRepeatableRandom("var1",size=1,limit=5)
-
-            # output 255,1,7,129,8, ==> repeat 255,1,7,129,8
-
-            STLVmFlowVarRepeatableRandom("var1",size=4,limit=100,min_value=0x12345678,
-                                         max_value=0x32345678)
-
-
-        """
-        super(STLVmFlowVarRepeatableRandom, self).__init__()
-        self.name = name
-        self.size = size
-        valid_fv_size(size)
-        self.limit = limit
-
-        if seed is None:
-            self.seed = random.randint(1, 32000)
-        else:
-            self.seed = seed
-
-        self.min_value = convert_val(min_value)
-
-        if max_value is None:
-            self.max_value = get_max_by_size(self.size)
-        else:
-            self.max_value = convert_val(max_value)
-
-        if self.min_value > self.max_value:
-            raise CTRexPacketBuildException(-11, ("max %d is lower than min %d ") % (self.max_value, self.min_value))
-
-    def get_obj(self):
-        return CTRexVmInsFlowVarRandLimit(self.name, self.size, self.limit, self.seed, self.min_value, self.max_value)
-
-    def get_var_name(self):
-        return [self.name]
-
-
-class STLVmFlowVarRepetableRandom(STLVmFlowVarRepeatableRandom):
-    def __init__(self, name, size=4, limit=100, seed=None, min_value=0, max_value=None):
-        super(STLVmFlowVarRepetableRandom, self).__init__(name, size, limit, seed, min_value, max_value)
-
-    def get_obj(self):
-        return CTRexVmInsFlowVarRandLimit(self.name, self.size, self.limit, self.seed, self.min_value, self.max_value)
-
-    def get_var_name(self):
-        return [self.name]
-
-
-class STLVmFixChecksumHw(CTRexVmDescBase):
-    def __init__(self, l3_offset, l4_offset, l4_type):
-        """
-        Fix Ipv4 header checksum and TCP/UDP checksum using hardware assist.
-        Use this if the packet header has changed or data payload has changed as it
-        is necessary to fix the checksums.
-        This instruction works on NICS that support this hardware offload.
-
-        For fixing only IPv4 header checksum use STLVmFixIpv4. This instruction should be used
-        if both L4 and L3 need to be fixed.
-
-        example for supported packets
-
-        Ether()/(IPv4|IPv6)/(UDP|TCP)
-        Ether()/(IPv4|IPv6)/(UDP|TCP)
-        SomeTunnel()/(IPv4|IPv6)/(UDP|TCP)
-        SomeTunnel()/(IPv4|IPv6)/(UDP|TCP)
-
-
-        :parameters:
-             l3_offset : offset in bytes
-                **IPv4/IPv6 header** offset from packet start. It is **not** the offset of
-                the checksum field itself.
-                in could be string in case of scapy packet. format IP[:[id]]
-
-             l4_offset : offset in bytes to UDP/TCP header
-
-             l4_type   : CTRexVmInsFixHwCs.L4_TYPE_UDP or CTRexVmInsFixHwCs.L4_TYPE_TCP
-
-             see full example stl/syn_attack_fix_cs_hw.py
-
-        .. code-block:: python
-
-            # Example2
-
-            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
-
-            # by offset
-            STLVmFixChecksumHw(l3_offset=14,l4_offset=14+20,l4_type=CTRexVmInsFixHwCs.L4_TYPE_UDP)
-
-            # in case of scapy packet can be defined by header name
-            STLVmFixChecksumHw(l3_offset="IP",l4_offset="UDP",l4_type=CTRexVmInsFixHwCs.L4_TYPE_UDP)
-
-            # string for second "IP" header in the packet is IP:1
-            STLVmFixChecksumHw(offset="IP:1")
-
-        """
-
-        super(STLVmFixChecksumHw, self).__init__()
-        self.l3_offset = l3_offset  # could be a name of offset
-        self.l4_offset = l4_offset  # could be a name of offset
-        self.l4_type = l4_type
-        self.l2_len = 0
-
-    def get_obj(self):
-        return CTRexVmInsFixHwCs(self.l2_len, self.l3_len, self.l4_type)
-
-    def compile(self, parent):
-        if type(self.l3_offset) == str:
-            self.l2_len = parent._pkt_layer_offset(self.l3_offset)
-        if type(self.l4_offset) == str:
-            self.l4_offset = parent._pkt_layer_offset(self.l4_offset)
-
-        assert self.l4_offset >= self.l2_len + 8, "l4_offset should be higher than l3_offset offset"
-        self.l3_len = self.l4_offset - self.l2_len
-
-
-class STLVmFixIpv4(CTRexVmDescBase):
-    def __init__(self, offset):
-        """
-        Fix IPv4 header checksum. Use this if the packet header has changed and it is
-        necessary to change the checksum.
-
-        :parameters:
-             offset : uint16_t or string
-                **IPv4 header** offset from packet start. It is **not** the offset of the
-                checksum field itself.
-                in could be string in case of scapy packet. format IP[:[id]]
-
-        .. code-block:: python
-
-            # Example2
-
-            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
-
-            # by offset
-            STLVmFixIpv4(offset=14)
-
-            # in case of scapy packet can be defined by header name
-            STLVmFixIpv4(offset="IP")
-
-            # string for second "IP" header in the packet is IP:1
-            STLVmFixIpv4(offset="IP:1")
-
-        """
-
-        super(STLVmFixIpv4, self).__init__()
-        self.offset = offset  # could be a name of offset
-
-    def get_obj(self):
-        return CTRexVmInsFixIpv4(self.offset)
-
-    def compile(self, parent):
-        if type(self.offset) == str:
-            self.offset = parent._pkt_layer_offset(self.offset)
-
-
-class STLVmWrFlowVar(CTRexVmDescBase):
-    def __init__(self, fv_name, pkt_offset, offset_fixup=0, add_val=0, is_big=True):
-        """
-        Write a stream variable into a packet field.
-        The write position is determined by the packet offset + offset fixup.
-        The size of the write is determined by the stream variable.
-        Example: Offset 10, fixup 0, variable size 4. This function writes at 10, 11, 12, and 13.
-
-        For inromation about chaning the write size, offset, or fixup, see
-        the `STLVmWrMaskFlowVar` command.
-        The Field name/offset can be given by name in the following format: ``header[:id].field``.
-
-
-        :parameters:
-            fv_name : string
-                Stream variable to write to a packet offset.
-
-            pkt_offset : string or in
-                Name of the field or offset in bytes from packet start.
-
-            offset_fixup : int
-                Number of bytes to move forward. If negative, move backward.
-
-             add_val     : int
-                Value to add to the stream variable before writing it to the packet field.
-                Can be used as a constant offset.
-
-             is_big      : bool
-                How to write the variable to the the packet. True=big-endian, False=little-endian
-
-        .. code-block:: python
-
-            # Example3
-
-            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
-
-
-            # write to ip.src offset
-            STLVmWrFlowVar(fv_name="tuple", pkt_offset= "IP.src" )
-
-            # packet offset is varible
-            STLVmWrFlowVar(fv_name="tuple", pkt_offset= 26 )
-
-            # add l3_len_fix before writing fv_rand into IP.len field
-            STLVmWrFlowVar(fv_name="fv_rand", pkt_offset= "IP.len", add_val=l3_len_fix)
-
-        """
-
-        super(STLVmWrFlowVar, self).__init__()
-        self.name = fv_name
-        self.offset_fixup = offset_fixup
-        self.pkt_offset = pkt_offset
-        self.add_val = add_val
-        self.is_big = is_big
-
-    def get_var_ref(self):
-        return self.name
-
-    def get_obj(self):
-        return CTRexVmInsWrFlowVar(self.name, self.pkt_offset + self.offset_fixup, self.add_val, self.is_big)
-
-    def compile(self, parent):
-        if type(self.pkt_offset) == str:
-            t = parent._name_to_offset(self.pkt_offset)
-            self.pkt_offset = t[0]
-
-
-class STLVmWrMaskFlowVar(CTRexVmDescBase):
-    def __init__(self, fv_name, pkt_offset, pkt_cast_size=1, mask=0xFF, shift=0, add_value=0, offset_fixup=0, is_big=True):
-        """
-        Write a stream variable into a packet field with some operations.
-        Using this instruction, the variable size and the field can have different sizes.
-
-        Pseudocode of this code::
-
-                uint32_t val=(cast_to_size)rd_from_variable("name") # read flow-var
-                val+=m_add_value                                    # add value
-
-                if(m_shift>0) {                                    # shift
-                    val=val<<m_shift
-                }else{
-                    if(m_shift<0) {
-                        val=val>>(-m_shift)
-                    }
-                }
-
-                pkt_val=rd_from_pkt(pkt_offset)                     # RMW to the packet
-                pkt_val =(pkt_val & ~m_mask) |(val & m_mask)
-                wr_to_pkt(pkt_offset,pkt_val)
-
-
-        :parameters:
-            fv_name : string
-                The stream variable name to write to a packet field
-
-            pkt_cast_size : uint8_t
-                The size in bytes of the packet field
-
-
-            mask          : uint32_t
-                The mask of the field. 1 means to write. 0 don't care
-
-            shift          : uint8_t
-                How many bits to shift
-
-            pkt_offset : string or in
-                the name of the field or offset in byte from packet start.
-
-            offset_fixup : int
-                how many bytes to go forward. In case of a negative value go backward
-
-             add_val     : int
-                value to add to stream variable before writing it to packet field.
-                can be used as a constant offset
-
-             is_big      : bool
-                how to write the variable to the the packet. is it big-edian or little edian
-
-        Example 1 - Cast from uint16_t(var) to uint8_t(pkt)::
-
-
-            base_pkt =  Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
-
-            vm = STLScVmRaw( [ STLVmFlowVar(name="mac_src",
-                                            min_value=1,
-                                            max_value=30,
-                                            size=2,
-                                            op="dec",step=1),
-                               STLVmWrMaskFlowVar(fv_name="mac_src",
-                                                  pkt_offset= 11,
-                                                  pkt_cast_size=1,
-                                                  mask=0xff) # mask command ->write it as one byte
-                          ]
-                       )
-
-            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
-
-        Example 2 - Change MSB of uint16_t variable::
-
-
-            vm = STLScVmRaw( [ STLVmFlowVar(name="mac_src",
-                                            min_value=1,
-                                            max_value=30,
-                                            size=2, op="dec",step=1),
-                               STLVmWrMaskFlowVar(fv_name="mac_src",
-                                                  pkt_offset= 10,
-                                                  pkt_cast_size=2,
-                                                  mask=0xff00,
-                                                  shift=8) # take the var shift it 8(x256)
-                                                  write only to LSB
-                              ]
-                            )
-
-
-
-        Example 3 - Every 2 packets, change the MAC(shift right)::
-
-                vm = STLScVmRaw( [ STLVmFlowVar(name="mac_src",
-                                                min_value=1,
-                                                max_value=30,
-                                                size=2, op="dec",step=1),
-                                   STLVmWrMaskFlowVar(fv_name="mac_src",
-                                                      pkt_offset= 10,
-                                                      pkt_cast_size=1,
-                                                      mask=0x1,
-                                                      shift=-1) # take var mac_src>>1 and write
-                                                      the LSB every two packet there should be
-                                                      a change
-                                 ]
-                                )
-
-
-        """
-
-        super(STLVmWrMaskFlowVar, self).__init__()
-        self.name = fv_name
-        self.offset_fixup = offset_fixup
-        self.pkt_offset = pkt_offset
-        self.pkt_cast_size = pkt_cast_size
-        if not (pkt_cast_size in [1, 2, 4]):
-            raise CTRexPacketBuildException(-10, "not valid cast size")
-
-        self.mask = mask
-        self.shift = shift
-        self.add_value = add_value
-
-        self.is_big = is_big
-
-    def get_var_ref(self):
-        return self.name
-
-    def get_obj(self):
-        return CTRexVmInsWrMaskFlowVar(
-            self.name,
-            self.pkt_offset + self.offset_fixup,
-            self.pkt_cast_size,
-            self.mask,
-            self.shift,
-            self.add_value,
-            self.is_big,
-        )
-
-    def compile(self, parent):
-        if type(self.pkt_offset) == str:
-            t = parent._name_to_offset(self.pkt_offset)
-            self.pkt_offset = t[0]
-
-
-class STLVmTrimPktSize(CTRexVmDescBase):
-    def __init__(self, fv_name):
-        """
-        Trim the packet size by the stream variable size. This instruction only
-        changes the total packet size, and does not repair the fields to match the new size.
-
-
-        :parameters:
-            fv_name : string
-                Stream variable name. The value of this variable is the new total packet size.
-
-
-        For Example::
-
-            def create_stream(self):
-                # pkt
-                p_l2  = Ether();
-                p_l3  = IP(src="16.0.0.1",dst="48.0.0.1")
-                p_l4  = UDP(dport=12,sport=1025)
-                pyld_size = max(0, self.max_pkt_size_l3 - len(p_l3/p_l4));
-                base_pkt = p_l2/p_l3/p_l4/('\x55'*(pyld_size))
-
-                l3_len_fix =-(len(p_l2));
-                l4_len_fix =-(len(p_l2/p_l3));
-
-
-                # vm
-                vm = STLScVmRaw( [ STLVmFlowVar(name="fv_rand", min_value=64,
-                                                max_value=len(base_pkt),
-                                                size=2, op="inc"),
-                                   # change total packet size <<<
-                                   STLVmTrimPktSize("fv_rand"),
-
-                                   STLVmWrFlowVar(fv_name="fv_rand",
-                                                  pkt_offset= "IP.len",
-                                                  add_val=l3_len_fix), # fix ip len
-                                   # fix checksum
-                                   STLVmFixIpv4(offset = "IP"),
-
-                                   STLVmWrFlowVar(fv_name="fv_rand",
-                                                  pkt_offset= "UDP.len",
-                                                  add_val=l4_len_fix) # fix udp len
-                                  ]
-                               )
-
-                pkt = STLPktBuilder(pkt = base_pkt,
-                                    vm = vm)
-
-                return STLStream(packet = pkt,
-                                 mode = STLTXCont())
-
-
-        """
-
-        super(STLVmTrimPktSize, self).__init__()
-        self.name = fv_name
-
-    def get_var_ref(self):
-        return self.name
-
-    def get_obj(self):
-        return CTRexVmInsTrimPktSize(self.name)
-
-
-class STLVmTupleGen(CTRexVmDescBase):
-    def __init__(self, name, ip_min="0.0.0.1", ip_max="0.0.0.10", port_min=1025, port_max=65535, limit_flows=100000, flags=0):
-        """
-        Generate a struct with two variables: ``var_name.ip`` as uint32_t
-        and ``var_name.port`` as uint16_t
-        The variables are dependent. When the ip variable value reaches its maximum,
-        the port is incremented.
-
-        For:
-
-        * ip_min      = 10.0.0.1
-        * ip_max      = 10.0.0.5
-        * port_min    = 1025
-        * port_max    = 1028
-        * limit_flows = 10
-
-        The result:
-
-        +------------+------------+-----------+
-        | ip         | port       | flow_id   |
-        +============+============+===========+
-        | 10.0.0.1   | 1025       | 1         |
-        +------------+------------+-----------+
-        | 10.0.0.2   | 1025       | 2         |
-        +------------+------------+-----------+
-        | 10.0.0.3   | 1025       | 3         |
-        +------------+------------+-----------+
-        | 10.0.0.4   | 1025       | 4         |
-        +------------+------------+-----------+
-        | 10.0.0.5   | 1025       | 5         |
-        +------------+------------+-----------+
-        | 10.0.0.1   | 1026       | 6         |
-        +------------+------------+-----------+
-        | 10.0.0.2   | 1026       | 7         |
-        +------------+------------+-----------+
-        | 10.0.0.3   | 1026       | 8         |
-        +------------+------------+-----------+
-        | 10.0.0.4   | 1026       | 9         |
-        +------------+------------+-----------+
-        | 10.0.0.5   | 1026       | 10        |
-        +------------+------------+-----------+
-        | 10.0.0.1   | 1025       | 1         |
-        +------------+------------+-----------+
-
-
-        :parameters:
-            name : string
-                Name of the stream struct.
-
-            ip_min : string or int
-                Min value of the ip value. Number or IPv4 format.
-
-            ip_max : string or int
-                Max value of the ip value. Number or IPv4 format.
-
-            port_min : int
-                Min value of port variable.
-
-            port_max : int
-                Max value of port variable.
-
-            limit_flows : int
-                Limit of number of flows.
-
-            flags       : 0
-
-            ="0.0.0.10", port_min=1025, port_max=65535, limit_flows=100000, flags=0
-
-        .. code-block:: python
-
-            # Example5
-
-            def create_stream(self):
-                # pkt
-                p_l2  = Ether();
-                p_l3  = IP(src="16.0.0.1",dst="48.0.0.1")
-                p_l4  = UDP(dport=12,sport=1025)
-                pyld_size = max(0, self.max_pkt_size_l3 - len(p_l3/p_l4));
-                base_pkt = p_l2/p_l3/p_l4/('\x55'*(pyld_size))
-
-                l3_len_fix =-(len(p_l2));
-                l4_len_fix =-(len(p_l2/p_l3));
-
-
-                # vm
-                vm = STLScVmRaw( [ STLVmFlowVar(name="fv_rand", min_value=64,
-                                                max_value=len(base_pkt),
-                                                size=2, op="inc"),
-                                   # change total packet size <<<
-                                   STLVmTrimPktSize("fv_rand"),
-
-                                   STLVmWrFlowVar(fv_name="fv_rand",
-                                                  pkt_offset= "IP.len",
-                                                  add_val=l3_len_fix), # fix ip len
-                                   # fix checksum
-                                   STLVmFixIpv4(offset = "IP"),
-
-                                   STLVmWrFlowVar(fv_name="fv_rand",
-                                                  pkt_offset= "UDP.len",
-                                                  add_val=l4_len_fix) # fix udp len
-                                  ]
-                               )
-
-                pkt = STLPktBuilder(pkt = base_pkt,
-                                    vm = vm)
-
-                return STLStream(packet = pkt,
-                                 mode = STLTXCont())
-
-
-        """
-
-        super(STLVmTupleGen, self).__init__()
-        self.name = name
-        self.ip_min = convert_val(ip_min)
-        self.ip_max = convert_val(ip_max)
-        self.port_min = port_min
-        self.port_max = port_max
-        self.limit_flows = limit_flows
-        self.flags = flags
-
-    def get_var_name(self):
-        return [self.name + ".ip", self.name + ".port"]
-
-    def get_obj(self):
-        return CTRexVmInsTupleGen(
-            self.name, self.ip_min, self.ip_max, self.port_min, self.port_max, self.limit_flows, self.flags
-        )
-
-
-# ###############################################################################################
-
-
-class STLPktBuilder:
-    # WTF
-    def __init__(self, pkt=None, pkt_buffer=None, vm=None, path_relative_to_profile=False, build_raw=False, remove_fcs=True):
-        """
-
-        This class defines a method for building a template packet and
-        Field Engine using the Scapy package.
-        Using this class the user can also define how TRex will handle the packet
-        by specifying the Field engine settings.
-        The pkt can be a Scapy pkt or pcap file name.
-        If using a pcap file, and path_relative_to_profile is True,
-        then the function loads the pcap file from a path relative to the profile.
-
-
-        .. code-block:: python
-
-            # Example6
-
-            # packet is scapy
-            STLPktBuilder( pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")
-                           /UDP(dport=12,sport=1025)/(10*'x') )
-
-
-            # packet is taken from pcap file relative to python
-            STLPktBuilder( pkt ="stl/yaml/udp_64B_no_crc.pcap")
-
-            # packet is taken from pcap file relative to profile file
-            STLPktBuilder( pkt ="stl/yaml/udp_64B_no_crc.pcap",
-                                path_relative_to_profile = True )
-
-
-            vm = STLScVmRaw( [   STLVmTupleGen( ip_min="16.0.0.1", ip_max="16.0.0.2",
-                                                   port_min=1025, port_max=65535,
-                                                    name="tuple"), # define tuple gen
-                             # write ip to packet IP.src
-                             STLVmWrFlowVar(fv_name="tuple.ip", pkt_offset= "IP.src" ),
-                             # fix checksum
-                             STLVmFixIpv4(offset = "IP"),
-                             #write udp.port
-                             STLVmWrFlowVar(fv_name="tuple.port", pkt_offset= "UDP.sport" )
-                             ]
-                           )
-
-            base_pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
-            pad = max(0, size - len(base_pkt)) * 'x'
-
-            STLPktBuilder(pkt = base_pkt/pad, vm= vm)
-
-
-        :parameters:
-
-             pkt : string,
-                Scapy object or pcap filename.
-
-             pkt_buffer : bytes
-                Packet as buffer.
-
-             vm   : list or base on :class:`trex_stl_lib.trex_stl_packet_builder_scapy.STLScVmRaw`
-                List of instructions to manipulate packet fields.
-
-             path_relative_to_profile : bool
-                If pkt is a pcap file, determines whether to load it relative to profile file.
-
-             build_raw : bool
-                If a buffer is specified(by pkt_buffer), determines whether to build Scapy.
-                Useful in cases where it is necessary to take the offset from Scapy.
-
-             remove_fcs : bool
-                If a buffer is specified(by pkt_buffer), determines whether to remove FCS.
-
-
-
-        """
-        super(STLPktBuilder, self).__init__()
-
-        self.pkt = None  # as input
-        self.pkt_raw = None  # from raw pcap file
-        self.vm_scripts = []  # list of high level instructions
-        self.vm_low_level = None
-        self.is_pkt_built = False
-        self.metadata = ""
-        self.path_relative_to_profile = path_relative_to_profile
-        self.remove_fcs = remove_fcs
-        self.is_binary_source = pkt_buffer is not None
-
-        if pkt is not None and pkt_buffer is not None:
-            raise CTRexPacketBuildException(-15, "Packet builder cannot be provided with both pkt and pkt_buffer.")
-
-        # process packet
-        if pkt is not None:
-            self.set_packet(pkt)
-
-        elif pkt_buffer is not None:
-            self.set_pkt_as_str(pkt_buffer)
-
-        # process VM
-        if vm is not None:
-            if not isinstance(vm, (STLScVmRaw, list)):
-                raise CTRexPacketBuildException(-14, "Bad value for variable vm.")
-
-            self.add_command(vm if isinstance(vm, STLScVmRaw) else STLScVmRaw(vm))
-
-        # raw source build to see MAC presence/ fields offset by name in VM
-        if build_raw and self.pkt_raw and not self.pkt:
-            self.__lazy_build_packet()
-
-        # if we have packet and VM - compile now
-        if self.pkt or self.pkt_raw and self.vm_scripts:
-            self.compile()
-
-    def dump_vm_data_as_yaml(self):
-        print((yaml.dump(self.get_vm_data(), default_flow_style=False)))
-
-    def get_vm_data(self):
-        """
-        Dumps the instructions
-
-        :parameters:
-            None
-
-        :return:
-            + json object of instructions
-
-        :raises:
-            + :exc:`AssertionError`, in case VM is not compiled(is None).
-        """
-
-        assert self.vm_low_level is not None, "vm_low_level is None, please use compile()"
-
-        return self.vm_low_level.get_json()
-
-    def dump_pkt(self, encode=True):
-        """
-        Dumps the packet as a decimal array of bytes(each item x gets value in range 0-255)
-
-        :parameters:
-            encode : bool
-                Encode using base64.(disable for debug)
-
-                Default: **True**
-
-        :return:
-            + packet representation as array of bytes
-
-        :raises:
-            + :exc:`AssertionError`, in case packet is empty.
-
-        """
-        pkt_buf = self._get_pkt_as_str()
-        return {"binary": base64.b64encode(pkt_buf).decode() if encode else pkt_buf, "meta": self.metadata}
-
-    def dump_pkt_to_pcap(self, file_path):
-        wrpcap(file_path, self._get_pkt_as_str())
-
-    def add_command(self, script):
-        self.vm_scripts.append(script.clone())
-
-    def dump_scripts(self):
-        self.vm_low_level.dump_as_yaml()
-
-    def dump_as_hex(self):
-        pkt_buf = self._get_pkt_as_str()
-        print((hexdump(pkt_buf)))
-
-    def pkt_layers_desc(self):
-        """
-        Return layer description in this format: IP:TCP:Pyload
-
-        """
-        pkt_buf = self._get_pkt_as_str()
-        return self.pkt_layers_desc_from_buffer(pkt_buf)
-
-    @staticmethod
-    def pkt_layers_desc_from_buffer(pkt_buf):
-        scapy_pkt = Ether(pkt_buf)
-        pkt_utl = CTRexScapyPktUtl(scapy_pkt)
-        return pkt_utl.get_pkt_layers()
-
-    def set_pkt_as_str(self, pkt_buffer):
-        self.pkt_raw = pkt_buffer
-
-    def set_pcap_file(self, pcap_file):
-        """
-        Load raw pcap file into a buffer. Loads only the first packet.
-
-        :parameters:
-            pcap_file : file_name
-
-        :raises:
-            + :exc:`AssertionError`, if packet is empty.
-
-        """
-        f_path = self._get_pcap_file_path(pcap_file)
-
-        p = RawPcapReader(f_path)
-        was_set = False
-
-        for pkt in p:
-            was_set = True
-            self.pkt_raw = pkt[0]
-            break
-        if not was_set:
-            raise CTRexPacketBuildException(-14, "Empty pcap file {0}".format(f_path))
-
-    def to_pkt_dump(self):
-        p = self.pkt
-        if p and isinstance(p, Ether):
-            p.show2()
-            hexdump(p)
-            return
-        p = self.pkt_raw
-        if p:
-            scapy_pkt = Ether(p)
-            scapy_pkt.show2()
-            hexdump(p)
-
-    def set_packet(self, pkt):
-        """
-        Scapy packet
-
-        Example::
-
-           pkt =Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)/IP()/('x'*10)
-
-        """
-        if isinstance(pkt, Ether):
-            self.pkt = pkt
-        else:
-            if isinstance(pkt, str):
-                self.set_pcap_file(pkt)
-            else:
-                raise CTRexPacketBuildException(-14, "bad packet")
-
-    def is_default_src_mac(self):
-        if self.is_binary_source:
-            return True
-        p = self.pkt
-        if isinstance(p, Packet):
-            if isinstance(p, Ether):
-                if "src" in p.fields:
-                    return False
-        return True
-
-    def is_default_dst_mac(self):
-        if self.is_binary_source:
-            return True
-        p = self.pkt
-        if isinstance(p, Packet):
-            if isinstance(p, Ether):
-                if "dst" in p.fields:
-                    return False
-        return True
-
-    def compile(self):
-        if self.pkt is None and self.pkt_raw is None:
-            raise CTRexPacketBuildException(-14, "Packet is empty")
-
-        self.vm_low_level = CTRexVmEngine()
-
-        # compile the VM
-        for sc in self.vm_scripts:
-            if isinstance(sc, STLScVmRaw):
-                self._compile_raw(sc)
-
-    def get_pkt_len(self):
-        if self.pkt:
-            return len(self.pkt)
-        elif self.pkt_raw:
-            return len(self.pkt_raw)
-        else:
-            raise CTRexPacketBuildException(-14, "Packet is empty")
-
-    ####################################################
-    # private
-
-    def _get_pcap_file_path(self, pcap_file_name):
-        f_path = pcap_file_name
-        if os.path.isabs(pcap_file_name):
-            f_path = pcap_file_name
-        else:
-            if self.path_relative_to_profile:
-                p = self._get_path_relative_to_profile()  # loader
-                if p:
-                    f_path = os.path.abspath(os.path.join(os.path.dirname(p), pcap_file_name))
-
-        return f_path
-
-    def _get_path_relative_to_profile(self):
-        p = inspect.stack()
-        for obj in p:
-            if obj[3] == "get_streams":
-                return obj[1]
-        return None
-
-    def _compile_raw(self, obj):
-
-        # make sure we have varibles once
-        vars = {}
-
-        # add it add var to dit
-        for desc in obj.commands:
-            var_names = desc.get_var_name()
-
-            if var_names:
-                for var_name in var_names:
-                    if var_name in vars:
-                        raise CTRexPacketBuildException(-11, ("Variable %s defined twice ") % (var_name))
-                    else:
-                        vars[var_name] = 1
-
-        # check that all write exits
-        for desc in obj.commands:
-            var_name = desc.get_var_ref()
-            if var_name:
-                if var_name not in vars:
-                    raise CTRexPacketBuildException(-11, ("Variable %s does not exist  ") % (var_name))
-            desc.compile(self)
-
-        for desc in obj.commands:
-            self.vm_low_level.add_ins(desc.get_obj())
-
-        # set split_by_var
-        if obj.split_by_field:
-            self.vm_low_level.split_by_var = obj.split_by_field
-
-        # set cache size
-        if obj.cache_size:
-            self.vm_low_level.cache_size = obj.cache_size
-
-    # lazy packet build only on demand
-
-    def __lazy_build_packet(self):
-        # alrady built ? bail out
-        if self.is_pkt_built:
-            return
-
-        # for buffer, promote to a scapy packet
-        if self.pkt_raw:
-            self.pkt = Ether(self.pkt_raw)
-            self.pkt_raw = None
-
-        # regular scapy packet
-        elif not self.pkt:
-            # should not reach here
-            raise CTRexPacketBuildException(-11, "Empty packet")
-
-        if self.remove_fcs and self.pkt.lastlayer().name == "Padding":
-            self.pkt.lastlayer().underlayer.remove_payload()
-
-        self.pkt.build()
-        self.is_pkt_built = True
-
-    def _pkt_layer_offset(self, layer_name):
-
-        self.__lazy_build_packet()
-
-        p_utl = CTRexScapyPktUtl(self.pkt)
-        return p_utl.get_layer_offet_by_str(layer_name)
-
-    def _name_to_offset(self, field_name):
-
-        self.__lazy_build_packet()
-
-        p_utl = CTRexScapyPktUtl(self.pkt)
-        return p_utl.get_field_offet_by_str(field_name)
-
-    def _get_pkt_as_str(self):
-
-        if self.pkt:
-            return bytes(self.pkt)
-
-        if self.pkt_raw:
-            return self.pkt_raw
-
-        raise CTRexPacketBuildException(-11, "Empty packet")
-
-    def _add_tuple_gen(self, tuple_gen):
-
-        pass
-
-
-def STLIPRange(src=None, dst=None, fix_chksum=True):
-
-    vm = []
-
-    if src:
-        vm += [
-            STLVmFlowVar(name="src", min_value=src["start"], max_value=src["end"], size=4, op="inc", step=src["step"]),
-            STLVmWrFlowVar(fv_name="src", pkt_offset="IP.src"),
-        ]
-
-    if dst:
-        vm += [
-            STLVmFlowVar(name="dst", min_value=dst["start"], max_value=dst["end"], size=4, op="inc", step=dst["step"]),
-            STLVmWrFlowVar(fv_name="dst", pkt_offset="IP.dst"),
-        ]
-
-    if fix_chksum:
-        vm.append(STLVmFixIpv4(offset="IP"))
-
-    return vm
-
-
-class STLVM(STLScVmRaw):
-    """
-    Defines a VM/Field Engine object
-
-    Describes the interaction on each packet
-
-    """
-
-    def __init__(self):
-        STLScVmRaw.__init__(self)
-
-    def set_cached(self, cache_size):
-        """
-        set VM as cached with a cache size
-        """
-        self.cache_size = cache_size
-
-    def var(self, name, min_value, max_value, size, op, step=1):
-        """
-        Defines a flow variable.
-        Allocates a variable on a stream context. The size argument determines the variable size.
-        The operation can be inc, dec, and random.
-        For increment and decrement operations, can set the "step" size.
-        For all operations, can set initialization value, minimum and maximum value.
-
-        :parameters:
-             name : string
-                Name of the stream variable
-
-             min_value  : int
-                Min value
-
-             max_value  : int
-                Max value
-
-             size  : int
-                Number of bytes of the variable. Possible values: 1,2,4,8 for uint8_t,
-                uint16_t, uint32_t, uint64_t
-
-             step  : int
-                Step in case of "inc" or "dec" operations
-
-             op    : string
-                Possible values: "inc", "dec", "random"
-        """
-        self.add_cmd(STLVmFlowVar(name=name, min_value=min_value, max_value=max_value, size=size, op=op, step=step))
-
-    def write(self, fv_name, pkt_offset, offset_fixup=0, add_val=0, byte_order="big"):
-        """
-        Write a previously defined varaible to the packet.
-
-        The write position is determined by the packet offset + offset fixup.
-        The size of the write is determined by the stream variable.
-        Example: Offset 10, fixup 0, variable size 4. This function writes at 10, 11, 12, and 13.
-
-        For inromation about chaning the write size, offset, or fixup,
-        see the `STLVmWrMaskFlowVar` command.
-        The Field name/offset can be given by name in the following format: ``header[:id].field``.
-
-
-        :parameters:
-            fv_name : string
-                Stream variable to write to a packet offset.
-
-            pkt_offset : string or in
-                Name of the field or offset in bytes from packet start.
-
-            offset_fixup : int
-                Number of bytes to move forward. If negative, move backward.
-
-             add_val     : int
-                Value to add to the stream variable before writing it to the packet field.
-                Can be used as a constant offset.
-
-             is_big      : bool
-                How to write the variable to the the packet. True=big-endian, False=little-endian
-        """
-        self.add_cmd(
-            STLVmWrFlowVar(
-                fv_name=fv_name,
-                pkt_offset=pkt_offset,
-                offset_fixup=offset_fixup,
-                add_val=add_val,
-                is_big=(byte_order == "big"),
-            )
-        )
-
-    def tuple_var(self, name, ip_min, ip_max, port_min, port_max, limit_flows=0):
-        """
-        Generate a struct with two variables: ``var_name.ip`` as uint32_t
-        and ``var_name.port`` as uint16_t
-        The variables are dependent. When the ip variable value reaches its maximum,
-        the port is incremented.
-
-        For:
-
-        * ip_min      = 10.0.0.1
-        * ip_max      = 10.0.0.5
-        * port_min    = 1025
-        * port_max    = 1028
-        * limit_flows = 10
-
-        The result:
-
-        +------------+------------+-----------+
-        | ip         | port       | flow_id   |
-        +============+============+===========+
-        | 10.0.0.1   | 1025       | 1         |
-        +------------+------------+-----------+
-        | 10.0.0.2   | 1025       | 2         |
-        +------------+------------+-----------+
-        | 10.0.0.3   | 1025       | 3         |
-        +------------+------------+-----------+
-        | 10.0.0.4   | 1025       | 4         |
-        +------------+------------+-----------+
-        | 10.0.0.5   | 1025       | 5         |
-        +------------+------------+-----------+
-        | 10.0.0.1   | 1026       | 6         |
-        +------------+------------+-----------+
-        | 10.0.0.2   | 1026       | 7         |
-        +------------+------------+-----------+
-        | 10.0.0.3   | 1026       | 8         |
-        +------------+------------+-----------+
-        | 10.0.0.4   | 1026       | 9         |
-        +------------+------------+-----------+
-        | 10.0.0.5   | 1026       | 10        |
-        +------------+------------+-----------+
-        | 10.0.0.1   | 1025       | 1         |
-        +------------+------------+-----------+
-
-
-        :parameters:
-            name : string
-                Name of the stream struct.
-
-            ip_min : string or int
-                Min value of the ip value. Number or IPv4 format.
-
-            ip_max : string or int
-                Max value of the ip value. Number or IPv4 format.
-
-            port_min : int
-                Min value of port variable.
-
-            port_max : int
-                Max value of port variable.
-
-            limit_flows : int
-                Limit of number of flows.
-
-        """
-
-        self.add_cmd(
-            STLVmTupleGen(
-                name=name, ip_min=ip_min, ip_max=ip_max, port_min=port_min, port_max=port_max, limit_flows=limit_flows
-            )
-        )
-
-    def fix_chksum(self, offset="IP"):
-        """
-        Fix IPv4 header checksum. Use this if the packet header has changed and it is
-        necessary to change the checksum.
-
-        :parameters:
-             offset : uint16_t or string
-                **IPv4 header** offset from packet start. It is **not** the offset of the
-                checksum field itself.
-                in could be string in case of scapy packet. format IP[:[id]]
-        """
-        self.add_cmd(STLVmFixIpv4(offset))
-
-    def trim(self, fv_name):
-        """
-        Trim the packet size by the stream variable size. This instruction only changes the
-        total packet size, and does not repair the fields to match the new size.
-
-
-        :parameters:
-            fv_name : string
-                Stream variable name. The value of this variable is the new total packet size.
-
-        """
-        self.add_cmd(STLVmTrimPktSize(fv_name=fv_name))
-
-
-class PacketBuffer:
-    """
-    Class to be used when sending packets via push_packets.
-
-    :parameters:
-        buffer : bytes or Scapy packet
-            Packet to send
-
-        port_src : bool
-            Override src MAC with TRex port src
-
-        port_dst : bool
-            Override dst MAC with TRex port dst
-    """
-
-    def __init__(self, buffer, port_src=False, port_dst=False):
-        if isinstance(buffer, dict):
-            self.buffer = bytes(buffer)
-        else:
-            self.buffer = buffer
-        self.port_src = port_src
-        self.port_dst = port_dst
+import base64
+import copy
+import inspect
+import json
+import os
+import random
+import socket
+import struct
+
+import yaml
+from scapy.all import RawPcapReader, mac2str
+from scapy.layers.l2 import Ether
+from scapy.packet import NoPayload, Packet
+from scapy.utils import hexdump, wrpcap
+
+
+class CTRexPacketBuildException(Exception):
+    """
+    This is the general Packet Building error exception class.
+    """
+
+    def __init__(self, code, message):
+        self.code = code
+        self.message = message
+
+    def __str__(self):
+        return self.__repr__()
+
+    def __repr__(self):
+        return "[errcode:%r] %r" % (self.code, self.message)
+
+
+# ###############################################################################################
+
+
+def safe_ord(c):
+    if type(c) is str:
+        return ord(c)
+    elif type(c) is int:
+        return c
+    else:
+        raise TypeError("Cannot convert: {0} of type: {1}".format(c, type(c)))
+
+
+def _buffer_to_num(str_buffer):
+    res = 0
+    for i in str_buffer:
+        res = res << 8
+        res += safe_ord(i)
+    return res
+
+
+def ipv4_str_to_num(ipv4_buffer):
+    assert len(ipv4_buffer) == 4, "Size of ipv4_buffer is not 4"
+    return _buffer_to_num(ipv4_buffer)
+
+
+def mac_str_to_num(mac_buffer):
+    assert len(mac_buffer) == 6, "Size of mac_buffer is not 6"
+    return _buffer_to_num(mac_buffer)
+
+
+def int2mac(val):
+    mac_arr = []
+    for _ in range(6):
+        val, char = divmod(val, 256)
+        mac_arr.insert(0, "%02x" % char)
+    return ":".join(mac_arr)
+
+
+def int2ip(val):
+    ip_arr = []
+    for _ in range(4):
+        val, char = divmod(val, 256)
+        ip_arr.insert(0, "%s" % char)
+    return ".".join(ip_arr)
+
+
+def increase_mac(mac_str, val=1):
+    if ":" in mac_str:
+        mac_str = mac2str(mac_str)
+    mac_val = mac_str_to_num(mac_str)
+    return int2mac((mac_val + val) % (1 << 48))
+
+
+def increase_ip(ip_str, val=1):
+    ip_val = ipv4_str_to_num(is_valid_ipv4_ret(ip_str))
+    return int2ip((ip_val + val) % (1 << 32))
+
+
+# RFC 3513
+
+
+def generate_ipv6(mac_str, prefix="fe80"):
+    mac_arr = mac_str.split(":")
+    assert len(mac_arr) == 6, "mac should be in format of 11:22:33:44:55:66, got: %s" % mac_str
+    mac_arr[0] = "%x" % (int(mac_arr[0], 16) ^ 2)  # invert second bit
+    return "%s::%s%s:%sff:fe%s:%s%s" % tuple([prefix] + mac_arr[:3] + mac_arr[3:])
+
+
+# RFC 4291
+
+
+def generate_ipv6_solicited_node(mac_str):
+    mac_arr = mac_str.split(":")
+    assert len(mac_arr) == 6, "mac should be in format of 11:22:33:44:55:66, got: %s" % mac_str
+    return "ff02::1:ff%s:%s%s" % tuple(mac_arr[3:])
+
+
+# return full ipv6 ff02::1 -> ff02:0:0:0:0:0:0:1
+def expand_ipv6(addr):
+    addr_arr = addr.split(":")
+    if addr.startswith(":"):
+        addr_arr[0] = "0"
+    if addr.endswith(":"):
+        addr_arr[-1] = "0"
+    for i, e in enumerate(addr_arr):
+        if not e:
+            return ":".join(addr_arr[:i] + ["0"] * (9 - len(addr_arr)) + addr_arr[i + 1 :])
+    return ":".join(addr_arr)
+
+
+# return multicast mac based on ipv6 ff02::1 -> 33:33:00:00:00:01
+def multicast_mac_from_ipv6(addr):
+    addr = expand_ipv6(addr)
+    addr_arr = addr.split(":")
+    return "33:33:%02x:%02x:%02x:%02x" % (divmod(int(addr_arr[-2], 16), 256) + divmod(int(addr_arr[-1], 16), 256))
+
+
+def is_valid_ipv4_ret(ip_addr):
+    """
+    Return buffer in network order
+    """
+    if type(ip_addr) == bytes and len(ip_addr) == 4:
+        return ip_addr
+
+    if type(ip_addr) == int:
+        ip_addr = socket.inet_ntoa(struct.pack("!I", ip_addr))
+
+    try:
+        return socket.inet_pton(socket.AF_INET, ip_addr)
+    except AttributeError:  # no inet_pton here, sorry
+        return socket.inet_aton(ip_addr)
+    except socket.error:  # not a valid address
+        raise CTRexPacketBuildException(-10, "Not valid ipv4 format")
+
+
+def is_valid_ipv6_ret(ipv6_addr):
+    """
+    Return buffer in network order
+    """
+    if type(ipv6_addr) == bytes and len(ipv6_addr) == 16:
+        return ipv6_addr
+    try:
+        return socket.inet_pton(socket.AF_INET6, ipv6_addr)
+    except AttributeError:  # no inet_pton here, sorry
+        raise CTRexPacketBuildException(-10, "No inet_pton function available")
+    except Exception:
+        raise CTRexPacketBuildException(-10, "Not valid ipv6 format")
+
+
+class CTRexScriptsBase(object):
+    """
+    VM Script base class
+    """
+
+    def clone(self):
+        return copy.deepcopy(self)
+
+
+class CTRexScFieldRangeBase(CTRexScriptsBase):
+
+    FILED_TYPES = ["inc", "dec", "rand"]
+
+    def __init__(self, field_name, field_type):
+        super(CTRexScFieldRangeBase, self).__init__()
+        self.field_name = field_name
+        self.field_type = field_type
+        if self.field_type not in CTRexScFieldRangeBase.FILED_TYPES:
+            raise CTRexPacketBuildException(-12, "Field type should be in %s" % CTRexScFieldRangeBase.FILED_TYPES)
+
+
+class CTRexScFieldRangeValue(CTRexScFieldRangeBase):
+    """
+    Range of field values
+    """
+
+    def __init__(self, field_name, field_type, min_value, max_value):
+        super(CTRexScFieldRangeValue, self).__init__(field_name, field_type)
+        self.min_value = min_value
+        self.max_value = max_value
+        if min_value > max_value:
+            raise CTRexPacketBuildException(-12, "Invalid range: min is greater than max.")
+        if min_value == max_value:
+            raise CTRexPacketBuildException(-13, "Invalid range: min value is equal to max value.")
+
+
+class CTRexScIpv4SimpleRange(CTRexScFieldRangeBase):
+    """
+    Range of ipv4 ip
+    """
+
+    def __init__(self, field_name, field_type, min_ip, max_ip):
+        super(CTRexScIpv4SimpleRange, self).__init__(field_name, field_type)
+        self.min_ip = min_ip
+        self.max_ip = max_ip
+        mmin = ipv4_str_to_num(is_valid_ipv4_ret(min_ip))
+        mmax = ipv4_str_to_num(is_valid_ipv4_ret(max_ip))
+        if mmin > mmax:
+            raise CTRexPacketBuildException(-11, "CTRexScIpv4SimpleRange m_min ip is bigger than max")
+
+
+class CTRexScIpv4TupleGen(CTRexScriptsBase):
+    """
+    Range tuple
+    """
+
+    FLAGS_ULIMIT_FLOWS = 1
+
+    def __init__(self, min_ipv4, max_ipv4, num_flows=100000, min_port=1025, max_port=65535, flags=0):
+        super(CTRexScIpv4TupleGen, self).__init__()
+        self.min_ip = min_ipv4
+        self.max_ip = max_ipv4
+        mmin = ipv4_str_to_num(is_valid_ipv4_ret(min_ipv4))
+        mmax = ipv4_str_to_num(is_valid_ipv4_ret(max_ipv4))
+        if mmin > mmax:
+            raise CTRexPacketBuildException(-11, "CTRexScIpv4SimpleRange m_min ip is bigger than max")
+
+        self.num_flows = num_flows
+
+        self.min_port = min_port
+        self.max_port = max_port
+        self.flags = flags
+
+
+class CTRexScTrimPacketSize(CTRexScriptsBase):
+    """
+    Trim packet size. Field type is CTRexScFieldRangeBase.FILED_TYPES = ["inc","dec","rand"]
+    """
+
+    def __init__(self, field_type="rand", min_pkt_size=None, max_pkt_size=None):
+        super(CTRexScTrimPacketSize, self).__init__()
+        self.field_type = field_type
+        self.min_pkt_size = min_pkt_size
+        self.max_pkt_size = max_pkt_size
+        if max_pkt_size is not None and min_pkt_size is not None:
+            if min_pkt_size == max_pkt_size:
+                raise CTRexPacketBuildException(-11, "CTRexScTrimPacketSize min_pkt_size is the same as max_pkt_size ")
+
+            if min_pkt_size > max_pkt_size:
+                raise CTRexPacketBuildException(-11, "CTRexScTrimPacketSize min_pkt_size is bigger than max_pkt_size ")
+
+
+class STLScVmRaw(CTRexScriptsBase):
+    """
+    Raw instructions
+    """
+
+    def __init__(self, list_of_commands=None, split_by_field=None, cache_size=None):
+        """
+        Include a list of a basic instructions objects.
+
+        :parameters:
+             list_of_commands : list
+                list of instructions
+
+             split_by_field : string
+                by which field to split to threads
+
+             cache_size     : uint16_t
+                In case it is bigger than zero, FE results will be cached
+                - this will speedup of the program at the cost of limiting the
+                number of possible packets to the number of cache.
+                The cache size is limited to the pool size
+
+        The following example splits the generated traffic by "ip_src" variable.
+
+        .. code-block:: python
+
+            # Split by
+
+            # TCP SYN
+            base_pkt  = Ether()/IP(dst="48.0.0.1")/TCP(dport=80,flags="S")
+
+
+            # vm
+            vm = STLScVmRaw( [ STLVmFlowVar(name="ip_src",
+                                                  min_value="16.0.0.0",
+                                                  max_value="16.0.0.254",
+                                                  size=4, op="inc"),
+
+
+                               STLVmWrFlowVar(fv_name="ip_src", pkt_offset= "IP.src" ),
+
+                               STLVmFixIpv4(offset = "IP"), # fix checksum
+                              ]
+                             ,split_by_field = "ip_src",
+                             cache_size = 1000
+                           )
+
+        """
+
+        super(STLScVmRaw, self).__init__()
+        self.split_by_field = split_by_field
+        self.cache_size = cache_size
+
+        if list_of_commands is None:
+            self.commands = []
+        else:
+            self.commands = list_of_commands
+
+    def add_cmd(self, cmd):
+        self.commands.append(cmd)
+
+
+################################################################################################
+# VM raw instructions
+################################################################################################
+
+
+class CTRexVmInsBase(object):
+    """
+    Instruction base
+    """
+
+    def __init__(self, ins_type):
+        self.type = ins_type
+
+
+class CTRexVmInsFixIpv4(CTRexVmInsBase):
+    def __init__(self, offset):
+        super(CTRexVmInsFixIpv4, self).__init__("fix_checksum_ipv4")
+        self.pkt_offset = offset
+
+
+class CTRexVmInsFixHwCs(CTRexVmInsBase):
+    L4_TYPE_UDP = 11
+    L4_TYPE_TCP = 13
+
+    def __init__(self, l2_len, l3_len, l4_type):
+        super(CTRexVmInsFixHwCs, self).__init__("fix_checksum_hw")
+        self.l2_len = l2_len
+        self.l3_len = l3_len
+        self.l4_type = l4_type
+
+
+class CTRexVmInsFlowVar(CTRexVmInsBase):
+    # TBD add more validation tests
+
+    OPERATIONS = ["inc", "dec", "random"]
+    VALID_SIZES = [1, 2, 4, 8]
+
+    def __init__(self, fv_name, size, op, init_value, min_value, max_value, step):
+        super(CTRexVmInsFlowVar, self).__init__("flow_var")
+        self.name = fv_name
+        self.size = size
+        self.op = op
+        self.init_value = init_value
+        assert init_value >= 0, "init_value(%s) is negative" % init_value
+        self.min_value = min_value
+        assert min_value >= 0, "min_value(%s) is negative" % min_value
+        self.max_value = max_value
+        assert max_value >= 0, "max_value(%s) is negative" % max_value
+        self.step = step
+        assert step >= 0, "step(%s) is negative" % step
+
+
+class CTRexVmInsFlowVarRandLimit(CTRexVmInsBase):
+    # TBD add more validation tests
+
+    VALID_SIZES = [1, 2, 4, 8]
+
+    def __init__(self, fv_name, size, limit, seed, min_value, max_value):
+        super(CTRexVmInsFlowVarRandLimit, self).__init__("flow_var_rand_limit")
+        self.name = fv_name
+        self.size = size
+        self.limit = limit
+        assert limit >= 0, "limit(%s) is negative" % limit
+        self.seed = seed
+        self.min_value = min_value
+        assert min_value >= 0, "min_value(%s) is negative" % min_value
+        self.max_value = max_value
+        assert max_value >= 0, "max_value(%s) is negative" % max_value
+
+
+class CTRexVmInsWrFlowVar(CTRexVmInsBase):
+    def __init__(self, fv_name, pkt_offset, add_value=0, is_big_endian=True):
+        super(CTRexVmInsWrFlowVar, self).__init__("write_flow_var")
+        self.name = fv_name
+        self.pkt_offset = pkt_offset
+        self.add_value = add_value
+        self.is_big_endian = is_big_endian
+
+
+class CTRexVmInsWrMaskFlowVar(CTRexVmInsBase):
+    def __init__(self, fv_name, pkt_offset, pkt_cast_size, mask, shift, add_value, is_big_endian=True):
+        super(CTRexVmInsWrMaskFlowVar, self).__init__("write_mask_flow_var")
+        self.name = fv_name
+        self.pkt_offset = pkt_offset
+        self.pkt_cast_size = pkt_cast_size
+        self.mask = mask
+        self.shift = shift
+        self.add_value = add_value
+        self.is_big_endian = is_big_endian
+
+
+class CTRexVmInsTrimPktSize(CTRexVmInsBase):
+    def __init__(self, fv_name):
+        super(CTRexVmInsTrimPktSize, self).__init__("trim_pkt_size")
+        self.name = fv_name
+
+
+class CTRexVmInsTupleGen(CTRexVmInsBase):
+    def __init__(self, fv_name, ip_min, ip_max, port_min, port_max, limit_flows, flags=0):
+        super(CTRexVmInsTupleGen, self).__init__("tuple_flow_var")
+        self.name = fv_name
+        self.ip_min = ip_min
+        self.ip_max = ip_max
+        self.port_min = port_min
+        self.port_max = port_max
+        self.limit_flows = limit_flows
+        self.flags = flags
+
+
+# ###############################################################################################
+#
+class CTRexVmEngine(object):
+    def __init__(self):
+        """
+        Inlcude list of instructions.
+        """
+        super(CTRexVmEngine, self).__init__()
+        self.ins = []
+        self.split_by_var = ""
+        self.cache_size = 0
+
+    # return as json
+
+    def get_json(self):
+        inst_array = []
+        # dump it as dict
+        for obj in self.ins:
+            inst_array.append(obj.__dict__)
+
+        d = {"instructions": inst_array, "split_by_var": self.split_by_var}
+        if self.cache_size > 0:
+            d["cache"] = self.cache_size
+        return d
+
+    def add_ins(self, ins):
+        # assert issubclass(ins, CTRexVmInsBase)
+        self.ins.append(ins)
+
+    def dump(self):
+        cnt = 0
+        for obj in self.ins:
+            print(("ins", cnt))
+            cnt = cnt + 1
+            print((obj.__dict__))
+
+    def dump_bjson(self):
+        print((json.dumps(self.get_json(), sort_keys=True, indent=4)))
+
+    def dump_as_yaml(self):
+        print((yaml.dump(self.get_json(), default_flow_style=False)))
+
+
+# ###############################################################################################
+
+
+class CTRexScapyPktUtl(object):
+    def __init__(self, scapy_pkt):
+        self.pkt = scapy_pkt
+
+    def pkt_iter(self):
+        p = self.pkt
+        while True:
+            yield p
+            p = p.payload
+            if p is None or isinstance(p, NoPayload):
+                break
+
+    def get_list_iter(self):
+        list_iter = list(self.pkt_iter())
+        return list_iter
+
+    def get_pkt_layers(self):
+        """
+        Return string 'IP:UDP:TCP'
+        """
+        list_iter = self.get_list_iter()
+        l1 = [p.name for p in list_iter]
+        return ":".join(l1)
+
+    def _layer_offset(self, name, cnt=0):
+        """
+        Return offset of layer. Example: 'IP',1 returns offfset of layer ip:1
+        """
+        save_cnt = cnt
+        for pkt in self.pkt_iter():
+            if name in (pkt.name, pkt.__class__.__name__):
+                if cnt == 0:
+                    return (pkt, pkt._offset)
+                else:
+                    cnt = cnt - 1
+
+        raise CTRexPacketBuildException(-11, ("no layer %s-%d" % (name, save_cnt)))
+
+    def layer_offset(self, name, cnt=0):
+        """
+        Return offset of layer. Example: 'IP',1 returns offfset of layer ip:1
+        """
+        save_cnt = cnt
+        for pkt in self.pkt_iter():
+            if pkt.name == name:
+                if cnt == 0:
+                    return pkt._offset
+                else:
+                    cnt = cnt - 1
+
+        raise CTRexPacketBuildException(-11, ("no layer %s-%d" % (name, save_cnt)))
+
+    def get_field_offet(self, layer, layer_cnt, field_name):
+        """
+        Return offset of layer. Example: 'IP',1 returns offfset of layer ip:1
+        """
+        t = self._layer_offset(layer, layer_cnt)
+        l_offset = t[1]
+        layer_pkt = t[0]
+        # layer_pkt.dump_fields_offsets()
+
+        for f in layer_pkt.fields_desc:
+            if f.name == field_name:
+                return (l_offset + f._offset, f.get_size_bytes())
+
+        raise CTRexPacketBuildException(-11, "No layer %s-%d." % (field_name, layer_cnt))
+
+    def get_layer_offet_by_str(self, layer_des):
+        """
+         Return layer offset by string.
+
+        :parameters:
+
+         IP:0
+         IP:1
+         return offset
+
+
+        """
+        l1 = layer_des.split(":")
+        layer = ""
+        layer_cnt = 0
+
+        if len(l1) == 1:
+            layer = l1[0]
+        else:
+            layer = l1[0]
+            layer_cnt = int(l1[1])
+
+        return self.layer_offset(layer, layer_cnt)
+
+    def get_field_offet_by_str(self, field_des):
+        """
+        Return field_des(offset,size) layer:cnt.field
+        Example:
+        802|1Q.vlan get 802.1Q->valn replace | with .
+        IP.src
+        IP:0.src (first IP.src like IP.src)
+        Example: IP:1.src  for internal IP
+
+        Return(offset, size) as tuple.
+
+
+        """
+
+        s = field_des.split(".")
+        if len(s) != 2:
+            raise CTRexPacketBuildException(-11, ("Field desription should be layer:cnt.field Example: IP.src or IP:1.src"))
+
+        layer_ex = s[0].replace("|", ".")
+        field = s[1]
+
+        l1 = layer_ex.split(":")
+        layer = ""
+        layer_cnt = 0
+
+        if len(l1) == 1:
+            layer = l1[0]
+        else:
+            layer = l1[0]
+            layer_cnt = int(l1[1])
+
+        return self.get_field_offet(layer, layer_cnt, field)
+
+    def has_IPv4(self):
+        return self.pkt.has_layer("IP")
+
+    def has_IPv6(self):
+        return self.pkt.has_layer("IPv6")
+
+    def has_UDP(self):
+        return self.pkt.has_layer("UDP")
+
+
+################################################################################################
+
+
+class CTRexVmDescBase(object):
+    """
+    Instruction base
+    """
+
+    def __init__(self):
+        pass
+
+    def get_obj(self):
+        return self
+
+    def get_json(self):
+        return self.get_obj().__dict__
+
+    def dump_bjson(self):
+        print((json.dumps(self.get_json(), sort_keys=True, indent=4)))
+
+    def dump_as_yaml(self):
+        print((yaml.dump(self.get_json(), default_flow_style=False)))
+
+    def get_var_ref(self):
+        """
+        Virtual function returns a ref var name.
+        """
+        return None
+
+    def get_var_name(self):
+        """
+        Virtual function returns the varible name if it exists.
+        """
+        return None
+
+    def compile(self, parent):
+        """
+        Virtual function to take parent that has function name_to_offset.
+        """
+        pass
+
+
+def valid_fv_size(size):
+    if not (size in CTRexVmInsFlowVar.VALID_SIZES):
+        raise CTRexPacketBuildException(-11, ("Flow var has invalid size %d ") % size)
+
+
+def valid_fv_ops(op):
+    if not (op in CTRexVmInsFlowVar.OPERATIONS):
+        raise CTRexPacketBuildException(-11, ("Flow var has invalid op %s ") % op)
+
+
+def get_max_by_size(size):
+    d = {1: ((1 << 8) - 1), 2: ((1 << 16) - 1), 4: ((1 << 32) - 1), 8: 0xFFFFFFFFFFFFFFFF}
+    return d[size]
+
+
+def convert_val(val):
+    if type(val) is int:
+        return val
+    if type(val) == str:
+        return ipv4_str_to_num(is_valid_ipv4_ret(val))
+    raise CTRexPacketBuildException(-11, ("init val invalid %s ") % val)
+
+
+class STLVmFlowVar(CTRexVmDescBase):
+    def __init__(self, name, init_value=None, min_value=0, max_value=255, size=4, step=1, op="inc"):
+        """
+        Flow variable instruction. Allocates a variable on a stream context.
+        The size argument determines the variable size.
+        The operation can be inc, dec, and random.
+        For increment and decrement operations, can set the "step" size.
+        For all operations, can set initialization value, minimum and maximum value.
+
+        :parameters:
+             name : string
+                Name of the stream variable
+
+             init_value : int
+                Init value of the variable. If not specified, it will be min_value
+
+             min_value  : int
+                Min value
+
+             max_value  : int
+                Max value
+
+             size  : int
+                Number of bytes of the variable. Possible values: 1,2,4,8 for uint8_t,
+                uint16_t, uint32_t, uint64_t
+
+             step  : int
+                Step in case of "inc" or "dec" operations
+
+             op    : string
+                Possible values: "inc", "dec", "random"
+
+        .. code-block:: python
+
+            # Example1
+
+            # input
+            STLVmFlowVar(min_value=0, max_value=3, size=1,op="inc")
+
+            # output 0,1,2,3,0,1,2,3 ..
+
+            # input
+            STLVmFlowVar(min_value=0, max_value=3, size=1,op="dec")
+
+            # output 0,3,2,1,0,3,2,1 ..
+
+
+            # input
+            STLVmFlowVar(min_value=0, max_value=3, size=1,op="random")
+
+            # output 1,1,2,3,1,2,1,0 ..
+
+            # input
+            STLVmFlowVar(min_value=0, max_value=10, size=1,op="inc",step=3)
+
+            # output 0,3,6,9,0,3,6,9,0..
+
+
+        """
+        super(STLVmFlowVar, self).__init__()
+        self.name = name
+        self.size = size
+        valid_fv_size(size)
+        self.op = op
+        valid_fv_ops(op)
+
+        # choose default value for init val
+        if init_value is None:
+            init_value = max_value if op == "dec" else min_value
+
+        self.init_value = convert_val(init_value)
+        self.min_value = convert_val(min_value)
+        self.max_value = convert_val(max_value)
+        self.step = convert_val(step)
+
+        if self.min_value > self.max_value:
+            raise CTRexPacketBuildException(-11, ("max %d is lower than min %d ") % (self.max_value, self.min_value))
+
+    def get_obj(self):
+        return CTRexVmInsFlowVar(self.name, self.size, self.op, self.init_value, self.min_value, self.max_value, self.step)
+
+    def get_var_name(self):
+        return [self.name]
+
+
+class STLVmFlowVarRepeatableRandom(CTRexVmDescBase):
+    def __init__(self, name, size=4, limit=100, seed=None, min_value=0, max_value=None):
+        """
+        Flow variable instruction for repeatable random with limit number of generating numbers.
+        Allocates memory on a stream context.
+        The size argument determines the variable size. Could be 1,2,4 or 8
+
+        1. The maximum number of distinct values will  'limit'. There could be a case of repetition
+        2. The values will be repeated  after 'limit' number of values.
+
+        :parameters:
+             name : string
+                Name of the stream variable
+
+             size  : int
+                Number of bytes of the variable. Possible values: 1,2,4,8 for uint8_t,
+                uint16_t, uint32_t, uint64_t
+
+             limit  : int
+                The number of distinct repetable random number
+
+             seed   : int
+                For deterministic result, you can set this to a uint16_t number
+
+             min_value  : int
+                Min value
+
+             max_value  : int
+                Max value
+
+
+        .. code-block:: python
+
+            # Example1
+
+            # input , 1 byte or random with limit of 5
+            STLVmFlowVarRepeatableRandom("var1",size=1,limit=5)
+
+            # output 255,1,7,129,8, ==> repeat 255,1,7,129,8
+
+            STLVmFlowVarRepeatableRandom("var1",size=4,limit=100,min_value=0x12345678,
+                                         max_value=0x32345678)
+
+
+        """
+        super(STLVmFlowVarRepeatableRandom, self).__init__()
+        self.name = name
+        self.size = size
+        valid_fv_size(size)
+        self.limit = limit
+
+        if seed is None:
+            self.seed = random.randint(1, 32000)
+        else:
+            self.seed = seed
+
+        self.min_value = convert_val(min_value)
+
+        if max_value is None:
+            self.max_value = get_max_by_size(self.size)
+        else:
+            self.max_value = convert_val(max_value)
+
+        if self.min_value > self.max_value:
+            raise CTRexPacketBuildException(-11, ("max %d is lower than min %d ") % (self.max_value, self.min_value))
+
+    def get_obj(self):
+        return CTRexVmInsFlowVarRandLimit(self.name, self.size, self.limit, self.seed, self.min_value, self.max_value)
+
+    def get_var_name(self):
+        return [self.name]
+
+
+class STLVmFlowVarRepetableRandom(STLVmFlowVarRepeatableRandom):
+    def __init__(self, name, size=4, limit=100, seed=None, min_value=0, max_value=None):
+        super(STLVmFlowVarRepetableRandom, self).__init__(name, size, limit, seed, min_value, max_value)
+
+    def get_obj(self):
+        return CTRexVmInsFlowVarRandLimit(self.name, self.size, self.limit, self.seed, self.min_value, self.max_value)
+
+    def get_var_name(self):
+        return [self.name]
+
+
+class STLVmFixChecksumHw(CTRexVmDescBase):
+    def __init__(self, l3_offset, l4_offset, l4_type):
+        """
+        Fix Ipv4 header checksum and TCP/UDP checksum using hardware assist.
+        Use this if the packet header has changed or data payload has changed as it
+        is necessary to fix the checksums.
+        This instruction works on NICS that support this hardware offload.
+
+        For fixing only IPv4 header checksum use STLVmFixIpv4. This instruction should be used
+        if both L4 and L3 need to be fixed.
+
+        example for supported packets
+
+        Ether()/(IPv4|IPv6)/(UDP|TCP)
+        Ether()/(IPv4|IPv6)/(UDP|TCP)
+        SomeTunnel()/(IPv4|IPv6)/(UDP|TCP)
+        SomeTunnel()/(IPv4|IPv6)/(UDP|TCP)
+
+
+        :parameters:
+             l3_offset : offset in bytes
+                **IPv4/IPv6 header** offset from packet start. It is **not** the offset of
+                the checksum field itself.
+                in could be string in case of scapy packet. format IP[:[id]]
+
+             l4_offset : offset in bytes to UDP/TCP header
+
+             l4_type   : CTRexVmInsFixHwCs.L4_TYPE_UDP or CTRexVmInsFixHwCs.L4_TYPE_TCP
+
+             see full example stl/syn_attack_fix_cs_hw.py
+
+        .. code-block:: python
+
+            # Example2
+
+            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
+
+            # by offset
+            STLVmFixChecksumHw(l3_offset=14,l4_offset=14+20,l4_type=CTRexVmInsFixHwCs.L4_TYPE_UDP)
+
+            # in case of scapy packet can be defined by header name
+            STLVmFixChecksumHw(l3_offset="IP",l4_offset="UDP",l4_type=CTRexVmInsFixHwCs.L4_TYPE_UDP)
+
+            # string for second "IP" header in the packet is IP:1
+            STLVmFixChecksumHw(offset="IP:1")
+
+        """
+
+        super(STLVmFixChecksumHw, self).__init__()
+        self.l3_offset = l3_offset  # could be a name of offset
+        self.l4_offset = l4_offset  # could be a name of offset
+        self.l4_type = l4_type
+        self.l2_len = 0
+
+    def get_obj(self):
+        return CTRexVmInsFixHwCs(self.l2_len, self.l3_len, self.l4_type)
+
+    def compile(self, parent):
+        if type(self.l3_offset) == str:
+            self.l2_len = parent._pkt_layer_offset(self.l3_offset)
+        if type(self.l4_offset) == str:
+            self.l4_offset = parent._pkt_layer_offset(self.l4_offset)
+
+        assert self.l4_offset >= self.l2_len + 8, "l4_offset should be higher than l3_offset offset"
+        self.l3_len = self.l4_offset - self.l2_len
+
+
+class STLVmFixIpv4(CTRexVmDescBase):
+    def __init__(self, offset):
+        """
+        Fix IPv4 header checksum. Use this if the packet header has changed and it is
+        necessary to change the checksum.
+
+        :parameters:
+             offset : uint16_t or string
+                **IPv4 header** offset from packet start. It is **not** the offset of the
+                checksum field itself.
+                in could be string in case of scapy packet. format IP[:[id]]
+
+        .. code-block:: python
+
+            # Example2
+
+            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
+
+            # by offset
+            STLVmFixIpv4(offset=14)
+
+            # in case of scapy packet can be defined by header name
+            STLVmFixIpv4(offset="IP")
+
+            # string for second "IP" header in the packet is IP:1
+            STLVmFixIpv4(offset="IP:1")
+
+        """
+
+        super(STLVmFixIpv4, self).__init__()
+        self.offset = offset  # could be a name of offset
+
+    def get_obj(self):
+        return CTRexVmInsFixIpv4(self.offset)
+
+    def compile(self, parent):
+        if type(self.offset) == str:
+            self.offset = parent._pkt_layer_offset(self.offset)
+
+
+class STLVmWrFlowVar(CTRexVmDescBase):
+    def __init__(self, fv_name, pkt_offset, offset_fixup=0, add_val=0, is_big=True):
+        """
+        Write a stream variable into a packet field.
+        The write position is determined by the packet offset + offset fixup.
+        The size of the write is determined by the stream variable.
+        Example: Offset 10, fixup 0, variable size 4. This function writes at 10, 11, 12, and 13.
+
+        For inromation about chaning the write size, offset, or fixup, see
+        the `STLVmWrMaskFlowVar` command.
+        The Field name/offset can be given by name in the following format: ``header[:id].field``.
+
+
+        :parameters:
+            fv_name : string
+                Stream variable to write to a packet offset.
+
+            pkt_offset : string or in
+                Name of the field or offset in bytes from packet start.
+
+            offset_fixup : int
+                Number of bytes to move forward. If negative, move backward.
+
+             add_val     : int
+                Value to add to the stream variable before writing it to the packet field.
+                Can be used as a constant offset.
+
+             is_big      : bool
+                How to write the variable to the the packet. True=big-endian, False=little-endian
+
+        .. code-block:: python
+
+            # Example3
+
+            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
+
+
+            # write to ip.src offset
+            STLVmWrFlowVar(fv_name="tuple", pkt_offset= "IP.src" )
+
+            # packet offset is varible
+            STLVmWrFlowVar(fv_name="tuple", pkt_offset= 26 )
+
+            # add l3_len_fix before writing fv_rand into IP.len field
+            STLVmWrFlowVar(fv_name="fv_rand", pkt_offset= "IP.len", add_val=l3_len_fix)
+
+        """
+
+        super(STLVmWrFlowVar, self).__init__()
+        self.name = fv_name
+        self.offset_fixup = offset_fixup
+        self.pkt_offset = pkt_offset
+        self.add_val = add_val
+        self.is_big = is_big
+
+    def get_var_ref(self):
+        return self.name
+
+    def get_obj(self):
+        return CTRexVmInsWrFlowVar(self.name, self.pkt_offset + self.offset_fixup, self.add_val, self.is_big)
+
+    def compile(self, parent):
+        if type(self.pkt_offset) == str:
+            t = parent._name_to_offset(self.pkt_offset)
+            self.pkt_offset = t[0]
+
+
+class STLVmWrMaskFlowVar(CTRexVmDescBase):
+    def __init__(self, fv_name, pkt_offset, pkt_cast_size=1, mask=0xFF, shift=0, add_value=0, offset_fixup=0, is_big=True):
+        """
+        Write a stream variable into a packet field with some operations.
+        Using this instruction, the variable size and the field can have different sizes.
+
+        Pseudocode of this code::
+
+                uint32_t val=(cast_to_size)rd_from_variable("name") # read flow-var
+                val+=m_add_value                                    # add value
+
+                if(m_shift>0) {                                    # shift
+                    val=val<<m_shift
+                }else{
+                    if(m_shift<0) {
+                        val=val>>(-m_shift)
+                    }
+                }
+
+                pkt_val=rd_from_pkt(pkt_offset)                     # RMW to the packet
+                pkt_val =(pkt_val & ~m_mask) |(val & m_mask)
+                wr_to_pkt(pkt_offset,pkt_val)
+
+
+        :parameters:
+            fv_name : string
+                The stream variable name to write to a packet field
+
+            pkt_cast_size : uint8_t
+                The size in bytes of the packet field
+
+
+            mask          : uint32_t
+                The mask of the field. 1 means to write. 0 don't care
+
+            shift          : uint8_t
+                How many bits to shift
+
+            pkt_offset : string or in
+                the name of the field or offset in byte from packet start.
+
+            offset_fixup : int
+                how many bytes to go forward. In case of a negative value go backward
+
+             add_val     : int
+                value to add to stream variable before writing it to packet field.
+                can be used as a constant offset
+
+             is_big      : bool
+                how to write the variable to the the packet. is it big-edian or little edian
+
+        Example 1 - Cast from uint16_t(var) to uint8_t(pkt)::
+
+
+            base_pkt =  Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
+
+            vm = STLScVmRaw( [ STLVmFlowVar(name="mac_src",
+                                            min_value=1,
+                                            max_value=30,
+                                            size=2,
+                                            op="dec",step=1),
+                               STLVmWrMaskFlowVar(fv_name="mac_src",
+                                                  pkt_offset= 11,
+                                                  pkt_cast_size=1,
+                                                  mask=0xff) # mask command ->write it as one byte
+                          ]
+                       )
+
+            pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
+
+        Example 2 - Change MSB of uint16_t variable::
+
+
+            vm = STLScVmRaw( [ STLVmFlowVar(name="mac_src",
+                                            min_value=1,
+                                            max_value=30,
+                                            size=2, op="dec",step=1),
+                               STLVmWrMaskFlowVar(fv_name="mac_src",
+                                                  pkt_offset= 10,
+                                                  pkt_cast_size=2,
+                                                  mask=0xff00,
+                                                  shift=8) # take the var shift it 8(x256)
+                                                  write only to LSB
+                              ]
+                            )
+
+
+
+        Example 3 - Every 2 packets, change the MAC(shift right)::
+
+                vm = STLScVmRaw( [ STLVmFlowVar(name="mac_src",
+                                                min_value=1,
+                                                max_value=30,
+                                                size=2, op="dec",step=1),
+                                   STLVmWrMaskFlowVar(fv_name="mac_src",
+                                                      pkt_offset= 10,
+                                                      pkt_cast_size=1,
+                                                      mask=0x1,
+                                                      shift=-1) # take var mac_src>>1 and write
+                                                      the LSB every two packet there should be
+                                                      a change
+                                 ]
+                                )
+
+
+        """
+
+        super(STLVmWrMaskFlowVar, self).__init__()
+        self.name = fv_name
+        self.offset_fixup = offset_fixup
+        self.pkt_offset = pkt_offset
+        self.pkt_cast_size = pkt_cast_size
+        if not (pkt_cast_size in [1, 2, 4]):
+            raise CTRexPacketBuildException(-10, "not valid cast size")
+
+        self.mask = mask
+        self.shift = shift
+        self.add_value = add_value
+
+        self.is_big = is_big
+
+    def get_var_ref(self):
+        return self.name
+
+    def get_obj(self):
+        return CTRexVmInsWrMaskFlowVar(
+            self.name,
+            self.pkt_offset + self.offset_fixup,
+            self.pkt_cast_size,
+            self.mask,
+            self.shift,
+            self.add_value,
+            self.is_big,
+        )
+
+    def compile(self, parent):
+        if type(self.pkt_offset) == str:
+            t = parent._name_to_offset(self.pkt_offset)
+            self.pkt_offset = t[0]
+
+
+class STLVmTrimPktSize(CTRexVmDescBase):
+    def __init__(self, fv_name):
+        """
+        Trim the packet size by the stream variable size. This instruction only
+        changes the total packet size, and does not repair the fields to match the new size.
+
+
+        :parameters:
+            fv_name : string
+                Stream variable name. The value of this variable is the new total packet size.
+
+
+        For Example::
+
+            def create_stream(self):
+                # pkt
+                p_l2  = Ether();
+                p_l3  = IP(src="16.0.0.1",dst="48.0.0.1")
+                p_l4  = UDP(dport=12,sport=1025)
+                pyld_size = max(0, self.max_pkt_size_l3 - len(p_l3/p_l4));
+                base_pkt = p_l2/p_l3/p_l4/('\x55'*(pyld_size))
+
+                l3_len_fix =-(len(p_l2));
+                l4_len_fix =-(len(p_l2/p_l3));
+
+
+                # vm
+                vm = STLScVmRaw( [ STLVmFlowVar(name="fv_rand", min_value=64,
+                                                max_value=len(base_pkt),
+                                                size=2, op="inc"),
+                                   # change total packet size <<<
+                                   STLVmTrimPktSize("fv_rand"),
+
+                                   STLVmWrFlowVar(fv_name="fv_rand",
+                                                  pkt_offset= "IP.len",
+                                                  add_val=l3_len_fix), # fix ip len
+                                   # fix checksum
+                                   STLVmFixIpv4(offset = "IP"),
+
+                                   STLVmWrFlowVar(fv_name="fv_rand",
+                                                  pkt_offset= "UDP.len",
+                                                  add_val=l4_len_fix) # fix udp len
+                                  ]
+                               )
+
+                pkt = STLPktBuilder(pkt = base_pkt,
+                                    vm = vm)
+
+                return STLStream(packet = pkt,
+                                 mode = STLTXCont())
+
+
+        """
+
+        super(STLVmTrimPktSize, self).__init__()
+        self.name = fv_name
+
+    def get_var_ref(self):
+        return self.name
+
+    def get_obj(self):
+        return CTRexVmInsTrimPktSize(self.name)
+
+
+class STLVmTupleGen(CTRexVmDescBase):
+    def __init__(self, name, ip_min="0.0.0.1", ip_max="0.0.0.10", port_min=1025, port_max=65535, limit_flows=100000, flags=0):
+        """
+        Generate a struct with two variables: ``var_name.ip`` as uint32_t
+        and ``var_name.port`` as uint16_t
+        The variables are dependent. When the ip variable value reaches its maximum,
+        the port is incremented.
+
+        For:
+
+        * ip_min      = 10.0.0.1
+        * ip_max      = 10.0.0.5
+        * port_min    = 1025
+        * port_max    = 1028
+        * limit_flows = 10
+
+        The result:
+
+        +------------+------------+-----------+
+        | ip         | port       | flow_id   |
+        +============+============+===========+
+        | 10.0.0.1   | 1025       | 1         |
+        +------------+------------+-----------+
+        | 10.0.0.2   | 1025       | 2         |
+        +------------+------------+-----------+
+        | 10.0.0.3   | 1025       | 3         |
+        +------------+------------+-----------+
+        | 10.0.0.4   | 1025       | 4         |
+        +------------+------------+-----------+
+        | 10.0.0.5   | 1025       | 5         |
+        +------------+------------+-----------+
+        | 10.0.0.1   | 1026       | 6         |
+        +------------+------------+-----------+
+        | 10.0.0.2   | 1026       | 7         |
+        +------------+------------+-----------+
+        | 10.0.0.3   | 1026       | 8         |
+        +------------+------------+-----------+
+        | 10.0.0.4   | 1026       | 9         |
+        +------------+------------+-----------+
+        | 10.0.0.5   | 1026       | 10        |
+        +------------+------------+-----------+
+        | 10.0.0.1   | 1025       | 1         |
+        +------------+------------+-----------+
+
+
+        :parameters:
+            name : string
+                Name of the stream struct.
+
+            ip_min : string or int
+                Min value of the ip value. Number or IPv4 format.
+
+            ip_max : string or int
+                Max value of the ip value. Number or IPv4 format.
+
+            port_min : int
+                Min value of port variable.
+
+            port_max : int
+                Max value of port variable.
+
+            limit_flows : int
+                Limit of number of flows.
+
+            flags       : 0
+
+            ="0.0.0.10", port_min=1025, port_max=65535, limit_flows=100000, flags=0
+
+        .. code-block:: python
+
+            # Example5
+
+            def create_stream(self):
+                # pkt
+                p_l2  = Ether();
+                p_l3  = IP(src="16.0.0.1",dst="48.0.0.1")
+                p_l4  = UDP(dport=12,sport=1025)
+                pyld_size = max(0, self.max_pkt_size_l3 - len(p_l3/p_l4));
+                base_pkt = p_l2/p_l3/p_l4/('\x55'*(pyld_size))
+
+                l3_len_fix =-(len(p_l2));
+                l4_len_fix =-(len(p_l2/p_l3));
+
+
+                # vm
+                vm = STLScVmRaw( [ STLVmFlowVar(name="fv_rand", min_value=64,
+                                                max_value=len(base_pkt),
+                                                size=2, op="inc"),
+                                   # change total packet size <<<
+                                   STLVmTrimPktSize("fv_rand"),
+
+                                   STLVmWrFlowVar(fv_name="fv_rand",
+                                                  pkt_offset= "IP.len",
+                                                  add_val=l3_len_fix), # fix ip len
+                                   # fix checksum
+                                   STLVmFixIpv4(offset = "IP"),
+
+                                   STLVmWrFlowVar(fv_name="fv_rand",
+                                                  pkt_offset= "UDP.len",
+                                                  add_val=l4_len_fix) # fix udp len
+                                  ]
+                               )
+
+                pkt = STLPktBuilder(pkt = base_pkt,
+                                    vm = vm)
+
+                return STLStream(packet = pkt,
+                                 mode = STLTXCont())
+
+
+        """
+
+        super(STLVmTupleGen, self).__init__()
+        self.name = name
+        self.ip_min = convert_val(ip_min)
+        self.ip_max = convert_val(ip_max)
+        self.port_min = port_min
+        self.port_max = port_max
+        self.limit_flows = limit_flows
+        self.flags = flags
+
+    def get_var_name(self):
+        return [self.name + ".ip", self.name + ".port"]
+
+    def get_obj(self):
+        return CTRexVmInsTupleGen(
+            self.name, self.ip_min, self.ip_max, self.port_min, self.port_max, self.limit_flows, self.flags
+        )
+
+
+# ###############################################################################################
+
+
+class STLPktBuilder:
+    # WTF
+    def __init__(self, pkt=None, pkt_buffer=None, vm=None, path_relative_to_profile=False, build_raw=False, remove_fcs=True):
+        """
+
+        This class defines a method for building a template packet and
+        Field Engine using the Scapy package.
+        Using this class the user can also define how TRex will handle the packet
+        by specifying the Field engine settings.
+        The pkt can be a Scapy pkt or pcap file name.
+        If using a pcap file, and path_relative_to_profile is True,
+        then the function loads the pcap file from a path relative to the profile.
+
+
+        .. code-block:: python
+
+            # Example6
+
+            # packet is scapy
+            STLPktBuilder( pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")
+                           /UDP(dport=12,sport=1025)/(10*'x') )
+
+
+            # packet is taken from pcap file relative to python
+            STLPktBuilder( pkt ="stl/yaml/udp_64B_no_crc.pcap")
+
+            # packet is taken from pcap file relative to profile file
+            STLPktBuilder( pkt ="stl/yaml/udp_64B_no_crc.pcap",
+                                path_relative_to_profile = True )
+
+
+            vm = STLScVmRaw( [   STLVmTupleGen( ip_min="16.0.0.1", ip_max="16.0.0.2",
+                                                   port_min=1025, port_max=65535,
+                                                    name="tuple"), # define tuple gen
+                             # write ip to packet IP.src
+                             STLVmWrFlowVar(fv_name="tuple.ip", pkt_offset= "IP.src" ),
+                             # fix checksum
+                             STLVmFixIpv4(offset = "IP"),
+                             #write udp.port
+                             STLVmWrFlowVar(fv_name="tuple.port", pkt_offset= "UDP.sport" )
+                             ]
+                           )
+
+            base_pkt = Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)
+            pad = max(0, size - len(base_pkt)) * 'x'
+
+            STLPktBuilder(pkt = base_pkt/pad, vm= vm)
+
+
+        :parameters:
+
+             pkt : string,
+                Scapy object or pcap filename.
+
+             pkt_buffer : bytes
+                Packet as buffer.
+
+             vm   : list or base on :class:`trex_stl_lib.trex_stl_packet_builder_scapy.STLScVmRaw`
+                List of instructions to manipulate packet fields.
+
+             path_relative_to_profile : bool
+                If pkt is a pcap file, determines whether to load it relative to profile file.
+
+             build_raw : bool
+                If a buffer is specified(by pkt_buffer), determines whether to build Scapy.
+                Useful in cases where it is necessary to take the offset from Scapy.
+
+             remove_fcs : bool
+                If a buffer is specified(by pkt_buffer), determines whether to remove FCS.
+
+
+
+        """
+        super(STLPktBuilder, self).__init__()
+
+        self.pkt = None  # as input
+        self.pkt_raw = None  # from raw pcap file
+        self.vm_scripts = []  # list of high level instructions
+        self.vm_low_level = None
+        self.is_pkt_built = False
+        self.metadata = ""
+        self.path_relative_to_profile = path_relative_to_profile
+        self.remove_fcs = remove_fcs
+        self.is_binary_source = pkt_buffer is not None
+
+        if pkt is not None and pkt_buffer is not None:
+            raise CTRexPacketBuildException(-15, "Packet builder cannot be provided with both pkt and pkt_buffer.")
+
+        # process packet
+        if pkt is not None:
+            self.set_packet(pkt)
+
+        elif pkt_buffer is not None:
+            self.set_pkt_as_str(pkt_buffer)
+
+        # process VM
+        if vm is not None:
+            if not isinstance(vm, (STLScVmRaw, list)):
+                raise CTRexPacketBuildException(-14, "Bad value for variable vm.")
+
+            self.add_command(vm if isinstance(vm, STLScVmRaw) else STLScVmRaw(vm))
+
+        # raw source build to see MAC presence/ fields offset by name in VM
+        if build_raw and self.pkt_raw and not self.pkt:
+            self.__lazy_build_packet()
+
+        # if we have packet and VM - compile now
+        if self.pkt or self.pkt_raw and self.vm_scripts:
+            self.compile()
+
+    def dump_vm_data_as_yaml(self):
+        print((yaml.dump(self.get_vm_data(), default_flow_style=False)))
+
+    def get_vm_data(self):
+        """
+        Dumps the instructions
+
+        :parameters:
+            None
+
+        :return:
+            + json object of instructions
+
+        :raises:
+            + :exc:`AssertionError`, in case VM is not compiled(is None).
+        """
+
+        assert self.vm_low_level is not None, "vm_low_level is None, please use compile()"
+
+        return self.vm_low_level.get_json()
+
+    def dump_pkt(self, encode=True):
+        """
+        Dumps the packet as a decimal array of bytes(each item x gets value in range 0-255)
+
+        :parameters:
+            encode : bool
+                Encode using base64.(disable for debug)
+
+                Default: **True**
+
+        :return:
+            + packet representation as array of bytes
+
+        :raises:
+            + :exc:`AssertionError`, in case packet is empty.
+
+        """
+        pkt_buf = self._get_pkt_as_str()
+        return {"binary": base64.b64encode(pkt_buf).decode() if encode else pkt_buf, "meta": self.metadata}
+
+    def dump_pkt_to_pcap(self, file_path):
+        wrpcap(file_path, self._get_pkt_as_str())
+
+    def add_command(self, script):
+        self.vm_scripts.append(script.clone())
+
+    def dump_scripts(self):
+        self.vm_low_level.dump_as_yaml()
+
+    def dump_as_hex(self):
+        pkt_buf = self._get_pkt_as_str()
+        print((hexdump(pkt_buf)))
+
+    def pkt_layers_desc(self):
+        """
+        Return layer description in this format: IP:TCP:Pyload
+
+        """
+        pkt_buf = self._get_pkt_as_str()
+        return self.pkt_layers_desc_from_buffer(pkt_buf)
+
+    @staticmethod
+    def pkt_layers_desc_from_buffer(pkt_buf):
+        scapy_pkt = Ether(pkt_buf)
+        pkt_utl = CTRexScapyPktUtl(scapy_pkt)
+        return pkt_utl.get_pkt_layers()
+
+    def set_pkt_as_str(self, pkt_buffer):
+        self.pkt_raw = pkt_buffer
+
+    def set_pcap_file(self, pcap_file):
+        """
+        Load raw pcap file into a buffer. Loads only the first packet.
+
+        :parameters:
+            pcap_file : file_name
+
+        :raises:
+            + :exc:`AssertionError`, if packet is empty.
+
+        """
+        f_path = self._get_pcap_file_path(pcap_file)
+
+        p = RawPcapReader(f_path)
+        was_set = False
+
+        for pkt in p:
+            was_set = True
+            self.pkt_raw = pkt[0]
+            break
+        if not was_set:
+            raise CTRexPacketBuildException(-14, "Empty pcap file {0}".format(f_path))
+
+    def to_pkt_dump(self):
+        p = self.pkt
+        if p and isinstance(p, Ether):
+            p.show2()
+            hexdump(p)
+            return
+        p = self.pkt_raw
+        if p:
+            scapy_pkt = Ether(p)
+            scapy_pkt.show2()
+            hexdump(p)
+
+    def set_packet(self, pkt):
+        """
+        Scapy packet
+
+        Example::
+
+           pkt =Ether()/IP(src="16.0.0.1",dst="48.0.0.1")/UDP(dport=12,sport=1025)/IP()/('x'*10)
+
+        """
+        if isinstance(pkt, Ether):
+            self.pkt = pkt
+        else:
+            if isinstance(pkt, str):
+                self.set_pcap_file(pkt)
+            else:
+                raise CTRexPacketBuildException(-14, "bad packet")
+
+    def is_default_src_mac(self) -> bool:
+        if self.is_binary_source:
+            return True
+        if isinstance(self.pkt, Packet) and isinstance(self.pkt, Ether) and "src" in self.pkt.fields:
+            return False
+        return True
+
+    def is_default_dst_mac(self):
+        if self.is_binary_source:
+            return True
+        if isinstance(self.pkt, Packet) and isinstance(self.pkt, Ether) and "dst" in self.pkt.fields:
+            return False
+        return True
+
+    def compile(self):
+        if self.pkt is None and self.pkt_raw is None:
+            raise CTRexPacketBuildException(-14, "Packet is empty")
+
+        self.vm_low_level = CTRexVmEngine()
+
+        # compile the VM
+        for sc in self.vm_scripts:
+            if isinstance(sc, STLScVmRaw):
+                self._compile_raw(sc)
+
+    def get_pkt_len(self):
+        if self.pkt:
+            return len(self.pkt)
+        elif self.pkt_raw:
+            return len(self.pkt_raw)
+        else:
+            raise CTRexPacketBuildException(-14, "Packet is empty")
+
+    ####################################################
+    # private
+
+    def _get_pcap_file_path(self, pcap_file_name):
+        f_path = pcap_file_name
+        if os.path.isabs(pcap_file_name):
+            f_path = pcap_file_name
+        else:
+            if self.path_relative_to_profile:
+                p = self._get_path_relative_to_profile()  # loader
+                if p:
+                    f_path = os.path.abspath(os.path.join(os.path.dirname(p), pcap_file_name))
+
+        return f_path
+
+    def _get_path_relative_to_profile(self):
+        p = inspect.stack()
+        for obj in p:
+            if obj[3] == "get_streams":
+                return obj[1]
+        return None
+
+    def _compile_raw(self, obj):
+
+        # make sure we have varibles once
+        vars = {}
+
+        # add it add var to dit
+        for desc in obj.commands:
+            var_names = desc.get_var_name()
+
+            if var_names:
+                for var_name in var_names:
+                    if var_name in vars:
+                        raise CTRexPacketBuildException(-11, ("Variable %s defined twice ") % (var_name))
+                    else:
+                        vars[var_name] = 1
+
+        # check that all write exits
+        for desc in obj.commands:
+            var_name = desc.get_var_ref()
+            if var_name:
+                if var_name not in vars:
+                    raise CTRexPacketBuildException(-11, ("Variable %s does not exist  ") % (var_name))
+            desc.compile(self)
+
+        for desc in obj.commands:
+            self.vm_low_level.add_ins(desc.get_obj())
+
+        # set split_by_var
+        if obj.split_by_field:
+            self.vm_low_level.split_by_var = obj.split_by_field
+
+        # set cache size
+        if obj.cache_size:
+            self.vm_low_level.cache_size = obj.cache_size
+
+    # lazy packet build only on demand
+
+    def __lazy_build_packet(self):
+        # alrady built ? bail out
+        if self.is_pkt_built:
+            return
+
+        # for buffer, promote to a scapy packet
+        if self.pkt_raw:
+            self.pkt = Ether(self.pkt_raw)
+            self.pkt_raw = None
+
+        # regular scapy packet
+        elif not self.pkt:
+            # should not reach here
+            raise CTRexPacketBuildException(-11, "Empty packet")
+
+        if self.remove_fcs and self.pkt.lastlayer().name == "Padding":
+            self.pkt.lastlayer().underlayer.remove_payload()
+
+        self.pkt.build()
+        self.is_pkt_built = True
+
+    def _pkt_layer_offset(self, layer_name):
+
+        self.__lazy_build_packet()
+
+        p_utl = CTRexScapyPktUtl(self.pkt)
+        return p_utl.get_layer_offet_by_str(layer_name)
+
+    def _name_to_offset(self, field_name):
+
+        self.__lazy_build_packet()
+
+        p_utl = CTRexScapyPktUtl(self.pkt)
+        return p_utl.get_field_offet_by_str(field_name)
+
+    def _get_pkt_as_str(self):
+
+        if self.pkt:
+            return bytes(self.pkt)
+
+        if self.pkt_raw:
+            return self.pkt_raw
+
+        raise CTRexPacketBuildException(-11, "Empty packet")
+
+    def _add_tuple_gen(self, tuple_gen):
+
+        pass
+
+
+def STLIPRange(src=None, dst=None, fix_chksum=True):
+
+    vm = []
+
+    if src:
+        vm += [
+            STLVmFlowVar(name="src", min_value=src["start"], max_value=src["end"], size=4, op="inc", step=src["step"]),
+            STLVmWrFlowVar(fv_name="src", pkt_offset="IP.src"),
+        ]
+
+    if dst:
+        vm += [
+            STLVmFlowVar(name="dst", min_value=dst["start"], max_value=dst["end"], size=4, op="inc", step=dst["step"]),
+            STLVmWrFlowVar(fv_name="dst", pkt_offset="IP.dst"),
+        ]
+
+    if fix_chksum:
+        vm.append(STLVmFixIpv4(offset="IP"))
+
+    return vm
+
+
+class STLVM(STLScVmRaw):
+    """
+    Defines a VM/Field Engine object
+
+    Describes the interaction on each packet
+
+    """
+
+    def __init__(self):
+        STLScVmRaw.__init__(self)
+
+    def set_cached(self, cache_size):
+        """
+        set VM as cached with a cache size
+        """
+        self.cache_size = cache_size
+
+    def var(self, name, min_value, max_value, size, op, step=1):
+        """
+        Defines a flow variable.
+        Allocates a variable on a stream context. The size argument determines the variable size.
+        The operation can be inc, dec, and random.
+        For increment and decrement operations, can set the "step" size.
+        For all operations, can set initialization value, minimum and maximum value.
+
+        :parameters:
+             name : string
+                Name of the stream variable
+
+             min_value  : int
+                Min value
+
+             max_value  : int
+                Max value
+
+             size  : int
+                Number of bytes of the variable. Possible values: 1,2,4,8 for uint8_t,
+                uint16_t, uint32_t, uint64_t
+
+             step  : int
+                Step in case of "inc" or "dec" operations
+
+             op    : string
+                Possible values: "inc", "dec", "random"
+        """
+        self.add_cmd(STLVmFlowVar(name=name, min_value=min_value, max_value=max_value, size=size, op=op, step=step))
+
+    def write(self, fv_name, pkt_offset, offset_fixup=0, add_val=0, byte_order="big"):
+        """
+        Write a previously defined varaible to the packet.
+
+        The write position is determined by the packet offset + offset fixup.
+        The size of the write is determined by the stream variable.
+        Example: Offset 10, fixup 0, variable size 4. This function writes at 10, 11, 12, and 13.
+
+        For inromation about chaning the write size, offset, or fixup,
+        see the `STLVmWrMaskFlowVar` command.
+        The Field name/offset can be given by name in the following format: ``header[:id].field``.
+
+
+        :parameters:
+            fv_name : string
+                Stream variable to write to a packet offset.
+
+            pkt_offset : string or in
+                Name of the field or offset in bytes from packet start.
+
+            offset_fixup : int
+                Number of bytes to move forward. If negative, move backward.
+
+             add_val     : int
+                Value to add to the stream variable before writing it to the packet field.
+                Can be used as a constant offset.
+
+             is_big      : bool
+                How to write the variable to the the packet. True=big-endian, False=little-endian
+        """
+        self.add_cmd(
+            STLVmWrFlowVar(
+                fv_name=fv_name,
+                pkt_offset=pkt_offset,
+                offset_fixup=offset_fixup,
+                add_val=add_val,
+                is_big=(byte_order == "big"),
+            )
+        )
+
+    def tuple_var(self, name, ip_min, ip_max, port_min, port_max, limit_flows=0):
+        """
+        Generate a struct with two variables: ``var_name.ip`` as uint32_t
+        and ``var_name.port`` as uint16_t
+        The variables are dependent. When the ip variable value reaches its maximum,
+        the port is incremented.
+
+        For:
+
+        * ip_min      = 10.0.0.1
+        * ip_max      = 10.0.0.5
+        * port_min    = 1025
+        * port_max    = 1028
+        * limit_flows = 10
+
+        The result:
+
+        +------------+------------+-----------+
+        | ip         | port       | flow_id   |
+        +============+============+===========+
+        | 10.0.0.1   | 1025       | 1         |
+        +------------+------------+-----------+
+        | 10.0.0.2   | 1025       | 2         |
+        +------------+------------+-----------+
+        | 10.0.0.3   | 1025       | 3         |
+        +------------+------------+-----------+
+        | 10.0.0.4   | 1025       | 4         |
+        +------------+------------+-----------+
+        | 10.0.0.5   | 1025       | 5         |
+        +------------+------------+-----------+
+        | 10.0.0.1   | 1026       | 6         |
+        +------------+------------+-----------+
+        | 10.0.0.2   | 1026       | 7         |
+        +------------+------------+-----------+
+        | 10.0.0.3   | 1026       | 8         |
+        +------------+------------+-----------+
+        | 10.0.0.4   | 1026       | 9         |
+        +------------+------------+-----------+
+        | 10.0.0.5   | 1026       | 10        |
+        +------------+------------+-----------+
+        | 10.0.0.1   | 1025       | 1         |
+        +------------+------------+-----------+
+
+
+        :parameters:
+            name : string
+                Name of the stream struct.
+
+            ip_min : string or int
+                Min value of the ip value. Number or IPv4 format.
+
+            ip_max : string or int
+                Max value of the ip value. Number or IPv4 format.
+
+            port_min : int
+                Min value of port variable.
+
+            port_max : int
+                Max value of port variable.
+
+            limit_flows : int
+                Limit of number of flows.
+
+        """
+
+        self.add_cmd(
+            STLVmTupleGen(
+                name=name, ip_min=ip_min, ip_max=ip_max, port_min=port_min, port_max=port_max, limit_flows=limit_flows
+            )
+        )
+
+    def fix_chksum(self, offset="IP"):
+        """
+        Fix IPv4 header checksum. Use this if the packet header has changed and it is
+        necessary to change the checksum.
+
+        :parameters:
+             offset : uint16_t or string
+                **IPv4 header** offset from packet start. It is **not** the offset of the
+                checksum field itself.
+                in could be string in case of scapy packet. format IP[:[id]]
+        """
+        self.add_cmd(STLVmFixIpv4(offset))
+
+    def trim(self, fv_name):
+        """
+        Trim the packet size by the stream variable size. This instruction only changes the
+        total packet size, and does not repair the fields to match the new size.
+
+
+        :parameters:
+            fv_name : string
+                Stream variable name. The value of this variable is the new total packet size.
+
+        """
+        self.add_cmd(STLVmTrimPktSize(fv_name=fv_name))
+
+
+class PacketBuffer:
+    """
+    Class to be used when sending packets via push_packets.
+
+    :parameters:
+        buffer : bytes or Scapy packet
+            Packet to send
+
+        port_src : bool
+            Override src MAC with TRex port src
+
+        port_dst : bool
+            Override dst MAC with TRex port dst
+    """
+
+    def __init__(self, buffer, port_src=False, port_dst=False):
+        if isinstance(buffer, dict):
+            self.buffer = bytes(buffer)
+        else:
+            self.buffer = buffer
+        self.port_src = port_src
+        self.port_dst = port_dst
```

## pytrex/trex_stream.py

```diff
@@ -1,355 +1,345 @@
-import base64
-import json
-import os
-from enum import Enum
-
-import yaml
-from scapy.layers.inet import IP
-from scapy.layers.l2 import Ether
-
-from .text_opts import format_num
-from .trex_object import TrexObject
-from .trex_statistics_view import TrexStreamStatistics
-from .trex_stl_packet_builder_scapy import STLPktBuilder
-
-
-def del_fields(dict, *entries):
-    for entry in entries:
-        try:
-            del dict[entry]
-        except Exception as _:
-            pass
-
-
-class TrexRateType(Enum):
-    pps = (0,)
-    bps_l1 = 1
-    bps_l2 = 2
-    percentage = 3
-
-
-class TrexTxType(Enum):
-    continuous = (0,)
-    single_burst = 1
-    multi_burst = 2
-
-
-class TrexFlowStatsType(Enum):
-    none = (0,)
-    stats = (1,)
-    latency = 2
-
-
-STLStreamDstMAC_CFG_FILE = 0
-STLStreamDstMAC_PKT = 1
-STLStreamDstMAC_ARP = 2
-
-
-class TrexStream(TrexObject):
-    def __init__(self, parent, index, name):
-        """Create stream object.
-
-        :param parent: parent port
-        :param index: stream index under port, zero based
-        :param name: stream name
-        """
-        super().__init__(objType="stream", parent=parent, index=index, name=name)
-        self.reset_fields()
-
-    def __repr__(self):
-        s = "Stream Name: {0}\n".format(self.name)
-        s += "Stream Next: {0}\n".format(self.next)
-        s += "Stream JSON:\n{0}\n".format(json.dumps(self.fields, indent=4, separators=(",", ": "), sort_keys=True))
-        return s
-
-    def reset_fields(self):
-        self.fields = {}
-        self.fields["enabled"] = True
-        self.fields["next_stream"] = None
-        self.fields["self_start"] = True
-        self.fields["action_count"] = 0
-        self.fields["isg"] = 0
-        self.fields["flags"] = 0x0
-        self.fields["mode"] = {}
-        self.fields["mode"]["rate"] = {}
-        self.fields["mode"]["rate"]["type"] = TrexRateType.pps.name
-        self.fields["mode"]["rate"]["value"] = 1
-        self.fields["mode"]["type"] = TrexTxType.continuous.name
-        self.fields["flow_stats"] = {}
-        self.fields["flow_stats"]["enabled"] = False
-        self.set_packet(STLPktBuilder(pkt=Ether() / IP()))
-
-    def set_next(self, stream):
-        self.fields["next_stream"] = stream.name if type(stream) == TrexStream else stream
-
-    def set_rate(self, type=TrexRateType.pps, value=1):
-        self.fields["mode"]["rate"] = {}
-        self.fields["mode"]["rate"]["type"] = type.name
-        self.fields["mode"]["rate"]["value"] = value
-
-    def set_tx_type(self, type=TrexTxType.continuous, packets=None, ibg=None, count=None):
-        self.fields["mode"]["type"] = type.name
-        if type == TrexTxType.single_burst:
-            self.fields["mode"]["total_pkts"] = packets
-            del_fields(self.fields["mode"], "pkts_per_burst", "ibg", "count")
-        elif type == TrexTxType.multi_burst:
-            self.fields["mode"]["pkts_per_burst"] = packets
-            self.fields["mode"]["ibg"] = ibg
-            self.fields["mode"]["count"] = count
-            del_fields(self.fields["mode"], "total_pkts")
-
-    def set_flow_stats(self, type, stream_id=None):
-        if type == TrexFlowStatsType.none:
-            self.fields["flow_stats"]["enabled"] = False
-            del_fields(self.fields["flow_stats"], "rule_type", "stream_id")
-        else:
-            self.fields["flow_stats"]["enabled"] = True
-            self.fields["flow_stats"]["rule_type"] = type.name
-            self.fields["flow_stats"]["stream_id"] = stream_id
-
-    def set_packet(self, packet=None, mac_src_override_by_pkt=None, mac_dst_override_mode=None, dummy_stream=False):
-        """Set packet headers.
-
-        :todo: packet should be Scapy packet.
-
-        :param packet: requested packet
-        :type packet: STLPktBuilder
-
-                  mac_src_override_by_pkt : bool
-                        Template packet sets src MAC.
-
-                  mac_dst_override_mode=None : STLStreamDstMAC_xx
-                        Template packet sets dst MAC.
-
-                  dummy_stream : bool
-                        For delay purposes, will not be sent.
-
-        :param packet:
-        :return:
-        """
-        # save for easy construct code from stream object
-        self.mac_src_override_by_pkt = mac_src_override_by_pkt
-        self.mac_dst_override_mode = mac_dst_override_mode
-        # self.id = stream_id
-
-        if mac_src_override_by_pkt is None:
-            int_mac_src_override_by_pkt = 0
-            if packet:
-                if packet.is_default_src_mac() is False:
-                    int_mac_src_override_by_pkt = 1
-
-        else:
-            int_mac_src_override_by_pkt = int(mac_src_override_by_pkt)
-
-        if mac_dst_override_mode is None:
-            int_mac_dst_override_mode = 0
-            if packet:
-                if packet.is_default_dst_mac() is False:
-                    int_mac_dst_override_mode = STLStreamDstMAC_PKT
-        else:
-            int_mac_dst_override_mode = int(mac_dst_override_mode)
-
-        self.is_default_mac = not (int_mac_src_override_by_pkt or int_mac_dst_override_mode)
-
-        self.fields["flags"] = (
-            (int_mac_src_override_by_pkt & 1) + ((int_mac_dst_override_mode & 3) << 1) + (int(dummy_stream) << 3)
-        )
-
-        if not packet:
-            packet = STLPktBuilder(pkt=Ether() / IP())
-            if dummy_stream:
-                self.packet_desc = "Dummy"
-
-        self.scapy_pkt_builder = packet
-        # packet builder
-        packet.compile()
-
-        # packet and VM
-        self.fields["packet"] = packet.dump_pkt()
-        self.fields["vm"] = packet.get_vm_data()
-
-        self.pkt = base64.b64decode(self.fields["packet"]["binary"])
-
-    def config(self, enabled=True, self_start=True, isg=0.0, action_count=0, random_seed=0):
-        """Configure stream.
-
-        :parameters:
-
-                  enabled : bool
-                      Indicates whether the stream is enabled.
-
-                  self_start : bool
-                      If False, another stream activates it.
-
-                  isg : float
-                     Inter-stream gap in usec. Time to wait until the stream
-                     sends the first packet.
-
-                  flow_stats : :class:`trex_stl_lib.trex_stl_streams.STLFlowStats`
-                      Per stream statistic object. See: STLFlowStats
-
-                  next : string
-                      Name of the stream to activate.
-
-                  action_count : uint16_t
-                        If there is a next stream, number of loops before stopping.
-                        Default: 0(unlimited).
-
-                  random_seed: uint16_t
-                       If given, the seed for this stream will be this value.
-                       Useful if you need a deterministic random value.
-        """
-        self.fields["action_count"] = action_count
-
-        # basic fields
-        self.fields["enabled"] = enabled
-        self.fields["self_start"] = self_start
-        self.fields["isg"] = isg
-
-        if random_seed != 0:
-            self.fields["random_seed"] = random_seed  # optional
-
-        # packet
-        self.fields["packet"] = {}
-        self.fields["vm"] = {}
-
-        # this is heavy, calculate lazy
-        self.packet_desc = None
-
-    def read_stats(self):
-        stream_stats_view = TrexStreamStatistics(self.server)
-        return stream_stats_view.read()[self]
-
-    def to_json(self) -> dict:
-        """Return json format."""
-        return dict(self.fields)
-
-    def has_custom_mac_addr(self) -> bool:
-        """Return True if src or dst MAC were set as custom."""
-        return not self.is_default_mac
-
-    def has_flow_stats(self):
-        """Return True if stream was configured with flow stats."""
-        return self.fields["flow_stats"]["enabled"]
-
-    def get_pkt_len(self, count_crc=True):
-        """Get packet number of bytes."""
-        pkt_len = len(self.get_pkt())
-        if count_crc:
-            pkt_len += 4
-
-        return pkt_len
-
-    def get_pkt_type(self):
-        """Get packet description. Example: IP:UDP."""
-        if self.packet_desc is None:
-            self.packet_desc = STLPktBuilder.pkt_layers_desc_from_buffer(self.get_pkt())
-
-        return self.packet_desc
-
-    @staticmethod
-    def get_rate_from_field(rate_json):
-        """Get rate from json."""
-        t = rate_json["type"]
-        v = rate_json["value"]
-
-        if t == "pps":
-            return format_num(v, suffix="pps")
-        elif t == "bps_L1":
-            return format_num(v, suffix="bps(L1)")
-        elif t == "bps_L2":
-            return format_num(v, suffix="bps(L2)")
-        elif t == "percentage":
-            return format_num(v, suffix="%")
-
-    def get_rate(self):
-        return self.get_rate_from_field(self.fields["mode"]["rate"])
-
-    def to_pkt_dump(self):
-        """Print packet description from Scapy."""
-        if self.name:
-            print(("Stream Name: ", self.name))
-        scapy_b = self.scapy_pkt_builder
-        if scapy_b and isinstance(scapy_b, STLPktBuilder):
-            dump = scapy_b.to_pkt_dump()
-        else:
-            print("Nothing to dump")
-        return dump
-
-
-class TrexYamlLoader:
-    def __init__(self, port, yaml_file):
-        self.port = port
-        self.yaml_path = os.path.dirname(yaml_file)
-        self.yaml_file = yaml_file
-
-    def __parse_packet(self, stream, packet_dict, mac_src_override_by_pkt, mac_dst_override_mode):
-
-        pkt_str = base64.b64decode(packet_dict["binary"])
-        builder = STLPktBuilder(pkt_buffer=pkt_str)
-        stream.set_packet(builder, mac_src_override_by_pkt, mac_dst_override_mode)
-
-    def __parse_mode(self, stream, mode_obj):
-
-        rate_type = TrexRateType[mode_obj.get("rate").get("type", "continuous")]
-        rate_value = mode_obj.get("rate").get("value", 1.0)
-        stream.set_rate(rate_type, rate_value)
-
-        tx_type = TrexTxType[mode_obj.get("type", "continuous")]
-        attributes = {}
-        if tx_type == TrexTxType.single_burst:
-            attributes["packets"] = mode_obj.get("total_pkts", 1)
-        elif tx_type == TrexTxType.multi_burst:
-            attributes["packets"] = mode_obj.get("pkts_per_burst", 1)
-            attributes["ibg"] = mode_obj.get("ibg", 0.0)
-            attributes["count"] = mode_obj.get("count", 2)
-        stream.set_tx_type(tx_type, **attributes)
-
-    def __parse_flow_stats(self, stream, flow_stats_obj):
-
-        if not flow_stats_obj.get("enabled"):
-            stream.set_flow_stats(TrexFlowStatsType.none)
-        stream.set_flow_stats(TrexFlowStatsType[flow_stats_obj.get("rule_type")], flow_stats_obj.get("stream_id"))
-
-    def __parse_stream(self, yaml_object):
-
-        # create the stream
-        s_obj = yaml_object["stream"]
-        stream = self.port.add_stream(name=yaml_object.get("name"))
-
-        stream.config(
-            enabled=s_obj.get("enabled", True),
-            self_start=s_obj.get("self_start", True),
-            isg=s_obj.get("isg", 0.0),
-            action_count=s_obj.get("action_count", 0),
-        )
-
-        stream.set_next(yaml_object.get("next"))
-
-        # mode
-        self.__parse_mode(stream, s_obj.get("mode"))
-
-        # packet
-        self.__parse_packet(
-            stream,
-            s_obj["packet"],
-            mac_src_override_by_pkt=s_obj.get("mac_src_override_by_pkt", 0),
-            mac_dst_override_mode=s_obj.get("mac_src_override_by_pkt", 0),
-        )
-
-        # rx stats
-        self.__parse_flow_stats(stream, s_obj.get("flow_stats"))
-
-        # hack the VM fields for now
-        if "vm" in s_obj:
-            stream.fields["vm"].update(s_obj["vm"])
-
-        return stream
-
-    def parse(self) -> list:
-        """Read YAML and pass it down to stream object."""
-        with open(self.yaml_file, "r") as f:
-            yaml_str = f.read()
-            objects = yaml.safe_load(yaml_str)
-            streams = [self.__parse_stream(obj) for obj in objects]
-            return streams
+import base64
+import json
+from enum import Enum
+from pathlib import Path
+
+import yaml
+from scapy.layers.inet import IP
+from scapy.layers.l2 import Ether
+
+from .text_opts import format_num
+from .trex_object import TrexObject
+from .trex_statistics_view import TrexStreamStatistics
+from .trex_stl_packet_builder_scapy import STLPktBuilder
+
+
+def del_fields(dict, *entries):
+    for entry in entries:
+        try:
+            del dict[entry]
+        except Exception as _:
+            pass
+
+
+class TrexRateType(Enum):
+    pps = (0,)
+    bps_l1 = 1
+    bps_l2 = 2
+    percentage = 3
+
+
+class TrexTxType(Enum):
+    continuous = (0,)
+    single_burst = 1
+    multi_burst = 2
+
+
+class TrexFlowStatsType(Enum):
+    none = (0,)
+    stats = (1,)
+    latency = 2
+
+
+STLStreamDstMAC_CFG_FILE = 0
+STLStreamDstMAC_PKT = 1
+STLStreamDstMAC_ARP = 2
+
+
+class TrexStream(TrexObject):
+    def __init__(self, port, index: int, name: str) -> None:
+        """Create stream object.
+
+        :param port: parent port
+        :param index: stream index under port, zero based
+        :param name: stream name
+        """
+        super().__init__(parent=port, objType="stream", index=str(index), name=name)
+        self.reset_fields()
+
+    def __repr__(self):
+        s = "Stream Name: {0}\n".format(self.name)
+        s += "Stream Next: {0}\n".format(self.next)
+        s += "Stream JSON:\n{0}\n".format(json.dumps(self.fields, indent=4, separators=(",", ": "), sort_keys=True))
+        return s
+
+    def reset_fields(self):
+        self.fields = {}
+        self.fields["enabled"] = True
+        self.fields["next_stream"] = None
+        self.fields["self_start"] = True
+        self.fields["action_count"] = 0
+        self.fields["isg"] = 0
+        self.fields["flags"] = 0x0
+        self.fields["mode"] = {}
+        self.fields["mode"]["rate"] = {}
+        self.fields["mode"]["rate"]["type"] = TrexRateType.pps.name
+        self.fields["mode"]["rate"]["value"] = 1
+        self.fields["mode"]["type"] = TrexTxType.continuous.name
+        self.fields["flow_stats"] = {}
+        self.fields["flow_stats"]["enabled"] = False
+        self.set_packet(STLPktBuilder(pkt=Ether() / IP()))
+
+    def set_next(self, stream):
+        self.fields["next_stream"] = stream.name if type(stream) == TrexStream else stream
+
+    def set_rate(self, type=TrexRateType.pps, value=1):
+        self.fields["mode"]["rate"] = {}
+        self.fields["mode"]["rate"]["type"] = type.name
+        self.fields["mode"]["rate"]["value"] = value
+
+    def set_tx_type(self, type=TrexTxType.continuous, packets=None, ibg=None, count=None):
+        self.fields["mode"]["type"] = type.name
+        if type == TrexTxType.single_burst:
+            self.fields["mode"]["total_pkts"] = packets
+            del_fields(self.fields["mode"], "pkts_per_burst", "ibg", "count")
+        elif type == TrexTxType.multi_burst:
+            self.fields["mode"]["pkts_per_burst"] = packets
+            self.fields["mode"]["ibg"] = ibg
+            self.fields["mode"]["count"] = count
+            del_fields(self.fields["mode"], "total_pkts")
+
+    def set_flow_stats(self, type, stream_id=None):
+        if type == TrexFlowStatsType.none:
+            self.fields["flow_stats"]["enabled"] = False
+            del_fields(self.fields["flow_stats"], "rule_type", "stream_id")
+        else:
+            self.fields["flow_stats"]["enabled"] = True
+            self.fields["flow_stats"]["rule_type"] = type.name
+            self.fields["flow_stats"]["stream_id"] = stream_id
+
+    def set_packet(
+        self,
+        packet: STLPktBuilder = None,
+        mac_src_override_by_pkt: int = None,
+        mac_dst_override_mode: int = None,
+        dummy_stream: bool = False,
+    ) -> None:
+        """Set packet headers.
+
+        :param packet: requested packet
+        :param mac_src_override_by_pkt: Template packet sets src MAC.
+        :param mac_dst_override_mode: Template packet sets dst MAC - STLStreamDstMAC_CFG_FILE/STLStreamDstMAC_PKT/STLStreamDstMAC_ARP
+        :param dummy_stream: For delay purposes, will not be sent.
+        """
+        # save for easy construct code from stream object
+        self.mac_src_override_by_pkt = mac_src_override_by_pkt
+        self.mac_dst_override_mode = mac_dst_override_mode
+        # self.id = stream_id
+
+        if mac_src_override_by_pkt is None:
+            int_mac_src_override_by_pkt = 0
+            if packet and packet.is_default_src_mac() is False:
+                int_mac_src_override_by_pkt = 1
+        else:
+            int_mac_src_override_by_pkt = int(mac_src_override_by_pkt)
+
+        if mac_dst_override_mode is None:
+            int_mac_dst_override_mode = 0
+            if packet and packet.is_default_dst_mac() is False:
+                int_mac_dst_override_mode = STLStreamDstMAC_PKT
+        else:
+            int_mac_dst_override_mode = int(mac_dst_override_mode)
+
+        self.is_default_mac = not (int_mac_src_override_by_pkt or int_mac_dst_override_mode)
+
+        self.fields["flags"] = (
+            (int_mac_src_override_by_pkt & 1) + ((int_mac_dst_override_mode & 3) << 1) + (int(dummy_stream) << 3)
+        )
+
+        if not packet:
+            packet = STLPktBuilder(pkt=Ether() / IP())
+            if dummy_stream:
+                self.packet_desc = "Dummy"
+
+        self.scapy_pkt_builder = packet
+        # packet builder
+        packet.compile()
+
+        # packet and VM
+        self.fields["packet"] = packet.dump_pkt()
+        self.fields["vm"] = packet.get_vm_data()
+
+        self.pkt = base64.b64decode(self.fields["packet"]["binary"])
+
+    def config(self, enabled=True, self_start=True, isg=0.0, action_count=0, random_seed=0):
+        """Configure stream.
+
+        :parameters:
+
+                  enabled : bool
+                      Indicates whether the stream is enabled.
+
+                  self_start : bool
+                      If False, another stream activates it.
+
+                  isg : float
+                     Inter-stream gap in usec. Time to wait until the stream
+                     sends the first packet.
+
+                  flow_stats : :class:`trex_stl_lib.trex_stl_streams.STLFlowStats`
+                      Per stream statistic object. See: STLFlowStats
+
+                  next : string
+                      Name of the stream to activate.
+
+                  action_count : uint16_t
+                        If there is a next stream, number of loops before stopping.
+                        Default: 0(unlimited).
+
+                  random_seed: uint16_t
+                       If given, the seed for this stream will be this value.
+                       Useful if you need a deterministic random value.
+        """
+        self.fields["action_count"] = action_count
+
+        # basic fields
+        self.fields["enabled"] = enabled
+        self.fields["self_start"] = self_start
+        self.fields["isg"] = isg
+
+        if random_seed != 0:
+            self.fields["random_seed"] = random_seed  # optional
+
+        # packet
+        self.fields["packet"] = {}
+        self.fields["vm"] = {}
+
+        # this is heavy, calculate lazy
+        self.packet_desc = None
+
+    def read_stats(self):
+        stream_stats_view = TrexStreamStatistics(self.server)
+        return stream_stats_view.read()[self]
+
+    def to_json(self) -> dict:
+        """Return json format."""
+        return dict(self.fields)
+
+    def has_custom_mac_addr(self) -> bool:
+        """Return True if src or dst MAC were set as custom."""
+        return not self.is_default_mac
+
+    def has_flow_stats(self):
+        """Return True if stream was configured with flow stats."""
+        return self.fields["flow_stats"]["enabled"]
+
+    def get_pkt_len(self, count_crc: bool = True) -> int:
+        """Get packet number of bytes.
+
+        :param count_crc: If True, add 4 bytes for CRC.
+        """
+        pkt_len = len(self.get_pkt())
+        if count_crc:
+            pkt_len += 4
+        return pkt_len
+
+    def get_pkt_type(self):
+        """Get packet description. Example: IP:UDP."""
+        if self.packet_desc is None:
+            self.packet_desc = STLPktBuilder.pkt_layers_desc_from_buffer(self.get_pkt())
+
+        return self.packet_desc
+
+    @staticmethod
+    def get_rate_from_field(rate_json):
+        """Get rate from json."""
+        t = rate_json["type"]
+        v = rate_json["value"]
+
+        if t == "pps":
+            return format_num(v, suffix="pps")
+        elif t == "bps_L1":
+            return format_num(v, suffix="bps(L1)")
+        elif t == "bps_L2":
+            return format_num(v, suffix="bps(L2)")
+        elif t == "percentage":
+            return format_num(v, suffix="%")
+
+    def get_rate(self):
+        return self.get_rate_from_field(self.fields["mode"]["rate"])
+
+    def to_pkt_dump(self):
+        """Print packet description from Scapy."""
+        if self.name:
+            print(("Stream Name: ", self.name))
+        scapy_b = self.scapy_pkt_builder
+        if scapy_b and isinstance(scapy_b, STLPktBuilder):
+            dump = scapy_b.to_pkt_dump()
+        else:
+            print("Nothing to dump")
+        return dump
+
+
+class TrexYamlLoader:
+    def __init__(self, port, yaml_file: Path) -> None:
+        self.port = port
+        self.yaml_file = yaml_file
+
+    def __parse_packet(
+        self, stream: TrexStream, packet_dict: dict, mac_src_override_by_pkt: int, mac_dst_override_mode: int
+    ) -> None:
+        pkt_str = base64.b64decode(packet_dict["binary"])
+        builder = STLPktBuilder(pkt_buffer=pkt_str)
+        stream.set_packet(builder, mac_src_override_by_pkt, mac_dst_override_mode)
+
+    def __parse_mode(self, stream, mode_obj):
+        rate_type = TrexRateType[mode_obj.get("rate").get("type", "continuous")]
+        rate_value = mode_obj.get("rate").get("value", 1.0)
+        stream.set_rate(rate_type, rate_value)
+
+        tx_type = TrexTxType[mode_obj.get("type", "continuous")]
+        attributes = {}
+        if tx_type == TrexTxType.single_burst:
+            attributes["packets"] = mode_obj.get("total_pkts", 1)
+        elif tx_type == TrexTxType.multi_burst:
+            attributes["packets"] = mode_obj.get("pkts_per_burst", 1)
+            attributes["ibg"] = mode_obj.get("ibg", 0.0)
+            attributes["count"] = mode_obj.get("count", 2)
+        stream.set_tx_type(tx_type, **attributes)
+
+    def __parse_flow_stats(self, stream, flow_stats_obj):
+        if not flow_stats_obj.get("enabled"):
+            stream.set_flow_stats(TrexFlowStatsType.none)
+        stream.set_flow_stats(TrexFlowStatsType[flow_stats_obj.get("rule_type")], flow_stats_obj.get("stream_id"))
+
+    def __parse_stream(self, yaml_object: dict) -> TrexStream:
+        # create the stream
+        s_obj = yaml_object["stream"]
+        stream = self.port.add_stream(name=yaml_object.get("name"))
+
+        stream.config(
+            enabled=s_obj.get("enabled", True),
+            self_start=s_obj.get("self_start", True),
+            isg=s_obj.get("isg", 0.0),
+            action_count=s_obj.get("action_count", 0),
+        )
+
+        stream.set_next(yaml_object.get("next"))
+
+        # mode
+        self.__parse_mode(stream, s_obj.get("mode"))
+
+        # packet
+        self.__parse_packet(
+            stream,
+            s_obj["packet"],
+            mac_src_override_by_pkt=s_obj["flags"] & 0x01,
+            mac_dst_override_mode=(s_obj["flags"] & 0x06) >> 1,
+        )
+
+        # rx stats
+        self.__parse_flow_stats(stream, s_obj.get("flow_stats"))
+
+        # hack the VM fields for now
+        if "vm" in s_obj:
+            stream.fields["vm"].update(s_obj["vm"])
+
+        return stream
+
+    def parse(self) -> list:
+        """Read YAML and pass it down to stream object."""
+        with open(self.yaml_file, "r") as f:
+            yaml_str = f.read()
+            objects = yaml.safe_load(yaml_str)
+        streams = [self.__parse_stream(obj) for obj in objects]
+        return streams
```

## pytrex/zipmsg.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-#!/usr/bin/python3
-
-import struct
-import zlib
-
-
-class ZippedMsg:
-
-    MSG_COMPRESS_THRESHOLD = 256
-    MSG_COMPRESS_HEADER_MAGIC = 0xABE85CEA
-
-    def check_threshold(self, msg):
-        return len(msg) >= self.MSG_COMPRESS_THRESHOLD
-
-    def compress(self, msg):
-        # compress
-        compressed = zlib.compress(msg)
-        new_msg = struct.pack(">II", self.MSG_COMPRESS_HEADER_MAGIC, len(msg)) + compressed
-        return new_msg
-
-    def decompress(self, msg):
-        if len(msg) < 8:
-            return None
-
-        t = struct.unpack(">II", msg[:8])
-        if t[0] != self.MSG_COMPRESS_HEADER_MAGIC:
-            return None
-
-        x = zlib.decompress(msg[8:])
-        if len(x) != t[1]:
-            return None
-
-        return x
-
-    def is_compressed(self, msg):
-        if len(msg) < 8:
-            return False
-
-        t = struct.unpack(">II", msg[:8])
-        if t[0] != self.MSG_COMPRESS_HEADER_MAGIC:
-            return False
-
-        return True
+#!/usr/bin/python3
+
+import struct
+import zlib
+
+
+class ZippedMsg:
+
+    MSG_COMPRESS_THRESHOLD = 256
+    MSG_COMPRESS_HEADER_MAGIC = 0xABE85CEA
+
+    def check_threshold(self, msg):
+        return len(msg) >= self.MSG_COMPRESS_THRESHOLD
+
+    def compress(self, msg):
+        # compress
+        compressed = zlib.compress(msg)
+        new_msg = struct.pack(">II", self.MSG_COMPRESS_HEADER_MAGIC, len(msg)) + compressed
+        return new_msg
+
+    def decompress(self, msg):
+        if len(msg) < 8:
+            return None
+
+        t = struct.unpack(">II", msg[:8])
+        if t[0] != self.MSG_COMPRESS_HEADER_MAGIC:
+            return None
+
+        x = zlib.decompress(msg[8:])
+        if len(x) != t[1]:
+            return None
+
+        return x
+
+    def is_compressed(self, msg):
+        if len(msg) < 8:
+            return False
+
+        t = struct.unpack(">II", msg[:8])
+        if t[0] != self.MSG_COMPRESS_HEADER_MAGIC:
+            return False
+
+        return True
```

## pytrex/api/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from collections import namedtuple
-
-RpcCmdData = namedtuple("RpcCmdData", ["method", "params", "api_class"])
+from collections import namedtuple
+
+RpcCmdData = namedtuple("RpcCmdData", ["method", "params", "api_class"])
```

## pytrex/api/trex_event.py

 * *Ordering differences only*

```diff
@@ -1,321 +1,321 @@
-import datetime
-import re
-import time
-import traceback
-
-from ..text_opts import format_text
-
-
-# an event
-class Event(object):
-    def __init__(self, origin, ev_type, msg):
-        self.origin = origin
-        self.ev_type = ev_type
-        self.msg = msg
-
-        self.ts = datetime.datetime.fromtimestamp(time.time()).strftime("%Y-%m-%d %H:%M:%S")
-
-    def __str__(self):
-
-        prefix = "[{:^}][{:^}]".format(self.origin, self.ev_type)
-
-        return "{:<10} - {:18} - {:}".format(self.ts, prefix, format_text(self.msg, "bold"))
-
-
-# handles different async events given to the client
-class EventsHandler:
-    EVENT_PORT_STARTED = 0
-    EVENT_PORT_STOPPED = 1
-    EVENT_PORT_PAUSED = 2
-    EVENT_PORT_RESUMED = 3
-    EVENT_PORT_JOB_DONE = 4
-    EVENT_PORT_ACQUIRED = 5
-    EVENT_PORT_RELEASED = 6
-    EVENT_PORT_ERROR = 7
-    EVENT_PORT_ATTR_CHG = 8
-
-    EVENT_SERVER_STOPPED = 100
-
-    def __init__(self, client):
-        self.client = client
-        self.logger = self.client.logger
-
-        self.events = []
-
-        # events are disabled by default until explicitly enabled
-        self.enabled = False
-
-    # will start handling events
-
-    def enable(self):
-        self.enabled = True
-
-    def disable(self):
-        self.enabled = False
-
-    def is_enabled(self):
-        return self.enabled
-
-    # public functions
-
-    def get_events(self, *types: str) -> list:
-        if types:
-            return [ev for ev in self.events if ev.ev_type in types]
-        else:
-            return [ev for ev in self.events]
-
-    def clear_events(self) -> None:
-        self.events = []
-
-    def log_warning(self, msg):
-        self.__add_event_log("local", "warning", msg)
-
-    # events called internally
-
-    def on_async_timeout(self, timeout_sec):
-        if self.client.conn.is_connected():
-            msg = "Connection lost - Subscriber timeout: no data from TRex "
-            +"server for more than {0} seconds".format(timeout_sec)
-            self.log_warning(msg)
-
-            # we cannot simply disconnect the connection - we mark it for disconnection
-            # later on, the main thread will execute an ordered disconnection
-            self.client.conn.mark_for_disconnect(msg)
-
-    def on_async_crash(self):
-        msg = "subscriber thread has crashed:\n\n{0}".format(traceback.format_exc())
-        self.log_warning(msg)
-
-        # if connected, mark as disconnected
-        if self.client.conn.is_connected():
-            self.client.conn.mark_for_disconnect(msg)
-
-    def on_async_alive(self):
-        pass
-
-    def on_async_rx_stats_event(self, data, baseline):
-        if not self.is_enabled():
-            return
-
-        self.client.flow_stats.update(data, baseline)
-
-    def on_async_latency_stats_event(self, data, baseline):
-        if not self.is_enabled():
-            return
-
-        self.client.latency_stats.update(data, baseline)
-
-    # handles an async stats update from the subscriber
-    def on_async_stats_update(self, dump_data, baseline):
-        if not self.is_enabled():
-            return
-
-        global_stats = {}
-        port_stats = {}
-
-        # filter the values per port and general
-        for key, value in list(dump_data.items()):
-            # match a pattern of ports
-            m = re.search(r"(.*)\-(\d+)", key)
-            if m:
-                port_id = int(m.group(2))
-                field_name = m.group(1)
-                if port_id in self.client.ports:
-                    if port_id not in port_stats:
-                        port_stats[port_id] = {}
-                    port_stats[port_id][field_name] = value
-                else:
-                    continue
-            else:
-                # no port match - general stats
-                global_stats[key] = value
-
-        # update the general object with the snapshot
-        self.client.global_stats.update(global_stats, baseline)
-
-        # update all ports
-        for port_id, data in list(port_stats.items()):
-            self.client.ports[port_id].port_stats.update(data, baseline)
-
-    # dispatcher for server async events(port started, port stopped and etc.)
-
-    def on_async_event(self, event_id, data):
-        if not self.is_enabled():
-            return
-
-        # default type info and do not show
-        ev_type = "info"
-        show_event = False
-
-        # port started
-        if event_id == self.EVENT_PORT_STARTED:
-            port_id = int(data["port_id"])
-            ev = "Port {0} has started".format(port_id)
-            self.__async_event_port_started(port_id)
-
-        # port stopped
-        elif event_id == self.EVENT_PORT_STOPPED:
-            port_id = int(data["port_id"])
-            ev = "Port {0} has stopped".format(port_id)
-
-            # call the handler
-            self.__async_event_port_stopped(port_id)
-
-        # port paused
-        elif event_id == self.EVENT_PORT_PAUSED:
-            port_id = int(data["port_id"])
-            ev = "Port {0} has paused".format(port_id)
-
-            # call the handler
-            self.__async_event_port_paused(port_id)
-
-        # port resumed
-        elif event_id == self.EVENT_PORT_RESUMED:
-            port_id = int(data["port_id"])
-            ev = "Port {0} has resumed".format(port_id)
-
-            # call the handler
-            self.__async_event_port_resumed(port_id)
-
-        # port finished traffic
-        elif event_id == self.EVENT_PORT_JOB_DONE:
-            port_id = int(data["port_id"])
-            ev = "Port {0} job done".format(port_id)
-
-            # call the handler
-            self.__async_event_port_job_done(port_id)
-
-            # mark the event for show
-            show_event = True
-
-        # port was acquired - maybe stolen...
-        elif event_id == self.EVENT_PORT_ACQUIRED:
-            session_id = data["session_id"]
-
-            port_id = int(data["port_id"])
-            who = data["who"]
-            force = data["force"]
-
-            # if we hold the port and it was not taken by this session - show it
-            if port_id in self.client.get_acquired_ports() and session_id != self.client.session_id:
-                ev_type = "warning"
-
-            # format the thief/us...
-            if session_id == self.client.session_id:
-                user = "you"
-            elif who == self.client.username:
-                user = "another session of you"
-            else:
-                user = "'{0}'".format(who)
-
-            if force:
-                ev = "Port {0} was forcely taken by {1}".format(port_id, user)
-            else:
-                ev = "Port {0} was taken by {1}".format(port_id, user)
-
-            # call the handler in case its not this session
-            if session_id != self.client.session_id:
-                self.__async_event_port_acquired(port_id, who)
-
-        # port was released
-        elif event_id == self.EVENT_PORT_RELEASED:
-            port_id = int(data["port_id"])
-            who = data["who"]
-            session_id = data["session_id"]
-
-            if session_id == self.client.session_id:
-                user = "you"
-            elif who == self.client.username:
-                user = "another session of you"
-            else:
-                user = "'{0}'".format(who)
-
-            ev = "Port {0} was released by {1}".format(port_id, user)
-
-            # call the handler in case its not this session
-            if session_id != self.client.session_id:
-                self.__async_event_port_released(port_id)
-
-        elif event_id == self.EVENT_PORT_ERROR:
-            port_id = int(data["port_id"])
-            ev = "port {0} job failed".format(port_id)
-            ev_type = "warning"
-
-        # port attr changed
-        elif event_id == self.EVENT_PORT_ATTR_CHG:
-
-            port_id = int(data["port_id"])
-
-            diff = self.__async_event_port_attr_changed(port_id, data["attr"])
-            if not diff:
-                return
-
-            ev = "port {0} attributes changed".format(port_id)
-            for key, (old_val, new_val) in list(diff.items()):
-                ev += "\n  {key}: {old} -> {new}".format(
-                    key=key,
-                    old=old_val.lower() if type(old_val) is str else old_val,
-                    new=new_val.lower() if type(new_val) is str else new_val,
-                )
-
-            ev_type = "info"
-            show_event = False
-
-        # server stopped
-        elif event_id == self.EVENT_SERVER_STOPPED:
-            ev = "Server has been shutdown - cause: '{0}'".format(data["cause"])
-            self.__async_event_server_stopped(ev)
-            ev_type = "warning"
-
-        else:
-            # unknown event - ignore
-            return
-
-        # showed events(port job done,
-        self.__add_event_log("server", ev_type, ev, show_event)
-
-    # private functions
-
-    # on rare cases events may come on a non existent prot
-    # (server was re-run with different config)
-
-    def __async_event_port_job_done(self, port_id):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_port_job_done()
-
-    def __async_event_port_stopped(self, port_id):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_port_stopped()
-
-    def __async_event_port_started(self, port_id):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_port_started()
-
-    def __async_event_port_paused(self, port_id):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_port_paused()
-
-    def __async_event_port_resumed(self, port_id):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_port_resumed()
-
-    def __async_event_port_acquired(self, port_id, who):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_acquired(who)
-
-    def __async_event_port_released(self, port_id):
-        if port_id in self.client.ports:
-            self.client.ports[port_id].async_event_released()
-
-    def __async_event_server_stopped(self, cause):
-        self.client.conn.mark_for_disconnect(cause)
-
-    def __async_event_port_attr_changed(self, port_id, attr):
-        if port_id in self.client.ports:
-            return self.client.ports[port_id].async_event_port_attr_changed(attr)
-
-    # add event to log
-    def __add_event_log(self, origin, ev_type, msg, show_event=False):
-
-        event = Event(origin, ev_type, msg)
-        self.events.append(event)
+import datetime
+import re
+import time
+import traceback
+
+from ..text_opts import format_text
+
+
+# an event
+class Event(object):
+    def __init__(self, origin, ev_type, msg):
+        self.origin = origin
+        self.ev_type = ev_type
+        self.msg = msg
+
+        self.ts = datetime.datetime.fromtimestamp(time.time()).strftime("%Y-%m-%d %H:%M:%S")
+
+    def __str__(self):
+
+        prefix = "[{:^}][{:^}]".format(self.origin, self.ev_type)
+
+        return "{:<10} - {:18} - {:}".format(self.ts, prefix, format_text(self.msg, "bold"))
+
+
+# handles different async events given to the client
+class EventsHandler:
+    EVENT_PORT_STARTED = 0
+    EVENT_PORT_STOPPED = 1
+    EVENT_PORT_PAUSED = 2
+    EVENT_PORT_RESUMED = 3
+    EVENT_PORT_JOB_DONE = 4
+    EVENT_PORT_ACQUIRED = 5
+    EVENT_PORT_RELEASED = 6
+    EVENT_PORT_ERROR = 7
+    EVENT_PORT_ATTR_CHG = 8
+
+    EVENT_SERVER_STOPPED = 100
+
+    def __init__(self, client):
+        self.client = client
+        self.logger = self.client.logger
+
+        self.events = []
+
+        # events are disabled by default until explicitly enabled
+        self.enabled = False
+
+    # will start handling events
+
+    def enable(self):
+        self.enabled = True
+
+    def disable(self):
+        self.enabled = False
+
+    def is_enabled(self):
+        return self.enabled
+
+    # public functions
+
+    def get_events(self, *types: str) -> list:
+        if types:
+            return [ev for ev in self.events if ev.ev_type in types]
+        else:
+            return [ev for ev in self.events]
+
+    def clear_events(self) -> None:
+        self.events = []
+
+    def log_warning(self, msg):
+        self.__add_event_log("local", "warning", msg)
+
+    # events called internally
+
+    def on_async_timeout(self, timeout_sec):
+        if self.client.conn.is_connected():
+            msg = "Connection lost - Subscriber timeout: no data from TRex "
+            +"server for more than {0} seconds".format(timeout_sec)
+            self.log_warning(msg)
+
+            # we cannot simply disconnect the connection - we mark it for disconnection
+            # later on, the main thread will execute an ordered disconnection
+            self.client.conn.mark_for_disconnect(msg)
+
+    def on_async_crash(self):
+        msg = "subscriber thread has crashed:\n\n{0}".format(traceback.format_exc())
+        self.log_warning(msg)
+
+        # if connected, mark as disconnected
+        if self.client.conn.is_connected():
+            self.client.conn.mark_for_disconnect(msg)
+
+    def on_async_alive(self):
+        pass
+
+    def on_async_rx_stats_event(self, data, baseline):
+        if not self.is_enabled():
+            return
+
+        self.client.flow_stats.update(data, baseline)
+
+    def on_async_latency_stats_event(self, data, baseline):
+        if not self.is_enabled():
+            return
+
+        self.client.latency_stats.update(data, baseline)
+
+    # handles an async stats update from the subscriber
+    def on_async_stats_update(self, dump_data, baseline):
+        if not self.is_enabled():
+            return
+
+        global_stats = {}
+        port_stats = {}
+
+        # filter the values per port and general
+        for key, value in list(dump_data.items()):
+            # match a pattern of ports
+            m = re.search(r"(.*)\-(\d+)", key)
+            if m:
+                port_id = int(m.group(2))
+                field_name = m.group(1)
+                if port_id in self.client.ports:
+                    if port_id not in port_stats:
+                        port_stats[port_id] = {}
+                    port_stats[port_id][field_name] = value
+                else:
+                    continue
+            else:
+                # no port match - general stats
+                global_stats[key] = value
+
+        # update the general object with the snapshot
+        self.client.global_stats.update(global_stats, baseline)
+
+        # update all ports
+        for port_id, data in list(port_stats.items()):
+            self.client.ports[port_id].port_stats.update(data, baseline)
+
+    # dispatcher for server async events(port started, port stopped and etc.)
+
+    def on_async_event(self, event_id, data):
+        if not self.is_enabled():
+            return
+
+        # default type info and do not show
+        ev_type = "info"
+        show_event = False
+
+        # port started
+        if event_id == self.EVENT_PORT_STARTED:
+            port_id = int(data["port_id"])
+            ev = "Port {0} has started".format(port_id)
+            self.__async_event_port_started(port_id)
+
+        # port stopped
+        elif event_id == self.EVENT_PORT_STOPPED:
+            port_id = int(data["port_id"])
+            ev = "Port {0} has stopped".format(port_id)
+
+            # call the handler
+            self.__async_event_port_stopped(port_id)
+
+        # port paused
+        elif event_id == self.EVENT_PORT_PAUSED:
+            port_id = int(data["port_id"])
+            ev = "Port {0} has paused".format(port_id)
+
+            # call the handler
+            self.__async_event_port_paused(port_id)
+
+        # port resumed
+        elif event_id == self.EVENT_PORT_RESUMED:
+            port_id = int(data["port_id"])
+            ev = "Port {0} has resumed".format(port_id)
+
+            # call the handler
+            self.__async_event_port_resumed(port_id)
+
+        # port finished traffic
+        elif event_id == self.EVENT_PORT_JOB_DONE:
+            port_id = int(data["port_id"])
+            ev = "Port {0} job done".format(port_id)
+
+            # call the handler
+            self.__async_event_port_job_done(port_id)
+
+            # mark the event for show
+            show_event = True
+
+        # port was acquired - maybe stolen...
+        elif event_id == self.EVENT_PORT_ACQUIRED:
+            session_id = data["session_id"]
+
+            port_id = int(data["port_id"])
+            who = data["who"]
+            force = data["force"]
+
+            # if we hold the port and it was not taken by this session - show it
+            if port_id in self.client.get_acquired_ports() and session_id != self.client.session_id:
+                ev_type = "warning"
+
+            # format the thief/us...
+            if session_id == self.client.session_id:
+                user = "you"
+            elif who == self.client.username:
+                user = "another session of you"
+            else:
+                user = "'{0}'".format(who)
+
+            if force:
+                ev = "Port {0} was forcely taken by {1}".format(port_id, user)
+            else:
+                ev = "Port {0} was taken by {1}".format(port_id, user)
+
+            # call the handler in case its not this session
+            if session_id != self.client.session_id:
+                self.__async_event_port_acquired(port_id, who)
+
+        # port was released
+        elif event_id == self.EVENT_PORT_RELEASED:
+            port_id = int(data["port_id"])
+            who = data["who"]
+            session_id = data["session_id"]
+
+            if session_id == self.client.session_id:
+                user = "you"
+            elif who == self.client.username:
+                user = "another session of you"
+            else:
+                user = "'{0}'".format(who)
+
+            ev = "Port {0} was released by {1}".format(port_id, user)
+
+            # call the handler in case its not this session
+            if session_id != self.client.session_id:
+                self.__async_event_port_released(port_id)
+
+        elif event_id == self.EVENT_PORT_ERROR:
+            port_id = int(data["port_id"])
+            ev = "port {0} job failed".format(port_id)
+            ev_type = "warning"
+
+        # port attr changed
+        elif event_id == self.EVENT_PORT_ATTR_CHG:
+
+            port_id = int(data["port_id"])
+
+            diff = self.__async_event_port_attr_changed(port_id, data["attr"])
+            if not diff:
+                return
+
+            ev = "port {0} attributes changed".format(port_id)
+            for key, (old_val, new_val) in list(diff.items()):
+                ev += "\n  {key}: {old} -> {new}".format(
+                    key=key,
+                    old=old_val.lower() if type(old_val) is str else old_val,
+                    new=new_val.lower() if type(new_val) is str else new_val,
+                )
+
+            ev_type = "info"
+            show_event = False
+
+        # server stopped
+        elif event_id == self.EVENT_SERVER_STOPPED:
+            ev = "Server has been shutdown - cause: '{0}'".format(data["cause"])
+            self.__async_event_server_stopped(ev)
+            ev_type = "warning"
+
+        else:
+            # unknown event - ignore
+            return
+
+        # showed events(port job done,
+        self.__add_event_log("server", ev_type, ev, show_event)
+
+    # private functions
+
+    # on rare cases events may come on a non existent prot
+    # (server was re-run with different config)
+
+    def __async_event_port_job_done(self, port_id):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_port_job_done()
+
+    def __async_event_port_stopped(self, port_id):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_port_stopped()
+
+    def __async_event_port_started(self, port_id):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_port_started()
+
+    def __async_event_port_paused(self, port_id):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_port_paused()
+
+    def __async_event_port_resumed(self, port_id):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_port_resumed()
+
+    def __async_event_port_acquired(self, port_id, who):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_acquired(who)
+
+    def __async_event_port_released(self, port_id):
+        if port_id in self.client.ports:
+            self.client.ports[port_id].async_event_released()
+
+    def __async_event_server_stopped(self, cause):
+        self.client.conn.mark_for_disconnect(cause)
+
+    def __async_event_port_attr_changed(self, port_id, attr):
+        if port_id in self.client.ports:
+            return self.client.ports[port_id].async_event_port_attr_changed(attr)
+
+    # add event to log
+    def __add_event_log(self, origin, ev_type, msg, show_event=False):
+
+        event = Event(origin, ev_type, msg)
+        self.events.append(event)
```

## pytrex/api/trex_stl_async_client.py

 * *Ordering differences only*

```diff
@@ -1,429 +1,429 @@
-#!/usr/bin/python3
-
-import datetime
-import json
-import os
-import random
-import re
-import signal
-import threading
-import time
-
-import zmq
-
-from .. import TrexError
-from ..text_opts import format_num
-from ..zipmsg import ZippedMsg
-
-
-# basic async stats class
-class CTRexAsyncStats(object):
-    def __init__(self):
-        self.ref_point = None
-        self.current = {}
-        self.last_update_ts = datetime.datetime.now()
-
-    def update(self, snapshot):
-
-        # update
-        self.last_update_ts = datetime.datetime.now()
-
-        self.current = snapshot
-
-        if self.ref_point is None:
-            self.ref_point = self.current
-
-    def clear(self):
-        self.ref_point = self.current
-
-    def get(self, field, format=False, suffix=""):
-
-        if field not in self.current:
-            return "N/A"
-
-        if not format:
-            return self.current[field]
-        else:
-            return format_num(self.current[field], suffix)
-
-    def get_rel(self, field, format=False, suffix=""):
-        if field not in self.current:
-            return "N/A"
-
-        if not format:
-            return self.current[field] - self.ref_point[field]
-        else:
-            return format_num(self.current[field] - self.ref_point[field], suffix)
-
-    # return true if new data has arrived in the past 2 seconds
-    def is_online(self):
-        delta_ms = (datetime.datetime.now() - self.last_update_ts).total_seconds() * 1000
-        return delta_ms < 2000
-
-
-# describes the general stats provided by TRex
-class CTRexAsyncStatsGeneral(CTRexAsyncStats):
-    def __init__(self):
-        super(CTRexAsyncStatsGeneral, self).__init__()
-
-
-# per port stats
-class CTRexAsyncStatsPort(CTRexAsyncStats):
-    def __init__(self):
-        super(CTRexAsyncStatsPort, self).__init__()
-
-    def get_stream_stats(self, stream_id):
-        return None
-
-
-# stats manager
-class CTRexAsyncStatsManager:
-    def __init__(self):
-
-        self.general_stats = CTRexAsyncStatsGeneral()
-        self.port_stats = {}
-
-    def get_general_stats(self):
-        return self.general_stats
-
-    def get_port_stats(self, port_id):
-
-        if not str(port_id) in self.port_stats:
-            return None
-
-        return self.port_stats[str(port_id)]
-
-    def update(self, data):
-        self.__handle_snapshot(data)
-
-    def __handle_snapshot(self, snapshot):
-
-        general_stats = {}
-        port_stats = {}
-
-        # filter the values per port and general
-        for key, value in list(snapshot.items()):
-
-            # match a pattern of ports
-            m = re.search(r"(.*)\-([0-8])", key)
-            if m:
-
-                port_id = m.group(2)
-                field_name = m.group(1)
-
-                if port_id not in port_stats:
-                    port_stats[port_id] = {}
-
-                port_stats[port_id][field_name] = value
-
-            else:
-                # no port match - general stats
-                general_stats[key] = value
-
-        # update the general object with the snapshot
-        self.general_stats.update(general_stats)
-
-        # update all ports
-        for port_id, data in list(port_stats.items()):
-
-            if port_id not in self.port_stats:
-                self.port_stats[port_id] = CTRexAsyncStatsPort()
-
-            self.port_stats[port_id].update(data)
-
-
-class CTRexAsyncClient:
-    THREAD_STATE_ACTIVE = 1
-    THREAD_STATE_ZOMBIE = 2
-    THREAD_STATE_DEAD = 3
-
-    def __init__(self, server, port, stateless_client):
-
-        self.port = port
-        self.server = server
-
-        self.stateless_client = stateless_client
-
-        self.event_handler = stateless_client.event_handler
-        self.logger = self.stateless_client.logger
-
-        self.raw_snapshot = {}
-
-        self.stats = CTRexAsyncStatsManager()
-
-        self.last_data_recv_ts = 0
-        self.async_barrier = None
-
-        self.monitor = AsyncUtil()
-
-        self.connected = False
-
-        self.zipped = ZippedMsg()
-
-        self.t_state = self.THREAD_STATE_DEAD
-
-    # connects the async channel
-    def connect(self):
-
-        if self.connected:
-            self.disconnect()
-
-        self.tr = "tcp://{0}:{1}".format(self.server, self.port)
-
-        #  Socket to talk to server
-        self.context = zmq.Context()
-        self.socket = self.context.socket(zmq.SUB)
-
-        # before running the thread - mark as active
-        self.t_state = self.THREAD_STATE_ACTIVE
-        self.t = threading.Thread(target=self._run_safe)
-
-        # kill this thread on exit and don't add it to the join list
-        self.t.setDaemon(True)
-        self.t.start()
-
-        self.connected = True
-
-        # v2.61 requires barrier, v2.99 does not
-        # https: // github.com / cisco - system - traffic - generator / trex - core / issues / 698
-        # first barrier - make sure async thread is up
-        try:
-            self.barrier()
-        except TrexError:
-            pass
-
-    # disconnect
-    def disconnect(self):
-        if not self.connected:
-            return
-
-        # mark for join
-        self.t_state = self.THREAD_STATE_DEAD
-        self.context.term()
-        self.t.join()
-
-        # done
-        self.connected = False
-
-    # set the thread as a zombie(in case of server death)
-
-    def set_as_zombie(self):
-        self.last_data_recv_ts = None
-        self.t_state = self.THREAD_STATE_ZOMBIE
-
-    # return the timeout in seconds for the ZMQ subscriber thread
-    def get_timeout_sec(self):
-        return 3
-
-    def _run_safe(self):
-
-        # socket must be created on the same thread
-        self.socket.setsockopt(zmq.SUBSCRIBE, b"")
-        self.socket.setsockopt(zmq.RCVTIMEO, self.get_timeout_sec() * 1000)
-        self.socket.connect(self.tr)
-
-        try:
-            self._run()
-        except Exception as error:
-            self.event_handler.on_async_crash()
-
-            # kill all the threads in the group
-            os.killpg(os.getppid(), signal.SIGTERM)
-            print("General failure: " + error)
-
-        finally:
-            # closing of socket must be from the same thread
-            self.socket.close(linger=0)
-
-    # thread function
-    def _run(self):
-
-        got_data = False
-
-        self.monitor.reset()
-
-        while self.t_state != self.THREAD_STATE_DEAD:
-            try:
-                with self.monitor:
-                    line = self.socket.recv()
-
-                # last data recv.
-                self.last_data_recv_ts = time.time()
-
-                # if thread was marked as zombie
-                # - it does nothing besides fetching messages
-                if self.t_state == self.THREAD_STATE_ZOMBIE:
-                    continue
-
-                self.monitor.on_recv_msg(line)
-
-                # try to decomrpess
-                unzipped = self.zipped.decompress(line)
-                if unzipped:
-                    line = unzipped
-
-                line = line.decode()
-
-                # signal once
-                if not got_data:
-                    self.event_handler.on_async_alive()
-                    got_data = True
-
-            # got a timeout - mark as not alive and retry
-            except zmq.Again:
-                # signal once
-                if got_data:
-                    self.event_handler.on_async_timeout(self.get_timeout_sec())
-                    got_data = False
-
-                continue
-
-            except zmq.ContextTerminated:
-                # outside thread signaled us to exit
-                assert self.t_state != self.THREAD_STATE_ACTIVE
-                break
-
-            msg = json.loads(line)
-            name = msg["name"]
-            data = msg["data"]
-            type = msg["type"]
-            baseline = msg.get("baseline", False)
-
-            self.raw_snapshot[name] = data
-
-            self.__dispatch(name, type, data, baseline)
-
-    def get_stats(self):
-        return self.stats
-
-    def get_raw_snapshot(self):
-        return self.raw_snapshot
-
-    # dispatch the message to the right place
-    def __dispatch(self, name, type, data, baseline):
-
-        # stats
-        if name == "trex-global":
-            self.event_handler.on_async_stats_update(data, baseline)
-
-        # events
-        elif name == "trex-event":
-            self.event_handler.on_async_event(type, data)
-
-        # barriers
-        elif name == "trex-barrier":
-            self.handle_async_barrier(type, data)
-
-        elif name == "flow_stats":
-            self.event_handler.on_async_rx_stats_event(data, baseline)
-
-        elif name == "latency_stats":
-            self.event_handler.on_async_latency_stats_event(data, baseline)
-
-        else:
-            pass
-
-    # async barrier handling routine
-    def handle_async_barrier(self, type, data):
-        if self.async_barrier["key"] == type:
-            self.async_barrier["ack"] = True
-
-    # block on barrier for async channel
-    def barrier(self, timeout=5, baseline=False):
-
-        # set a random key
-        key = random.getrandbits(32)
-        self.async_barrier = {"key": key, "ack": False}
-
-        # expr time
-        expr = time.time() + timeout
-
-        while not self.async_barrier["ack"]:
-
-            # inject
-            rc = self.stateless_client.transmit("publish_now", params={"key": key, "baseline": baseline})
-            if not rc:
-                return rc
-
-            # fast loop
-            for _ in range(0, 100):
-                if self.async_barrier["ack"]:
-                    break
-                time.sleep(0.001)
-
-            if time.time() > expr:
-                raise TrexError("*** [subscriber] - timeout " + "- no data flow from server at : " + self.tr)
-
-
-# a class to measure util. of async subscriber thread
-class AsyncUtil(object):
-
-    STATE_SLEEP = 1
-    STATE_AWAKE = 2
-
-    def __init__(self):
-        self.reset()
-
-    def reset(self):
-        self.state = self.STATE_AWAKE
-        self.clock = time.time()
-
-        # reset the current interval
-        self.interval = {"ts": time.time(), "total_sleep": 0, "total_bits": 0}
-
-        # global counters
-        self.cpu_util = 0
-        self.bps = 0
-
-    def on_recv_msg(self, message):
-        self.interval["total_bits"] += len(message) * 8.0
-
-        self._tick()
-
-    def __enter__(self):
-        assert self.state == self.STATE_AWAKE
-        self.state = self.STATE_SLEEP
-
-        self.sleep_start_ts = time.time()
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        assert self.state == self.STATE_SLEEP
-        self.state = self.STATE_AWAKE
-
-        # measure total sleep time for interval
-        self.interval["total_sleep"] += time.time() - self.sleep_start_ts
-
-        self._tick()
-
-    def _tick(self):
-        # how much time did the current interval lasted
-        ts = time.time() - self.interval["ts"]
-        if ts < 1:
-            return
-
-        # if tick is in the middle of sleep - add the interval and reset
-        if self.state == self.STATE_SLEEP:
-            self.interval["total_sleep"] += time.time() - self.sleep_start_ts
-            self.sleep_start_ts = time.time()
-
-        # add the interval
-        if self.interval["total_sleep"] > 0:
-            # calculate
-            self.cpu_util = self.cpu_util * 0.75 + (float(ts - self.interval["total_sleep"]) / ts) * 0.25
-            self.interval["total_sleep"] = 0
-
-        if self.interval["total_bits"] > 0:
-            # calculate
-            self.bps = self.bps * 0.75 + (self.interval["total_bits"] / ts) * 0.25
-            self.interval["total_bits"] = 0
-
-        # reset the interval's clock
-        self.interval["ts"] = time.time()
-
-    def get_cpu_util(self):
-        self._tick()
-        return self.cpu_util * 100
-
-    def get_bps(self):
-        self._tick()
-        return self.bps
+#!/usr/bin/python3
+
+import datetime
+import json
+import os
+import random
+import re
+import signal
+import threading
+import time
+
+import zmq
+
+from .. import TrexError
+from ..text_opts import format_num
+from ..zipmsg import ZippedMsg
+
+
+# basic async stats class
+class CTRexAsyncStats(object):
+    def __init__(self):
+        self.ref_point = None
+        self.current = {}
+        self.last_update_ts = datetime.datetime.now()
+
+    def update(self, snapshot):
+
+        # update
+        self.last_update_ts = datetime.datetime.now()
+
+        self.current = snapshot
+
+        if self.ref_point is None:
+            self.ref_point = self.current
+
+    def clear(self):
+        self.ref_point = self.current
+
+    def get(self, field, format=False, suffix=""):
+
+        if field not in self.current:
+            return "N/A"
+
+        if not format:
+            return self.current[field]
+        else:
+            return format_num(self.current[field], suffix)
+
+    def get_rel(self, field, format=False, suffix=""):
+        if field not in self.current:
+            return "N/A"
+
+        if not format:
+            return self.current[field] - self.ref_point[field]
+        else:
+            return format_num(self.current[field] - self.ref_point[field], suffix)
+
+    # return true if new data has arrived in the past 2 seconds
+    def is_online(self):
+        delta_ms = (datetime.datetime.now() - self.last_update_ts).total_seconds() * 1000
+        return delta_ms < 2000
+
+
+# describes the general stats provided by TRex
+class CTRexAsyncStatsGeneral(CTRexAsyncStats):
+    def __init__(self):
+        super(CTRexAsyncStatsGeneral, self).__init__()
+
+
+# per port stats
+class CTRexAsyncStatsPort(CTRexAsyncStats):
+    def __init__(self):
+        super(CTRexAsyncStatsPort, self).__init__()
+
+    def get_stream_stats(self, stream_id):
+        return None
+
+
+# stats manager
+class CTRexAsyncStatsManager:
+    def __init__(self):
+
+        self.general_stats = CTRexAsyncStatsGeneral()
+        self.port_stats = {}
+
+    def get_general_stats(self):
+        return self.general_stats
+
+    def get_port_stats(self, port_id):
+
+        if not str(port_id) in self.port_stats:
+            return None
+
+        return self.port_stats[str(port_id)]
+
+    def update(self, data):
+        self.__handle_snapshot(data)
+
+    def __handle_snapshot(self, snapshot):
+
+        general_stats = {}
+        port_stats = {}
+
+        # filter the values per port and general
+        for key, value in list(snapshot.items()):
+
+            # match a pattern of ports
+            m = re.search(r"(.*)\-([0-8])", key)
+            if m:
+
+                port_id = m.group(2)
+                field_name = m.group(1)
+
+                if port_id not in port_stats:
+                    port_stats[port_id] = {}
+
+                port_stats[port_id][field_name] = value
+
+            else:
+                # no port match - general stats
+                general_stats[key] = value
+
+        # update the general object with the snapshot
+        self.general_stats.update(general_stats)
+
+        # update all ports
+        for port_id, data in list(port_stats.items()):
+
+            if port_id not in self.port_stats:
+                self.port_stats[port_id] = CTRexAsyncStatsPort()
+
+            self.port_stats[port_id].update(data)
+
+
+class CTRexAsyncClient:
+    THREAD_STATE_ACTIVE = 1
+    THREAD_STATE_ZOMBIE = 2
+    THREAD_STATE_DEAD = 3
+
+    def __init__(self, server, port, stateless_client):
+
+        self.port = port
+        self.server = server
+
+        self.stateless_client = stateless_client
+
+        self.event_handler = stateless_client.event_handler
+        self.logger = self.stateless_client.logger
+
+        self.raw_snapshot = {}
+
+        self.stats = CTRexAsyncStatsManager()
+
+        self.last_data_recv_ts = 0
+        self.async_barrier = None
+
+        self.monitor = AsyncUtil()
+
+        self.connected = False
+
+        self.zipped = ZippedMsg()
+
+        self.t_state = self.THREAD_STATE_DEAD
+
+    # connects the async channel
+    def connect(self):
+
+        if self.connected:
+            self.disconnect()
+
+        self.tr = "tcp://{0}:{1}".format(self.server, self.port)
+
+        #  Socket to talk to server
+        self.context = zmq.Context()
+        self.socket = self.context.socket(zmq.SUB)
+
+        # before running the thread - mark as active
+        self.t_state = self.THREAD_STATE_ACTIVE
+        self.t = threading.Thread(target=self._run_safe)
+
+        # kill this thread on exit and don't add it to the join list
+        self.t.setDaemon(True)
+        self.t.start()
+
+        self.connected = True
+
+        # v2.61 requires barrier, v2.99 does not
+        # https: // github.com / cisco - system - traffic - generator / trex - core / issues / 698
+        # first barrier - make sure async thread is up
+        try:
+            self.barrier()
+        except TrexError:
+            pass
+
+    # disconnect
+    def disconnect(self):
+        if not self.connected:
+            return
+
+        # mark for join
+        self.t_state = self.THREAD_STATE_DEAD
+        self.context.term()
+        self.t.join()
+
+        # done
+        self.connected = False
+
+    # set the thread as a zombie(in case of server death)
+
+    def set_as_zombie(self):
+        self.last_data_recv_ts = None
+        self.t_state = self.THREAD_STATE_ZOMBIE
+
+    # return the timeout in seconds for the ZMQ subscriber thread
+    def get_timeout_sec(self):
+        return 3
+
+    def _run_safe(self):
+
+        # socket must be created on the same thread
+        self.socket.setsockopt(zmq.SUBSCRIBE, b"")
+        self.socket.setsockopt(zmq.RCVTIMEO, self.get_timeout_sec() * 1000)
+        self.socket.connect(self.tr)
+
+        try:
+            self._run()
+        except Exception as error:
+            self.event_handler.on_async_crash()
+
+            # kill all the threads in the group
+            os.killpg(os.getppid(), signal.SIGTERM)
+            print("General failure: " + error)
+
+        finally:
+            # closing of socket must be from the same thread
+            self.socket.close(linger=0)
+
+    # thread function
+    def _run(self):
+
+        got_data = False
+
+        self.monitor.reset()
+
+        while self.t_state != self.THREAD_STATE_DEAD:
+            try:
+                with self.monitor:
+                    line = self.socket.recv()
+
+                # last data recv.
+                self.last_data_recv_ts = time.time()
+
+                # if thread was marked as zombie
+                # - it does nothing besides fetching messages
+                if self.t_state == self.THREAD_STATE_ZOMBIE:
+                    continue
+
+                self.monitor.on_recv_msg(line)
+
+                # try to decomrpess
+                unzipped = self.zipped.decompress(line)
+                if unzipped:
+                    line = unzipped
+
+                line = line.decode()
+
+                # signal once
+                if not got_data:
+                    self.event_handler.on_async_alive()
+                    got_data = True
+
+            # got a timeout - mark as not alive and retry
+            except zmq.Again:
+                # signal once
+                if got_data:
+                    self.event_handler.on_async_timeout(self.get_timeout_sec())
+                    got_data = False
+
+                continue
+
+            except zmq.ContextTerminated:
+                # outside thread signaled us to exit
+                assert self.t_state != self.THREAD_STATE_ACTIVE
+                break
+
+            msg = json.loads(line)
+            name = msg["name"]
+            data = msg["data"]
+            type = msg["type"]
+            baseline = msg.get("baseline", False)
+
+            self.raw_snapshot[name] = data
+
+            self.__dispatch(name, type, data, baseline)
+
+    def get_stats(self):
+        return self.stats
+
+    def get_raw_snapshot(self):
+        return self.raw_snapshot
+
+    # dispatch the message to the right place
+    def __dispatch(self, name, type, data, baseline):
+
+        # stats
+        if name == "trex-global":
+            self.event_handler.on_async_stats_update(data, baseline)
+
+        # events
+        elif name == "trex-event":
+            self.event_handler.on_async_event(type, data)
+
+        # barriers
+        elif name == "trex-barrier":
+            self.handle_async_barrier(type, data)
+
+        elif name == "flow_stats":
+            self.event_handler.on_async_rx_stats_event(data, baseline)
+
+        elif name == "latency_stats":
+            self.event_handler.on_async_latency_stats_event(data, baseline)
+
+        else:
+            pass
+
+    # async barrier handling routine
+    def handle_async_barrier(self, type, data):
+        if self.async_barrier["key"] == type:
+            self.async_barrier["ack"] = True
+
+    # block on barrier for async channel
+    def barrier(self, timeout=5, baseline=False):
+
+        # set a random key
+        key = random.getrandbits(32)
+        self.async_barrier = {"key": key, "ack": False}
+
+        # expr time
+        expr = time.time() + timeout
+
+        while not self.async_barrier["ack"]:
+
+            # inject
+            rc = self.stateless_client.transmit("publish_now", params={"key": key, "baseline": baseline})
+            if not rc:
+                return rc
+
+            # fast loop
+            for _ in range(0, 100):
+                if self.async_barrier["ack"]:
+                    break
+                time.sleep(0.001)
+
+            if time.time() > expr:
+                raise TrexError("*** [subscriber] - timeout " + "- no data flow from server at : " + self.tr)
+
+
+# a class to measure util. of async subscriber thread
+class AsyncUtil(object):
+
+    STATE_SLEEP = 1
+    STATE_AWAKE = 2
+
+    def __init__(self):
+        self.reset()
+
+    def reset(self):
+        self.state = self.STATE_AWAKE
+        self.clock = time.time()
+
+        # reset the current interval
+        self.interval = {"ts": time.time(), "total_sleep": 0, "total_bits": 0}
+
+        # global counters
+        self.cpu_util = 0
+        self.bps = 0
+
+    def on_recv_msg(self, message):
+        self.interval["total_bits"] += len(message) * 8.0
+
+        self._tick()
+
+    def __enter__(self):
+        assert self.state == self.STATE_AWAKE
+        self.state = self.STATE_SLEEP
+
+        self.sleep_start_ts = time.time()
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        assert self.state == self.STATE_SLEEP
+        self.state = self.STATE_AWAKE
+
+        # measure total sleep time for interval
+        self.interval["total_sleep"] += time.time() - self.sleep_start_ts
+
+        self._tick()
+
+    def _tick(self):
+        # how much time did the current interval lasted
+        ts = time.time() - self.interval["ts"]
+        if ts < 1:
+            return
+
+        # if tick is in the middle of sleep - add the interval and reset
+        if self.state == self.STATE_SLEEP:
+            self.interval["total_sleep"] += time.time() - self.sleep_start_ts
+            self.sleep_start_ts = time.time()
+
+        # add the interval
+        if self.interval["total_sleep"] > 0:
+            # calculate
+            self.cpu_util = self.cpu_util * 0.75 + (float(ts - self.interval["total_sleep"]) / ts) * 0.25
+            self.interval["total_sleep"] = 0
+
+        if self.interval["total_bits"] > 0:
+            # calculate
+            self.bps = self.bps * 0.75 + (self.interval["total_bits"] / ts) * 0.25
+            self.interval["total_bits"] = 0
+
+        # reset the interval's clock
+        self.interval["ts"] = time.time()
+
+    def get_cpu_util(self):
+        self._tick()
+        return self.cpu_util * 100
+
+    def get_bps(self):
+        self._tick()
+        return self.bps
```

## pytrex/api/trex_stl_conn.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-import os
-import signal
-import time
-
-from .trex_stl_async_client import CTRexAsyncClient
-from .trex_stl_jsonrpc_client import JsonRpcClient
-
-# ###########################     RPC layer     #############################
-# ###########################                   #############################
-# ###########################                   #############################
-
-
-class CCommLink(object):
-    """Describes the connectivity of the stateless client method"""
-
-    def __init__(self, server="localhost", port=5050, virtual=False, client=None):
-        self.virtual = virtual
-        self.server = server
-        self.port = port
-        self.rpc_link = JsonRpcClient(self.server, self.port, client)
-
-    @property
-    def is_connected(self):
-        if not self.virtual:
-            return self.rpc_link.connected
-        else:
-            return True
-
-    def get_server(self):
-        return self.server
-
-    def get_port(self):
-        return self.port
-
-    def connect(self):
-        if not self.virtual:
-            self.rpc_link.connect()
-
-    def disconnect(self):
-        if not self.virtual:
-            return self.rpc_link.disconnect()
-
-    def transmit(self, method_name, params=None, api_class="core", retry=0):
-        if self.virtual:
-            self._prompt_virtual_tx_msg()
-            _, msg = self.rpc_link.create_jsonrpc_v2(method_name, params, api_class)
-            print(msg)
-            return
-        else:
-            return self.rpc_link.invoke_rpc_method(method_name, params, api_class, retry=retry)
-
-    def transmit_batch(self, batch_list, retry=0):
-        if self.virtual:
-            self._prompt_virtual_tx_msg()
-            print(
-                [
-                    msg
-                    for _, msg in [
-                        self.rpc_link.create_jsonrpc_v2(command.method, command.params, command.api_class)
-                        for command in batch_list
-                    ]
-                ]
-            )
-        else:
-            batch = self.rpc_link.create_batch()
-            for command in batch_list:
-                batch.add(command.method, command.params, command.api_class)
-            # invoke the batch
-            return batch.invoke(retry=retry)
-
-    def _prompt_virtual_tx_msg(self):
-        print(("Transmitting virtually over tcp://{server}:{port}".format(server=self.server, port=self.port)))
-
-
-class Connection:
-    """Manage that connection to the server.
-
-    Connection state object describes the connection to the server state.
-
-    Can be either fully disconnected, fully connected or marked for disconnection.
-    """
-
-    DISCONNECTED = 1
-    CONNECTED = 2
-    MARK_FOR_DISCONNECT = 3
-
-    def __init__(self, conn_info, logger, client):
-
-        self.conn_info = conn_info
-        self.logger = logger
-        self.sigint_on_conn_lost = False
-
-        # API classes
-        self.api_vers = [{"type": "core", "major": 5, "minor": 1}]
-
-        # low level RPC layer
-        self.rpc = CCommLink(
-            self.conn_info["server"],
-            self.conn_info["sync_port"],
-            self.conn_info["virtual"],
-            client,
-        )
-
-        self.async_connection = CTRexAsyncClient(self.conn_info["server"], self.conn_info["async_port"], client)
-
-        # save pointers
-        self.conn_info = conn_info
-
-        # init state
-        self.state = (self.DISCONNECTED, None)
-        self.api_h = {"core": None}
-
-    def disconnect(self):
-        """
-        disconnect from both channels
-        sync and async
-        """
-        try:
-            self.rpc.disconnect()
-            self.async_connection.disconnect()
-
-        finally:
-            self.state = (self.DISCONNECTED, None)
-            self.api_h = {"core": None}
-
-    def connect(self):
-        """Connect to the server(two channels)."""
-
-        # first disconnect if already connected
-        if self.is_connected():
-            self.disconnect()
-
-        # connect
-        self.__connect()
-
-    def get_api_h(self):
-        """
-        return the API handlers for each component
-        """
-        return self.api_h
-
-    def barrier(self):
-        """
-        executes a barrier
-        when it retruns, an async barrier is guaranteed
-        """
-        return self.async_connection.barrier()
-
-    def sync(self):
-        """
-        fully sync the client with the server
-        must be called after all the config
-        was done
-        """
-        return self.async_connection.barrier(baseline=True)
-
-    def mark_for_disconnect(self, cause):
-        """
-        A multithread safe call
-        any thread can mark the current connection
-        as not valid
-        and will require the main thread to reconnect
-        """
-
-        # avoid any messages handling for the async thread
-        self.async_connection.set_as_zombie()
-
-        # change state
-        self.state = (self.MARK_FOR_DISCONNECT, cause)
-
-        # if the flag is on, a SIGINT will be sent to the main thread
-        # causing the ZMQ RPC to stop what it's doing and report an error
-        if self.sigint_on_conn_lost:
-            os.kill(os.getpid(), signal.SIGINT)
-
-    def sigint_on_conn_lost_enable(self):
-        """
-        when enabled, if connection
-        is lost a SIGINT will be sent
-        to the main thread
-        """
-        self.sigint_on_conn_lost = True
-
-    def sigint_on_conn_lost_disable(self):
-        """
-        disable SIGINT dispatching
-        on case of connection lost
-        """
-        self.sigint_on_conn_lost = False
-
-    def is_alive(self):
-        """
-        return True if any data has arrived
-        the server in the last 3 seconds
-        """
-        return self.async_connection.last_data_recv_ts is not None and (
-            (time.time() - self.async_connection.last_data_recv_ts) <= 3
-        )
-
-    def is_connected(self):
-        return self.state[0] == self.CONNECTED
-
-    def is_marked_for_disconnect(self):
-        return self.state[0] == self.MARK_FOR_DISCONNECT
-
-    def get_disconnection_cause(self):
-        return self.state[1]
-
-    # ######### private ################
-
-    def __connect(self):
-
-        # start with the sync channel
-        self.logger.info(f'Connecting to RPC server on {self.conn_info["server"]}:{self.conn_info["sync_port"]}')
-        self.rpc.connect()
-
-        # API sync
-        rc = self.rpc.transmit("api_sync", params={"api_vers": self.api_vers}, api_class=None)
-        self.logger.debug(rc)
-
-        # get the API_H
-        for api in rc["result"]["api_vers"]:
-            self.api_h[api["type"]] = api["api_h"]
-
-        # connect async channel
-        self.logger.info(f'Connecting to publisher server on {self.conn_info["server"]}:{self.conn_info["async_port"]}')
-        rc = self.async_connection.connect()
-        self.logger.debug(rc)
-
-        if not rc:
-            return rc
-
-        self.state = (self.CONNECTED, None)
+import os
+import signal
+import time
+
+from .trex_stl_async_client import CTRexAsyncClient
+from .trex_stl_jsonrpc_client import JsonRpcClient
+
+# ###########################     RPC layer     #############################
+# ###########################                   #############################
+# ###########################                   #############################
+
+
+class CCommLink(object):
+    """Describes the connectivity of the stateless client method"""
+
+    def __init__(self, server="localhost", port=5050, virtual=False, client=None):
+        self.virtual = virtual
+        self.server = server
+        self.port = port
+        self.rpc_link = JsonRpcClient(self.server, self.port, client)
+
+    @property
+    def is_connected(self):
+        if not self.virtual:
+            return self.rpc_link.connected
+        else:
+            return True
+
+    def get_server(self):
+        return self.server
+
+    def get_port(self):
+        return self.port
+
+    def connect(self):
+        if not self.virtual:
+            self.rpc_link.connect()
+
+    def disconnect(self):
+        if not self.virtual:
+            return self.rpc_link.disconnect()
+
+    def transmit(self, method_name, params=None, api_class="core", retry=0):
+        if self.virtual:
+            self._prompt_virtual_tx_msg()
+            _, msg = self.rpc_link.create_jsonrpc_v2(method_name, params, api_class)
+            print(msg)
+            return
+        else:
+            return self.rpc_link.invoke_rpc_method(method_name, params, api_class, retry=retry)
+
+    def transmit_batch(self, batch_list, retry=0):
+        if self.virtual:
+            self._prompt_virtual_tx_msg()
+            print(
+                [
+                    msg
+                    for _, msg in [
+                        self.rpc_link.create_jsonrpc_v2(command.method, command.params, command.api_class)
+                        for command in batch_list
+                    ]
+                ]
+            )
+        else:
+            batch = self.rpc_link.create_batch()
+            for command in batch_list:
+                batch.add(command.method, command.params, command.api_class)
+            # invoke the batch
+            return batch.invoke(retry=retry)
+
+    def _prompt_virtual_tx_msg(self):
+        print(("Transmitting virtually over tcp://{server}:{port}".format(server=self.server, port=self.port)))
+
+
+class Connection:
+    """Manage that connection to the server.
+
+    Connection state object describes the connection to the server state.
+
+    Can be either fully disconnected, fully connected or marked for disconnection.
+    """
+
+    DISCONNECTED = 1
+    CONNECTED = 2
+    MARK_FOR_DISCONNECT = 3
+
+    def __init__(self, conn_info, logger, client):
+
+        self.conn_info = conn_info
+        self.logger = logger
+        self.sigint_on_conn_lost = False
+
+        # API classes
+        self.api_vers = [{"type": "core", "major": 5, "minor": 1}]
+
+        # low level RPC layer
+        self.rpc = CCommLink(
+            self.conn_info["server"],
+            self.conn_info["sync_port"],
+            self.conn_info["virtual"],
+            client,
+        )
+
+        self.async_connection = CTRexAsyncClient(self.conn_info["server"], self.conn_info["async_port"], client)
+
+        # save pointers
+        self.conn_info = conn_info
+
+        # init state
+        self.state = (self.DISCONNECTED, None)
+        self.api_h = {"core": None}
+
+    def disconnect(self):
+        """
+        disconnect from both channels
+        sync and async
+        """
+        try:
+            self.rpc.disconnect()
+            self.async_connection.disconnect()
+
+        finally:
+            self.state = (self.DISCONNECTED, None)
+            self.api_h = {"core": None}
+
+    def connect(self):
+        """Connect to the server(two channels)."""
+
+        # first disconnect if already connected
+        if self.is_connected():
+            self.disconnect()
+
+        # connect
+        self.__connect()
+
+    def get_api_h(self):
+        """
+        return the API handlers for each component
+        """
+        return self.api_h
+
+    def barrier(self):
+        """
+        executes a barrier
+        when it retruns, an async barrier is guaranteed
+        """
+        return self.async_connection.barrier()
+
+    def sync(self):
+        """
+        fully sync the client with the server
+        must be called after all the config
+        was done
+        """
+        return self.async_connection.barrier(baseline=True)
+
+    def mark_for_disconnect(self, cause):
+        """
+        A multithread safe call
+        any thread can mark the current connection
+        as not valid
+        and will require the main thread to reconnect
+        """
+
+        # avoid any messages handling for the async thread
+        self.async_connection.set_as_zombie()
+
+        # change state
+        self.state = (self.MARK_FOR_DISCONNECT, cause)
+
+        # if the flag is on, a SIGINT will be sent to the main thread
+        # causing the ZMQ RPC to stop what it's doing and report an error
+        if self.sigint_on_conn_lost:
+            os.kill(os.getpid(), signal.SIGINT)
+
+    def sigint_on_conn_lost_enable(self):
+        """
+        when enabled, if connection
+        is lost a SIGINT will be sent
+        to the main thread
+        """
+        self.sigint_on_conn_lost = True
+
+    def sigint_on_conn_lost_disable(self):
+        """
+        disable SIGINT dispatching
+        on case of connection lost
+        """
+        self.sigint_on_conn_lost = False
+
+    def is_alive(self):
+        """
+        return True if any data has arrived
+        the server in the last 3 seconds
+        """
+        return self.async_connection.last_data_recv_ts is not None and (
+            (time.time() - self.async_connection.last_data_recv_ts) <= 3
+        )
+
+    def is_connected(self):
+        return self.state[0] == self.CONNECTED
+
+    def is_marked_for_disconnect(self):
+        return self.state[0] == self.MARK_FOR_DISCONNECT
+
+    def get_disconnection_cause(self):
+        return self.state[1]
+
+    # ######### private ################
+
+    def __connect(self):
+
+        # start with the sync channel
+        self.logger.info(f'Connecting to RPC server on {self.conn_info["server"]}:{self.conn_info["sync_port"]}')
+        self.rpc.connect()
+
+        # API sync
+        rc = self.rpc.transmit("api_sync", params={"api_vers": self.api_vers}, api_class=None)
+        self.logger.debug(rc)
+
+        # get the API_H
+        for api in rc["result"]["api_vers"]:
+            self.api_h[api["type"]] = api["api_h"]
+
+        # connect async channel
+        self.logger.info(f'Connecting to publisher server on {self.conn_info["server"]}:{self.conn_info["async_port"]}')
+        rc = self.async_connection.connect()
+        self.logger.debug(rc)
+
+        if not rc:
+            return rc
+
+        self.state = (self.CONNECTED, None)
```

## pytrex/api/trex_stl_jsonrpc_client.py

 * *Ordering differences only*

```diff
@@ -1,239 +1,239 @@
-import json
-from threading import Lock
-
-import zmq
-
-from .. import TrexError
-from ..common import random_id_gen
-from ..zipmsg import ZippedMsg
-
-
-# sub class to describe a batch
-class BatchMessage:
-    def __init__(self, rpc_client):
-        self.rpc_client = rpc_client
-        self.batch_list = []
-
-    def add(self, method_name, params=None, api_class="core"):
-
-        id, msg = self.rpc_client.create_jsonrpc_v2(method_name, params, api_class, encode=False)
-        self.batch_list.append(msg)
-
-    def invoke(self, chunk_size=500000, retry=0) -> list:
-        if not self.rpc_client.connected:
-            raise TrexError("Not connected to server")
-
-        if chunk_size:
-            response_batch = []
-            size = 0
-            new_batch = []
-            for msg in self.batch_list:
-                size += len(json.dumps(msg))
-                new_batch.append(msg)
-
-                if size > chunk_size:
-                    batch_json = json.dumps(new_batch)
-                    response_batch += self.rpc_client.send_msg(batch_json)
-                    size = 0
-                    new_batch = []
-            if new_batch:
-                batch_json = json.dumps(new_batch)
-                response_batch = self.rpc_client.send_msg(batch_json)
-            return response_batch
-        else:
-            batch_json = json.dumps(self.batch_list)
-            return self.rpc_client.send_msg(batch_json, retry=retry)
-
-
-# JSON RPC v2.0 client
-class JsonRpcClient:
-    def __init__(self, default_server, default_port, client):
-        self.get_api_h = client._get_api_h
-        self.logger = client.logger
-        self.connected = False
-
-        # default values
-        self.port = default_port
-        self.server = default_server
-
-        self.id_gen = random_id_gen()
-        self.zipper = ZippedMsg()
-
-        self.lock = Lock()
-
-    def get_connection_details(self):
-        rc = {}
-        rc["server"] = self.server
-        rc["port"] = self.port
-
-        return rc
-
-    # batch messages
-
-    def create_batch(self):
-        return BatchMessage(self)
-
-    def create_jsonrpc_v2(self, method_name, params=None, api_class="core", encode=True):
-        msg = {}
-        msg["jsonrpc"] = "2.0"
-        msg["method"] = method_name
-        msg["id"] = next(self.id_gen)
-        msg["params"] = params if params is not None else {}
-
-        # if this RPC has an API class - add it's handler
-        if api_class:
-            msg["params"]["api_h"] = self.get_api_h()[api_class]
-
-        # pdb.set_trace()
-        if encode:
-            return id, json.dumps(msg)
-        else:
-            return id, msg
-
-    def invoke_rpc_method(self, method_name, params=None, api_class="core", retry=0):
-        if not self.connected:
-            raise TrexError("Not connected to server")
-
-        id, msg = self.create_jsonrpc_v2(method_name, params, api_class)
-
-        return self.send_msg(msg, retry=retry)
-
-    def send_msg(self, msg, retry=0):
-        # REQ/RESP pattern in ZMQ requires no interrupts during the send
-        with self.lock:
-            rc = self.__send_msg(msg, retry)
-        return rc
-
-    def __send_msg(self, msg, retry=0):
-        # print before
-        pretty_json = json.dumps(json.loads(msg), indent=4, separators=(",", ": "), sort_keys=True)
-        self.logger.debug(f"Sending Request To Server:\n{pretty_json}\n")
-
-        # encode string to buffer
-        buffer = msg.encode("utf-8", "ignore")
-
-        if self.zipper.check_threshold(buffer):
-            response = self.send_raw_msg(self.zipper.compress(buffer), retry=retry)
-        else:
-            response = self.send_raw_msg(buffer, retry=retry)
-
-        if not response:
-            return response
-        elif self.zipper.is_compressed(response):
-            response = self.zipper.decompress(response)
-
-        # return to string
-        response = response.decode()
-
-        # print after
-        pretty_json = json.dumps(json.loads(response), indent=4, separators=(",", ": "), sort_keys=True)
-        self.logger.debug(f"Server Response:\n{pretty_json}\n")
-
-        # process response(batch and regular)
-        try:
-            response_json = json.loads(response)
-        except (TypeError, ValueError):
-            raise TrexError("*** [RPC] - Failed to decode response from server")
-
-        response_list = response_json if isinstance(response_json, list) else [response_json]
-        for response in response_list:
-            self.check_response(response)
-
-        return response_json
-
-    # low level send of string message
-
-    def send_raw_msg(self, msg, retry=0):
-        try:
-            return self._send_raw_msg_safe(msg, retry)
-        except KeyboardInterrupt as e:
-            # must restore the socket to a sane state
-            self.reconnect()
-            raise e
-
-    def _send_raw_msg_safe(self, msg, retry):
-
-        retry_left = retry
-        while True:
-            try:
-                # pdb.set_trace()
-                if isinstance(msg, str):
-                    msg = msg  # .encode("utf-8", "ignore")
-
-                if isinstance(msg, bytes):
-                    self.socket.send(msg)
-                else:
-                    raise TrexError("*** [RPC] - failed to understand message to server")
-                break
-            except zmq.Again:
-                retry_left -= 1
-                if retry_left < 0:
-                    self.disconnect()
-                    raise TrexError("*** [RPC] - Failed to send message to server")
-
-        retry_left = retry
-        while True:
-            try:
-                response = self.socket.recv()
-                break
-            except zmq.Again:
-                retry_left -= 1
-                if retry_left < 0:
-                    self.disconnect()
-                    raise TrexError(f"*** [RPC] - Failed to get server response from {self.transport}")
-
-        return response
-
-    @staticmethod
-    def check_response(response: dict) -> None:
-        """Check single response from server for errors."""
-        if response.get("jsonrpc") != "2.0":
-            raise TrexError(f"Malformed Response({response})")
-
-        # error reported by server
-        if "error" in response:
-            if "specific_err" in response["error"]:
-                raise TrexError(response["error"]["specific_err"])
-            else:
-                raise TrexError(response["error"]["message"])
-
-        # if no error there should be a result
-        if "result" not in response:
-            raise TrexError(f"Malformed Response({str(response)})")
-
-    def disconnect(self) -> None:
-        if self.connected:
-            self.socket.close(linger=0)
-            self.context.destroy(linger=0)
-            self.connected = False
-
-    def connect(self, server=None, port=None) -> None:
-        if self.connected:
-            self.disconnect()
-
-        self.context = zmq.Context()
-
-        self.server = server if server else self.server
-        self.port = port if port else self.port
-
-        #  Socket to talk to server
-        self.transport = "tcp://{0}:{1}".format(self.server, self.port)
-
-        self.socket = self.context.socket(zmq.REQ)
-        self.socket.connect(self.transport)
-
-        self.socket.setsockopt(zmq.SNDTIMEO, 10000)
-        self.socket.setsockopt(zmq.RCVTIMEO, 10000)
-
-        self.connected = True
-
-    def reconnect(self):
-        # connect using current values
-        return self.connect()
-
-    def is_connected(self):
-        return self.connected
-
-    def __del__(self):
-        if hasattr(self, "context"):
-            self.context.destroy(linger=0)
+import json
+from threading import Lock
+
+import zmq
+
+from .. import TrexError
+from ..common import random_id_gen
+from ..zipmsg import ZippedMsg
+
+
+# sub class to describe a batch
+class BatchMessage:
+    def __init__(self, rpc_client):
+        self.rpc_client = rpc_client
+        self.batch_list = []
+
+    def add(self, method_name, params=None, api_class="core"):
+
+        id, msg = self.rpc_client.create_jsonrpc_v2(method_name, params, api_class, encode=False)
+        self.batch_list.append(msg)
+
+    def invoke(self, chunk_size=500000, retry=0) -> list:
+        if not self.rpc_client.connected:
+            raise TrexError("Not connected to server")
+
+        if chunk_size:
+            response_batch = []
+            size = 0
+            new_batch = []
+            for msg in self.batch_list:
+                size += len(json.dumps(msg))
+                new_batch.append(msg)
+
+                if size > chunk_size:
+                    batch_json = json.dumps(new_batch)
+                    response_batch += self.rpc_client.send_msg(batch_json)
+                    size = 0
+                    new_batch = []
+            if new_batch:
+                batch_json = json.dumps(new_batch)
+                response_batch = self.rpc_client.send_msg(batch_json)
+            return response_batch
+        else:
+            batch_json = json.dumps(self.batch_list)
+            return self.rpc_client.send_msg(batch_json, retry=retry)
+
+
+# JSON RPC v2.0 client
+class JsonRpcClient:
+    def __init__(self, default_server, default_port, client):
+        self.get_api_h = client._get_api_h
+        self.logger = client.logger
+        self.connected = False
+
+        # default values
+        self.port = default_port
+        self.server = default_server
+
+        self.id_gen = random_id_gen()
+        self.zipper = ZippedMsg()
+
+        self.lock = Lock()
+
+    def get_connection_details(self):
+        rc = {}
+        rc["server"] = self.server
+        rc["port"] = self.port
+
+        return rc
+
+    # batch messages
+
+    def create_batch(self):
+        return BatchMessage(self)
+
+    def create_jsonrpc_v2(self, method_name, params=None, api_class="core", encode=True):
+        msg = {}
+        msg["jsonrpc"] = "2.0"
+        msg["method"] = method_name
+        msg["id"] = next(self.id_gen)
+        msg["params"] = params if params is not None else {}
+
+        # if this RPC has an API class - add it's handler
+        if api_class:
+            msg["params"]["api_h"] = self.get_api_h()[api_class]
+
+        # pdb.set_trace()
+        if encode:
+            return id, json.dumps(msg)
+        else:
+            return id, msg
+
+    def invoke_rpc_method(self, method_name, params=None, api_class="core", retry=0):
+        if not self.connected:
+            raise TrexError("Not connected to server")
+
+        id, msg = self.create_jsonrpc_v2(method_name, params, api_class)
+
+        return self.send_msg(msg, retry=retry)
+
+    def send_msg(self, msg, retry=0):
+        # REQ/RESP pattern in ZMQ requires no interrupts during the send
+        with self.lock:
+            rc = self.__send_msg(msg, retry)
+        return rc
+
+    def __send_msg(self, msg, retry=0):
+        # print before
+        pretty_json = json.dumps(json.loads(msg), indent=4, separators=(",", ": "), sort_keys=True)
+        self.logger.debug(f"Sending Request To Server:\n{pretty_json}\n")
+
+        # encode string to buffer
+        buffer = msg.encode("utf-8", "ignore")
+
+        if self.zipper.check_threshold(buffer):
+            response = self.send_raw_msg(self.zipper.compress(buffer), retry=retry)
+        else:
+            response = self.send_raw_msg(buffer, retry=retry)
+
+        if not response:
+            return response
+        elif self.zipper.is_compressed(response):
+            response = self.zipper.decompress(response)
+
+        # return to string
+        response = response.decode()
+
+        # print after
+        pretty_json = json.dumps(json.loads(response), indent=4, separators=(",", ": "), sort_keys=True)
+        self.logger.debug(f"Server Response:\n{pretty_json}\n")
+
+        # process response(batch and regular)
+        try:
+            response_json = json.loads(response)
+        except (TypeError, ValueError):
+            raise TrexError("*** [RPC] - Failed to decode response from server")
+
+        response_list = response_json if isinstance(response_json, list) else [response_json]
+        for response in response_list:
+            self.check_response(response)
+
+        return response_json
+
+    # low level send of string message
+
+    def send_raw_msg(self, msg, retry=0):
+        try:
+            return self._send_raw_msg_safe(msg, retry)
+        except KeyboardInterrupt as e:
+            # must restore the socket to a sane state
+            self.reconnect()
+            raise e
+
+    def _send_raw_msg_safe(self, msg, retry):
+
+        retry_left = retry
+        while True:
+            try:
+                # pdb.set_trace()
+                if isinstance(msg, str):
+                    msg = msg  # .encode("utf-8", "ignore")
+
+                if isinstance(msg, bytes):
+                    self.socket.send(msg)
+                else:
+                    raise TrexError("*** [RPC] - failed to understand message to server")
+                break
+            except zmq.Again:
+                retry_left -= 1
+                if retry_left < 0:
+                    self.disconnect()
+                    raise TrexError("*** [RPC] - Failed to send message to server")
+
+        retry_left = retry
+        while True:
+            try:
+                response = self.socket.recv()
+                break
+            except zmq.Again:
+                retry_left -= 1
+                if retry_left < 0:
+                    self.disconnect()
+                    raise TrexError(f"*** [RPC] - Failed to get server response from {self.transport}")
+
+        return response
+
+    @staticmethod
+    def check_response(response: dict) -> None:
+        """Check single response from server for errors."""
+        if response.get("jsonrpc") != "2.0":
+            raise TrexError(f"Malformed Response({response})")
+
+        # error reported by server
+        if "error" in response:
+            if "specific_err" in response["error"]:
+                raise TrexError(response["error"]["specific_err"])
+            else:
+                raise TrexError(response["error"]["message"])
+
+        # if no error there should be a result
+        if "result" not in response:
+            raise TrexError(f"Malformed Response({str(response)})")
+
+    def disconnect(self) -> None:
+        if self.connected:
+            self.socket.close(linger=0)
+            self.context.destroy(linger=0)
+            self.connected = False
+
+    def connect(self, server=None, port=None) -> None:
+        if self.connected:
+            self.disconnect()
+
+        self.context = zmq.Context()
+
+        self.server = server if server else self.server
+        self.port = port if port else self.port
+
+        #  Socket to talk to server
+        self.transport = "tcp://{0}:{1}".format(self.server, self.port)
+
+        self.socket = self.context.socket(zmq.REQ)
+        self.socket.connect(self.transport)
+
+        self.socket.setsockopt(zmq.SNDTIMEO, 10000)
+        self.socket.setsockopt(zmq.RCVTIMEO, 10000)
+
+        self.connected = True
+
+    def reconnect(self):
+        # connect using current values
+        return self.connect()
+
+    def is_connected(self):
+        return self.connected
+
+    def __del__(self):
+        if hasattr(self, "context"):
+            self.context.destroy(linger=0)
```

## Comparing `pytrex-0.9.1.dist-info/METADATA` & `pytrex-0.9.2.dist-info/METADATA`

 * *Files 23% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-Metadata-Version: 2.1
-Name: pytrex
-Version: 0.9.1
-Summary: Python OO API package to manage TRex traffic generator stateless traffic generator
-Home-page: https://github.com/shmir/PyTRex
-Author: Yoram Shamir
-Author-email: yoram@ignissoft.com
-License: Apache Software License
-Keywords: cisco,trex,l2l3,tg,traffic generator,test automation,automation api
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Natural Language :: English
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Intended Audience :: Developers
-Classifier: Operating System :: OS Independent
-Classifier: Topic :: Software Development :: Testing
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Description-Content-Type: text/markdown
-Requires-Dist: requests
-Requires-Dist: scapy (==2.4.3)
-Requires-Dist: pyzmq
-Requires-Dist: pytrafficgen (<4.1.0,>=4.0.0)
-Requires-Dist: pyyaml
-
-## Python OO API for Cisco TRex.
-
-The package was tested with TRex version 2.61.
-
-### TRex Resources
-- https://trex-tgn.cisco.com/trex/doc/trex_manual.html
-- https://trex-tgn.cisco.com/trex/doc/trex_rpc_server_spec.html
+Metadata-Version: 2.1
+Name: pytrex
+Version: 0.9.2
+Summary: Python OO API package to manage TRex traffic generator stateless traffic generator
+Home-page: https://github.com/shmir/PyTRex
+Author: Yoram Shamir
+Author-email: yoram@ignissoft.com
+License: Apache Software License
+Keywords: cisco,trex,l2l3,tg,traffic generator,test automation,automation api
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Natural Language :: English
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Intended Audience :: Developers
+Classifier: Operating System :: OS Independent
+Classifier: Topic :: Software Development :: Testing
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Description-Content-Type: text/markdown
+Requires-Dist: scapy
+Requires-Dist: pytrafficgen (<4.1.0,>=4.0.0)
+Requires-Dist: pyyaml
+Requires-Dist: pyzmq
+
+## Python OO API for Cisco TRex.
+
+The package was tested with TRex version 2.61.
+
+### TRex Resources
+- https://trex-tgn.cisco.com/trex/doc/trex_manual.html
+- https://trex-tgn.cisco.com/trex/doc/trex_rpc_server_spec.html
+- https://trex-tgn.cisco.com/trex/doc/cp_stl_docs/
```

